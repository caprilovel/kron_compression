nohup: ignoring input
[2024-03-01 09:00:40,471] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-01 09:00:41,844] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 09:00:41,850] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         458     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       560 MACs
fwd flops per GPU:                                                      1.12 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.12 K  
fwd latency:                                                            6.69 ms 
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    167.34 KFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '458'}
    MACs        - {'KronLayer': '560 MACs'}
    fwd latency - {'KronLayer': '6.69 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  458 = 100% Params, 560 MACs = 100% MACs, 6.69 ms = 100% latency, 167.34 KFLOPS
  (kronlinear): KronLinear(458 = 100% Params, 560 MACs = 100% MACs, 6.37 ms = 95.23% latency, 175.72 KFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 47.21 us = 0.71% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 09:00:41,984] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.12 K 458
[2024-03-01 09:00:41,985] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 09:00:41,985] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            321.39 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.74 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '321.39 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 321.39 us = 100% latency, 1.74 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 213.38 us = 66.39% latency, 2.62 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 29.09 us = 9.05% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 09:00:41,987] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
fold 0 accuracy: 0.47735714285714287
sparse ratio: 0.535714328289032
fold 1 accuracy: 0.48292857142857143
sparse ratio: 0.535714328289032
fold 2 accuracy: 0.4794285714285714
sparse ratio: 0.535714328289032
fold 3 accuracy: 0.49407142857142855
sparse ratio: 0.535714328289032
fold 4 accuracy: 0.4848571428571429
sparse ratio: 0.535714328289032
0.48372857142857145 0.005796092983859982
[tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0')]
[2024-03-01 09:19:25,276] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 09:19:25,277] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            359.77 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.56 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '359.77 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 359.77 us = 100% latency, 1.56 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 262.5 us = 72.96% latency, 2.13 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 29.56 us = 8.22% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 09:19:25,279] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
fold 0 accuracy: 0.45871428571428574
sparse ratio: 0.535714328289032
fold 1 accuracy: 0.45121428571428573
sparse ratio: 0.5535714626312256
fold 2 accuracy: 0.4392857142857143
sparse ratio: 0.5535714626312256
fold 3 accuracy: 0.43864285714285717
sparse ratio: 0.5535714626312256
fold 4 accuracy: 0.43585714285714283
sparse ratio: 0.5535714626312256
0.44474285714285716 0.008752445722336655
[tensor(0.5357, device='cuda:0'), tensor(0.5536, device='cuda:0'), tensor(0.5536, device='cuda:0'), tensor(0.5536, device='cuda:0'), tensor(0.5536, device='cuda:0')]
[2024-03-01 09:38:16,081] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 09:38:16,085] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            425.1 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.32 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '425.1 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 425.1 us = 100% latency, 1.32 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 329.49 us = 77.51% latency, 1.7 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 29.33 us = 6.9% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 09:38:16,086] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
fold 0 accuracy: 0.49257142857142855
sparse ratio: 0.535714328289032
fold 1 accuracy: 0.5098571428571429
sparse ratio: 0.535714328289032
fold 2 accuracy: 0.5150714285714286
sparse ratio: 0.535714328289032
fold 3 accuracy: 0.5190714285714285
sparse ratio: 0.535714328289032
fold 4 accuracy: 0.5202142857142857
sparse ratio: 0.535714328289032
0.5113571428571428 0.010068844653338405
[tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0')]
[2024-03-01 09:57:05,100] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 09:57:05,107] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            299.93 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.87 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '299.93 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 299.93 us = 100% latency, 1.87 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 210.52 us = 70.19% latency, 2.66 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 26.7 us = 8.9% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 09:57:05,108] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
fold 0 accuracy: 0.42378571428571427
sparse ratio: 0.5892857313156128
fold 1 accuracy: 0.4170714285714286
sparse ratio: 0.5892857313156128
fold 2 accuracy: 0.4057142857142857
sparse ratio: 0.5892857313156128
fold 3 accuracy: 0.40685714285714286
sparse ratio: 0.5892857313156128
fold 4 accuracy: 0.4024285714285714
sparse ratio: 0.5892857313156128
0.4111714285714285 0.007984500291061377
[tensor(0.5893, device='cuda:0'), tensor(0.5893, device='cuda:0'), tensor(0.5893, device='cuda:0'), tensor(0.5893, device='cuda:0'), tensor(0.5893, device='cuda:0')]
[2024-03-01 10:15:41,599] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 10:15:41,600] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            315.67 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.77 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '315.67 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 315.67 us = 100% latency, 1.77 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 218.63 us = 69.26% latency, 2.56 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 26.46 us = 8.38% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 10:15:41,601] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
fold 0 accuracy: 0.46014285714285713
sparse ratio: 0.5535714626312256
fold 1 accuracy: 0.4737142857142857
sparse ratio: 0.5535714626312256
fold 2 accuracy: 0.48828571428571427
sparse ratio: 0.5535714626312256
fold 3 accuracy: 0.48807142857142854
sparse ratio: 0.5535714626312256
fold 4 accuracy: 0.4888571428571429
sparse ratio: 0.5535714626312256
0.47981428571428564 0.011365684119761738
[tensor(0.5536, device='cuda:0'), tensor(0.5536, device='cuda:0'), tensor(0.5536, device='cuda:0'), tensor(0.5536, device='cuda:0'), tensor(0.5536, device='cuda:0')]
[2024-03-01 10:34:16,792] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 10:34:16,793] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            324.73 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.72 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '324.73 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 324.73 us = 100% latency, 1.72 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 232.22 us = 71.51% latency, 2.41 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 7.93% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 10:34:16,794] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
fold 0 accuracy: 0.4387857142857143
sparse ratio: 0.5892857313156128
fold 1 accuracy: 0.42978571428571427
sparse ratio: 0.5892857313156128
fold 2 accuracy: 0.42692857142857144
sparse ratio: 0.5892857313156128
fold 3 accuracy: 0.42164285714285715
sparse ratio: 0.5892857313156128
fold 4 accuracy: 0.4232142857142857
sparse ratio: 0.5892857313156128
0.42807142857142855 0.006065627480705836
[tensor(0.5893, device='cuda:0'), tensor(0.5893, device='cuda:0'), tensor(0.5893, device='cuda:0'), tensor(0.5893, device='cuda:0'), tensor(0.5893, device='cuda:0')]
[2024-03-01 10:52:54,230] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 10:52:54,233] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            308.75 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.81 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '308.75 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 308.75 us = 100% latency, 1.81 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 215.29 us = 69.73% latency, 2.6 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 27.66 us = 8.96% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 10:52:54,234] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
fold 0 accuracy: 0.41464285714285715
sparse ratio: 0.5178571939468384
fold 1 accuracy: 0.4205
sparse ratio: 0.5178571939468384
fold 2 accuracy: 0.42928571428571427
sparse ratio: 0.5178571939468384
fold 3 accuracy: 0.4218571428571429
sparse ratio: 0.5178571939468384
fold 4 accuracy: 0.43028571428571427
sparse ratio: 0.5178571939468384
0.4233142857142857 0.005822300548191351
[tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0')]
[2024-03-01 11:11:31,093] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 11:11:31,097] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            335.45 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.67 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '335.45 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 335.45 us = 100% latency, 1.67 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 242.23 us = 72.21% latency, 2.31 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 7.68% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 11:11:31,098] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
fold 0 accuracy: 0.45421428571428574
sparse ratio: 0.3750000298023224
fold 1 accuracy: 0.4695
sparse ratio: 0.3750000298023224
fold 2 accuracy: 0.4635714285714286
sparse ratio: 0.392857164144516
fold 3 accuracy: 0.475
sparse ratio: 0.4107142984867096
fold 4 accuracy: 0.4732142857142857
sparse ratio: 0.4107142984867096
0.46709999999999996 0.0075372002597185765
[tensor(0.3750, device='cuda:0'), tensor(0.3750, device='cuda:0'), tensor(0.3929, device='cuda:0'), tensor(0.4107, device='cuda:0'), tensor(0.4107, device='cuda:0')]
[2024-03-01 11:30:07,083] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 11:30:07,084] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            313.04 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.79 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '313.04 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 313.04 us = 100% latency, 1.79 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 219.82 us = 70.22% latency, 2.55 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 27.42 us = 8.76% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 11:30:07,086] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
fold 0 accuracy: 0.39785714285714285
sparse ratio: 0.5714285969734192
fold 1 accuracy: 0.40264285714285714
sparse ratio: 0.5714285969734192
fold 2 accuracy: 0.41585714285714287
sparse ratio: 0.5714285969734192
fold 3 accuracy: 0.45507142857142857
sparse ratio: 0.5714285969734192
fold 4 accuracy: 0.45585714285714285
sparse ratio: 0.5714285969734192
0.42545714285714287 0.02520149817380073
[tensor(0.5714, device='cuda:0'), tensor(0.5714, device='cuda:0'), tensor(0.5714, device='cuda:0'), tensor(0.5714, device='cuda:0'), tensor(0.5714, device='cuda:0')]
[2024-03-01 11:48:56,295] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 11:48:56,296] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         458     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       560 MACs
fwd flops per GPU:                                                      1.12 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.12 K  
fwd latency:                                                            325.2 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.44 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '458'}
    MACs        - {'KronLayer': '560 MACs'}
    fwd latency - {'KronLayer': '325.2 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  458 = 100% Params, 560 MACs = 100% MACs, 325.2 us = 100% latency, 3.44 MFLOPS
  (kronlinear): KronLinear(458 = 100% Params, 560 MACs = 100% MACs, 229.6 us = 70.6% latency, 4.88 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 28.37 us = 8.72% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 11:48:56,298] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.12 K 458
fold 0 accuracy: 0.6555714285714286
sparse ratio: 0.5178571939468384
fold 1 accuracy: 0.7033571428571429
sparse ratio: 0.5178571939468384
fold 2 accuracy: 0.7208571428571429
sparse ratio: 0.5178571939468384
fold 3 accuracy: 0.7226428571428571
sparse ratio: 0.5178571939468384
fold 4 accuracy: 0.7214285714285714
sparse ratio: 0.5178571939468384
0.7047714285714286 0.025605619600041714
[tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0')]
[2024-03-01 12:09:19,080] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 12:09:19,081] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         458     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       560 MACs
fwd flops per GPU:                                                      1.12 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.12 K  
fwd latency:                                                            305.18 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.67 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '458'}
    MACs        - {'KronLayer': '560 MACs'}
    fwd latency - {'KronLayer': '305.18 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  458 = 100% Params, 560 MACs = 100% MACs, 305.18 us = 100% latency, 3.67 MFLOPS
  (kronlinear): KronLinear(458 = 100% Params, 560 MACs = 100% MACs, 216.96 us = 71.09% latency, 5.16 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 26.23 us = 8.59% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 12:09:19,082] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.12 K 458
fold 0 accuracy: 0.6373571428571428
sparse ratio: 0.4285714626312256
fold 1 accuracy: 0.6359285714285714
sparse ratio: 0.4107142984867096
fold 2 accuracy: 0.6558571428571428
sparse ratio: 0.4107142984867096
fold 3 accuracy: 0.6499285714285714
sparse ratio: 0.4107142984867096
fold 4 accuracy: 0.6512857142857142
sparse ratio: 0.4107142984867096
0.6460714285714284 0.007957925070555219
[tensor(0.4286, device='cuda:0'), tensor(0.4107, device='cuda:0'), tensor(0.4107, device='cuda:0'), tensor(0.4107, device='cuda:0'), tensor(0.4107, device='cuda:0')]
[2024-03-01 12:29:08,935] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 12:29:08,936] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         458     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       560 MACs
fwd flops per GPU:                                                      1.12 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.12 K  
fwd latency:                                                            324.25 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.45 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '458'}
    MACs        - {'KronLayer': '560 MACs'}
    fwd latency - {'KronLayer': '324.25 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  458 = 100% Params, 560 MACs = 100% MACs, 324.25 us = 100% latency, 3.45 MFLOPS
  (kronlinear): KronLinear(458 = 100% Params, 560 MACs = 100% MACs, 236.75 us = 73.01% latency, 4.73 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 7.94% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 12:29:08,938] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.12 K 458
fold 0 accuracy: 0.5948571428571429
sparse ratio: 0.5178571939468384
fold 1 accuracy: 0.6223571428571428
sparse ratio: 0.5178571939468384
fold 2 accuracy: 0.6534285714285715
sparse ratio: 0.5178571939468384
fold 3 accuracy: 0.6673571428571429
sparse ratio: 0.5178571939468384
fold 4 accuracy: 0.7186428571428571
sparse ratio: 0.5
0.6513285714285715 0.042002152520156995
[tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5000, device='cuda:0')]
[2024-03-01 12:47:38,843] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 12:47:38,844] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         458     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       560 MACs
fwd flops per GPU:                                                      1.12 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.12 K  
fwd latency:                                                            553.13 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    2.02 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '458'}
    MACs        - {'KronLayer': '560 MACs'}
    fwd latency - {'KronLayer': '553.13 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  458 = 100% Params, 560 MACs = 100% MACs, 553.13 us = 100% latency, 2.02 MFLOPS
  (kronlinear): KronLinear(458 = 100% Params, 560 MACs = 100% MACs, 246.05 us = 44.48% latency, 4.55 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 26.23 us = 4.74% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 12:47:38,845] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.12 K 458
fold 0 accuracy: 0.6294285714285714
sparse ratio: 0.5178571939468384
fold 1 accuracy: 0.6493571428571429
sparse ratio: 0.535714328289032
fold 2 accuracy: 0.6649285714285714
sparse ratio: 0.535714328289032
fold 3 accuracy: 0.6809285714285714
sparse ratio: 0.535714328289032
fold 4 accuracy: 0.714
sparse ratio: 0.535714328289032
0.6677285714285713 0.02873160103369081
[tensor(0.5179, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0')]
[2024-03-01 13:06:01,946] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 13:06:01,947] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         458     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       560 MACs
fwd flops per GPU:                                                      1.12 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.12 K  
fwd latency:                                                            323.06 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.47 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '458'}
    MACs        - {'KronLayer': '560 MACs'}
    fwd latency - {'KronLayer': '323.06 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  458 = 100% Params, 560 MACs = 100% MACs, 323.06 us = 100% latency, 3.47 MFLOPS
  (kronlinear): KronLinear(458 = 100% Params, 560 MACs = 100% MACs, 232.7 us = 72.03% latency, 4.81 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 26.46 us = 8.19% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 13:06:01,948] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.12 K 458
fold 0 accuracy: 0.6862857142857143
sparse ratio: 0.535714328289032
fold 1 accuracy: 0.7107857142857142
sparse ratio: 0.535714328289032
fold 2 accuracy: 0.7137857142857142
sparse ratio: 0.535714328289032
fold 3 accuracy: 0.7224285714285714
sparse ratio: 0.5178571939468384
fold 4 accuracy: 0.7192857142857143
sparse ratio: 0.5178571939468384
0.7105142857142857 0.012780358016128038
[tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0')]
[2024-03-01 13:24:29,809] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 13:24:29,810] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         654     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       840 MACs
fwd flops per GPU:                                                      1.68 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.68 K  
fwd latency:                                                            352.14 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    4.77 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '654'}
    MACs        - {'KronLayer': '840 MACs'}
    fwd latency - {'KronLayer': '352.14 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  654 = 100% Params, 840 MACs = 100% MACs, 352.14 us = 100% latency, 4.77 MFLOPS
  (kronlinear): KronLinear(654 = 100% Params, 840 MACs = 100% MACs, 237.7 us = 67.5% latency, 7.07 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 48.64 us = 13.81% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 13:24:29,813] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.68 K 654
fold 0 accuracy: 0.7782857142857142
sparse ratio: 0.535714328289032
fold 1 accuracy: 0.7969285714285714
sparse ratio: 0.535714328289032
fold 2 accuracy: 0.8029285714285714
sparse ratio: 0.535714328289032
fold 3 accuracy: 0.7967142857142857
sparse ratio: 0.535714328289032
fold 4 accuracy: 0.8068571428571428
sparse ratio: 0.535714328289032
0.7963428571428571 0.009802686012579647
[tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0')]
[2024-03-01 13:42:59,477] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 13:42:59,478] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         654     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       840 MACs
fwd flops per GPU:                                                      1.68 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.68 K  
fwd latency:                                                            318.05 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    5.28 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '654'}
    MACs        - {'KronLayer': '840 MACs'}
    fwd latency - {'KronLayer': '318.05 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  654 = 100% Params, 840 MACs = 100% MACs, 318.05 us = 100% latency, 5.28 MFLOPS
  (kronlinear): KronLinear(654 = 100% Params, 840 MACs = 100% MACs, 229.12 us = 72.04% latency, 7.33 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 26.94 us = 8.47% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 13:42:59,479] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.68 K 654
fold 0 accuracy: 0.8070714285714286
sparse ratio: 0.5535714626312256
fold 1 accuracy: 0.8097142857142857
sparse ratio: 0.535714328289032
fold 2 accuracy: 0.8160714285714286
sparse ratio: 0.535714328289032
fold 3 accuracy: 0.8235714285714286
sparse ratio: 0.535714328289032
fold 4 accuracy: 0.8161428571428572
sparse ratio: 0.535714328289032
0.8145142857142857 0.005754040284168766
[tensor(0.5536, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0'), tensor(0.5357, device='cuda:0')]
[2024-03-01 14:01:27,895] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 14:01:27,896] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         654     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       840 MACs
fwd flops per GPU:                                                      1.68 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.68 K  
fwd latency:                                                            390.53 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    4.3 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '654'}
    MACs        - {'KronLayer': '840 MACs'}
    fwd latency - {'KronLayer': '390.53 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  654 = 100% Params, 840 MACs = 100% MACs, 390.53 us = 100% latency, 4.3 MFLOPS
  (kronlinear): KronLinear(654 = 100% Params, 840 MACs = 100% MACs, 279.9 us = 71.67% latency, 6 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 32.9 us = 8.42% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 14:01:27,898] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.68 K 654
fold 0 accuracy: 0.8050714285714285
sparse ratio: 0.392857164144516
fold 1 accuracy: 0.8171428571428572
sparse ratio: 0.392857164144516
fold 2 accuracy: 0.8109285714285714
sparse ratio: 0.3750000298023224
fold 3 accuracy: 0.812
sparse ratio: 0.3750000298023224
fold 4 accuracy: 0.8212142857142857
sparse ratio: 0.3750000298023224
0.8132714285714286 0.0055212761021269634
[tensor(0.3929, device='cuda:0'), tensor(0.3929, device='cuda:0'), tensor(0.3750, device='cuda:0'), tensor(0.3750, device='cuda:0'), tensor(0.3750, device='cuda:0')]
[2024-03-01 14:19:57,717] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 14:19:57,717] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         654     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       840 MACs
fwd flops per GPU:                                                      1.68 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.68 K  
fwd latency:                                                            522.38 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.22 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '654'}
    MACs        - {'KronLayer': '840 MACs'}
    fwd latency - {'KronLayer': '522.38 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  654 = 100% Params, 840 MACs = 100% MACs, 522.38 us = 100% latency, 3.22 MFLOPS
  (kronlinear): KronLinear(654 = 100% Params, 840 MACs = 100% MACs, 237.23 us = 45.41% latency, 7.08 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 41.72 us = 7.99% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 14:19:57,719] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.68 K 654
fold 0 accuracy: 0.8037142857142857
sparse ratio: 0.5892857313156128
fold 1 accuracy: 0.8125
sparse ratio: 0.5714285969734192
fold 2 accuracy: 0.81
sparse ratio: 0.5714285969734192
fold 3 accuracy: 0.8207142857142857
sparse ratio: 0.5714285969734192
fold 4 accuracy: 0.8173571428571429
sparse ratio: 0.5714285969734192
0.812857142857143 0.0058969033727673675
[tensor(0.5893, device='cuda:0'), tensor(0.5714, device='cuda:0'), tensor(0.5714, device='cuda:0'), tensor(0.5714, device='cuda:0'), tensor(0.5714, device='cuda:0')]
[2024-03-01 14:38:27,705] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 14:38:27,706] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         654     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       840 MACs
fwd flops per GPU:                                                      1.68 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.68 K  
fwd latency:                                                            331.88 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    5.06 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '654'}
    MACs        - {'KronLayer': '840 MACs'}
    fwd latency - {'KronLayer': '331.88 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  654 = 100% Params, 840 MACs = 100% MACs, 331.88 us = 100% latency, 5.06 MFLOPS
  (kronlinear): KronLinear(654 = 100% Params, 840 MACs = 100% MACs, 246.52 us = 74.28% latency, 6.81 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 25.99 us = 7.83% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 14:38:27,708] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.68 K 654
fold 0 accuracy: 0.7827142857142857
sparse ratio: 0.5714285969734192
fold 1 accuracy: 0.8182142857142857
sparse ratio: 0.5892857313156128
fold 2 accuracy: 0.8191428571428572
sparse ratio: 0.5892857313156128
fold 3 accuracy: 0.8184285714285714
sparse ratio: 0.5892857313156128
fold 4 accuracy: 0.8174285714285714
sparse ratio: 0.5892857313156128
0.8111857142857142 0.014246202501518982
[tensor(0.5714, device='cuda:0'), tensor(0.5893, device='cuda:0'), tensor(0.5893, device='cuda:0'), tensor(0.5893, device='cuda:0'), tensor(0.5893, device='cuda:0')]
[2024-03-01 14:56:51,853] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 14:56:51,854] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         850     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.12 KMACs
fwd flops per GPU:                                                      2.24 K  
fwd flops of model = fwd flops per GPU * mp_size:                       2.24 K  
fwd latency:                                                            345.23 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    6.49 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '850'}
    MACs        - {'KronLayer': '1.12 KMACs'}
    fwd latency - {'KronLayer': '345.23 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  850 = 100% Params, 1.12 KMACs = 100% MACs, 345.23 us = 100% latency, 6.49 MFLOPS
  (kronlinear): KronLinear(850 = 100% Params, 1.12 KMACs = 100% MACs, 253.2 us = 73.34% latency, 8.85 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 25.99 us = 7.53% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 14:56:51,855] [INFO] [profiler.py:226:end_profile] Flops profiler finished
2.24 K 850
fold 0 accuracy: 0.8390714285714286
sparse ratio: 0.392857164144516
fold 1 accuracy: 0.8488571428571429
sparse ratio: 0.392857164144516
fold 2 accuracy: 0.8521428571428571
sparse ratio: 0.392857164144516
fold 3 accuracy: 0.847
sparse ratio: 0.392857164144516
fold 4 accuracy: 0.8512142857142857
sparse ratio: 0.392857164144516
0.8476571428571429 0.0046553458408542595
[tensor(0.3929, device='cuda:0'), tensor(0.3929, device='cuda:0'), tensor(0.3929, device='cuda:0'), tensor(0.3929, device='cuda:0'), tensor(0.3929, device='cuda:0')]
[2024-03-01 15:15:14,575] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 15:15:14,576] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         850     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.12 KMACs
fwd flops per GPU:                                                      2.24 K  
fwd flops of model = fwd flops per GPU * mp_size:                       2.24 K  
fwd latency:                                                            327.59 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    6.84 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '850'}
    MACs        - {'KronLayer': '1.12 KMACs'}
    fwd latency - {'KronLayer': '327.59 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  850 = 100% Params, 1.12 KMACs = 100% MACs, 327.59 us = 100% latency, 6.84 MFLOPS
  (kronlinear): KronLinear(850 = 100% Params, 1.12 KMACs = 100% MACs, 236.27 us = 72.13% latency, 9.48 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 27.42 us = 8.37% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 15:15:14,577] [INFO] [profiler.py:226:end_profile] Flops profiler finished
2.24 K 850
fold 0 accuracy: 0.8037142857142857
sparse ratio: 0.5178571939468384
fold 1 accuracy: 0.8173571428571429
sparse ratio: 0.5178571939468384
fold 2 accuracy: 0.8252142857142857
sparse ratio: 0.5178571939468384
fold 3 accuracy: 0.8299285714285715
sparse ratio: 0.5178571939468384
fold 4 accuracy: 0.8332857142857143
sparse ratio: 0.5178571939468384
0.8219000000000001 0.010549262475429751
[tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0')]
[2024-03-01 15:33:40,085] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 15:33:40,086] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         850     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.12 KMACs
fwd flops per GPU:                                                      2.24 K  
fwd flops of model = fwd flops per GPU * mp_size:                       2.24 K  
fwd latency:                                                            315.9 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    7.09 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '850'}
    MACs        - {'KronLayer': '1.12 KMACs'}
    fwd latency - {'KronLayer': '315.9 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  850 = 100% Params, 1.12 KMACs = 100% MACs, 315.9 us = 100% latency, 7.09 MFLOPS
  (kronlinear): KronLinear(850 = 100% Params, 1.12 KMACs = 100% MACs, 229.84 us = 72.75% latency, 9.75 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 8.15% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 15:33:40,087] [INFO] [profiler.py:226:end_profile] Flops profiler finished
2.24 K 850
fold 0 accuracy: 0.7966428571428571
sparse ratio: 0.5178571939468384
fold 1 accuracy: 0.8081428571428572
sparse ratio: 0.5178571939468384
fold 2 accuracy: 0.8317857142857142
sparse ratio: 0.5178571939468384
fold 3 accuracy: 0.8415
sparse ratio: 0.5178571939468384
fold 4 accuracy: 0.829
sparse ratio: 0.5178571939468384
0.8214142857142857 0.016482111515215527
[tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0')]
[2024-03-01 15:52:03,239] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 15:52:03,240] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         850     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.12 KMACs
fwd flops per GPU:                                                      2.24 K  
fwd flops of model = fwd flops per GPU * mp_size:                       2.24 K  
fwd latency:                                                            478.98 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    4.68 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '850'}
    MACs        - {'KronLayer': '1.12 KMACs'}
    fwd latency - {'KronLayer': '478.98 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  850 = 100% Params, 1.12 KMACs = 100% MACs, 478.98 us = 100% latency, 4.68 MFLOPS
  (kronlinear): KronLinear(850 = 100% Params, 1.12 KMACs = 100% MACs, 236.99 us = 49.48% latency, 9.45 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 26.23 us = 5.48% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 15:52:03,242] [INFO] [profiler.py:226:end_profile] Flops profiler finished
2.24 K 850
fold 0 accuracy: 0.8398571428571429
sparse ratio: 0.5
fold 1 accuracy: 0.8515
sparse ratio: 0.5
fold 2 accuracy: 0.8537142857142858
sparse ratio: 0.5
fold 3 accuracy: 0.8484285714285714
sparse ratio: 0.5
fold 4 accuracy: 0.8469285714285715
sparse ratio: 0.5
0.8480857142857143 0.004743502537843817
[tensor(0.5000, device='cuda:0'), tensor(0.5000, device='cuda:0'), tensor(0.5000, device='cuda:0'), tensor(0.5000, device='cuda:0'), tensor(0.5000, device='cuda:0')]
[2024-03-01 16:10:26,958] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 16:10:26,959] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         850     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.12 KMACs
fwd flops per GPU:                                                      2.24 K  
fwd flops of model = fwd flops per GPU * mp_size:                       2.24 K  
fwd latency:                                                            331.88 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    6.75 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '850'}
    MACs        - {'KronLayer': '1.12 KMACs'}
    fwd latency - {'KronLayer': '331.88 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  850 = 100% Params, 1.12 KMACs = 100% MACs, 331.88 us = 100% latency, 6.75 MFLOPS
  (kronlinear): KronLinear(850 = 100% Params, 1.12 KMACs = 100% MACs, 242.23 us = 72.99% latency, 9.25 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 7.76% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 16:10:26,961] [INFO] [profiler.py:226:end_profile] Flops profiler finished
2.24 K 850
fold 0 accuracy: 0.8282857142857143
sparse ratio: 0.3571428656578064
fold 1 accuracy: 0.8352142857142857
sparse ratio: 0.3571428656578064
fold 2 accuracy: 0.824
sparse ratio: 0.3571428656578064
fold 3 accuracy: 0.8277857142857142
sparse ratio: 0.3571428656578064
fold 4 accuracy: 0.8339285714285715
sparse ratio: 0.3571428656578064
0.8298428571428571 0.004155497956359293
[tensor(0.3571, device='cuda:0'), tensor(0.3571, device='cuda:0'), tensor(0.3571, device='cuda:0'), tensor(0.3571, device='cuda:0'), tensor(0.3571, device='cuda:0')]
[2024-03-01 16:28:52,928] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 16:28:52,929] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         1.05 K  
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.4 KMACs
fwd flops per GPU:                                                      2.8 K   
fwd flops of model = fwd flops per GPU * mp_size:                       2.8 K   
fwd latency:                                                            292.06 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    9.59 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '1.05 K'}
    MACs        - {'KronLayer': '1.4 KMACs'}
    fwd latency - {'KronLayer': '292.06 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  1.05 K = 100% Params, 1.4 KMACs = 100% MACs, 292.06 us = 100% latency, 9.59 MFLOPS
  (kronlinear): KronLinear(1.05 K = 100% Params, 1.4 KMACs = 100% MACs, 212.67 us = 72.82% latency, 13.17 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 23.37 us = 8% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 16:28:52,930] [INFO] [profiler.py:226:end_profile] Flops profiler finished
2.8 K 1.05 K
fold 0 accuracy: 0.8589285714285714
sparse ratio: 0.5714285969734192
fold 1 accuracy: 0.871
sparse ratio: 0.5714285969734192
fold 2 accuracy: 0.8722142857142857
sparse ratio: 0.5714285969734192
fold 3 accuracy: 0.8795714285714286
sparse ratio: 0.5714285969734192
fold 4 accuracy: 0.8780714285714286
sparse ratio: 0.5714285969734192
0.8719571428571428 0.007295036053400182
[tensor(0.5714, device='cuda:0'), tensor(0.5714, device='cuda:0'), tensor(0.5714, device='cuda:0'), tensor(0.5714, device='cuda:0'), tensor(0.5714, device='cuda:0')]
[2024-03-01 16:47:45,159] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 16:47:45,159] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         1.05 K  
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.4 KMACs
fwd flops per GPU:                                                      2.8 K   
fwd flops of model = fwd flops per GPU * mp_size:                       2.8 K   
fwd latency:                                                            515.46 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    5.43 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '1.05 K'}
    MACs        - {'KronLayer': '1.4 KMACs'}
    fwd latency - {'KronLayer': '515.46 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  1.05 K = 100% Params, 1.4 KMACs = 100% MACs, 515.46 us = 100% latency, 5.43 MFLOPS
  (kronlinear): KronLinear(1.05 K = 100% Params, 1.4 KMACs = 100% MACs, 405.31 us = 78.63% latency, 6.91 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 23.6 us = 4.58% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 16:47:45,163] [INFO] [profiler.py:226:end_profile] Flops profiler finished
2.8 K 1.05 K
fold 0 accuracy: 0.8115
sparse ratio: 0.4285714626312256
fold 1 accuracy: 0.8335
sparse ratio: 0.4285714626312256
fold 2 accuracy: 0.8432857142857143
sparse ratio: 0.4285714626312256
fold 3 accuracy: 0.8515
sparse ratio: 0.4285714626312256
fold 4 accuracy: 0.843
sparse ratio: 0.4285714626312256
0.8365571428571428 0.013764298687813597
[tensor(0.4286, device='cuda:0'), tensor(0.4286, device='cuda:0'), tensor(0.4286, device='cuda:0'), tensor(0.4286, device='cuda:0'), tensor(0.4286, device='cuda:0')]
[2024-03-01 17:06:20,986] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 17:06:20,987] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         1.05 K  
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.4 KMACs
fwd flops per GPU:                                                      2.8 K   
fwd flops of model = fwd flops per GPU * mp_size:                       2.8 K   
fwd latency:                                                            1.81 ms 
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.55 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '1.05 K'}
    MACs        - {'KronLayer': '1.4 KMACs'}
    fwd latency - {'KronLayer': '1.81 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  1.05 K = 100% Params, 1.4 KMACs = 100% MACs, 1.81 ms = 100% latency, 1.55 MFLOPS
  (kronlinear): KronLinear(1.05 K = 100% Params, 1.4 KMACs = 100% MACs, 1.6 ms = 88.62% latency, 1.75 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 62.23 us = 3.44% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 17:06:21,011] [INFO] [profiler.py:226:end_profile] Flops profiler finished
2.8 K 1.05 K
fold 0 accuracy: 0.8644285714285714
sparse ratio: 0.4107142984867096
fold 1 accuracy: 0.8680714285714286
sparse ratio: 0.4285714626312256
fold 2 accuracy: 0.8794285714285714
sparse ratio: 0.4285714626312256
fold 3 accuracy: 0.8698571428571429
sparse ratio: 0.4464285969734192
fold 4 accuracy: 0.8723571428571428
sparse ratio: 0.4464285969734192
0.8708285714285715 0.00501601516776667
[tensor(0.4107, device='cuda:0'), tensor(0.4286, device='cuda:0'), tensor(0.4286, device='cuda:0'), tensor(0.4464, device='cuda:0'), tensor(0.4464, device='cuda:0')]
[2024-03-01 17:24:53,455] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 17:24:53,456] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         1.05 K  
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.4 KMACs
fwd flops per GPU:                                                      2.8 K   
fwd flops of model = fwd flops per GPU * mp_size:                       2.8 K   
fwd latency:                                                            503.3 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    5.56 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '1.05 K'}
    MACs        - {'KronLayer': '1.4 KMACs'}
    fwd latency - {'KronLayer': '503.3 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  1.05 K = 100% Params, 1.4 KMACs = 100% MACs, 503.3 us = 100% latency, 5.56 MFLOPS
  (kronlinear): KronLinear(1.05 K = 100% Params, 1.4 KMACs = 100% MACs, 242.23 us = 48.13% latency, 11.56 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 27.89 us = 5.54% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 17:24:53,459] [INFO] [profiler.py:226:end_profile] Flops profiler finished
2.8 K 1.05 K
fold 0 accuracy: 0.8564285714285714
sparse ratio: 0.5178571939468384
fold 1 accuracy: 0.8625
sparse ratio: 0.5178571939468384
fold 2 accuracy: 0.8662857142857143
sparse ratio: 0.5178571939468384
fold 3 accuracy: 0.8614285714285714
sparse ratio: 0.5178571939468384
fold 4 accuracy: 0.8719285714285714
sparse ratio: 0.5178571939468384
0.8637142857142857 0.0051743085903099145
[tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0')]
[2024-03-01 17:43:20,258] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 17:43:20,259] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         1.05 K  
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.4 KMACs
fwd flops per GPU:                                                      2.8 K   
fwd flops of model = fwd flops per GPU * mp_size:                       2.8 K   
fwd latency:                                                            394.58 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    7.1 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '1.05 K'}
    MACs        - {'KronLayer': '1.4 KMACs'}
    fwd latency - {'KronLayer': '394.58 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  1.05 K = 100% Params, 1.4 KMACs = 100% MACs, 394.58 us = 100% latency, 7.1 MFLOPS
  (kronlinear): KronLinear(1.05 K = 100% Params, 1.4 KMACs = 100% MACs, 303.27 us = 76.86% latency, 9.23 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 26.7 us = 6.77% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 17:43:20,261] [INFO] [profiler.py:226:end_profile] Flops profiler finished
2.8 K 1.05 K
fold 0 accuracy: 0.8272142857142857
sparse ratio: 0.4285714626312256
fold 1 accuracy: 0.8506428571428571
sparse ratio: 0.4285714626312256
fold 2 accuracy: 0.8624285714285714
sparse ratio: 0.4285714626312256
fold 3 accuracy: 0.8624285714285714
sparse ratio: 0.4107142984867096
fold 4 accuracy: 0.8620714285714286
sparse ratio: 0.4285714626312256
0.852957142857143 0.013642116659037043
[tensor(0.4286, device='cuda:0'), tensor(0.4286, device='cuda:0'), tensor(0.4286, device='cuda:0'), tensor(0.4107, device='cuda:0'), tensor(0.4286, device='cuda:0')]
[2024-03-01 18:01:44,471] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 18:01:44,472] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         1.24 K  
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.68 KMACs
fwd flops per GPU:                                                      3.36 K  
fwd flops of model = fwd flops per GPU * mp_size:                       3.36 K  
fwd latency:                                                            328.3 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    10.23 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '1.24 K'}
    MACs        - {'KronLayer': '1.68 KMACs'}
    fwd latency - {'KronLayer': '328.3 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  1.24 K = 100% Params, 1.68 KMACs = 100% MACs, 328.3 us = 100% latency, 10.23 MFLOPS
  (kronlinear): KronLinear(1.24 K = 100% Params, 1.68 KMACs = 100% MACs, 235.08 us = 71.6% latency, 14.29 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 27.42 us = 8.35% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 18:01:44,474] [INFO] [profiler.py:226:end_profile] Flops profiler finished
3.36 K 1.24 K
fold 0 accuracy: 0.8795
sparse ratio: 0.4107142984867096
fold 1 accuracy: 0.8765714285714286
sparse ratio: 0.4107142984867096
fold 2 accuracy: 0.8832857142857143
sparse ratio: 0.4107142984867096
fold 3 accuracy: 0.8742142857142857
sparse ratio: 0.4107142984867096
fold 4 accuracy: 0.8832857142857143
sparse ratio: 0.4107142984867096
0.8793714285714286 0.003608210591618726
[tensor(0.4107, device='cuda:0'), tensor(0.4107, device='cuda:0'), tensor(0.4107, device='cuda:0'), tensor(0.4107, device='cuda:0'), tensor(0.4107, device='cuda:0')]
[2024-03-01 18:20:12,645] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 18:20:12,646] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         1.24 K  
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.68 KMACs
fwd flops per GPU:                                                      3.36 K  
fwd flops of model = fwd flops per GPU * mp_size:                       3.36 K  
fwd latency:                                                            327.83 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    10.25 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '1.24 K'}
    MACs        - {'KronLayer': '1.68 KMACs'}
    fwd latency - {'KronLayer': '327.83 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  1.24 K = 100% Params, 1.68 KMACs = 100% MACs, 327.83 us = 100% latency, 10.25 MFLOPS
  (kronlinear): KronLinear(1.24 K = 100% Params, 1.68 KMACs = 100% MACs, 236.03 us = 72% latency, 14.24 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 27.18 us = 8.29% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 18:20:12,648] [INFO] [profiler.py:226:end_profile] Flops profiler finished
3.36 K 1.24 K
fold 0 accuracy: 0.8745714285714286
sparse ratio: 0.5178571939468384
fold 1 accuracy: 0.8774285714285714
sparse ratio: 0.5178571939468384
fold 2 accuracy: 0.8824285714285715
sparse ratio: 0.5178571939468384
fold 3 accuracy: 0.8831428571428571
sparse ratio: 0.5178571939468384
fold 4 accuracy: 0.8817857142857143
sparse ratio: 0.5
0.8798714285714284 0.00331551701265798
[tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5000, device='cuda:0')]
[2024-03-01 18:38:39,429] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 18:38:39,430] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         1.24 K  
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.68 KMACs
fwd flops per GPU:                                                      3.36 K  
fwd flops of model = fwd flops per GPU * mp_size:                       3.36 K  
fwd latency:                                                            320.67 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    10.48 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '1.24 K'}
    MACs        - {'KronLayer': '1.68 KMACs'}
    fwd latency - {'KronLayer': '320.67 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  1.24 K = 100% Params, 1.68 KMACs = 100% MACs, 320.67 us = 100% latency, 10.48 MFLOPS
  (kronlinear): KronLinear(1.24 K = 100% Params, 1.68 KMACs = 100% MACs, 230.07 us = 71.75% latency, 14.6 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 26.94 us = 8.4% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 18:38:39,431] [INFO] [profiler.py:226:end_profile] Flops profiler finished
3.36 K 1.24 K
fold 0 accuracy: 0.8868571428571429
sparse ratio: 0.5178571939468384
fold 1 accuracy: 0.8872142857142857
sparse ratio: 0.5178571939468384
fold 2 accuracy: 0.8863571428571428
sparse ratio: 0.5178571939468384
fold 3 accuracy: 0.8864285714285715
sparse ratio: 0.5178571939468384
fold 4 accuracy: 0.8900714285714286
sparse ratio: 0.5178571939468384
0.8873857142857144 0.0013781087306411748
[tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0'), tensor(0.5179, device='cuda:0')]
[2024-03-01 18:57:04,245] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 18:57:04,246] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         1.24 K  
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.68 KMACs
fwd flops per GPU:                                                      3.36 K  
fwd flops of model = fwd flops per GPU * mp_size:                       3.36 K  
fwd latency:                                                            519.04 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    6.47 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '1.24 K'}
    MACs        - {'KronLayer': '1.68 KMACs'}
    fwd latency - {'KronLayer': '519.04 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  1.24 K = 100% Params, 1.68 KMACs = 100% MACs, 519.04 us = 100% latency, 6.47 MFLOPS
  (kronlinear): KronLinear(1.24 K = 100% Params, 1.68 KMACs = 100% MACs, 247 us = 47.59% latency, 13.6 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 27.18 us = 5.24% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 18:57:04,250] [INFO] [profiler.py:226:end_profile] Flops profiler finished
3.36 K 1.24 K
fold 0 accuracy: 0.8851428571428571
sparse ratio: 0.4464285969734192
fold 1 accuracy: 0.8862857142857142
sparse ratio: 0.4464285969734192
fold 2 accuracy: 0.8862857142857142
sparse ratio: 0.4464285969734192
fold 3 accuracy: 0.8877142857142857
sparse ratio: 0.4464285969734192
fold 4 accuracy: 0.8841428571428571
sparse ratio: 0.4464285969734192
0.8859142857142857 0.001204074714611734
[tensor(0.4464, device='cuda:0'), tensor(0.4464, device='cuda:0'), tensor(0.4464, device='cuda:0'), tensor(0.4464, device='cuda:0'), tensor(0.4464, device='cuda:0')]
[2024-03-01 19:15:26,510] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 19:15:26,511] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         1.24 K  
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.68 KMACs
fwd flops per GPU:                                                      3.36 K  
fwd flops of model = fwd flops per GPU * mp_size:                       3.36 K  
fwd latency:                                                            331.64 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    10.13 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '1.24 K'}
    MACs        - {'KronLayer': '1.68 KMACs'}
    fwd latency - {'KronLayer': '331.64 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  1.24 K = 100% Params, 1.68 KMACs = 100% MACs, 331.64 us = 100% latency, 10.13 MFLOPS
  (kronlinear): KronLinear(1.24 K = 100% Params, 1.68 KMACs = 100% MACs, 238.9 us = 72.03% latency, 14.06 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 26.94 us = 8.12% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 19:15:26,516] [INFO] [profiler.py:226:end_profile] Flops profiler finished
3.36 K 1.24 K
fold 0 accuracy: 0.8403571428571428
sparse ratio: 0.4821428656578064
fold 1 accuracy: 0.8575
sparse ratio: 0.4821428656578064
fold 2 accuracy: 0.8560714285714286
sparse ratio: 0.4821428656578064
fold 3 accuracy: 0.8796428571428572
sparse ratio: 0.4821428656578064
fold 4 accuracy: 0.8767857142857143
sparse ratio: 0.4821428656578064
0.8620714285714286 0.014516703960526008
[tensor(0.4821, device='cuda:0'), tensor(0.4821, device='cuda:0'), tensor(0.4821, device='cuda:0'), tensor(0.4821, device='cuda:0'), tensor(0.4821, device='cuda:0')]
[2024-03-01 19:34:00,632] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 19:34:00,633] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         1.44 K  
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.96 KMACs
fwd flops per GPU:                                                      3.92 K  
fwd flops of model = fwd flops per GPU * mp_size:                       3.92 K  
fwd latency:                                                            296.83 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    13.21 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '1.44 K'}
    MACs        - {'KronLayer': '1.96 KMACs'}
    fwd latency - {'KronLayer': '296.83 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  1.44 K = 100% Params, 1.96 KMACs = 100% MACs, 296.83 us = 100% latency, 13.21 MFLOPS
  (kronlinear): KronLinear(1.44 K = 100% Params, 1.96 KMACs = 100% MACs, 216.72 us = 73.01% latency, 18.09 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 24.08 us = 8.11% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 19:34:00,634] [INFO] [profiler.py:226:end_profile] Flops profiler finished
3.92 K 1.44 K
fold 0 accuracy: 0.8930714285714285
sparse ratio: 0.4821428656578064
fold 1 accuracy: 0.8868571428571429
sparse ratio: 0.4821428656578064
fold 2 accuracy: 0.8862142857142857
sparse ratio: 0.4821428656578064
fold 3 accuracy: 0.8975714285714286
sparse ratio: 0.4821428656578064
fold 4 accuracy: 0.8912142857142857
sparse ratio: 0.4821428656578064
0.8909857142857144 0.004185348594459334
[tensor(0.4821, device='cuda:0'), tensor(0.4821, device='cuda:0'), tensor(0.4821, device='cuda:0'), tensor(0.4821, device='cuda:0'), tensor(0.4821, device='cuda:0')]
[2024-03-01 19:52:47,949] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 19:52:47,950] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         1.44 K  
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.96 KMACs
fwd flops per GPU:                                                      3.92 K  
fwd flops of model = fwd flops per GPU * mp_size:                       3.92 K  
fwd latency:                                                            320.91 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    12.22 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '1.44 K'}
    MACs        - {'KronLayer': '1.96 KMACs'}
    fwd latency - {'KronLayer': '320.91 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  1.44 K = 100% Params, 1.96 KMACs = 100% MACs, 320.91 us = 100% latency, 12.22 MFLOPS
  (kronlinear): KronLinear(1.44 K = 100% Params, 1.96 KMACs = 100% MACs, 233.65 us = 72.81% latency, 16.78 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 7.95% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 19:52:47,952] [INFO] [profiler.py:226:end_profile] Flops profiler finished
3.92 K 1.44 K
fold 0 accuracy: 0.8872142857142857
sparse ratio: 0.5178571939468384
fold 1 accuracy: 0.8863571428571428
sparse ratio: 0.5
fold 2 accuracy: 0.8907142857142857
sparse ratio: 0.5
fold 3 accuracy: 0.8927857142857143
sparse ratio: 0.5
fold 4 accuracy: 0.893
sparse ratio: 0.5
0.8900142857142856 0.0027676705006196125
[tensor(0.5179, device='cuda:0'), tensor(0.5000, device='cuda:0'), tensor(0.5000, device='cuda:0'), tensor(0.5000, device='cuda:0'), tensor(0.5000, device='cuda:0')]
[2024-03-01 20:11:33,608] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 20:11:33,609] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         1.44 K  
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.96 KMACs
fwd flops per GPU:                                                      3.92 K  
fwd flops of model = fwd flops per GPU * mp_size:                       3.92 K  
fwd latency:                                                            321.39 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    12.2 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '1.44 K'}
    MACs        - {'KronLayer': '1.96 KMACs'}
    fwd latency - {'KronLayer': '321.39 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  1.44 K = 100% Params, 1.96 KMACs = 100% MACs, 321.39 us = 100% latency, 12.2 MFLOPS
  (kronlinear): KronLinear(1.44 K = 100% Params, 1.96 KMACs = 100% MACs, 232.22 us = 72.26% latency, 16.88 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 25.99 us = 8.09% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 20:11:33,611] [INFO] [profiler.py:226:end_profile] Flops profiler finished
3.92 K 1.44 K
fold 0 accuracy: 0.8951428571428571
sparse ratio: 0.4285714626312256
fold 1 accuracy: 0.8992857142857142
sparse ratio: 0.4285714626312256
fold 2 accuracy: 0.8937857142857143
sparse ratio: 0.4285714626312256
fold 3 accuracy: 0.8997142857142857
sparse ratio: 0.4285714626312256
fold 4 accuracy: 0.8928571428571429
sparse ratio: 0.4285714626312256
0.896157142857143 0.002827849835807658
[tensor(0.4286, device='cuda:0'), tensor(0.4286, device='cuda:0'), tensor(0.4286, device='cuda:0'), tensor(0.4286, device='cuda:0'), tensor(0.4286, device='cuda:0')]
[2024-03-01 20:30:03,102] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 20:30:03,103] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         1.44 K  
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.96 KMACs
fwd flops per GPU:                                                      3.92 K  
fwd flops of model = fwd flops per GPU * mp_size:                       3.92 K  
fwd latency:                                                            1.57 ms 
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    2.5 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '1.44 K'}
    MACs        - {'KronLayer': '1.96 KMACs'}
    fwd latency - {'KronLayer': '1.57 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  1.44 K = 100% Params, 1.96 KMACs = 100% MACs, 1.57 ms = 100% latency, 2.5 MFLOPS
  (kronlinear): KronLinear(1.44 K = 100% Params, 1.96 KMACs = 100% MACs, 223.16 us = 14.25% latency, 17.57 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 23.84 us = 1.52% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 20:30:03,106] [INFO] [profiler.py:226:end_profile] Flops profiler finished
3.92 K 1.44 K
fold 0 accuracy: 0.8917857142857143
sparse ratio: 0.3750000298023224
fold 1 accuracy: 0.8921428571428571
sparse ratio: 0.3750000298023224
fold 2 accuracy: 0.891
sparse ratio: 0.3750000298023224
fold 3 accuracy: 0.8981428571428571
sparse ratio: 0.3750000298023224
fold 4 accuracy: 0.8914285714285715
sparse ratio: 0.3750000298023224
0.8929 0.002648680842246114
[tensor(0.3750, device='cuda:0'), tensor(0.3750, device='cuda:0'), tensor(0.3750, device='cuda:0'), tensor(0.3750, device='cuda:0'), tensor(0.3750, device='cuda:0')]
[2024-03-01 20:48:40,477] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-03-01 20:48:40,478] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         1.44 K  
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       1.96 KMACs
fwd flops per GPU:                                                      3.92 K  
fwd flops of model = fwd flops per GPU * mp_size:                       3.92 K  
fwd latency:                                                            327.59 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    11.97 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '1.44 K'}
    MACs        - {'KronLayer': '1.96 KMACs'}
    fwd latency - {'KronLayer': '327.59 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  1.44 K = 100% Params, 1.96 KMACs = 100% MACs, 327.59 us = 100% latency, 11.97 MFLOPS
  (kronlinear): KronLinear(1.44 K = 100% Params, 1.96 KMACs = 100% MACs, 239.13 us = 73% latency, 16.39 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 25.99 us = 7.93% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-03-01 20:48:40,480] [INFO] [profiler.py:226:end_profile] Flops profiler finished
3.92 K 1.44 K
fold 0 accuracy: 0.8792857142857143
sparse ratio: 0.4107142984867096
fold 1 accuracy: 0.8832142857142857
sparse ratio: 0.4107142984867096
fold 2 accuracy: 0.8843571428571428
sparse ratio: 0.4107142984867096
fold 3 accuracy: 0.8846428571428572
sparse ratio: 0.4107142984867096
fold 4 accuracy: 0.8875714285714286
sparse ratio: 0.4107142984867096
0.8838142857142858 0.0026819882968591975
[tensor(0.4107, device='cuda:0'), tensor(0.4107, device='cuda:0'), tensor(0.4107, device='cuda:0'), tensor(0.4107, device='cuda:0'), tensor(0.4107, device='cuda:0')]
[0.48372857142857145, 0.44474285714285716, 0.5113571428571428, 0.4111714285714285, 0.47981428571428564, 0.42807142857142855, 0.4233142857142857, 0.46709999999999996, 0.42545714285714287, 0.7047714285714286, 0.6460714285714284, 0.6513285714285715, 0.6677285714285713, 0.7105142857142857, 0.7963428571428571, 0.8145142857142857, 0.8132714285714286, 0.812857142857143, 0.8111857142857142, 0.8476571428571429, 0.8219000000000001, 0.8214142857142857, 0.8480857142857143, 0.8298428571428571, 0.8719571428571428, 0.8365571428571428, 0.8708285714285715, 0.8637142857142857, 0.852957142857143, 0.8793714285714286, 0.8798714285714284, 0.8873857142857144, 0.8859142857142857, 0.8620714285714286, 0.8909857142857144, 0.8900142857142856, 0.896157142857143, 0.8929, 0.8838142857142858]
[0.005796092983859982, 0.008752445722336655, 0.010068844653338405, 0.007984500291061377, 0.011365684119761738, 0.006065627480705836, 0.005822300548191351, 0.0075372002597185765, 0.02520149817380073, 0.025605619600041714, 0.007957925070555219, 0.042002152520156995, 0.02873160103369081, 0.012780358016128038, 0.009802686012579647, 0.005754040284168766, 0.0055212761021269634, 0.0058969033727673675, 0.014246202501518982, 0.0046553458408542595, 0.010549262475429751, 0.016482111515215527, 0.004743502537843817, 0.004155497956359293, 0.007295036053400182, 0.013764298687813597, 0.00501601516776667, 0.0051743085903099145, 0.013642116659037043, 0.003608210591618726, 0.00331551701265798, 0.0013781087306411748, 0.001204074714611734, 0.014516703960526008, 0.004185348594459334, 0.0027676705006196125, 0.002827849835807658, 0.002648680842246114, 0.0026819882968591975]
