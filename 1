nohup: ignoring input
[2024-02-28 22:11:36,610] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-28 22:11:37,438] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-28 22:11:37,441] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         458     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       560 MACs
fwd flops per GPU:                                                      1.12 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.12 K  
fwd latency:                                                            1.56 ms 
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    716.54 KFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '458'}
    MACs        - {'KronLayer': '560 MACs'}
    fwd latency - {'KronLayer': '1.56 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  458 = 100% Params, 560 MACs = 100% MACs, 1.56 ms = 100% latency, 716.54 KFLOPS
  (kronlinear): KronLinear(458 = 100% Params, 560 MACs = 100% MACs, 916.72 us = 58.65% latency, 1.22 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 75.1 us = 4.8% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-28 22:11:37,571] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.12 K 458
[2024-02-28 22:11:37,573] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-28 22:11:37,573] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            350.48 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.6 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '350.48 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 350.48 us = 100% latency, 1.6 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 240.8 us = 68.71% latency, 2.33 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 28.37 us = 8.1% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-28 22:11:37,575] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
iteration 0, loss 2.3060667514801025
iteration 100, loss 2.330170154571533
iteration 200, loss 2.2397427558898926
iteration 300, loss 2.16215443611145
iteration 400, loss 2.082426071166992
iteration 500, loss 2.0605287551879883
iteration 600, loss 2.100283145904541
iteration 700, loss 2.0356571674346924
iteration 800, loss 1.9527889490127563
iteration 0, loss 2.051851749420166
iteration 100, loss 1.9871405363082886
iteration 200, loss 1.9495383501052856
iteration 300, loss 1.996978521347046
iteration 400, loss 1.9822934865951538
iteration 500, loss 1.958436131477356
iteration 600, loss 1.94000244140625
iteration 700, loss 1.9717297554016113
iteration 800, loss 1.9127960205078125
iteration 0, loss 1.9794141054153442
iteration 100, loss 1.96222722530365
iteration 200, loss 1.900378942489624
iteration 300, loss 1.8593600988388062
iteration 400, loss 2.014261484146118
iteration 500, loss 2.0187289714813232
iteration 600, loss 1.909435510635376
iteration 700, loss 1.9417134523391724
iteration 800, loss 1.938244104385376
iteration 0, loss 1.965234637260437
iteration 100, loss 1.9881547689437866
iteration 200, loss 1.9567900896072388
iteration 300, loss 1.953709602355957
iteration 400, loss 1.937737226486206
iteration 500, loss 1.880074381828308
iteration 600, loss 1.9105857610702515
iteration 700, loss 1.976212501525879
iteration 800, loss 1.927194595336914
iteration 0, loss 1.935361385345459
iteration 100, loss 1.9345641136169434
iteration 200, loss 1.9231998920440674
iteration 300, loss 1.9676202535629272
iteration 400, loss 1.9349195957183838
iteration 500, loss 1.9241089820861816
iteration 600, loss 1.9855202436447144
iteration 700, loss 1.9765443801879883
iteration 800, loss 1.941202163696289
iteration 0, loss 1.9487842321395874
iteration 100, loss 1.961617350578308
iteration 200, loss 1.955782175064087
iteration 300, loss 1.9392588138580322
iteration 400, loss 1.865206003189087
iteration 500, loss 1.8968713283538818
iteration 600, loss 1.9642932415008545
iteration 700, loss 1.892467737197876
iteration 800, loss 1.8733563423156738
iteration 0, loss 1.8851579427719116
iteration 100, loss 1.9672191143035889
iteration 200, loss 1.9081958532333374
iteration 300, loss 1.9105968475341797
iteration 400, loss 1.9325203895568848
iteration 500, loss 1.8995516300201416
iteration 600, loss 1.927707314491272
iteration 700, loss 1.9224134683609009
iteration 800, loss 1.9758296012878418
iteration 0, loss 1.903597116470337
iteration 100, loss 1.9200360774993896
iteration 200, loss 1.8516604900360107
iteration 300, loss 1.8733031749725342
iteration 400, loss 1.940526008605957
iteration 500, loss 1.8722732067108154
iteration 600, loss 1.9014716148376465
iteration 700, loss 1.8498224020004272
iteration 800, loss 1.9027721881866455
iteration 0, loss 1.8988579511642456
iteration 100, loss 1.8591071367263794
iteration 200, loss 1.8731342554092407
iteration 300, loss 1.8070476055145264
iteration 400, loss 1.9488472938537598
iteration 500, loss 1.9110171794891357
iteration 600, loss 1.8864690065383911
iteration 700, loss 1.9009069204330444
iteration 800, loss 1.906097173690796
iteration 0, loss 1.9480183124542236
iteration 100, loss 1.88833749294281
iteration 200, loss 1.8612762689590454
iteration 300, loss 1.8305634260177612
iteration 400, loss 1.876592755317688
iteration 500, loss 1.836796522140503
iteration 600, loss 1.9181393384933472
iteration 700, loss 1.8717708587646484
iteration 800, loss 1.8472661972045898
iteration 0, loss 1.8864493370056152
iteration 100, loss 1.9348441362380981
iteration 200, loss 1.8720958232879639
iteration 300, loss 1.8709300756454468
iteration 400, loss 1.8845621347427368
iteration 500, loss 1.9083393812179565
iteration 600, loss 1.8636053800582886
iteration 700, loss 1.8693053722381592
iteration 800, loss 1.925075650215149
iteration 0, loss 1.85298752784729
iteration 100, loss 1.8841712474822998
iteration 200, loss 1.8939110040664673
iteration 300, loss 1.8743374347686768
iteration 400, loss 1.8882079124450684
iteration 500, loss 1.9059118032455444
iteration 600, loss 1.8944296836853027
iteration 700, loss 1.917213797569275
iteration 800, loss 1.933752179145813
iteration 0, loss 1.9260536432266235
iteration 100, loss 1.8899319171905518
iteration 200, loss 1.8522255420684814
iteration 300, loss 1.9096864461898804
iteration 400, loss 1.8734041452407837
iteration 500, loss 1.8766225576400757
iteration 600, loss 1.8253908157348633
iteration 700, loss 1.8633660078048706
iteration 800, loss 1.888476014137268
iteration 0, loss 1.8506666421890259
iteration 100, loss 1.8111512660980225
iteration 200, loss 1.819726586341858
iteration 300, loss 1.8889992237091064
iteration 400, loss 1.8792622089385986
iteration 500, loss 1.8953384160995483
iteration 600, loss 1.9176970720291138
iteration 700, loss 1.9640014171600342
iteration 800, loss 1.843929648399353
iteration 0, loss 1.8513442277908325
iteration 100, loss 1.813293218612671
iteration 200, loss 1.9262516498565674
iteration 300, loss 1.9296414852142334
iteration 400, loss 1.8098437786102295
iteration 500, loss 1.8467447757720947
iteration 600, loss 1.8296757936477661
iteration 700, loss 1.8322081565856934
iteration 800, loss 1.9021077156066895
iteration 0, loss 1.8859342336654663
iteration 100, loss 1.8280329704284668
iteration 200, loss 1.8891578912734985
iteration 300, loss 1.8987208604812622
iteration 400, loss 1.850911259651184
iteration 500, loss 1.791264533996582
iteration 600, loss 1.8137130737304688
iteration 700, loss 1.8635026216506958
iteration 800, loss 1.7998853921890259
iteration 0, loss 1.8286188840866089
iteration 100, loss 1.8804693222045898
iteration 200, loss 1.8181477785110474
iteration 300, loss 1.8093547821044922
iteration 400, loss 1.7937755584716797
iteration 500, loss 1.8259028196334839
iteration 600, loss 1.842163324356079
iteration 700, loss 1.884005069732666
iteration 800, loss 1.8749467134475708
iteration 0, loss 1.8294858932495117
iteration 100, loss 1.828096866607666
iteration 200, loss 1.849041223526001
iteration 300, loss 1.786738395690918
iteration 400, loss 1.9259651899337769
iteration 500, loss 1.8060072660446167
iteration 600, loss 1.9206163883209229
iteration 700, loss 1.8501465320587158
iteration 800, loss 1.7984113693237305
iteration 0, loss 1.8471077680587769
iteration 100, loss 1.877955436706543
iteration 200, loss 1.8518617153167725
iteration 300, loss 1.8060555458068848
iteration 400, loss 1.8031721115112305
iteration 500, loss 1.7987226247787476
iteration 600, loss 1.8318381309509277
iteration 700, loss 1.8557337522506714
iteration 800, loss 1.7796103954315186
iteration 0, loss 1.9036738872528076
iteration 100, loss 1.865604043006897
iteration 200, loss 1.8824403285980225
iteration 300, loss 1.821696162223816
iteration 400, loss 1.830711841583252
iteration 500, loss 1.770347237586975
iteration 600, loss 1.8661805391311646
iteration 700, loss 1.920827865600586
iteration 800, loss 1.8209432363510132
iteration 0, loss 1.8048568964004517
iteration 100, loss 1.789943814277649
iteration 200, loss 1.8105486631393433
iteration 300, loss 1.8863885402679443
iteration 400, loss 1.8616822957992554
iteration 500, loss 1.8254351615905762
iteration 600, loss 1.8220778703689575
iteration 700, loss 1.8165783882141113
iteration 800, loss 1.820631980895996
iteration 0, loss 1.8222798109054565
iteration 100, loss 1.7846124172210693
iteration 200, loss 1.8690201044082642
iteration 300, loss 1.8169565200805664
iteration 400, loss 1.7911098003387451
iteration 500, loss 1.8026541471481323
iteration 600, loss 1.839350938796997
iteration 700, loss 1.8067822456359863
iteration 800, loss 1.8256090879440308
iteration 0, loss 1.7635095119476318
iteration 100, loss 1.78435218334198
iteration 200, loss 1.7896798849105835
iteration 300, loss 1.875946044921875
iteration 400, loss 1.8162862062454224
iteration 500, loss 1.8703205585479736
iteration 600, loss 1.8469067811965942
iteration 700, loss 1.8930221796035767
iteration 800, loss 1.8466746807098389
iteration 0, loss 1.8567804098129272
iteration 100, loss 1.8441293239593506
iteration 200, loss 1.8358688354492188
iteration 300, loss 1.8329832553863525
iteration 400, loss 1.8547160625457764
iteration 500, loss 1.9150118827819824
iteration 600, loss 1.8353880643844604
iteration 700, loss 1.8881115913391113
iteration 800, loss 1.7998448610305786
iteration 0, loss 1.8800079822540283
iteration 100, loss 1.8553907871246338
iteration 200, loss 1.7847800254821777
iteration 300, loss 1.8347233533859253
iteration 400, loss 1.8038578033447266
iteration 500, loss 1.8668873310089111
iteration 600, loss 1.8961116075515747
iteration 700, loss 1.8790937662124634
iteration 800, loss 1.8243820667266846
iteration 0, loss 1.8262284994125366
iteration 100, loss 1.7548701763153076
iteration 200, loss 1.8732796907424927
iteration 300, loss 1.8104321956634521
iteration 400, loss 1.9263275861740112
iteration 500, loss 1.8317921161651611
iteration 600, loss 1.8541615009307861
iteration 700, loss 1.7702966928482056
iteration 800, loss 1.8015226125717163
iteration 0, loss 1.8969461917877197
iteration 100, loss 1.877942442893982
iteration 200, loss 1.849649429321289
iteration 300, loss 1.8302958011627197
iteration 400, loss 1.8767234086990356
iteration 500, loss 1.8099850416183472
iteration 600, loss 1.8717845678329468
iteration 700, loss 1.7265955209732056
iteration 800, loss 1.850494384765625
iteration 0, loss 1.8698616027832031
iteration 100, loss 1.8361788988113403
iteration 200, loss 1.772818684577942
iteration 300, loss 1.8418080806732178
iteration 400, loss 1.855316400527954
iteration 500, loss 1.8537554740905762
iteration 600, loss 1.9279652833938599
iteration 700, loss 1.8560113906860352
iteration 800, loss 1.8579654693603516
iteration 0, loss 1.7850006818771362
iteration 100, loss 1.8702874183654785
iteration 200, loss 1.8033491373062134
iteration 300, loss 1.8702393770217896
iteration 400, loss 1.7805073261260986
iteration 500, loss 1.77684485912323
iteration 600, loss 1.874414324760437
iteration 700, loss 1.7706915140151978
iteration 800, loss 1.8570952415466309
iteration 0, loss 1.7898938655853271
iteration 100, loss 1.8535507917404175
iteration 200, loss 1.8173812627792358
iteration 300, loss 1.8514564037322998
iteration 400, loss 1.8260078430175781
iteration 500, loss 1.8648830652236938
iteration 600, loss 1.7891933917999268
iteration 700, loss 1.9001269340515137
iteration 800, loss 1.8145630359649658
iteration 0, loss 1.77585768699646
iteration 100, loss 1.8471888303756714
iteration 200, loss 1.8602008819580078
iteration 300, loss 1.8474187850952148
iteration 400, loss 1.7847670316696167
iteration 500, loss 1.8535473346710205
iteration 600, loss 1.8362869024276733
iteration 700, loss 1.8077963590621948
iteration 800, loss 1.8386906385421753
iteration 0, loss 1.8797837495803833
iteration 100, loss 1.8246761560440063
iteration 200, loss 1.7999509572982788
iteration 300, loss 1.914347767829895
iteration 400, loss 1.8561583757400513
iteration 500, loss 1.8515406847000122
iteration 600, loss 1.8494948148727417
iteration 700, loss 1.8863937854766846
iteration 800, loss 1.8804874420166016
iteration 0, loss 1.8426305055618286
iteration 100, loss 1.878751277923584
iteration 200, loss 1.8443337678909302
iteration 300, loss 1.848254680633545
iteration 400, loss 1.792402744293213
iteration 500, loss 1.7441328763961792
iteration 600, loss 1.8346259593963623
iteration 700, loss 1.87278413772583
iteration 800, loss 1.8614321947097778
iteration 0, loss 1.8462262153625488
iteration 100, loss 1.8079129457473755
iteration 200, loss 1.7954813241958618
iteration 300, loss 1.797217607498169
iteration 400, loss 1.7964929342269897
iteration 500, loss 1.8478643894195557
iteration 600, loss 1.8091422319412231
iteration 700, loss 1.8956280946731567
iteration 800, loss 1.7965284585952759
iteration 0, loss 1.8355692625045776
iteration 100, loss 1.8689732551574707
iteration 200, loss 1.938011884689331
iteration 300, loss 1.8016515970230103
iteration 400, loss 1.860059142112732
iteration 500, loss 1.8537722826004028
iteration 600, loss 1.8391767740249634
iteration 700, loss 1.9238008260726929
iteration 800, loss 1.8645918369293213
iteration 0, loss 1.8758348226547241
iteration 100, loss 1.887473464012146
iteration 200, loss 1.828322172164917
iteration 300, loss 1.840691089630127
iteration 400, loss 1.8177212476730347
iteration 500, loss 1.7828054428100586
iteration 600, loss 1.8654670715332031
iteration 700, loss 1.8554699420928955
iteration 800, loss 1.8446214199066162
iteration 0, loss 1.8442004919052124
iteration 100, loss 1.8671115636825562
iteration 200, loss 1.8174309730529785
iteration 300, loss 1.8928797245025635
iteration 400, loss 1.790562629699707
iteration 500, loss 1.81269371509552
iteration 600, loss 1.8074331283569336
iteration 700, loss 1.8449841737747192
iteration 800, loss 1.8522621393203735
iteration 0, loss 1.8346216678619385
iteration 100, loss 1.8956432342529297
iteration 200, loss 1.8070299625396729
iteration 300, loss 1.8005053997039795
iteration 400, loss 1.7850943803787231
iteration 500, loss 1.8753252029418945
iteration 600, loss 1.8136940002441406
iteration 700, loss 1.7608224153518677
iteration 800, loss 1.9118986129760742
iteration 0, loss 1.8259024620056152
iteration 100, loss 1.8112562894821167
iteration 200, loss 1.835411787033081
iteration 300, loss 1.8151116371154785
iteration 400, loss 1.8391276597976685
iteration 500, loss 1.797232985496521
iteration 600, loss 1.7888544797897339
iteration 700, loss 1.832829475402832
iteration 800, loss 1.804755449295044
iteration 0, loss 1.8812267780303955
iteration 100, loss 1.826575756072998
iteration 200, loss 1.8912982940673828
iteration 300, loss 1.8446345329284668
iteration 400, loss 1.834995985031128
iteration 500, loss 1.8647487163543701
iteration 600, loss 1.8301564455032349
iteration 700, loss 1.7511550188064575
iteration 800, loss 1.8946670293807983
iteration 0, loss 1.8748900890350342
iteration 100, loss 1.7926937341690063
iteration 200, loss 1.8817797899246216
iteration 300, loss 1.7829841375350952
iteration 400, loss 1.8268462419509888
iteration 500, loss 1.7708489894866943
iteration 600, loss 1.8365037441253662
iteration 700, loss 1.8290590047836304
iteration 800, loss 1.8203282356262207
iteration 0, loss 1.74751615524292
iteration 100, loss 1.7214597463607788
iteration 200, loss 1.9191176891326904
iteration 300, loss 1.8956917524337769
iteration 400, loss 1.8207038640975952
iteration 500, loss 1.7924388647079468
iteration 600, loss 1.8060482740402222
iteration 700, loss 1.8848572969436646
iteration 800, loss 1.8962534666061401
iteration 0, loss 1.818922519683838
iteration 100, loss 1.8500396013259888
iteration 200, loss 1.8483390808105469
iteration 300, loss 1.894924283027649
iteration 400, loss 1.8036813735961914
iteration 500, loss 1.812015175819397
iteration 600, loss 1.8159465789794922
iteration 700, loss 1.7326581478118896
iteration 800, loss 1.7967458963394165
iteration 0, loss 1.848581314086914
iteration 100, loss 1.8172281980514526
iteration 200, loss 1.7918729782104492
iteration 300, loss 1.7909009456634521
iteration 400, loss 1.8068535327911377
iteration 500, loss 1.8022663593292236
iteration 600, loss 1.7788379192352295
iteration 700, loss 1.7673754692077637
iteration 800, loss 1.816755771636963
iteration 0, loss 1.7936257123947144
iteration 100, loss 1.8642665147781372
iteration 200, loss 1.8101208209991455
iteration 300, loss 1.8195558786392212
iteration 400, loss 1.7842297554016113
iteration 500, loss 1.8471405506134033
iteration 600, loss 1.8086626529693604
iteration 700, loss 1.8060411214828491
iteration 800, loss 1.8034470081329346
iteration 0, loss 1.7857214212417603
iteration 100, loss 1.8405665159225464
iteration 200, loss 1.8541505336761475
iteration 300, loss 1.8679001331329346
iteration 400, loss 1.7995226383209229
iteration 500, loss 1.8092635869979858
iteration 600, loss 1.898343563079834
iteration 700, loss 1.7942289113998413
iteration 800, loss 1.8008967638015747
iteration 0, loss 1.852344274520874
iteration 100, loss 1.8032115697860718
iteration 200, loss 1.8078323602676392
iteration 300, loss 1.8818674087524414
iteration 400, loss 1.815466046333313
iteration 500, loss 1.829912543296814
iteration 600, loss 1.7912720441818237
iteration 700, loss 1.8243614435195923
iteration 800, loss 1.8050378561019897
iteration 0, loss 1.8805867433547974
iteration 100, loss 1.7659367322921753
iteration 200, loss 1.816698670387268
iteration 300, loss 1.9016188383102417
iteration 400, loss 1.8098206520080566
iteration 500, loss 1.77382493019104
iteration 600, loss 1.7875769138336182
iteration 700, loss 1.8046300411224365
iteration 800, loss 1.8641287088394165
iteration 0, loss 1.8545414209365845
iteration 100, loss 1.8015846014022827
iteration 200, loss 1.8711693286895752
iteration 300, loss 1.7927803993225098
iteration 400, loss 1.8522610664367676
iteration 500, loss 1.793601155281067
iteration 600, loss 1.7684416770935059
iteration 700, loss 1.7181804180145264
iteration 800, loss 1.8294970989227295
iteration 0, loss 1.8530309200286865
iteration 100, loss 1.8023799657821655
iteration 200, loss 1.8291593790054321
iteration 300, loss 1.8173878192901611
iteration 400, loss 1.828049898147583
iteration 500, loss 1.8224607706069946
iteration 600, loss 1.8734487295150757
iteration 700, loss 1.7857033014297485
iteration 800, loss 1.7841392755508423
fold 0 accuracy: 0.4764285714285714
iteration 0, loss 1.8181370496749878
iteration 100, loss 1.7896554470062256
iteration 200, loss 1.7718039751052856
iteration 300, loss 1.9202396869659424
iteration 400, loss 1.8519848585128784
iteration 500, loss 1.8470226526260376
iteration 600, loss 1.8075274229049683
iteration 700, loss 1.8025246858596802
iteration 800, loss 1.8598039150238037
iteration 0, loss 1.9136887788772583
iteration 100, loss 1.7928500175476074
iteration 200, loss 1.8568929433822632
iteration 300, loss 1.8584413528442383
iteration 400, loss 1.7396125793457031
iteration 500, loss 1.8801507949829102
iteration 600, loss 1.8617048263549805
iteration 700, loss 1.7877840995788574
iteration 800, loss 1.8318760395050049
iteration 0, loss 1.7901124954223633
iteration 100, loss 1.8495211601257324
iteration 200, loss 1.8299070596694946
iteration 300, loss 1.8994174003601074
iteration 400, loss 1.8160399198532104
iteration 500, loss 1.850669026374817
iteration 600, loss 1.8524352312088013
iteration 700, loss 1.7947423458099365
iteration 800, loss 1.8984307050704956
iteration 0, loss 1.8158631324768066
iteration 100, loss 1.8772393465042114
iteration 200, loss 1.7837268114089966
iteration 300, loss 1.8301922082901
iteration 400, loss 1.8358253240585327
iteration 500, loss 1.8014084100723267
iteration 600, loss 1.8593858480453491
iteration 700, loss 1.797347068786621
iteration 800, loss 1.8150805234909058
iteration 0, loss 1.7900633811950684
iteration 100, loss 1.8130757808685303
iteration 200, loss 1.806316614151001
iteration 300, loss 1.8204636573791504
iteration 400, loss 1.902651309967041
iteration 500, loss 1.7946743965148926
iteration 600, loss 1.785109043121338
iteration 700, loss 1.814640998840332
iteration 800, loss 1.8083213567733765
iteration 0, loss 1.8082274198532104
iteration 100, loss 1.8421975374221802
iteration 200, loss 1.8138961791992188
iteration 300, loss 1.8088868856430054
iteration 400, loss 1.8048033714294434
iteration 500, loss 1.809768795967102
iteration 600, loss 1.8698508739471436
iteration 700, loss 1.8054234981536865
iteration 800, loss 1.9196889400482178
iteration 0, loss 1.76109778881073
iteration 100, loss 1.7722111940383911
iteration 200, loss 1.8359661102294922
iteration 300, loss 1.7738128900527954
iteration 400, loss 1.7504583597183228
iteration 500, loss 1.8161745071411133
iteration 600, loss 2.004014015197754
iteration 700, loss 1.8397223949432373
iteration 800, loss 1.8171021938323975
iteration 0, loss 1.8667349815368652
iteration 100, loss 1.8406891822814941
iteration 200, loss 1.821143627166748
iteration 300, loss 1.8280507326126099
iteration 400, loss 1.9068514108657837
iteration 500, loss 1.9244979619979858
iteration 600, loss 1.777050495147705
iteration 700, loss 1.8503798246383667
iteration 800, loss 1.8666611909866333
iteration 0, loss 1.8764220476150513
iteration 100, loss 1.8471133708953857
iteration 200, loss 1.823264479637146
iteration 300, loss 1.8388913869857788
iteration 400, loss 1.9118866920471191
iteration 500, loss 1.7942346334457397
iteration 600, loss 1.7986936569213867
iteration 700, loss 1.8454153537750244
iteration 800, loss 1.7361382246017456
iteration 0, loss 1.7946245670318604
iteration 100, loss 1.7445374727249146
iteration 200, loss 1.9002934694290161
iteration 300, loss 1.849725604057312
iteration 400, loss 1.83318030834198
iteration 500, loss 1.81690514087677
iteration 600, loss 1.8443405628204346
iteration 700, loss 1.830430507659912
iteration 800, loss 1.7836347818374634
iteration 0, loss 1.776211142539978
iteration 100, loss 1.8290234804153442
iteration 200, loss 1.8186482191085815
iteration 300, loss 1.7809579372406006
iteration 400, loss 1.7933557033538818
iteration 500, loss 1.8065271377563477
iteration 600, loss 1.794265627861023
iteration 700, loss 1.8266512155532837
iteration 800, loss 1.7920275926589966
iteration 0, loss 1.8925597667694092
iteration 100, loss 1.80528724193573
iteration 200, loss 1.8736696243286133
iteration 300, loss 1.8003525733947754
iteration 400, loss 1.8394204378128052
iteration 500, loss 1.8236939907073975
iteration 600, loss 1.7991036176681519
iteration 700, loss 1.78475821018219
iteration 800, loss 1.8467960357666016
iteration 0, loss 1.8200008869171143
iteration 100, loss 1.793365478515625
iteration 200, loss 1.818220615386963
iteration 300, loss 1.849623203277588
iteration 400, loss 1.8479458093643188
iteration 500, loss 1.8581345081329346
iteration 600, loss 1.7612695693969727
iteration 700, loss 1.878568172454834
iteration 800, loss 1.888512134552002
iteration 0, loss 1.814171552658081
iteration 100, loss 1.8087148666381836
iteration 200, loss 1.7929497957229614
iteration 300, loss 1.8804422616958618
iteration 400, loss 1.9373564720153809
iteration 500, loss 1.8808529376983643
iteration 600, loss 1.8587918281555176
iteration 700, loss 1.9412492513656616
iteration 800, loss 1.8019989728927612
iteration 0, loss 1.7918888330459595
iteration 100, loss 1.8263275623321533
iteration 200, loss 1.8918700218200684
iteration 300, loss 1.8813709020614624
iteration 400, loss 1.8410444259643555
iteration 500, loss 1.7831220626831055
iteration 600, loss 1.793278694152832
iteration 700, loss 1.8742154836654663
iteration 800, loss 1.8113903999328613
iteration 0, loss 1.8162733316421509
iteration 100, loss 1.8279461860656738
iteration 200, loss 1.7855198383331299
iteration 300, loss 1.8147389888763428
iteration 400, loss 1.829289197921753
iteration 500, loss 1.8089967966079712
iteration 600, loss 1.8831005096435547
iteration 700, loss 1.7932997941970825
iteration 800, loss 1.8031439781188965
iteration 0, loss 1.7960249185562134
iteration 100, loss 1.7913953065872192
iteration 200, loss 1.7680672407150269
iteration 300, loss 1.7995100021362305
iteration 400, loss 1.7512906789779663
iteration 500, loss 1.7990199327468872
iteration 600, loss 1.8436031341552734
iteration 700, loss 1.8718873262405396
iteration 800, loss 1.8656086921691895
iteration 0, loss 1.8649566173553467
iteration 100, loss 1.7978849411010742
iteration 200, loss 1.7536251544952393
iteration 300, loss 1.8059799671173096
iteration 400, loss 1.8694815635681152
iteration 500, loss 1.880130410194397
iteration 600, loss 1.7795344591140747
iteration 700, loss 1.7796274423599243
iteration 800, loss 1.7982516288757324
iteration 0, loss 1.8588879108428955
iteration 100, loss 1.812809705734253
iteration 200, loss 1.8439193964004517
iteration 300, loss 1.8728824853897095
iteration 400, loss 1.7796299457550049
iteration 500, loss 1.8856428861618042
iteration 600, loss 1.7355787754058838
iteration 700, loss 1.8532527685165405
iteration 800, loss 1.7576996088027954
iteration 0, loss 1.8332594633102417
iteration 100, loss 1.8634611368179321
iteration 200, loss 1.8258007764816284
iteration 300, loss 1.8062093257904053
iteration 400, loss 1.8098020553588867
iteration 500, loss 1.8327841758728027
iteration 600, loss 1.8617345094680786
iteration 700, loss 1.7879211902618408
iteration 800, loss 1.7881405353546143
iteration 0, loss 1.7318817377090454
iteration 100, loss 1.8158081769943237
iteration 200, loss 1.7900766134262085
iteration 300, loss 1.7792128324508667
iteration 400, loss 1.7586746215820312
iteration 500, loss 1.7972139120101929
iteration 600, loss 1.8318926095962524
iteration 700, loss 1.7737071514129639
iteration 800, loss 1.7936145067214966
iteration 0, loss 1.8625465631484985
iteration 100, loss 1.7693026065826416
iteration 200, loss 1.8763502836227417
iteration 300, loss 1.8856332302093506
iteration 400, loss 1.8183339834213257
iteration 500, loss 1.8324310779571533
iteration 600, loss 1.7661209106445312
iteration 700, loss 1.8727996349334717
iteration 800, loss 1.8035941123962402
iteration 0, loss 1.8103148937225342
iteration 100, loss 1.8261653184890747
iteration 200, loss 1.7978538274765015
iteration 300, loss 1.880183458328247
iteration 400, loss 1.7769404649734497
iteration 500, loss 1.781736135482788
iteration 600, loss 1.837851643562317
iteration 700, loss 1.8572616577148438
iteration 800, loss 1.8284207582473755
iteration 0, loss 1.742072582244873
iteration 100, loss 1.806096076965332
iteration 200, loss 1.8551892042160034
iteration 300, loss 1.829228162765503
iteration 400, loss 1.8532969951629639
iteration 500, loss 1.7451565265655518
iteration 600, loss 1.814420223236084
iteration 700, loss 1.8459104299545288
iteration 800, loss 1.8598370552062988
iteration 0, loss 1.7571110725402832
iteration 100, loss 1.8002463579177856
iteration 200, loss 1.8044164180755615
iteration 300, loss 1.8138091564178467
iteration 400, loss 1.8851304054260254
iteration 500, loss 1.8047950267791748
iteration 600, loss 1.7536982297897339
iteration 700, loss 1.8402185440063477
iteration 800, loss 1.7826733589172363
iteration 0, loss 1.8007065057754517
iteration 100, loss 1.810670256614685
iteration 200, loss 1.7707676887512207
iteration 300, loss 1.8507657051086426
iteration 400, loss 1.7944940328598022
iteration 500, loss 1.8123903274536133
iteration 600, loss 1.8334745168685913
iteration 700, loss 1.8446297645568848
iteration 800, loss 1.8266503810882568
iteration 0, loss 1.7684639692306519
iteration 100, loss 1.7965985536575317
iteration 200, loss 1.8594346046447754
iteration 300, loss 1.8103801012039185
iteration 400, loss 1.8162133693695068
iteration 500, loss 1.831472396850586
iteration 600, loss 1.7648088932037354
iteration 700, loss 1.7330231666564941
iteration 800, loss 1.8372622728347778
iteration 0, loss 1.8131356239318848
iteration 100, loss 1.7168831825256348
iteration 200, loss 1.7660105228424072
iteration 300, loss 1.8907485008239746
iteration 400, loss 1.744879126548767
iteration 500, loss 1.8258459568023682
iteration 600, loss 1.7585703134536743
iteration 700, loss 1.8888479471206665
iteration 800, loss 1.8454049825668335
iteration 0, loss 1.8349640369415283
iteration 100, loss 1.8109149932861328
iteration 200, loss 1.8465015888214111
iteration 300, loss 1.7947648763656616
iteration 400, loss 1.846252679824829
iteration 500, loss 1.7288874387741089
iteration 600, loss 1.8621783256530762
iteration 700, loss 1.8563919067382812
iteration 800, loss 1.8407410383224487
iteration 0, loss 1.8479758501052856
iteration 100, loss 1.8936792612075806
iteration 200, loss 1.88326096534729
iteration 300, loss 1.8135648965835571
iteration 400, loss 1.8864552974700928
iteration 500, loss 1.786769151687622
iteration 600, loss 1.7460497617721558
iteration 700, loss 1.8334532976150513
iteration 800, loss 1.8521146774291992
iteration 0, loss 1.873363971710205
iteration 100, loss 1.841829776763916
iteration 200, loss 1.8286911249160767
iteration 300, loss 1.8229988813400269
iteration 400, loss 1.879326343536377
iteration 500, loss 1.855167031288147
iteration 600, loss 1.7961323261260986
iteration 700, loss 1.8504877090454102
iteration 800, loss 1.788792610168457
iteration 0, loss 1.8010507822036743
iteration 100, loss 1.9238276481628418
iteration 200, loss 1.8795926570892334
iteration 300, loss 1.7616091966629028
iteration 400, loss 1.9288520812988281
iteration 500, loss 1.793939232826233
iteration 600, loss 1.8432022333145142
iteration 700, loss 1.8291302919387817
iteration 800, loss 1.8755333423614502
iteration 0, loss 1.7205255031585693
iteration 100, loss 1.753511905670166
iteration 200, loss 1.7727057933807373
iteration 300, loss 1.780772089958191
iteration 400, loss 1.817817211151123
iteration 500, loss 1.767022967338562
iteration 600, loss 1.8407942056655884
iteration 700, loss 1.853193998336792
iteration 800, loss 1.8601725101470947
iteration 0, loss 1.8240208625793457
iteration 100, loss 1.8582943677902222
iteration 200, loss 1.7833623886108398
iteration 300, loss 1.7962005138397217
iteration 400, loss 1.8693863153457642
iteration 500, loss 1.8584598302841187
iteration 600, loss 1.786653757095337
iteration 700, loss 1.8856837749481201
iteration 800, loss 1.8827370405197144
iteration 0, loss 1.822103500366211
iteration 100, loss 1.8439185619354248
iteration 200, loss 1.7914179563522339
iteration 300, loss 1.7492743730545044
iteration 400, loss 1.8265150785446167
iteration 500, loss 1.8493914604187012
iteration 600, loss 1.831043004989624
iteration 700, loss 1.8288650512695312
iteration 800, loss 1.8362305164337158
iteration 0, loss 1.833885908126831
iteration 100, loss 1.8983595371246338
iteration 200, loss 1.830386996269226
iteration 300, loss 1.8326255083084106
iteration 400, loss 1.836458444595337
iteration 500, loss 1.8172792196273804
iteration 600, loss 1.7521957159042358
iteration 700, loss 1.7752338647842407
iteration 800, loss 1.8633047342300415
iteration 0, loss 1.8838398456573486
iteration 100, loss 1.7604619264602661
iteration 200, loss 1.7781039476394653
iteration 300, loss 1.8200936317443848
iteration 400, loss 1.9386601448059082
iteration 500, loss 1.8277353048324585
iteration 600, loss 1.778171181678772
iteration 700, loss 1.9233825206756592
iteration 800, loss 1.7493764162063599
iteration 0, loss 1.7632300853729248
iteration 100, loss 1.7273818254470825
iteration 200, loss 1.8041549921035767
iteration 300, loss 1.7817832231521606
iteration 400, loss 1.786421537399292
iteration 500, loss 1.797561526298523
iteration 600, loss 1.7545807361602783
iteration 700, loss 1.8234678506851196
iteration 800, loss 1.8610420227050781
iteration 0, loss 1.813905954360962
iteration 100, loss 1.8244245052337646
iteration 200, loss 1.8026118278503418
iteration 300, loss 1.8308117389678955
iteration 400, loss 1.8020075559616089
iteration 500, loss 1.7700856924057007
iteration 600, loss 1.7630094289779663
iteration 700, loss 1.76629638671875
iteration 800, loss 1.8194996118545532
iteration 0, loss 1.861311435699463
iteration 100, loss 1.7639517784118652
iteration 200, loss 1.741668701171875
iteration 300, loss 1.7973852157592773
iteration 400, loss 1.8032060861587524
iteration 500, loss 1.79123055934906
iteration 600, loss 1.78401780128479
iteration 700, loss 1.809035062789917
iteration 800, loss 1.7456364631652832
iteration 0, loss 1.8278721570968628
iteration 100, loss 1.8125981092453003
iteration 200, loss 1.8438385725021362
iteration 300, loss 1.7867615222930908
iteration 400, loss 1.7114503383636475
iteration 500, loss 1.8349820375442505
iteration 600, loss 1.7766342163085938
iteration 700, loss 1.8219702243804932
iteration 800, loss 1.857603907585144
iteration 0, loss 1.896865963935852
iteration 100, loss 1.7856216430664062
iteration 200, loss 1.818393588066101
iteration 300, loss 1.7833259105682373
iteration 400, loss 1.8042069673538208
iteration 500, loss 1.7802824974060059
iteration 600, loss 1.8403425216674805
iteration 700, loss 1.8810195922851562
iteration 800, loss 1.8589119911193848
iteration 0, loss 1.8883521556854248
iteration 100, loss 1.8794618844985962
iteration 200, loss 1.7987887859344482
iteration 300, loss 1.8344197273254395
iteration 400, loss 1.8318707942962646
iteration 500, loss 1.8504327535629272
iteration 600, loss 1.7610337734222412
iteration 700, loss 1.8071463108062744
iteration 800, loss 1.8061070442199707
iteration 0, loss 1.8862091302871704
iteration 100, loss 1.8378605842590332
iteration 200, loss 1.903945803642273
iteration 300, loss 1.8042386770248413
iteration 400, loss 1.8593932390213013
iteration 500, loss 1.8008310794830322
iteration 600, loss 1.8110876083374023
iteration 700, loss 1.7853569984436035
iteration 800, loss 1.822464108467102
iteration 0, loss 1.8653497695922852
iteration 100, loss 1.8305269479751587
iteration 200, loss 1.8254585266113281
iteration 300, loss 1.8876068592071533
iteration 400, loss 1.835369348526001
iteration 500, loss 1.815457820892334
iteration 600, loss 1.8223401308059692
iteration 700, loss 1.7735575437545776
iteration 800, loss 1.8198320865631104
iteration 0, loss 1.8036080598831177
iteration 100, loss 1.8587945699691772
iteration 200, loss 1.8400421142578125
iteration 300, loss 1.860469102859497
iteration 400, loss 1.8943500518798828
iteration 500, loss 1.932706594467163
iteration 600, loss 1.7922836542129517
iteration 700, loss 1.8727213144302368
iteration 800, loss 1.7878413200378418
iteration 0, loss 1.8251594305038452
iteration 100, loss 1.8618378639221191
iteration 200, loss 1.8042535781860352
iteration 300, loss 1.8682773113250732
iteration 400, loss 1.764082908630371
iteration 500, loss 1.835973858833313
iteration 600, loss 1.7991877794265747
iteration 700, loss 1.7928693294525146
iteration 800, loss 1.807026982307434
iteration 0, loss 1.8132482767105103
iteration 100, loss 1.7881461381912231
iteration 200, loss 1.8276423215866089
iteration 300, loss 1.835516333580017
iteration 400, loss 1.7586543560028076
iteration 500, loss 1.8355660438537598
iteration 600, loss 1.8048375844955444
iteration 700, loss 1.841672420501709
iteration 800, loss 1.8577998876571655
iteration 0, loss 1.7933605909347534
iteration 100, loss 1.8497295379638672
iteration 200, loss 1.7728503942489624
iteration 300, loss 1.8341445922851562
iteration 400, loss 1.8477504253387451
iteration 500, loss 1.8308464288711548
iteration 600, loss 1.8188591003417969
iteration 700, loss 1.842166543006897
iteration 800, loss 1.7968062162399292
iteration 0, loss 1.860896110534668
iteration 100, loss 1.900344967842102
iteration 200, loss 1.8214212656021118
iteration 300, loss 1.74440598487854
iteration 400, loss 1.8385252952575684
iteration 500, loss 1.830201506614685
iteration 600, loss 1.9105231761932373
iteration 700, loss 1.8476494550704956
iteration 800, loss 1.852670431137085
fold 1 accuracy: 0.49407142857142855
iteration 0, loss 1.768315315246582
iteration 100, loss 1.9004032611846924
iteration 200, loss 1.8608142137527466
iteration 300, loss 1.8128308057785034
iteration 400, loss 1.8328657150268555
iteration 500, loss 1.8624030351638794
iteration 600, loss 1.7998318672180176
iteration 700, loss 1.8077472448349
iteration 800, loss 1.8450560569763184
iteration 0, loss 1.8197405338287354
iteration 100, loss 1.8363851308822632
iteration 200, loss 1.872140645980835
iteration 300, loss 1.7846715450286865
iteration 400, loss 1.7916522026062012
iteration 500, loss 1.8066848516464233
iteration 600, loss 1.717036247253418
iteration 700, loss 1.8231443166732788
iteration 800, loss 1.8040143251419067
iteration 0, loss 1.8054872751235962
iteration 100, loss 1.7477132081985474
iteration 200, loss 1.895331621170044
iteration 300, loss 1.7728008031845093
iteration 400, loss 1.7996913194656372
iteration 500, loss 1.7844104766845703
iteration 600, loss 1.808078408241272
iteration 700, loss 1.8377137184143066
iteration 800, loss 1.8341962099075317
iteration 0, loss 1.7509434223175049
iteration 100, loss 1.8115872144699097
iteration 200, loss 1.8391953706741333
iteration 300, loss 1.8327211141586304
iteration 400, loss 1.8059167861938477
iteration 500, loss 1.7337381839752197
iteration 600, loss 1.8374613523483276
iteration 700, loss 1.7897584438323975
iteration 800, loss 1.8360426425933838
iteration 0, loss 1.804960012435913
iteration 100, loss 1.7245819568634033
iteration 200, loss 1.8074167966842651
iteration 300, loss 1.805370569229126
iteration 400, loss 1.7459614276885986
iteration 500, loss 1.8139266967773438
iteration 600, loss 1.8239213228225708
iteration 700, loss 1.8372446298599243
iteration 800, loss 1.9372494220733643
iteration 0, loss 1.8543366193771362
iteration 100, loss 1.8633841276168823
iteration 200, loss 1.7540878057479858
iteration 300, loss 1.8649811744689941
iteration 400, loss 1.8356183767318726
iteration 500, loss 1.7346001863479614
iteration 600, loss 1.8021326065063477
iteration 700, loss 1.803049921989441
iteration 800, loss 1.839202642440796
iteration 0, loss 1.8442541360855103
iteration 100, loss 1.795844316482544
iteration 200, loss 1.8178547620773315
iteration 300, loss 1.8688764572143555
iteration 400, loss 1.8186144828796387
iteration 500, loss 1.7674877643585205
iteration 600, loss 1.8841192722320557
iteration 700, loss 1.744127869606018
iteration 800, loss 1.8014976978302002
iteration 0, loss 1.7775381803512573
iteration 100, loss 1.792339563369751
iteration 200, loss 1.891073226928711
iteration 300, loss 1.8002229928970337
iteration 400, loss 1.8916102647781372
iteration 500, loss 1.8052892684936523
iteration 600, loss 1.818779706954956
iteration 700, loss 1.8597421646118164
iteration 800, loss 1.7820755243301392
iteration 0, loss 1.7587817907333374
iteration 100, loss 1.8081244230270386
iteration 200, loss 1.803902506828308
iteration 300, loss 1.9015287160873413
iteration 400, loss 1.863403558731079
iteration 500, loss 1.796889305114746
iteration 600, loss 1.808276891708374
iteration 700, loss 1.8262218236923218
iteration 800, loss 1.791805386543274
iteration 0, loss 1.7873133420944214
iteration 100, loss 1.8321455717086792
iteration 200, loss 1.8129302263259888
iteration 300, loss 1.8761552572250366
iteration 400, loss 1.847127079963684
iteration 500, loss 1.7806932926177979
iteration 600, loss 1.830462098121643
iteration 700, loss 1.818692684173584
iteration 800, loss 1.8567959070205688
iteration 0, loss 1.887967824935913
iteration 100, loss 1.8358359336853027
iteration 200, loss 1.7887214422225952
iteration 300, loss 1.8124099969863892
iteration 400, loss 1.861923098564148
iteration 500, loss 1.827034831047058
iteration 600, loss 1.8500391244888306
iteration 700, loss 1.752691388130188
iteration 800, loss 1.7989885807037354
iteration 0, loss 1.793766736984253
iteration 100, loss 1.7729547023773193
iteration 200, loss 1.8854539394378662
iteration 300, loss 1.8838306665420532
iteration 400, loss 1.7331609725952148
iteration 500, loss 1.7904390096664429
iteration 600, loss 1.9074060916900635
iteration 700, loss 1.8129167556762695
iteration 800, loss 1.820908784866333
iteration 0, loss 1.7888038158416748
iteration 100, loss 1.8622769117355347
iteration 200, loss 1.8237735033035278
iteration 300, loss 1.8195148706436157
iteration 400, loss 1.7864199876785278
iteration 500, loss 1.7690491676330566
iteration 600, loss 1.772834062576294
iteration 700, loss 1.8539977073669434
iteration 800, loss 1.8544166088104248
iteration 0, loss 1.7754164934158325
iteration 100, loss 1.922999620437622
iteration 200, loss 1.8258241415023804
iteration 300, loss 1.7959096431732178
iteration 400, loss 1.8170263767242432
iteration 500, loss 1.8640700578689575
iteration 600, loss 1.8586688041687012
iteration 700, loss 1.8341543674468994
iteration 800, loss 1.8688666820526123
iteration 0, loss 1.8520252704620361
iteration 100, loss 1.8292384147644043
iteration 200, loss 1.806736946105957
iteration 300, loss 1.8224804401397705
iteration 400, loss 1.830945372581482
iteration 500, loss 1.7800840139389038
iteration 600, loss 1.8922450542449951
iteration 700, loss 1.8316700458526611
iteration 800, loss 1.8162895441055298
iteration 0, loss 1.8260802030563354
iteration 100, loss 1.857485294342041
iteration 200, loss 1.776788353919983
iteration 300, loss 1.8447473049163818
iteration 400, loss 1.8214757442474365
iteration 500, loss 1.7746927738189697
iteration 600, loss 1.8505970239639282
iteration 700, loss 1.8393614292144775
iteration 800, loss 1.7523679733276367
iteration 0, loss 1.7810430526733398
iteration 100, loss 1.830112338066101
iteration 200, loss 1.7964228391647339
iteration 300, loss 1.7622230052947998
iteration 400, loss 1.7954057455062866
iteration 500, loss 1.794481635093689
iteration 600, loss 1.9134304523468018
iteration 700, loss 1.804808259010315
iteration 800, loss 1.8297542333602905
iteration 0, loss 1.8227499723434448
iteration 100, loss 1.7864776849746704
iteration 200, loss 1.7544509172439575
iteration 300, loss 1.8329505920410156
iteration 400, loss 1.7426023483276367
iteration 500, loss 1.7829482555389404
iteration 600, loss 1.8859344720840454
iteration 700, loss 1.718272089958191
iteration 800, loss 1.7335758209228516
iteration 0, loss 1.7835296392440796
iteration 100, loss 1.8042224645614624
iteration 200, loss 1.8200173377990723
iteration 300, loss 1.874389886856079
iteration 400, loss 1.784687876701355
iteration 500, loss 1.8609328269958496
iteration 600, loss 1.8292653560638428
iteration 700, loss 1.8294252157211304
iteration 800, loss 1.8100000619888306
iteration 0, loss 1.7613624334335327
iteration 100, loss 1.9021120071411133
iteration 200, loss 1.8199104070663452
iteration 300, loss 1.827219009399414
iteration 400, loss 1.7949050664901733
iteration 500, loss 1.7884340286254883
iteration 600, loss 1.7219276428222656
iteration 700, loss 1.8194899559020996
iteration 800, loss 1.8243352174758911
iteration 0, loss 1.764221429824829
iteration 100, loss 1.8247458934783936
iteration 200, loss 1.7540831565856934
iteration 300, loss 1.7401478290557861
iteration 400, loss 1.9487826824188232
iteration 500, loss 1.7969245910644531
iteration 600, loss 1.80866277217865
iteration 700, loss 1.868780255317688
iteration 800, loss 1.8740793466567993
iteration 0, loss 1.7626829147338867
iteration 100, loss 1.8217101097106934
iteration 200, loss 1.8285502195358276
iteration 300, loss 1.8154246807098389
iteration 400, loss 1.8306035995483398
iteration 500, loss 1.83187997341156
iteration 600, loss 1.779370903968811
iteration 700, loss 1.7967689037322998
iteration 800, loss 1.851771593093872
iteration 0, loss 1.8461153507232666
iteration 100, loss 1.8240376710891724
iteration 200, loss 1.8763070106506348
iteration 300, loss 1.8196542263031006
iteration 400, loss 1.876531720161438
iteration 500, loss 1.815665364265442
iteration 600, loss 1.8248745203018188
iteration 700, loss 1.8366129398345947
iteration 800, loss 1.8098406791687012
iteration 0, loss 1.7852565050125122
iteration 100, loss 1.9514617919921875
iteration 200, loss 1.8752657175064087
iteration 300, loss 1.8000667095184326
iteration 400, loss 1.8062149286270142
iteration 500, loss 1.8682531118392944
iteration 600, loss 1.80179762840271
iteration 700, loss 1.8203797340393066
iteration 800, loss 1.7886054515838623
iteration 0, loss 1.8337059020996094
iteration 100, loss 1.784735918045044
iteration 200, loss 1.8472115993499756
iteration 300, loss 1.8414591550827026
iteration 400, loss 1.8349008560180664
iteration 500, loss 1.773886799812317
iteration 600, loss 1.8457210063934326
iteration 700, loss 1.9214879274368286
iteration 800, loss 1.8875799179077148
iteration 0, loss 1.8286519050598145
iteration 100, loss 1.781343936920166
iteration 200, loss 1.73274564743042
iteration 300, loss 1.8132811784744263
iteration 400, loss 1.7916513681411743
iteration 500, loss 1.814631700515747
iteration 600, loss 1.7608859539031982
iteration 700, loss 1.785895824432373
iteration 800, loss 1.751448392868042
iteration 0, loss 1.7828456163406372
iteration 100, loss 1.941799521446228
iteration 200, loss 1.8795441389083862
iteration 300, loss 1.7677701711654663
iteration 400, loss 1.8550688028335571
iteration 500, loss 1.777964472770691
iteration 600, loss 1.8438851833343506
iteration 700, loss 1.7937312126159668
iteration 800, loss 1.8154584169387817
iteration 0, loss 1.7183256149291992
iteration 100, loss 1.7733724117279053
iteration 200, loss 1.8437731266021729
iteration 300, loss 1.8612680435180664
iteration 400, loss 1.90415358543396
iteration 500, loss 1.8447030782699585
iteration 600, loss 1.7864668369293213
iteration 700, loss 1.7962263822555542
iteration 800, loss 1.7790610790252686
iteration 0, loss 1.820863127708435
iteration 100, loss 1.8477604389190674
iteration 200, loss 1.812185287475586
iteration 300, loss 1.7959480285644531
iteration 400, loss 1.8025553226470947
iteration 500, loss 1.831392765045166
iteration 600, loss 1.863919734954834
iteration 700, loss 1.794026494026184
iteration 800, loss 1.8491839170455933
iteration 0, loss 1.8099437952041626
iteration 100, loss 1.8273135423660278
iteration 200, loss 1.8317065238952637
iteration 300, loss 1.8276679515838623
iteration 400, loss 1.7784689664840698
iteration 500, loss 1.763384461402893
iteration 600, loss 1.7906748056411743
iteration 700, loss 1.806646704673767
iteration 800, loss 1.8714619874954224
iteration 0, loss 1.8643653392791748
iteration 100, loss 1.7921593189239502
iteration 200, loss 1.8034322261810303
iteration 300, loss 1.850831151008606
iteration 400, loss 1.80257248878479
iteration 500, loss 1.8352469205856323
iteration 600, loss 1.7398266792297363
iteration 700, loss 1.7517848014831543
iteration 800, loss 1.8612803220748901
iteration 0, loss 1.9127197265625
iteration 100, loss 1.8025012016296387
iteration 200, loss 1.8418630361557007
iteration 300, loss 1.8196951150894165
iteration 400, loss 1.8498201370239258
iteration 500, loss 1.8469611406326294
iteration 600, loss 1.8012559413909912
iteration 700, loss 1.8275182247161865
iteration 800, loss 1.8268591165542603
iteration 0, loss 1.7868034839630127
iteration 100, loss 1.8537741899490356
iteration 200, loss 1.8953660726547241
iteration 300, loss 1.8968838453292847
iteration 400, loss 1.7951281070709229
iteration 500, loss 1.8384133577346802
iteration 600, loss 1.7690736055374146
iteration 700, loss 1.8378527164459229
iteration 800, loss 1.8158013820648193
iteration 0, loss 1.834864616394043
iteration 100, loss 1.82953941822052
iteration 200, loss 1.830037236213684
iteration 300, loss 1.8150274753570557
iteration 400, loss 1.8187886476516724
iteration 500, loss 1.8262107372283936
iteration 600, loss 1.788794755935669
iteration 700, loss 1.8278074264526367
iteration 800, loss 1.8768996000289917
iteration 0, loss 1.7752406597137451
iteration 100, loss 1.7937060594558716
iteration 200, loss 1.7662171125411987
iteration 300, loss 1.7835561037063599
iteration 400, loss 1.8999717235565186
iteration 500, loss 1.775182843208313
iteration 600, loss 1.8693901300430298
iteration 700, loss 1.8830879926681519
iteration 800, loss 1.8708992004394531
iteration 0, loss 1.7980899810791016
iteration 100, loss 1.8626258373260498
iteration 200, loss 1.8734866380691528
iteration 300, loss 1.8440074920654297
iteration 400, loss 1.844014286994934
iteration 500, loss 1.8389036655426025
iteration 600, loss 1.8807053565979004
iteration 700, loss 1.7555028200149536
iteration 800, loss 1.8141956329345703
iteration 0, loss 1.8250106573104858
iteration 100, loss 1.8352941274642944
iteration 200, loss 1.8253310918807983
iteration 300, loss 1.8646879196166992
iteration 400, loss 1.9382011890411377
iteration 500, loss 1.7419250011444092
iteration 600, loss 1.809754729270935
iteration 700, loss 1.8307877779006958
iteration 800, loss 1.689273476600647
iteration 0, loss 1.7028722763061523
iteration 100, loss 1.7246923446655273
iteration 200, loss 1.8196682929992676
iteration 300, loss 1.9135210514068604
iteration 400, loss 1.8108752965927124
iteration 500, loss 1.7808310985565186
iteration 600, loss 1.87276029586792
iteration 700, loss 1.7405916452407837
iteration 800, loss 1.8123149871826172
iteration 0, loss 1.8061373233795166
iteration 100, loss 1.8391846418380737
iteration 200, loss 1.822504997253418
iteration 300, loss 1.7857152223587036
iteration 400, loss 1.8259798288345337
iteration 500, loss 1.8510043621063232
iteration 600, loss 1.815322756767273
iteration 700, loss 1.7735028266906738
iteration 800, loss 1.833975911140442
iteration 0, loss 1.8116554021835327
iteration 100, loss 1.8340753316879272
iteration 200, loss 1.7032012939453125
iteration 300, loss 1.7987573146820068
iteration 400, loss 1.7945677042007446
iteration 500, loss 1.8626861572265625
iteration 600, loss 1.8181096315383911
iteration 700, loss 1.8934835195541382
iteration 800, loss 1.7520660161972046
iteration 0, loss 1.8063015937805176
iteration 100, loss 1.81354820728302
iteration 200, loss 1.737827181816101
iteration 300, loss 1.839719533920288
iteration 400, loss 1.865057110786438
iteration 500, loss 1.7364815473556519
iteration 600, loss 1.8391520977020264
iteration 700, loss 1.8816165924072266
iteration 800, loss 1.8053295612335205
iteration 0, loss 1.873415231704712
iteration 100, loss 1.7731735706329346
iteration 200, loss 1.7744768857955933
iteration 300, loss 1.7876373529434204
iteration 400, loss 1.7495037317276
iteration 500, loss 1.821431040763855
iteration 600, loss 1.7828162908554077
iteration 700, loss 1.805685043334961
iteration 800, loss 1.7965847253799438
iteration 0, loss 1.8134605884552002
iteration 100, loss 1.7747248411178589
iteration 200, loss 1.8357876539230347
iteration 300, loss 1.7979354858398438
iteration 400, loss 1.792555332183838
iteration 500, loss 1.7379354238510132
iteration 600, loss 1.8528152704238892
iteration 700, loss 1.8866150379180908
iteration 800, loss 1.8664610385894775
iteration 0, loss 1.769187331199646
iteration 100, loss 1.8329055309295654
iteration 200, loss 1.8557038307189941
iteration 300, loss 1.8029389381408691
iteration 400, loss 1.8261688947677612
iteration 500, loss 1.7970774173736572
iteration 600, loss 1.7442965507507324
iteration 700, loss 1.8130784034729004
iteration 800, loss 1.7710180282592773
iteration 0, loss 1.8205540180206299
iteration 100, loss 1.8118391036987305
iteration 200, loss 1.763153314590454
iteration 300, loss 1.906843662261963
iteration 400, loss 1.822935700416565
iteration 500, loss 1.7685866355895996
iteration 600, loss 1.8319017887115479
iteration 700, loss 1.7731187343597412
iteration 800, loss 1.762764573097229
iteration 0, loss 1.9784822463989258
iteration 100, loss 1.79787278175354
iteration 200, loss 1.811065912246704
iteration 300, loss 1.7964524030685425
iteration 400, loss 1.8402291536331177
iteration 500, loss 1.8453805446624756
iteration 600, loss 1.850046157836914
iteration 700, loss 1.864835500717163
iteration 800, loss 1.8097739219665527
iteration 0, loss 1.8663302659988403
iteration 100, loss 1.8257224559783936
iteration 200, loss 1.8199795484542847
iteration 300, loss 1.7742727994918823
iteration 400, loss 1.7692960500717163
iteration 500, loss 1.8402076959609985
iteration 600, loss 1.7556995153427124
iteration 700, loss 1.8053646087646484
iteration 800, loss 1.8561151027679443
iteration 0, loss 1.778914451599121
iteration 100, loss 1.7618324756622314
iteration 200, loss 1.7721312046051025
iteration 300, loss 1.7924574613571167
iteration 400, loss 1.8272547721862793
iteration 500, loss 1.8194433450698853
iteration 600, loss 1.7947971820831299
iteration 700, loss 1.9070457220077515
iteration 800, loss 1.740091323852539
iteration 0, loss 1.8579719066619873
iteration 100, loss 1.8061407804489136
iteration 200, loss 1.8004029989242554
iteration 300, loss 1.8079078197479248
iteration 400, loss 1.8461014032363892
iteration 500, loss 1.7517611980438232
iteration 600, loss 1.7613409757614136
iteration 700, loss 1.8293710947036743
iteration 800, loss 1.8218867778778076
iteration 0, loss 1.8021639585494995
iteration 100, loss 1.924212098121643
iteration 200, loss 1.7712961435317993
iteration 300, loss 1.8481909036636353
iteration 400, loss 1.8019416332244873
iteration 500, loss 1.7874590158462524
iteration 600, loss 1.7779746055603027
iteration 700, loss 1.8929131031036377
iteration 800, loss 1.7837995290756226
fold 2 accuracy: 0.49714285714285716
iteration 0, loss 1.8038336038589478
iteration 100, loss 1.815176010131836
iteration 200, loss 1.8220000267028809
iteration 300, loss 1.7189096212387085
iteration 400, loss 1.768680214881897
iteration 500, loss 1.7648184299468994
iteration 600, loss 1.8420847654342651
iteration 700, loss 1.7496120929718018
iteration 800, loss 1.8822067975997925
iteration 0, loss 1.8503471612930298
iteration 100, loss 1.8224409818649292
iteration 200, loss 1.8005496263504028
iteration 300, loss 1.793814778327942
iteration 400, loss 1.8628816604614258
iteration 500, loss 1.744794487953186
iteration 600, loss 1.7726662158966064
iteration 700, loss 1.841552972793579
iteration 800, loss 1.7738075256347656
iteration 0, loss 1.766654372215271
iteration 100, loss 1.885310411453247
iteration 200, loss 1.7833187580108643
iteration 300, loss 1.7770631313323975
iteration 400, loss 1.8162789344787598
iteration 500, loss 1.7701096534729004
iteration 600, loss 1.7941310405731201
iteration 700, loss 1.7853742837905884
iteration 800, loss 1.776576280593872
iteration 0, loss 1.8143484592437744
iteration 100, loss 1.8195654153823853
iteration 200, loss 1.74362051486969
iteration 300, loss 1.7730295658111572
iteration 400, loss 1.8480424880981445
iteration 500, loss 1.769557237625122
iteration 600, loss 1.7784700393676758
iteration 700, loss 1.7899560928344727
iteration 800, loss 1.884084939956665
iteration 0, loss 1.9117414951324463
iteration 100, loss 1.8167760372161865
iteration 200, loss 1.7973887920379639
iteration 300, loss 1.8534225225448608
iteration 400, loss 1.8466458320617676
iteration 500, loss 1.769897699356079
iteration 600, loss 1.8399523496627808
iteration 700, loss 1.8501698970794678
iteration 800, loss 1.7562971115112305
iteration 0, loss 1.8194698095321655
iteration 100, loss 1.8609760999679565
iteration 200, loss 1.8042834997177124
iteration 300, loss 1.7720131874084473
iteration 400, loss 1.7941840887069702
iteration 500, loss 1.9124445915222168
iteration 600, loss 1.8164185285568237
iteration 700, loss 1.9073472023010254
iteration 800, loss 1.7596654891967773
iteration 0, loss 1.78850257396698
iteration 100, loss 1.763171911239624
iteration 200, loss 1.754596471786499
iteration 300, loss 1.8401830196380615
iteration 400, loss 1.7930828332901
iteration 500, loss 1.8448231220245361
iteration 600, loss 1.7700419425964355
iteration 700, loss 1.8463670015335083
iteration 800, loss 1.8262948989868164
iteration 0, loss 1.7979040145874023
iteration 100, loss 1.7915632724761963
iteration 200, loss 1.8583801984786987
iteration 300, loss 1.808740496635437
iteration 400, loss 1.7849589586257935
iteration 500, loss 1.8060829639434814
iteration 600, loss 1.7703289985656738
iteration 700, loss 1.8815821409225464
iteration 800, loss 1.7437318563461304
iteration 0, loss 1.7610068321228027
iteration 100, loss 1.8169001340866089
iteration 200, loss 1.8481792211532593
iteration 300, loss 1.8245439529418945
iteration 400, loss 1.7967009544372559
iteration 500, loss 1.680649757385254
iteration 600, loss 1.8717302083969116
iteration 700, loss 1.9100852012634277
iteration 800, loss 1.849648118019104
iteration 0, loss 1.8632930517196655
iteration 100, loss 1.7989939451217651
iteration 200, loss 1.7568142414093018
iteration 300, loss 1.805626392364502
iteration 400, loss 1.8290313482284546
iteration 500, loss 1.8594939708709717
iteration 600, loss 1.8018287420272827
iteration 700, loss 1.8083852529525757
iteration 800, loss 1.8618000745773315
iteration 0, loss 1.7140885591506958
iteration 100, loss 1.7740775346755981
iteration 200, loss 1.8510404825210571
iteration 300, loss 1.8470988273620605
iteration 400, loss 1.786826491355896
iteration 500, loss 1.8084393739700317
iteration 600, loss 1.844967246055603
iteration 700, loss 1.7818502187728882
iteration 800, loss 1.850428581237793
iteration 0, loss 1.8371480703353882
iteration 100, loss 1.7934350967407227
iteration 200, loss 1.7888106107711792
iteration 300, loss 1.75107741355896
iteration 400, loss 1.783782958984375
iteration 500, loss 1.8159410953521729
iteration 600, loss 1.8197383880615234
iteration 700, loss 1.8644720315933228
iteration 800, loss 1.8054161071777344
iteration 0, loss 1.7703169584274292
iteration 100, loss 1.7870047092437744
iteration 200, loss 1.8209055662155151
iteration 300, loss 1.8312597274780273
iteration 400, loss 1.8760420083999634
iteration 500, loss 1.8339978456497192
iteration 600, loss 1.7982394695281982
iteration 700, loss 1.8391180038452148
iteration 800, loss 1.8710871934890747
iteration 0, loss 1.8140844106674194
iteration 100, loss 1.827487587928772
iteration 200, loss 1.8616218566894531
iteration 300, loss 1.7936078310012817
iteration 400, loss 1.8558286428451538
iteration 500, loss 1.8257570266723633
iteration 600, loss 1.8821078538894653
iteration 700, loss 1.766214370727539
iteration 800, loss 1.8280465602874756
iteration 0, loss 1.780394196510315
iteration 100, loss 1.841787576675415
iteration 200, loss 1.8409854173660278
iteration 300, loss 1.8673747777938843
iteration 400, loss 1.844165325164795
iteration 500, loss 1.7132922410964966
iteration 600, loss 1.8044629096984863
iteration 700, loss 1.8491671085357666
iteration 800, loss 1.7112290859222412
iteration 0, loss 1.8284677267074585
iteration 100, loss 1.9319111108779907
iteration 200, loss 1.8798587322235107
iteration 300, loss 1.8347816467285156
iteration 400, loss 1.8163923025131226
iteration 500, loss 1.772438883781433
iteration 600, loss 1.8342024087905884
iteration 700, loss 1.8206055164337158
iteration 800, loss 1.8303977251052856
iteration 0, loss 1.8153276443481445
iteration 100, loss 1.7820261716842651
iteration 200, loss 1.817948341369629
iteration 300, loss 1.8326623439788818
iteration 400, loss 1.7338489294052124
iteration 500, loss 1.8247355222702026
iteration 600, loss 1.8468811511993408
iteration 700, loss 1.8525642156600952
iteration 800, loss 1.75918710231781
iteration 0, loss 1.7033522129058838
iteration 100, loss 1.7704716920852661
iteration 200, loss 1.7767000198364258
iteration 300, loss 1.7623317241668701
iteration 400, loss 1.7954442501068115
iteration 500, loss 1.7803155183792114
iteration 600, loss 1.7802269458770752
iteration 700, loss 1.7824374437332153
iteration 800, loss 1.7205814123153687
iteration 0, loss 1.8301887512207031
iteration 100, loss 1.8201656341552734
iteration 200, loss 1.8615964651107788
iteration 300, loss 1.772976279258728
iteration 400, loss 1.808921217918396
iteration 500, loss 1.8194621801376343
iteration 600, loss 1.88155198097229
iteration 700, loss 1.8028349876403809
iteration 800, loss 1.7269405126571655
iteration 0, loss 1.8189433813095093
iteration 100, loss 1.7863740921020508
iteration 200, loss 1.764087200164795
iteration 300, loss 1.800162434577942
iteration 400, loss 1.8618242740631104
iteration 500, loss 1.8559800386428833
iteration 600, loss 1.8189347982406616
iteration 700, loss 1.814069151878357
iteration 800, loss 1.8307030200958252
iteration 0, loss 1.8298730850219727
iteration 100, loss 1.8105907440185547
iteration 200, loss 1.8780384063720703
iteration 300, loss 1.8231958150863647
iteration 400, loss 1.7753446102142334
iteration 500, loss 1.8173106908798218
iteration 600, loss 1.887802004814148
iteration 700, loss 1.8570353984832764
iteration 800, loss 1.7639025449752808
iteration 0, loss 1.781997561454773
iteration 100, loss 1.7672934532165527
iteration 200, loss 1.7431532144546509
iteration 300, loss 1.7906410694122314
iteration 400, loss 1.8226369619369507
iteration 500, loss 1.838460922241211
iteration 600, loss 1.8769985437393188
iteration 700, loss 1.7594945430755615
iteration 800, loss 1.8734214305877686
iteration 0, loss 1.801762580871582
iteration 100, loss 1.8583005666732788
iteration 200, loss 1.8289742469787598
iteration 300, loss 1.8743290901184082
iteration 400, loss 1.8903777599334717
iteration 500, loss 1.843640923500061
iteration 600, loss 1.777239203453064
iteration 700, loss 1.7960879802703857
iteration 800, loss 1.8760162591934204
iteration 0, loss 1.7006992101669312
iteration 100, loss 1.8999989032745361
iteration 200, loss 1.799363136291504
iteration 300, loss 1.838229775428772
iteration 400, loss 1.7713640928268433
iteration 500, loss 1.826034665107727
iteration 600, loss 1.8348058462142944
iteration 700, loss 1.8155745267868042
iteration 800, loss 1.7477986812591553
iteration 0, loss 1.8547497987747192
iteration 100, loss 1.7902432680130005
iteration 200, loss 1.7798234224319458
iteration 300, loss 1.8214690685272217
iteration 400, loss 1.7877836227416992
iteration 500, loss 1.7822383642196655
iteration 600, loss 1.869935154914856
iteration 700, loss 1.8749276399612427
iteration 800, loss 1.8658567667007446
iteration 0, loss 1.728173851966858
iteration 100, loss 1.835166096687317
iteration 200, loss 1.7514270544052124
iteration 300, loss 1.7725980281829834
iteration 400, loss 1.8016867637634277
iteration 500, loss 1.8785817623138428
iteration 600, loss 1.7915297746658325
iteration 700, loss 1.8017932176589966
iteration 800, loss 1.7851955890655518
iteration 0, loss 1.8386965990066528
iteration 100, loss 1.8764890432357788
iteration 200, loss 1.7630717754364014
iteration 300, loss 1.7575547695159912
iteration 400, loss 1.750575304031372
iteration 500, loss 1.7856472730636597
iteration 600, loss 1.8180367946624756
iteration 700, loss 1.8144670724868774
iteration 800, loss 1.7178921699523926
iteration 0, loss 1.8572064638137817
iteration 100, loss 1.8363186120986938
iteration 200, loss 1.8539707660675049
iteration 300, loss 1.8350250720977783
iteration 400, loss 1.789263367652893
iteration 500, loss 1.7894721031188965
iteration 600, loss 1.8602334260940552
iteration 700, loss 1.781660795211792
iteration 800, loss 1.9018715620040894
iteration 0, loss 1.782208800315857
iteration 100, loss 1.790615439414978
iteration 200, loss 1.8582899570465088
iteration 300, loss 1.8355140686035156
iteration 400, loss 1.858895182609558
iteration 500, loss 1.8324034214019775
iteration 600, loss 1.8443440198898315
iteration 700, loss 1.7390966415405273
iteration 800, loss 1.7777236700057983
iteration 0, loss 1.8109806776046753
iteration 100, loss 1.7329909801483154
iteration 200, loss 1.8324888944625854
iteration 300, loss 1.837357759475708
iteration 400, loss 1.814833164215088
iteration 500, loss 1.923836350440979
iteration 600, loss 1.7796554565429688
iteration 700, loss 1.7114949226379395
iteration 800, loss 1.8592931032180786
iteration 0, loss 1.7469476461410522
iteration 100, loss 1.8174448013305664
iteration 200, loss 1.7760604619979858
iteration 300, loss 1.7957383394241333
iteration 400, loss 1.7693090438842773
iteration 500, loss 1.7821146249771118
iteration 600, loss 1.8227286338806152
iteration 700, loss 1.792155146598816
iteration 800, loss 1.7961366176605225
iteration 0, loss 1.7604634761810303
iteration 100, loss 1.825554609298706
iteration 200, loss 1.8488627672195435
iteration 300, loss 1.8677257299423218
iteration 400, loss 1.8002320528030396
iteration 500, loss 1.8476548194885254
iteration 600, loss 1.8128457069396973
iteration 700, loss 1.8620377779006958
iteration 800, loss 1.7353774309158325
iteration 0, loss 1.779702067375183
iteration 100, loss 1.8582067489624023
iteration 200, loss 1.8252538442611694
iteration 300, loss 1.7534370422363281
iteration 400, loss 1.8374152183532715
iteration 500, loss 1.91182541847229
iteration 600, loss 1.7516902685165405
iteration 700, loss 1.8215855360031128
iteration 800, loss 1.7709810733795166
iteration 0, loss 1.8523179292678833
iteration 100, loss 1.7903244495391846
iteration 200, loss 1.819191336631775
iteration 300, loss 1.8112369775772095
iteration 400, loss 1.7731175422668457
iteration 500, loss 1.798569917678833
iteration 600, loss 1.7424771785736084
iteration 700, loss 1.8892858028411865
iteration 800, loss 1.7948863506317139
iteration 0, loss 1.8749313354492188
iteration 100, loss 1.8342936038970947
iteration 200, loss 1.782720923423767
iteration 300, loss 1.8418751955032349
iteration 400, loss 1.8613897562026978
iteration 500, loss 1.7539198398590088
iteration 600, loss 1.9229270219802856
iteration 700, loss 1.808497667312622
iteration 800, loss 1.7961320877075195
iteration 0, loss 1.802430272102356
iteration 100, loss 1.7795337438583374
iteration 200, loss 1.8447552919387817
iteration 300, loss 1.8277363777160645
iteration 400, loss 1.7403696775436401
iteration 500, loss 1.8624732494354248
iteration 600, loss 1.7563661336898804
iteration 700, loss 1.8051860332489014
iteration 800, loss 1.7989954948425293
iteration 0, loss 1.7934015989303589
iteration 100, loss 1.8552062511444092
iteration 200, loss 1.8317766189575195
iteration 300, loss 1.7754626274108887
iteration 400, loss 1.8508353233337402
iteration 500, loss 1.7773666381835938
iteration 600, loss 1.7971044778823853
iteration 700, loss 1.852627158164978
iteration 800, loss 1.8172736167907715
iteration 0, loss 1.7791115045547485
iteration 100, loss 1.8128163814544678
iteration 200, loss 1.8545408248901367
iteration 300, loss 1.745208978652954
iteration 400, loss 1.7683202028274536
iteration 500, loss 1.8097374439239502
iteration 600, loss 1.7289875745773315
iteration 700, loss 1.7796014547348022
iteration 800, loss 1.8114962577819824
iteration 0, loss 1.830971598625183
iteration 100, loss 1.7834452390670776
iteration 200, loss 1.7742531299591064
iteration 300, loss 1.7853035926818848
iteration 400, loss 1.8094180822372437
iteration 500, loss 1.7903395891189575
iteration 600, loss 1.7777719497680664
iteration 700, loss 1.8641796112060547
iteration 800, loss 1.7882981300354004
iteration 0, loss 1.766543984413147
iteration 100, loss 1.786167860031128
iteration 200, loss 1.7553863525390625
iteration 300, loss 1.78146231174469
iteration 400, loss 1.869747281074524
iteration 500, loss 1.8292829990386963
iteration 600, loss 1.860436201095581
iteration 700, loss 1.7327828407287598
iteration 800, loss 1.9282212257385254
iteration 0, loss 1.8136392831802368
iteration 100, loss 1.8942612409591675
iteration 200, loss 1.7956926822662354
iteration 300, loss 1.834507703781128
iteration 400, loss 1.794426679611206
iteration 500, loss 1.8281490802764893
iteration 600, loss 1.8536561727523804
iteration 700, loss 1.808339238166809
iteration 800, loss 1.8015742301940918
iteration 0, loss 1.8592685461044312
iteration 100, loss 1.8395001888275146
iteration 200, loss 1.918806791305542
iteration 300, loss 1.8889901638031006
iteration 400, loss 1.8379267454147339
iteration 500, loss 1.8065844774246216
iteration 600, loss 1.8672794103622437
iteration 700, loss 1.718511939048767
iteration 800, loss 1.8309050798416138
iteration 0, loss 1.786095380783081
iteration 100, loss 1.8254616260528564
iteration 200, loss 1.863762617111206
iteration 300, loss 1.7869434356689453
iteration 400, loss 1.833040475845337
iteration 500, loss 1.8581987619400024
iteration 600, loss 1.8699636459350586
iteration 700, loss 1.7799588441848755
iteration 800, loss 1.824831247329712
iteration 0, loss 1.7559137344360352
iteration 100, loss 1.8569869995117188
iteration 200, loss 1.814461588859558
iteration 300, loss 1.8072634935379028
iteration 400, loss 1.7431782484054565
iteration 500, loss 1.7551288604736328
iteration 600, loss 1.9298195838928223
iteration 700, loss 1.828707218170166
iteration 800, loss 1.764879822731018
iteration 0, loss 1.7658953666687012
iteration 100, loss 1.8366540670394897
iteration 200, loss 1.7914775609970093
iteration 300, loss 1.7655740976333618
iteration 400, loss 1.8359735012054443
iteration 500, loss 1.867550015449524
iteration 600, loss 1.8294601440429688
iteration 700, loss 1.7851042747497559
iteration 800, loss 1.793216347694397
iteration 0, loss 1.7621891498565674
iteration 100, loss 1.7850613594055176
iteration 200, loss 1.7817753553390503
iteration 300, loss 1.745955228805542
iteration 400, loss 1.8612689971923828
iteration 500, loss 1.7324273586273193
iteration 600, loss 1.7268553972244263
iteration 700, loss 1.8363070487976074
iteration 800, loss 1.784637689590454
iteration 0, loss 1.7782750129699707
iteration 100, loss 1.835823655128479
iteration 200, loss 1.852870225906372
iteration 300, loss 1.8547186851501465
iteration 400, loss 1.8389828205108643
iteration 500, loss 1.792585015296936
iteration 600, loss 1.7930413484573364
iteration 700, loss 1.780303716659546
iteration 800, loss 1.8196909427642822
iteration 0, loss 1.8345476388931274
iteration 100, loss 1.8220497369766235
iteration 200, loss 1.8597813844680786
iteration 300, loss 1.8272438049316406
iteration 400, loss 1.8283604383468628
iteration 500, loss 1.7730896472930908
iteration 600, loss 1.8132712841033936
iteration 700, loss 1.785296082496643
iteration 800, loss 1.8698699474334717
iteration 0, loss 1.9565120935440063
iteration 100, loss 1.7793716192245483
iteration 200, loss 1.847174048423767
iteration 300, loss 1.6904937028884888
iteration 400, loss 1.78142511844635
iteration 500, loss 1.8488030433654785
iteration 600, loss 1.8229795694351196
iteration 700, loss 1.8407129049301147
iteration 800, loss 1.8028829097747803
iteration 0, loss 1.796812891960144
iteration 100, loss 1.742170810699463
iteration 200, loss 1.810417652130127
iteration 300, loss 1.8806018829345703
iteration 400, loss 1.8318140506744385
iteration 500, loss 1.8250880241394043
iteration 600, loss 1.804091215133667
iteration 700, loss 1.824715256690979
iteration 800, loss 1.7657368183135986
fold 3 accuracy: 0.48314285714285715
iteration 0, loss 1.7961291074752808
iteration 100, loss 1.8739304542541504
iteration 200, loss 1.8424358367919922
iteration 300, loss 1.8517512083053589
iteration 400, loss 1.890166163444519
iteration 500, loss 1.8052901029586792
iteration 600, loss 1.8364152908325195
iteration 700, loss 1.8053512573242188
iteration 800, loss 1.8085970878601074
iteration 0, loss 1.9744648933410645
iteration 100, loss 1.8368682861328125
iteration 200, loss 1.7711896896362305
iteration 300, loss 1.850925087928772
iteration 400, loss 1.9200776815414429
iteration 500, loss 1.7845232486724854
iteration 600, loss 1.8714641332626343
iteration 700, loss 1.7609422206878662
iteration 800, loss 1.8129558563232422
iteration 0, loss 1.8495991230010986
iteration 100, loss 1.7958934307098389
iteration 200, loss 1.7777811288833618
iteration 300, loss 1.7136000394821167
iteration 400, loss 1.7800108194351196
iteration 500, loss 1.8437719345092773
iteration 600, loss 1.8335007429122925
iteration 700, loss 1.7905144691467285
iteration 800, loss 1.8442933559417725
iteration 0, loss 1.8300997018814087
iteration 100, loss 1.7904257774353027
iteration 200, loss 1.7218213081359863
iteration 300, loss 1.8669564723968506
iteration 400, loss 1.8048865795135498
iteration 500, loss 1.792293906211853
iteration 600, loss 1.7632824182510376
iteration 700, loss 1.804415225982666
iteration 800, loss 1.7217546701431274
iteration 0, loss 1.784460425376892
iteration 100, loss 1.8609070777893066
iteration 200, loss 1.8241865634918213
iteration 300, loss 1.8185858726501465
iteration 400, loss 1.7392524480819702
iteration 500, loss 1.7573696374893188
iteration 600, loss 1.755948543548584
iteration 700, loss 1.73745596408844
iteration 800, loss 1.836597204208374
iteration 0, loss 1.7332513332366943
iteration 100, loss 1.8196632862091064
iteration 200, loss 1.9083142280578613
iteration 300, loss 1.9490137100219727
iteration 400, loss 1.8522720336914062
iteration 500, loss 1.8502165079116821
iteration 600, loss 1.8584760427474976
iteration 700, loss 1.8122808933258057
iteration 800, loss 1.804209589958191
iteration 0, loss 1.7945263385772705
iteration 100, loss 1.816688060760498
iteration 200, loss 1.827000379562378
iteration 300, loss 1.7549524307250977
iteration 400, loss 1.7999868392944336
iteration 500, loss 1.7665050029754639
iteration 600, loss 1.9111162424087524
iteration 700, loss 1.7710473537445068
iteration 800, loss 1.829672932624817
iteration 0, loss 1.8855445384979248
iteration 100, loss 1.8086391687393188
iteration 200, loss 1.8023494482040405
iteration 300, loss 1.796687364578247
iteration 400, loss 1.7265841960906982
iteration 500, loss 1.8483773469924927
iteration 600, loss 1.7836369276046753
iteration 700, loss 1.797412633895874
iteration 800, loss 1.8290472030639648
iteration 0, loss 1.7711472511291504
iteration 100, loss 1.8692349195480347
iteration 200, loss 1.8068618774414062
iteration 300, loss 1.7273534536361694
iteration 400, loss 1.8035187721252441
iteration 500, loss 1.8086439371109009
iteration 600, loss 1.7432043552398682
iteration 700, loss 1.8369691371917725
iteration 800, loss 1.8576630353927612
iteration 0, loss 1.7782586812973022
iteration 100, loss 1.8029253482818604
iteration 200, loss 1.7222180366516113
iteration 300, loss 1.8326764106750488
iteration 400, loss 1.9099376201629639
iteration 500, loss 1.8499693870544434
iteration 600, loss 1.7737048864364624
iteration 700, loss 1.786482810974121
iteration 800, loss 1.838017463684082
iteration 0, loss 1.8106545209884644
iteration 100, loss 1.7819504737854004
iteration 200, loss 1.7720897197723389
iteration 300, loss 1.807157039642334
iteration 400, loss 1.7938263416290283
iteration 500, loss 1.7459031343460083
iteration 600, loss 1.7713741064071655
iteration 700, loss 1.7999725341796875
iteration 800, loss 1.7863860130310059
iteration 0, loss 1.8288644552230835
iteration 100, loss 1.830371379852295
iteration 200, loss 1.751783847808838
iteration 300, loss 1.8398598432540894
iteration 400, loss 1.7937748432159424
iteration 500, loss 1.7851054668426514
iteration 600, loss 1.8055839538574219
iteration 700, loss 1.8137799501419067
iteration 800, loss 1.7666635513305664
iteration 0, loss 1.762831211090088
iteration 100, loss 1.7971866130828857
iteration 200, loss 1.803642988204956
iteration 300, loss 1.7586464881896973
iteration 400, loss 1.8359205722808838
iteration 500, loss 1.7884105443954468
iteration 600, loss 1.7983365058898926
iteration 700, loss 1.890382170677185
iteration 800, loss 1.8244102001190186
iteration 0, loss 1.8403886556625366
iteration 100, loss 1.7982736825942993
iteration 200, loss 1.8748910427093506
iteration 300, loss 1.794609785079956
iteration 400, loss 1.8688572645187378
iteration 500, loss 1.798287034034729
iteration 600, loss 1.7431033849716187
iteration 700, loss 1.7982217073440552
iteration 800, loss 1.8031854629516602
iteration 0, loss 1.8295888900756836
iteration 100, loss 1.7507084608078003
iteration 200, loss 1.7838530540466309
iteration 300, loss 1.8713065385818481
iteration 400, loss 1.788112998008728
iteration 500, loss 1.838598608970642
iteration 600, loss 1.801748275756836
iteration 700, loss 1.8452088832855225
iteration 800, loss 1.8468550443649292
iteration 0, loss 1.898292064666748
iteration 100, loss 1.8865745067596436
iteration 200, loss 1.7985426187515259
iteration 300, loss 1.7552675008773804
iteration 400, loss 1.756518840789795
iteration 500, loss 1.7752389907836914
iteration 600, loss 1.89037024974823
iteration 700, loss 1.7604877948760986
iteration 800, loss 1.80226731300354
iteration 0, loss 1.8453247547149658
iteration 100, loss 1.8171956539154053
iteration 200, loss 1.8181642293930054
iteration 300, loss 1.857085108757019
iteration 400, loss 1.8226194381713867
iteration 500, loss 1.783068299293518
iteration 600, loss 1.7474254369735718
iteration 700, loss 1.7598377466201782
iteration 800, loss 1.7577335834503174
iteration 0, loss 1.7698123455047607
iteration 100, loss 1.8632943630218506
iteration 200, loss 1.829261064529419
iteration 300, loss 1.8854330778121948
iteration 400, loss 1.8410295248031616
iteration 500, loss 1.7325694561004639
iteration 600, loss 1.8306858539581299
iteration 700, loss 1.7894655466079712
iteration 800, loss 1.9109385013580322
iteration 0, loss 1.802553653717041
iteration 100, loss 1.7591516971588135
iteration 200, loss 1.7987442016601562
iteration 300, loss 1.840232253074646
iteration 400, loss 1.814574956893921
iteration 500, loss 1.8163726329803467
iteration 600, loss 1.8034586906433105
iteration 700, loss 1.824528694152832
iteration 800, loss 1.7633768320083618
iteration 0, loss 1.8017269372940063
iteration 100, loss 1.8836724758148193
iteration 200, loss 1.8055801391601562
iteration 300, loss 1.9078898429870605
iteration 400, loss 1.8964545726776123
iteration 500, loss 1.9037684202194214
iteration 600, loss 1.8395771980285645
iteration 700, loss 1.892106056213379
iteration 800, loss 1.8013708591461182
iteration 0, loss 1.7949304580688477
iteration 100, loss 1.942575216293335
iteration 200, loss 1.8160513639450073
iteration 300, loss 1.797808289527893
iteration 400, loss 1.8318451642990112
iteration 500, loss 1.7696120738983154
iteration 600, loss 1.8757445812225342
iteration 700, loss 1.8011473417282104
iteration 800, loss 1.7988998889923096
iteration 0, loss 1.8060275316238403
iteration 100, loss 1.8207674026489258
iteration 200, loss 1.8527828454971313
iteration 300, loss 1.7631711959838867
iteration 400, loss 1.859458088874817
iteration 500, loss 1.7674776315689087
iteration 600, loss 1.8056902885437012
iteration 700, loss 1.730188012123108
iteration 800, loss 1.8775333166122437
iteration 0, loss 1.7852641344070435
iteration 100, loss 1.8657910823822021
iteration 200, loss 1.7877987623214722
iteration 300, loss 1.7732380628585815
iteration 400, loss 1.7997902631759644
iteration 500, loss 1.7611241340637207
iteration 600, loss 1.8080086708068848
iteration 700, loss 1.8406221866607666
iteration 800, loss 1.8084982633590698
iteration 0, loss 1.783835530281067
iteration 100, loss 1.8274879455566406
iteration 200, loss 1.8272777795791626
iteration 300, loss 1.86807119846344
iteration 400, loss 1.8348097801208496
iteration 500, loss 1.7623755931854248
iteration 600, loss 1.7721747159957886
iteration 700, loss 1.785220742225647
iteration 800, loss 1.8462119102478027
iteration 0, loss 1.8363254070281982
iteration 100, loss 1.8238540887832642
iteration 200, loss 1.7879515886306763
iteration 300, loss 1.8161664009094238
iteration 400, loss 1.8447849750518799
iteration 500, loss 1.7877460718154907
iteration 600, loss 1.799615740776062
iteration 700, loss 1.8569037914276123
iteration 800, loss 1.8114382028579712
iteration 0, loss 1.7714449167251587
iteration 100, loss 1.766640543937683
iteration 200, loss 1.7574933767318726
iteration 300, loss 1.8507152795791626
iteration 400, loss 1.7567404508590698
iteration 500, loss 1.8157896995544434
iteration 600, loss 1.8178499937057495
iteration 700, loss 1.7506588697433472
iteration 800, loss 1.82711923122406
iteration 0, loss 1.817911148071289
iteration 100, loss 1.8764970302581787
iteration 200, loss 1.8944377899169922
iteration 300, loss 1.8435221910476685
iteration 400, loss 1.7691595554351807
iteration 500, loss 1.8731237649917603
iteration 600, loss 1.8705482482910156
iteration 700, loss 1.8118579387664795
iteration 800, loss 1.7700120210647583
iteration 0, loss 1.8258546590805054
iteration 100, loss 1.802679419517517
iteration 200, loss 1.7895156145095825
iteration 300, loss 1.8774704933166504
iteration 400, loss 1.7680609226226807
iteration 500, loss 1.832384705543518
iteration 600, loss 1.7740163803100586
iteration 700, loss 1.9091119766235352
iteration 800, loss 1.7457457780838013
iteration 0, loss 1.8817603588104248
iteration 100, loss 1.8098344802856445
iteration 200, loss 1.8111008405685425
iteration 300, loss 1.8186980485916138
iteration 400, loss 1.7793378829956055
iteration 500, loss 1.7473753690719604
iteration 600, loss 1.868360161781311
iteration 700, loss 1.8575695753097534
iteration 800, loss 1.7666999101638794
iteration 0, loss 1.8307944536209106
iteration 100, loss 1.8601888418197632
iteration 200, loss 1.8140418529510498
iteration 300, loss 1.8119499683380127
iteration 400, loss 1.8369309902191162
iteration 500, loss 1.7563588619232178
iteration 600, loss 1.8244889974594116
iteration 700, loss 1.837685465812683
iteration 800, loss 1.7279330492019653
iteration 0, loss 1.7818918228149414
iteration 100, loss 1.7467949390411377
iteration 200, loss 1.8560899496078491
iteration 300, loss 1.8376622200012207
iteration 400, loss 1.8474370241165161
iteration 500, loss 1.7510327100753784
iteration 600, loss 1.7846908569335938
iteration 700, loss 1.8122092485427856
iteration 800, loss 1.8010119199752808
iteration 0, loss 1.8316521644592285
iteration 100, loss 1.85523521900177
iteration 200, loss 1.7980129718780518
iteration 300, loss 1.8325855731964111
iteration 400, loss 1.8200347423553467
iteration 500, loss 1.767505168914795
iteration 600, loss 1.7782057523727417
iteration 700, loss 1.804348349571228
iteration 800, loss 1.7809754610061646
iteration 0, loss 1.7960532903671265
iteration 100, loss 1.7920957803726196
iteration 200, loss 1.7536438703536987
iteration 300, loss 1.8311684131622314
iteration 400, loss 1.8748852014541626
iteration 500, loss 1.7755918502807617
iteration 600, loss 1.8290374279022217
iteration 700, loss 1.8112014532089233
iteration 800, loss 1.8667244911193848
iteration 0, loss 1.8226792812347412
iteration 100, loss 1.8431135416030884
iteration 200, loss 1.7702186107635498
iteration 300, loss 1.844465970993042
iteration 400, loss 1.8498519659042358
iteration 500, loss 1.8356609344482422
iteration 600, loss 1.8085250854492188
iteration 700, loss 1.7982944250106812
iteration 800, loss 1.8162620067596436
iteration 0, loss 1.807674765586853
iteration 100, loss 1.831246256828308
iteration 200, loss 1.7715167999267578
iteration 300, loss 1.7620898485183716
iteration 400, loss 1.7865177392959595
iteration 500, loss 1.8007075786590576
iteration 600, loss 1.7694324254989624
iteration 700, loss 1.8711633682250977
iteration 800, loss 1.8268331289291382
iteration 0, loss 1.7959016561508179
iteration 100, loss 1.7978090047836304
iteration 200, loss 1.8256776332855225
iteration 300, loss 1.7340874671936035
iteration 400, loss 1.7017484903335571
iteration 500, loss 1.8379473686218262
iteration 600, loss 1.8324836492538452
iteration 700, loss 1.8535734415054321
iteration 800, loss 1.7903858423233032
iteration 0, loss 1.8177746534347534
iteration 100, loss 1.7637555599212646
iteration 200, loss 1.7798815965652466
iteration 300, loss 1.7812598943710327
iteration 400, loss 1.8228591680526733
iteration 500, loss 1.7558975219726562
iteration 600, loss 1.7804350852966309
iteration 700, loss 1.7614562511444092
iteration 800, loss 1.8758537769317627
iteration 0, loss 1.7864294052124023
iteration 100, loss 1.7759549617767334
iteration 200, loss 1.8880634307861328
iteration 300, loss 1.8990105390548706
iteration 400, loss 1.7811150550842285
iteration 500, loss 1.8421916961669922
iteration 600, loss 1.7981131076812744
iteration 700, loss 1.7786842584609985
iteration 800, loss 1.8007962703704834
iteration 0, loss 1.8430498838424683
iteration 100, loss 1.8353307247161865
iteration 200, loss 1.8653793334960938
iteration 300, loss 1.775139331817627
iteration 400, loss 1.8175305128097534
iteration 500, loss 1.8257927894592285
iteration 600, loss 1.7929543256759644
iteration 700, loss 1.7806181907653809
iteration 800, loss 1.8311550617218018
iteration 0, loss 1.793197512626648
iteration 100, loss 1.7749004364013672
iteration 200, loss 1.8292585611343384
iteration 300, loss 1.8327382802963257
iteration 400, loss 1.7926346063613892
iteration 500, loss 1.7435733079910278
iteration 600, loss 1.8407158851623535
iteration 700, loss 1.7994418144226074
iteration 800, loss 1.8065427541732788
iteration 0, loss 1.8500598669052124
iteration 100, loss 1.8134976625442505
iteration 200, loss 1.8335868120193481
iteration 300, loss 1.8312095403671265
iteration 400, loss 1.8056561946868896
iteration 500, loss 1.872735619544983
iteration 600, loss 1.8367550373077393
iteration 700, loss 1.8143821954727173
iteration 800, loss 1.868963599205017
iteration 0, loss 1.7928012609481812
iteration 100, loss 1.8736964464187622
iteration 200, loss 1.8140966892242432
iteration 300, loss 1.8217856884002686
iteration 400, loss 1.8145029544830322
iteration 500, loss 1.8111481666564941
iteration 600, loss 1.7300044298171997
iteration 700, loss 1.8802762031555176
iteration 800, loss 1.8410807847976685
iteration 0, loss 1.7349913120269775
iteration 100, loss 1.888809323310852
iteration 200, loss 1.806444525718689
iteration 300, loss 1.7934280633926392
iteration 400, loss 1.7330894470214844
iteration 500, loss 1.8391304016113281
iteration 600, loss 1.817868947982788
iteration 700, loss 1.7793011665344238
iteration 800, loss 1.8239091634750366
iteration 0, loss 1.856092929840088
iteration 100, loss 1.7921234369277954
iteration 200, loss 1.8496485948562622
iteration 300, loss 1.821165680885315
iteration 400, loss 1.8631999492645264
iteration 500, loss 1.733269453048706
iteration 600, loss 1.855096697807312
iteration 700, loss 1.8463585376739502
iteration 800, loss 1.8094810247421265
iteration 0, loss 1.7647260427474976
iteration 100, loss 1.79864501953125
iteration 200, loss 1.724957823753357
iteration 300, loss 1.867182970046997
iteration 400, loss 1.8697930574417114
iteration 500, loss 1.8124799728393555
iteration 600, loss 1.822799801826477
iteration 700, loss 1.7805033922195435
iteration 800, loss 1.798433542251587
iteration 0, loss 1.7803422212600708
iteration 100, loss 1.8127912282943726
iteration 200, loss 1.7808862924575806
iteration 300, loss 1.8015657663345337
iteration 400, loss 1.7513707876205444
iteration 500, loss 1.7919962406158447
iteration 600, loss 1.7363944053649902
iteration 700, loss 1.821493148803711
iteration 800, loss 1.7875531911849976
iteration 0, loss 1.7642755508422852
iteration 100, loss 1.825981616973877
iteration 200, loss 1.8334721326828003
iteration 300, loss 1.775171160697937
iteration 400, loss 1.830114722251892
iteration 500, loss 1.8095970153808594
iteration 600, loss 1.8184502124786377
iteration 700, loss 1.8188101053237915
iteration 800, loss 1.7545231580734253
iteration 0, loss 1.8268510103225708
iteration 100, loss 1.7653838396072388
iteration 200, loss 1.7589302062988281
iteration 300, loss 1.8598589897155762
iteration 400, loss 1.8625168800354004
iteration 500, loss 1.8454527854919434
iteration 600, loss 1.8762831687927246
iteration 700, loss 1.8990914821624756
iteration 800, loss 1.8030505180358887
iteration 0, loss 1.7811994552612305
iteration 100, loss 1.8253610134124756
iteration 200, loss 1.776702880859375
iteration 300, loss 1.7801456451416016
iteration 400, loss 1.761471152305603
iteration 500, loss 1.8142001628875732
iteration 600, loss 1.7823312282562256
iteration 700, loss 1.7933955192565918
iteration 800, loss 1.7954084873199463
iteration 0, loss 1.892630696296692
iteration 100, loss 1.8238158226013184
iteration 200, loss 1.7911278009414673
iteration 300, loss 1.8480451107025146
iteration 400, loss 1.7882378101348877
iteration 500, loss 1.827134370803833
iteration 600, loss 1.793637752532959
iteration 700, loss 1.8873155117034912
iteration 800, loss 1.8844006061553955
fold 4 accuracy: 0.486
[2024-02-28 22:30:37,889] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-28 22:30:37,893] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            328.06 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.71 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '328.06 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 328.06 us = 100% latency, 1.71 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 226.02 us = 68.9% latency, 2.48 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 30.28 us = 9.23% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-28 22:30:37,894] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
iteration 0, loss 2.3218793869018555
iteration 100, loss 2.2854208946228027
iteration 200, loss 2.171004056930542
iteration 300, loss 2.1429731845855713
iteration 400, loss 2.0925824642181396
iteration 500, loss 2.0715489387512207
iteration 600, loss 2.036588430404663
iteration 700, loss 2.0657663345336914
iteration 800, loss 1.9848817586898804
iteration 0, loss 2.0409135818481445
iteration 100, loss 2.037640333175659
iteration 200, loss 2.0148227214813232
iteration 300, loss 2.0316483974456787
iteration 400, loss 1.9774090051651
iteration 500, loss 1.974988341331482
iteration 600, loss 2.0118727684020996
iteration 700, loss 1.9217277765274048
iteration 800, loss 2.0220987796783447
iteration 0, loss 1.9704393148422241
iteration 100, loss 1.9993549585342407
iteration 200, loss 1.9592530727386475
iteration 300, loss 2.0247209072113037
iteration 400, loss 1.9277862310409546
iteration 500, loss 1.9019901752471924
iteration 600, loss 1.887029767036438
iteration 700, loss 2.016908884048462
iteration 800, loss 1.9521788358688354
iteration 0, loss 2.0176634788513184
iteration 100, loss 1.978730320930481
iteration 200, loss 1.969279408454895
iteration 300, loss 1.9450703859329224
iteration 400, loss 2.0001773834228516
iteration 500, loss 1.973433494567871
iteration 600, loss 1.9946959018707275
iteration 700, loss 1.9535795450210571
iteration 800, loss 1.9458515644073486
iteration 0, loss 1.9771738052368164
iteration 100, loss 1.9944294691085815
iteration 200, loss 1.9924629926681519
iteration 300, loss 1.9170821905136108
iteration 400, loss 2.005889892578125
iteration 500, loss 1.9231362342834473
iteration 600, loss 1.928877830505371
iteration 700, loss 1.985111951828003
iteration 800, loss 1.9549212455749512
iteration 0, loss 1.9677107334136963
iteration 100, loss 1.9510152339935303
iteration 200, loss 1.9446340799331665
iteration 300, loss 1.9030249118804932
iteration 400, loss 1.9301483631134033
iteration 500, loss 1.9033609628677368
iteration 600, loss 1.9022107124328613
iteration 700, loss 1.9814468622207642
iteration 800, loss 1.9409852027893066
iteration 0, loss 1.9514201879501343
iteration 100, loss 1.9323921203613281
iteration 200, loss 1.8920862674713135
iteration 300, loss 1.9025495052337646
iteration 400, loss 1.9336748123168945
iteration 500, loss 1.9452998638153076
iteration 600, loss 2.0132248401641846
iteration 700, loss 1.8972147703170776
iteration 800, loss 2.0081586837768555
iteration 0, loss 1.8810288906097412
iteration 100, loss 1.9370838403701782
iteration 200, loss 1.9586803913116455
iteration 300, loss 1.9521739482879639
iteration 400, loss 1.98286771774292
iteration 500, loss 1.8726685047149658
iteration 600, loss 1.9354779720306396
iteration 700, loss 1.9099010229110718
iteration 800, loss 1.936413049697876
iteration 0, loss 2.0108373165130615
iteration 100, loss 1.9100337028503418
iteration 200, loss 1.8531678915023804
iteration 300, loss 1.9181967973709106
iteration 400, loss 1.9412167072296143
iteration 500, loss 1.9468395709991455
iteration 600, loss 1.9124923944473267
iteration 700, loss 1.9030537605285645
iteration 800, loss 1.9338948726654053
iteration 0, loss 1.9070570468902588
iteration 100, loss 1.917436957359314
iteration 200, loss 1.9634093046188354
iteration 300, loss 1.971725583076477
iteration 400, loss 1.995176076889038
iteration 500, loss 1.8851479291915894
iteration 600, loss 1.9179034233093262
iteration 700, loss 1.9740945100784302
iteration 800, loss 2.002849578857422
iteration 0, loss 1.9583994150161743
iteration 100, loss 1.8522504568099976
iteration 200, loss 1.937428593635559
iteration 300, loss 1.9533193111419678
iteration 400, loss 1.9712566137313843
iteration 500, loss 1.9140667915344238
iteration 600, loss 1.9213855266571045
iteration 700, loss 1.861242413520813
iteration 800, loss 1.8642834424972534
iteration 0, loss 1.9237409830093384
iteration 100, loss 1.877894639968872
iteration 200, loss 1.9434977769851685
iteration 300, loss 1.88307785987854
iteration 400, loss 1.8473576307296753
iteration 500, loss 1.928428053855896
iteration 600, loss 1.9139262437820435
iteration 700, loss 1.9443691968917847
iteration 800, loss 1.9512070417404175
iteration 0, loss 1.9835790395736694
iteration 100, loss 1.9504919052124023
iteration 200, loss 1.9244028329849243
iteration 300, loss 1.9607044458389282
iteration 400, loss 1.8985254764556885
iteration 500, loss 1.8985000848770142
iteration 600, loss 1.9420809745788574
iteration 700, loss 1.9432969093322754
iteration 800, loss 1.9604425430297852
iteration 0, loss 1.969617247581482
iteration 100, loss 1.9183951616287231
iteration 200, loss 1.865551233291626
iteration 300, loss 2.013700246810913
iteration 400, loss 1.8852934837341309
iteration 500, loss 1.9756487607955933
iteration 600, loss 1.9626219272613525
iteration 700, loss 1.8863633871078491
iteration 800, loss 1.9533401727676392
iteration 0, loss 1.9393315315246582
iteration 100, loss 1.9955471754074097
iteration 200, loss 1.9498982429504395
iteration 300, loss 1.9004019498825073
iteration 400, loss 1.9425482749938965
iteration 500, loss 1.9409500360488892
iteration 600, loss 1.9085817337036133
iteration 700, loss 1.8941189050674438
iteration 800, loss 1.952250361442566
iteration 0, loss 2.0201592445373535
iteration 100, loss 1.9082626104354858
iteration 200, loss 1.9479128122329712
iteration 300, loss 1.927526831626892
iteration 400, loss 1.9678767919540405
iteration 500, loss 1.9427671432495117
iteration 600, loss 1.9068578481674194
iteration 700, loss 1.824723720550537
iteration 800, loss 1.934571385383606
iteration 0, loss 1.9538853168487549
iteration 100, loss 1.9307667016983032
iteration 200, loss 1.988645076751709
iteration 300, loss 1.8990000486373901
iteration 400, loss 1.9063113927841187
iteration 500, loss 1.827620029449463
iteration 600, loss 1.9528799057006836
iteration 700, loss 1.9082573652267456
iteration 800, loss 1.8935521841049194
iteration 0, loss 1.8809956312179565
iteration 100, loss 1.9113606214523315
iteration 200, loss 1.9511696100234985
iteration 300, loss 1.9640369415283203
iteration 400, loss 1.9551208019256592
iteration 500, loss 1.9569810628890991
iteration 600, loss 2.0309934616088867
iteration 700, loss 1.9333827495574951
iteration 800, loss 1.9366679191589355
iteration 0, loss 1.944485068321228
iteration 100, loss 1.9092316627502441
iteration 200, loss 1.8710334300994873
iteration 300, loss 1.9123488664627075
iteration 400, loss 1.9026693105697632
iteration 500, loss 1.8900949954986572
iteration 600, loss 1.9273077249526978
iteration 700, loss 1.9311476945877075
iteration 800, loss 1.92283034324646
iteration 0, loss 1.8925745487213135
iteration 100, loss 1.918645977973938
iteration 200, loss 1.9361951351165771
iteration 300, loss 1.9612385034561157
iteration 400, loss 1.9107487201690674
iteration 500, loss 1.9115136861801147
iteration 600, loss 1.9531811475753784
iteration 700, loss 1.9303959608078003
iteration 800, loss 1.945528268814087
iteration 0, loss 1.9260627031326294
iteration 100, loss 1.9535876512527466
iteration 200, loss 1.9755722284317017
iteration 300, loss 1.9889143705368042
iteration 400, loss 1.858087420463562
iteration 500, loss 1.876083493232727
iteration 600, loss 1.9369934797286987
iteration 700, loss 1.9122390747070312
iteration 800, loss 1.9013782739639282
iteration 0, loss 1.841963291168213
iteration 100, loss 1.9102120399475098
iteration 200, loss 1.8572967052459717
iteration 300, loss 1.9004011154174805
iteration 400, loss 1.9355887174606323
iteration 500, loss 1.9866812229156494
iteration 600, loss 1.851560354232788
iteration 700, loss 1.929196834564209
iteration 800, loss 1.8449122905731201
iteration 0, loss 1.9185253381729126
iteration 100, loss 1.916925311088562
iteration 200, loss 1.9865518808364868
iteration 300, loss 1.934232234954834
iteration 400, loss 1.9186171293258667
iteration 500, loss 1.931951642036438
iteration 600, loss 1.9307990074157715
iteration 700, loss 1.9109915494918823
iteration 800, loss 1.9520750045776367
iteration 0, loss 1.9617348909378052
iteration 100, loss 1.8795989751815796
iteration 200, loss 1.892424464225769
iteration 300, loss 1.9188971519470215
iteration 400, loss 1.967597246170044
iteration 500, loss 1.9335362911224365
iteration 600, loss 1.9477295875549316
iteration 700, loss 1.9317162036895752
iteration 800, loss 1.9346215724945068
iteration 0, loss 1.9810529947280884
iteration 100, loss 1.962839126586914
iteration 200, loss 1.869929552078247
iteration 300, loss 1.9148602485656738
iteration 400, loss 1.9021977186203003
iteration 500, loss 1.8629121780395508
iteration 600, loss 1.8584412336349487
iteration 700, loss 1.9577988386154175
iteration 800, loss 1.9149672985076904
iteration 0, loss 1.8443342447280884
iteration 100, loss 1.9054484367370605
iteration 200, loss 1.9383584260940552
iteration 300, loss 1.9099338054656982
iteration 400, loss 1.8836266994476318
iteration 500, loss 1.8913203477859497
iteration 600, loss 1.8689864873886108
iteration 700, loss 1.9160988330841064
iteration 800, loss 1.9259926080703735
iteration 0, loss 1.844412922859192
iteration 100, loss 1.8869917392730713
iteration 200, loss 1.9234675168991089
iteration 300, loss 1.9018661975860596
iteration 400, loss 1.8879815340042114
iteration 500, loss 1.8923252820968628
iteration 600, loss 1.9245508909225464
iteration 700, loss 1.9008808135986328
iteration 800, loss 1.9083917140960693
iteration 0, loss 1.8143742084503174
iteration 100, loss 1.9559991359710693
iteration 200, loss 1.949790358543396
iteration 300, loss 1.836426019668579
iteration 400, loss 1.911977767944336
iteration 500, loss 1.8628616333007812
iteration 600, loss 1.9179490804672241
iteration 700, loss 1.9457404613494873
iteration 800, loss 1.8567801713943481
iteration 0, loss 1.9421669244766235
iteration 100, loss 1.8894370794296265
iteration 200, loss 1.8051031827926636
iteration 300, loss 1.9715266227722168
iteration 400, loss 1.959283471107483
iteration 500, loss 1.8508496284484863
iteration 600, loss 1.8396053314208984
iteration 700, loss 1.984373927116394
iteration 800, loss 1.9631106853485107
iteration 0, loss 1.8419935703277588
iteration 100, loss 1.9230653047561646
iteration 200, loss 1.971892237663269
iteration 300, loss 1.917522668838501
iteration 400, loss 1.9033585786819458
iteration 500, loss 1.9068217277526855
iteration 600, loss 1.9213998317718506
iteration 700, loss 1.9074019193649292
iteration 800, loss 1.8586140871047974
iteration 0, loss 1.8734911680221558
iteration 100, loss 1.848905324935913
iteration 200, loss 1.829822063446045
iteration 300, loss 1.9111878871917725
iteration 400, loss 1.8969284296035767
iteration 500, loss 1.855285882949829
iteration 600, loss 1.935744047164917
iteration 700, loss 1.934141755104065
iteration 800, loss 1.9042545557022095
iteration 0, loss 1.9379748106002808
iteration 100, loss 1.847321629524231
iteration 200, loss 1.9181660413742065
iteration 300, loss 1.8832881450653076
iteration 400, loss 1.9869587421417236
iteration 500, loss 1.8910530805587769
iteration 600, loss 1.8981757164001465
iteration 700, loss 1.9474629163742065
iteration 800, loss 1.9502235651016235
iteration 0, loss 1.999631643295288
iteration 100, loss 1.8406144380569458
iteration 200, loss 1.9425650835037231
iteration 300, loss 1.9312303066253662
iteration 400, loss 1.8878257274627686
iteration 500, loss 1.8402490615844727
iteration 600, loss 1.8342580795288086
iteration 700, loss 1.9279602766036987
iteration 800, loss 1.9958950281143188
iteration 0, loss 1.8641443252563477
iteration 100, loss 1.8665528297424316
iteration 200, loss 1.817408800125122
iteration 300, loss 1.799231767654419
iteration 400, loss 1.9264720678329468
iteration 500, loss 1.8555020093917847
iteration 600, loss 1.829457402229309
iteration 700, loss 1.9107450246810913
iteration 800, loss 1.8995866775512695
iteration 0, loss 1.8877744674682617
iteration 100, loss 1.874372959136963
iteration 200, loss 1.9207391738891602
iteration 300, loss 1.8689581155776978
iteration 400, loss 1.8992443084716797
iteration 500, loss 1.8771525621414185
iteration 600, loss 1.881995439529419
iteration 700, loss 1.8725334405899048
iteration 800, loss 1.8892762660980225
iteration 0, loss 1.8368639945983887
iteration 100, loss 1.8879379034042358
iteration 200, loss 1.8980079889297485
iteration 300, loss 1.8015154600143433
iteration 400, loss 1.8177722692489624
iteration 500, loss 1.874763011932373
iteration 600, loss 1.885326862335205
iteration 700, loss 1.9247816801071167
iteration 800, loss 1.836800456047058
iteration 0, loss 1.8252805471420288
iteration 100, loss 1.8906463384628296
iteration 200, loss 1.8706631660461426
iteration 300, loss 1.889553189277649
iteration 400, loss 1.9194650650024414
iteration 500, loss 1.93192720413208
iteration 600, loss 1.8787474632263184
iteration 700, loss 1.8406776189804077
iteration 800, loss 1.9135127067565918
iteration 0, loss 1.9631166458129883
iteration 100, loss 1.8834329843521118
iteration 200, loss 1.9057148694992065
iteration 300, loss 1.8962578773498535
iteration 400, loss 1.9363481998443604
iteration 500, loss 1.8793020248413086
iteration 600, loss 1.8058685064315796
iteration 700, loss 1.9102730751037598
iteration 800, loss 1.8601531982421875
iteration 0, loss 1.930345058441162
iteration 100, loss 1.8953611850738525
iteration 200, loss 1.8944697380065918
iteration 300, loss 1.8279223442077637
iteration 400, loss 1.8874225616455078
iteration 500, loss 1.966016173362732
iteration 600, loss 1.9352940320968628
iteration 700, loss 1.8085711002349854
iteration 800, loss 1.871228814125061
iteration 0, loss 1.883111596107483
iteration 100, loss 1.8482638597488403
iteration 200, loss 1.8279755115509033
iteration 300, loss 1.8540693521499634
iteration 400, loss 1.9000740051269531
iteration 500, loss 1.8939071893692017
iteration 600, loss 1.977306604385376
iteration 700, loss 1.807847023010254
iteration 800, loss 1.8533339500427246
iteration 0, loss 1.907111644744873
iteration 100, loss 1.8645873069763184
iteration 200, loss 1.8179638385772705
iteration 300, loss 1.8358820676803589
iteration 400, loss 1.7953197956085205
iteration 500, loss 1.893568515777588
iteration 600, loss 1.8001500368118286
iteration 700, loss 1.8426976203918457
iteration 800, loss 1.8639178276062012
iteration 0, loss 1.7873966693878174
iteration 100, loss 1.7868359088897705
iteration 200, loss 1.8046915531158447
iteration 300, loss 1.9156631231307983
iteration 400, loss 1.8398137092590332
iteration 500, loss 1.863722801208496
iteration 600, loss 1.8888505697250366
iteration 700, loss 1.87052321434021
iteration 800, loss 1.9343111515045166
iteration 0, loss 1.7836540937423706
iteration 100, loss 1.8862015008926392
iteration 200, loss 1.869672179222107
iteration 300, loss 1.8681467771530151
iteration 400, loss 1.836287260055542
iteration 500, loss 1.8568975925445557
iteration 600, loss 1.8559373617172241
iteration 700, loss 1.8822245597839355
iteration 800, loss 1.96315598487854
iteration 0, loss 1.8452905416488647
iteration 100, loss 1.8361341953277588
iteration 200, loss 1.8736765384674072
iteration 300, loss 1.8280208110809326
iteration 400, loss 1.830601692199707
iteration 500, loss 1.8514033555984497
iteration 600, loss 1.9129257202148438
iteration 700, loss 1.8306493759155273
iteration 800, loss 1.8094114065170288
iteration 0, loss 1.9739105701446533
iteration 100, loss 1.920546531677246
iteration 200, loss 1.829614520072937
iteration 300, loss 1.9371403455734253
iteration 400, loss 1.9684678316116333
iteration 500, loss 1.9152671098709106
iteration 600, loss 1.880849838256836
iteration 700, loss 1.901090145111084
iteration 800, loss 1.886904001235962
iteration 0, loss 1.840097188949585
iteration 100, loss 1.852940320968628
iteration 200, loss 1.8281275033950806
iteration 300, loss 1.8769391775131226
iteration 400, loss 1.8745460510253906
iteration 500, loss 1.9209319353103638
iteration 600, loss 1.851752519607544
iteration 700, loss 1.9300258159637451
iteration 800, loss 1.8477976322174072
iteration 0, loss 1.878650188446045
iteration 100, loss 1.8779151439666748
iteration 200, loss 1.8575186729431152
iteration 300, loss 1.8362066745758057
iteration 400, loss 1.8256103992462158
iteration 500, loss 1.83046293258667
iteration 600, loss 1.8741989135742188
iteration 700, loss 1.7968460321426392
iteration 800, loss 1.7790330648422241
iteration 0, loss 1.780672311782837
iteration 100, loss 1.8700834512710571
iteration 200, loss 1.8776893615722656
iteration 300, loss 1.88764488697052
iteration 400, loss 1.8473986387252808
iteration 500, loss 1.8819553852081299
iteration 600, loss 1.8738353252410889
iteration 700, loss 1.906365156173706
iteration 800, loss 1.8723925352096558
iteration 0, loss 1.8865249156951904
iteration 100, loss 1.8632279634475708
iteration 200, loss 1.9331430196762085
iteration 300, loss 1.904251217842102
iteration 400, loss 1.9455018043518066
iteration 500, loss 1.9039170742034912
iteration 600, loss 1.8195114135742188
iteration 700, loss 1.8382359743118286
iteration 800, loss 1.925066590309143
iteration 0, loss 1.8895812034606934
iteration 100, loss 1.798149824142456
iteration 200, loss 1.9017605781555176
iteration 300, loss 1.8442524671554565
iteration 400, loss 1.8567122220993042
iteration 500, loss 1.888708472251892
iteration 600, loss 1.8467531204223633
iteration 700, loss 1.7952733039855957
iteration 800, loss 1.9792649745941162
fold 0 accuracy: 0.47314285714285714
iteration 0, loss 1.8791521787643433
iteration 100, loss 1.810056209564209
iteration 200, loss 1.8387842178344727
iteration 300, loss 1.8745758533477783
iteration 400, loss 1.8585394620895386
iteration 500, loss 1.9221696853637695
iteration 600, loss 1.8469656705856323
iteration 700, loss 1.8798161745071411
iteration 800, loss 1.8258472681045532
iteration 0, loss 1.7725365161895752
iteration 100, loss 1.8672932386398315
iteration 200, loss 1.8409481048583984
iteration 300, loss 1.8618165254592896
iteration 400, loss 1.8748185634613037
iteration 500, loss 1.8655287027359009
iteration 600, loss 1.848880648612976
iteration 700, loss 1.9234853982925415
iteration 800, loss 1.8461778163909912
iteration 0, loss 1.9006434679031372
iteration 100, loss 1.8847203254699707
iteration 200, loss 1.8860515356063843
iteration 300, loss 1.7980060577392578
iteration 400, loss 1.880850076675415
iteration 500, loss 1.8710750341415405
iteration 600, loss 1.8529229164123535
iteration 700, loss 1.8814038038253784
iteration 800, loss 1.8861627578735352
iteration 0, loss 1.9404312372207642
iteration 100, loss 1.9019349813461304
iteration 200, loss 1.880709171295166
iteration 300, loss 1.8768216371536255
iteration 400, loss 1.910186529159546
iteration 500, loss 1.8795067071914673
iteration 600, loss 1.7945753335952759
iteration 700, loss 1.7991105318069458
iteration 800, loss 1.8656805753707886
iteration 0, loss 1.8403483629226685
iteration 100, loss 1.868827223777771
iteration 200, loss 1.7795366048812866
iteration 300, loss 1.8909733295440674
iteration 400, loss 1.8055084943771362
iteration 500, loss 1.8708155155181885
iteration 600, loss 1.8341255187988281
iteration 700, loss 1.8265812397003174
iteration 800, loss 1.8345972299575806
iteration 0, loss 1.904224157333374
iteration 100, loss 1.904308557510376
iteration 200, loss 1.825462818145752
iteration 300, loss 1.8763957023620605
iteration 400, loss 1.9525508880615234
iteration 500, loss 1.8232600688934326
iteration 600, loss 1.8937156200408936
iteration 700, loss 1.8661794662475586
iteration 800, loss 1.922019362449646
iteration 0, loss 1.7981042861938477
iteration 100, loss 1.764297366142273
iteration 200, loss 1.8323925733566284
iteration 300, loss 1.8050819635391235
iteration 400, loss 1.8452829122543335
iteration 500, loss 1.802125334739685
iteration 600, loss 1.8714289665222168
iteration 700, loss 1.8508965969085693
iteration 800, loss 1.805403232574463
iteration 0, loss 1.8211458921432495
iteration 100, loss 1.8528223037719727
iteration 200, loss 1.8133329153060913
iteration 300, loss 1.8021149635314941
iteration 400, loss 1.8967077732086182
iteration 500, loss 1.805752158164978
iteration 600, loss 1.8571302890777588
iteration 700, loss 1.8407955169677734
iteration 800, loss 1.8512643575668335
iteration 0, loss 1.7846636772155762
iteration 100, loss 1.9571205377578735
iteration 200, loss 1.8936461210250854
iteration 300, loss 1.8460938930511475
iteration 400, loss 1.8868465423583984
iteration 500, loss 1.8581985235214233
iteration 600, loss 1.8444286584854126
iteration 700, loss 1.8868576288223267
iteration 800, loss 1.8422174453735352
iteration 0, loss 1.8023661375045776
iteration 100, loss 1.8231122493743896
iteration 200, loss 1.9015179872512817
iteration 300, loss 1.8619340658187866
iteration 400, loss 1.7955505847930908
iteration 500, loss 1.8423153162002563
iteration 600, loss 1.8365671634674072
iteration 700, loss 1.8294639587402344
iteration 800, loss 1.8574879169464111
iteration 0, loss 1.937222957611084
iteration 100, loss 1.88784921169281
iteration 200, loss 1.8444980382919312
iteration 300, loss 1.8501378297805786
iteration 400, loss 1.8858290910720825
iteration 500, loss 1.791762113571167
iteration 600, loss 1.8338358402252197
iteration 700, loss 1.8523778915405273
iteration 800, loss 1.8764909505844116
iteration 0, loss 1.884077787399292
iteration 100, loss 1.8825114965438843
iteration 200, loss 1.8355658054351807
iteration 300, loss 1.804166555404663
iteration 400, loss 1.9193662405014038
iteration 500, loss 1.8352035284042358
iteration 600, loss 1.8965307474136353
iteration 700, loss 1.8524169921875
iteration 800, loss 1.870052456855774
iteration 0, loss 1.8006861209869385
iteration 100, loss 1.8558260202407837
iteration 200, loss 1.8243393898010254
iteration 300, loss 1.8631227016448975
iteration 400, loss 1.8451244831085205
iteration 500, loss 1.817814588546753
iteration 600, loss 1.881888508796692
iteration 700, loss 1.8095202445983887
iteration 800, loss 1.8866413831710815
iteration 0, loss 1.8708882331848145
iteration 100, loss 1.907232403755188
iteration 200, loss 1.7915329933166504
iteration 300, loss 1.8242766857147217
iteration 400, loss 1.7989574670791626
iteration 500, loss 1.8344298601150513
iteration 600, loss 1.8145211935043335
iteration 700, loss 1.8601915836334229
iteration 800, loss 1.7591441869735718
iteration 0, loss 1.9248933792114258
iteration 100, loss 1.8497893810272217
iteration 200, loss 1.8824352025985718
iteration 300, loss 1.8370240926742554
iteration 400, loss 1.8625640869140625
iteration 500, loss 1.8172061443328857
iteration 600, loss 1.8268420696258545
iteration 700, loss 1.9231855869293213
iteration 800, loss 1.8476438522338867
iteration 0, loss 1.8290941715240479
iteration 100, loss 1.8596997261047363
iteration 200, loss 1.8521819114685059
iteration 300, loss 1.828737735748291
iteration 400, loss 1.8655446767807007
iteration 500, loss 1.8410379886627197
iteration 600, loss 1.8359732627868652
iteration 700, loss 1.8614850044250488
iteration 800, loss 1.8325037956237793
iteration 0, loss 1.8568612337112427
iteration 100, loss 1.8375352621078491
iteration 200, loss 1.7789726257324219
iteration 300, loss 1.9388256072998047
iteration 400, loss 1.8018527030944824
iteration 500, loss 1.8697590827941895
iteration 600, loss 1.8804112672805786
iteration 700, loss 1.841511845588684
iteration 800, loss 1.7970120906829834
iteration 0, loss 1.8379566669464111
iteration 100, loss 1.855586051940918
iteration 200, loss 1.7768306732177734
iteration 300, loss 1.8678566217422485
iteration 400, loss 1.856250286102295
iteration 500, loss 1.8280556201934814
iteration 600, loss 1.8146361112594604
iteration 700, loss 1.831795334815979
iteration 800, loss 1.8100059032440186
iteration 0, loss 1.907202959060669
iteration 100, loss 1.8498483896255493
iteration 200, loss 1.8184312582015991
iteration 300, loss 1.8436925411224365
iteration 400, loss 1.938435673713684
iteration 500, loss 1.858356237411499
iteration 600, loss 1.899935245513916
iteration 700, loss 1.91492760181427
iteration 800, loss 1.8021513223648071
iteration 0, loss 1.8045257329940796
iteration 100, loss 1.7696678638458252
iteration 200, loss 1.8443138599395752
iteration 300, loss 1.8294177055358887
iteration 400, loss 1.797299861907959
iteration 500, loss 1.8473976850509644
iteration 600, loss 1.818468689918518
iteration 700, loss 1.890228509902954
iteration 800, loss 1.855851650238037
iteration 0, loss 1.8397127389907837
iteration 100, loss 1.8572113513946533
iteration 200, loss 1.8298450708389282
iteration 300, loss 1.8144103288650513
iteration 400, loss 1.849555492401123
iteration 500, loss 1.7969956398010254
iteration 600, loss 1.8388350009918213
iteration 700, loss 1.8456230163574219
iteration 800, loss 1.8459631204605103
iteration 0, loss 1.814556360244751
iteration 100, loss 1.8109943866729736
iteration 200, loss 1.7717117071151733
iteration 300, loss 1.9020222425460815
iteration 400, loss 1.8501886129379272
iteration 500, loss 1.8931968212127686
iteration 600, loss 1.9068915843963623
iteration 700, loss 1.7809760570526123
iteration 800, loss 1.8444008827209473
iteration 0, loss 1.7993649244308472
iteration 100, loss 1.8456206321716309
iteration 200, loss 1.8672832250595093
iteration 300, loss 1.7934740781784058
iteration 400, loss 1.7888389825820923
iteration 500, loss 1.8025248050689697
iteration 600, loss 1.9026496410369873
iteration 700, loss 1.873143196105957
iteration 800, loss 1.8012343645095825
iteration 0, loss 1.911360740661621
iteration 100, loss 1.7960669994354248
iteration 200, loss 1.9181214570999146
iteration 300, loss 1.7985223531723022
iteration 400, loss 1.8177824020385742
iteration 500, loss 1.8145891427993774
iteration 600, loss 1.894909381866455
iteration 700, loss 1.8404041528701782
iteration 800, loss 1.8578882217407227
iteration 0, loss 1.8704851865768433
iteration 100, loss 1.7677603960037231
iteration 200, loss 1.8648046255111694
iteration 300, loss 1.8507436513900757
iteration 400, loss 1.8125481605529785
iteration 500, loss 1.8834000825881958
iteration 600, loss 1.8528072834014893
iteration 700, loss 1.7579879760742188
iteration 800, loss 1.774388313293457
iteration 0, loss 1.7217055559158325
iteration 100, loss 1.9197520017623901
iteration 200, loss 1.9093307256698608
iteration 300, loss 1.8244476318359375
iteration 400, loss 1.812500238418579
iteration 500, loss 1.9274846315383911
iteration 600, loss 1.8942582607269287
iteration 700, loss 1.8764454126358032
iteration 800, loss 1.8249173164367676
iteration 0, loss 1.7808845043182373
iteration 100, loss 1.8488166332244873
iteration 200, loss 1.780346155166626
iteration 300, loss 1.7706915140151978
iteration 400, loss 1.821998119354248
iteration 500, loss 1.8373934030532837
iteration 600, loss 1.9302622079849243
iteration 700, loss 1.8189293146133423
iteration 800, loss 1.9003437757492065
iteration 0, loss 1.953326940536499
iteration 100, loss 1.9485714435577393
iteration 200, loss 1.8410301208496094
iteration 300, loss 1.8961538076400757
iteration 400, loss 1.950104832649231
iteration 500, loss 1.8111191987991333
iteration 600, loss 1.8485249280929565
iteration 700, loss 1.8510236740112305
iteration 800, loss 1.7426793575286865
iteration 0, loss 1.781670331954956
iteration 100, loss 1.8247803449630737
iteration 200, loss 1.9263968467712402
iteration 300, loss 1.9038630723953247
iteration 400, loss 1.7850297689437866
iteration 500, loss 1.8667653799057007
iteration 600, loss 1.8703926801681519
iteration 700, loss 1.8778936862945557
iteration 800, loss 1.8972115516662598
iteration 0, loss 1.8051316738128662
iteration 100, loss 1.88621187210083
iteration 200, loss 1.86016845703125
iteration 300, loss 1.9111026525497437
iteration 400, loss 1.8156977891921997
iteration 500, loss 1.7711597681045532
iteration 600, loss 1.901990532875061
iteration 700, loss 1.7892814874649048
iteration 800, loss 1.8093699216842651
iteration 0, loss 1.8761699199676514
iteration 100, loss 1.8359929323196411
iteration 200, loss 1.8323217630386353
iteration 300, loss 1.8659565448760986
iteration 400, loss 1.877216100692749
iteration 500, loss 1.8092342615127563
iteration 600, loss 1.865708589553833
iteration 700, loss 1.778848648071289
iteration 800, loss 1.8221864700317383
iteration 0, loss 1.8787848949432373
iteration 100, loss 1.879942536354065
iteration 200, loss 1.8530290126800537
iteration 300, loss 1.8910993337631226
iteration 400, loss 1.8117095232009888
iteration 500, loss 1.800917148590088
iteration 600, loss 1.878200888633728
iteration 700, loss 1.8755381107330322
iteration 800, loss 1.7911945581436157
iteration 0, loss 1.8608746528625488
iteration 100, loss 1.8634366989135742
iteration 200, loss 1.903376579284668
iteration 300, loss 1.835660696029663
iteration 400, loss 1.7908598184585571
iteration 500, loss 1.8305171728134155
iteration 600, loss 1.936225414276123
iteration 700, loss 1.936212420463562
iteration 800, loss 1.8128284215927124
iteration 0, loss 1.803400993347168
iteration 100, loss 1.79073166847229
iteration 200, loss 1.8380054235458374
iteration 300, loss 1.848258376121521
iteration 400, loss 1.8569135665893555
iteration 500, loss 1.9006073474884033
iteration 600, loss 1.8615598678588867
iteration 700, loss 1.910895586013794
iteration 800, loss 1.823928713798523
iteration 0, loss 1.7873916625976562
iteration 100, loss 1.8367785215377808
iteration 200, loss 1.8210244178771973
iteration 300, loss 1.8299014568328857
iteration 400, loss 1.884568214416504
iteration 500, loss 1.8829777240753174
iteration 600, loss 1.8484320640563965
iteration 700, loss 1.82037353515625
iteration 800, loss 1.8470914363861084
iteration 0, loss 1.8711435794830322
iteration 100, loss 1.8758530616760254
iteration 200, loss 1.810410737991333
iteration 300, loss 1.7920433282852173
iteration 400, loss 1.8357232809066772
iteration 500, loss 1.7866744995117188
iteration 600, loss 1.8038407564163208
iteration 700, loss 1.9191200733184814
iteration 800, loss 1.853553295135498
iteration 0, loss 1.8444430828094482
iteration 100, loss 1.8083891868591309
iteration 200, loss 1.8620301485061646
iteration 300, loss 1.7753791809082031
iteration 400, loss 1.865474820137024
iteration 500, loss 1.7959059476852417
iteration 600, loss 1.87750244140625
iteration 700, loss 1.8943785429000854
iteration 800, loss 1.8557164669036865
iteration 0, loss 1.843462347984314
iteration 100, loss 1.8296089172363281
iteration 200, loss 1.762085199356079
iteration 300, loss 1.8844455480575562
iteration 400, loss 1.8390309810638428
iteration 500, loss 1.8356207609176636
iteration 600, loss 1.8337123394012451
iteration 700, loss 1.8875067234039307
iteration 800, loss 1.8410944938659668
iteration 0, loss 1.8004546165466309
iteration 100, loss 1.8196513652801514
iteration 200, loss 1.847599983215332
iteration 300, loss 1.8108859062194824
iteration 400, loss 1.8107866048812866
iteration 500, loss 1.8308182954788208
iteration 600, loss 1.8452061414718628
iteration 700, loss 1.8851878643035889
iteration 800, loss 1.7491601705551147
iteration 0, loss 1.9101682901382446
iteration 100, loss 1.8230171203613281
iteration 200, loss 1.7934424877166748
iteration 300, loss 1.818718671798706
iteration 400, loss 1.8017950057983398
iteration 500, loss 1.9112523794174194
iteration 600, loss 1.8172370195388794
iteration 700, loss 1.8294081687927246
iteration 800, loss 1.8678317070007324
iteration 0, loss 1.814086675643921
iteration 100, loss 1.7685734033584595
iteration 200, loss 1.85267174243927
iteration 300, loss 1.8325287103652954
iteration 400, loss 1.8207772970199585
iteration 500, loss 1.8702332973480225
iteration 600, loss 1.8113372325897217
iteration 700, loss 1.783499836921692
iteration 800, loss 1.8093478679656982
iteration 0, loss 1.8713479042053223
iteration 100, loss 1.7426378726959229
iteration 200, loss 1.8364555835723877
iteration 300, loss 1.8100559711456299
iteration 400, loss 1.8304883241653442
iteration 500, loss 1.9057037830352783
iteration 600, loss 1.8715533018112183
iteration 700, loss 1.8498542308807373
iteration 800, loss 1.8030288219451904
iteration 0, loss 1.8650825023651123
iteration 100, loss 1.8517531156539917
iteration 200, loss 1.8301455974578857
iteration 300, loss 1.8577966690063477
iteration 400, loss 1.8747315406799316
iteration 500, loss 1.8394588232040405
iteration 600, loss 1.8457305431365967
iteration 700, loss 1.7975924015045166
iteration 800, loss 1.799726128578186
iteration 0, loss 1.8037656545639038
iteration 100, loss 1.799959421157837
iteration 200, loss 1.9369115829467773
iteration 300, loss 1.8761695623397827
iteration 400, loss 1.8576606512069702
iteration 500, loss 1.8555384874343872
iteration 600, loss 1.8370357751846313
iteration 700, loss 1.8237907886505127
iteration 800, loss 1.8450520038604736
iteration 0, loss 1.816516637802124
iteration 100, loss 1.7955865859985352
iteration 200, loss 1.8491753339767456
iteration 300, loss 1.864892601966858
iteration 400, loss 1.9240717887878418
iteration 500, loss 1.8235281705856323
iteration 600, loss 1.855970025062561
iteration 700, loss 1.7914769649505615
iteration 800, loss 1.9653539657592773
iteration 0, loss 1.777972936630249
iteration 100, loss 1.8410276174545288
iteration 200, loss 1.861667275428772
iteration 300, loss 1.7661247253417969
iteration 400, loss 1.8413907289505005
iteration 500, loss 1.9116816520690918
iteration 600, loss 1.9345225095748901
iteration 700, loss 1.8291888236999512
iteration 800, loss 1.7491317987442017
iteration 0, loss 1.8567218780517578
iteration 100, loss 1.8136451244354248
iteration 200, loss 1.9084107875823975
iteration 300, loss 1.865919589996338
iteration 400, loss 1.891799807548523
iteration 500, loss 1.9482203722000122
iteration 600, loss 1.925756573677063
iteration 700, loss 1.7938746213912964
iteration 800, loss 1.7716037034988403
iteration 0, loss 1.834926962852478
iteration 100, loss 1.8287216424942017
iteration 200, loss 1.841057300567627
iteration 300, loss 1.8621491193771362
iteration 400, loss 1.8027423620224
iteration 500, loss 1.9472370147705078
iteration 600, loss 1.8436830043792725
iteration 700, loss 1.8763272762298584
iteration 800, loss 1.8773256540298462
iteration 0, loss 1.884139060974121
iteration 100, loss 1.8547292947769165
iteration 200, loss 1.8333697319030762
iteration 300, loss 1.86286199092865
iteration 400, loss 1.8648706674575806
iteration 500, loss 1.8840032815933228
iteration 600, loss 1.8112387657165527
iteration 700, loss 1.7673076391220093
iteration 800, loss 1.8218196630477905
iteration 0, loss 1.9056425094604492
iteration 100, loss 1.8361035585403442
iteration 200, loss 1.8185067176818848
iteration 300, loss 1.9222331047058105
iteration 400, loss 1.7388616800308228
iteration 500, loss 1.8681058883666992
iteration 600, loss 1.9177110195159912
iteration 700, loss 1.8721431493759155
iteration 800, loss 1.9005852937698364
fold 1 accuracy: 0.42614285714285716
iteration 0, loss 1.806795597076416
iteration 100, loss 1.8707820177078247
iteration 200, loss 1.7724705934524536
iteration 300, loss 1.8065646886825562
iteration 400, loss 1.7901594638824463
iteration 500, loss 1.8509328365325928
iteration 600, loss 1.7991737127304077
iteration 700, loss 1.797140121459961
iteration 800, loss 1.8341444730758667
iteration 0, loss 1.855958104133606
iteration 100, loss 1.8643690347671509
iteration 200, loss 1.8501371145248413
iteration 300, loss 1.849735975265503
iteration 400, loss 1.8556342124938965
iteration 500, loss 1.8330235481262207
iteration 600, loss 1.889325499534607
iteration 700, loss 1.9047456979751587
iteration 800, loss 1.8091048002243042
iteration 0, loss 1.8313227891921997
iteration 100, loss 1.861037015914917
iteration 200, loss 1.8457525968551636
iteration 300, loss 1.815910816192627
iteration 400, loss 1.8402496576309204
iteration 500, loss 1.886794090270996
iteration 600, loss 1.9259834289550781
iteration 700, loss 1.8473725318908691
iteration 800, loss 1.7616026401519775
iteration 0, loss 1.8447842597961426
iteration 100, loss 1.8693032264709473
iteration 200, loss 1.8410935401916504
iteration 300, loss 1.8142781257629395
iteration 400, loss 1.7719497680664062
iteration 500, loss 1.8610342741012573
iteration 600, loss 1.9144659042358398
iteration 700, loss 1.7720955610275269
iteration 800, loss 1.849252462387085
iteration 0, loss 1.800006628036499
iteration 100, loss 1.8437602519989014
iteration 200, loss 1.7850300073623657
iteration 300, loss 1.809942603111267
iteration 400, loss 1.8635379076004028
iteration 500, loss 1.9074574708938599
iteration 600, loss 1.934593677520752
iteration 700, loss 1.9168705940246582
iteration 800, loss 1.8808640241622925
iteration 0, loss 1.8364566564559937
iteration 100, loss 1.8511337041854858
iteration 200, loss 1.8270344734191895
iteration 300, loss 1.8703001737594604
iteration 400, loss 1.8685415983200073
iteration 500, loss 1.8468668460845947
iteration 600, loss 1.8246047496795654
iteration 700, loss 1.8951034545898438
iteration 800, loss 1.849838376045227
iteration 0, loss 1.8139275312423706
iteration 100, loss 1.780371069908142
iteration 200, loss 1.8022102117538452
iteration 300, loss 1.8697305917739868
iteration 400, loss 1.8780722618103027
iteration 500, loss 1.8064454793930054
iteration 600, loss 1.827042818069458
iteration 700, loss 1.8630421161651611
iteration 800, loss 1.8145694732666016
iteration 0, loss 1.8385897874832153
iteration 100, loss 1.8157176971435547
iteration 200, loss 1.8657445907592773
iteration 300, loss 1.910466194152832
iteration 400, loss 1.8578879833221436
iteration 500, loss 1.839308500289917
iteration 600, loss 1.8432464599609375
iteration 700, loss 1.8315550088882446
iteration 800, loss 1.8331222534179688
iteration 0, loss 1.8414855003356934
iteration 100, loss 1.8436458110809326
iteration 200, loss 1.830865502357483
iteration 300, loss 1.7954908609390259
iteration 400, loss 1.9322928190231323
iteration 500, loss 1.787207007408142
iteration 600, loss 1.8677130937576294
iteration 700, loss 1.7948766946792603
iteration 800, loss 1.808988094329834
iteration 0, loss 1.8646190166473389
iteration 100, loss 1.7887425422668457
iteration 200, loss 1.9297479391098022
iteration 300, loss 1.8226789236068726
iteration 400, loss 1.93025803565979
iteration 500, loss 1.8085442781448364
iteration 600, loss 1.8322994709014893
iteration 700, loss 1.8758631944656372
iteration 800, loss 1.8005849123001099
iteration 0, loss 1.8538981676101685
iteration 100, loss 1.8252397775650024
iteration 200, loss 1.7665945291519165
iteration 300, loss 1.894661784172058
iteration 400, loss 1.8180153369903564
iteration 500, loss 1.8464158773422241
iteration 600, loss 1.9088163375854492
iteration 700, loss 1.8000404834747314
iteration 800, loss 1.93929123878479
iteration 0, loss 1.8369660377502441
iteration 100, loss 1.7965786457061768
iteration 200, loss 1.8778457641601562
iteration 300, loss 1.759129285812378
iteration 400, loss 1.7612555027008057
iteration 500, loss 1.8227555751800537
iteration 600, loss 1.8384836912155151
iteration 700, loss 1.8696396350860596
iteration 800, loss 1.8789801597595215
iteration 0, loss 1.7915058135986328
iteration 100, loss 1.8270761966705322
iteration 200, loss 1.8671823740005493
iteration 300, loss 1.8395004272460938
iteration 400, loss 1.8246124982833862
iteration 500, loss 1.777058482170105
iteration 600, loss 1.883925437927246
iteration 700, loss 1.8662976026535034
iteration 800, loss 1.8295549154281616
iteration 0, loss 1.8299497365951538
iteration 100, loss 1.8679646253585815
iteration 200, loss 1.808241367340088
iteration 300, loss 1.8281943798065186
iteration 400, loss 1.8746140003204346
iteration 500, loss 1.8461741209030151
iteration 600, loss 1.8594300746917725
iteration 700, loss 1.806874394416809
iteration 800, loss 1.8540570735931396
iteration 0, loss 1.785189151763916
iteration 100, loss 1.7695037126541138
iteration 200, loss 1.8542351722717285
iteration 300, loss 1.825754165649414
iteration 400, loss 1.8223683834075928
iteration 500, loss 1.7906121015548706
iteration 600, loss 1.8226934671401978
iteration 700, loss 1.8334124088287354
iteration 800, loss 1.8134821653366089
iteration 0, loss 1.88224196434021
iteration 100, loss 1.8349478244781494
iteration 200, loss 1.8254774808883667
iteration 300, loss 1.8340853452682495
iteration 400, loss 1.8378791809082031
iteration 500, loss 1.806837558746338
iteration 600, loss 1.9172828197479248
iteration 700, loss 1.9108487367630005
iteration 800, loss 1.8742619752883911
iteration 0, loss 1.8074145317077637
iteration 100, loss 1.8342256546020508
iteration 200, loss 1.7966278791427612
iteration 300, loss 1.8814693689346313
iteration 400, loss 1.8093441724777222
iteration 500, loss 1.8995699882507324
iteration 600, loss 1.8362218141555786
iteration 700, loss 1.8693381547927856
iteration 800, loss 1.7976832389831543
iteration 0, loss 1.834067702293396
iteration 100, loss 1.83029043674469
iteration 200, loss 1.8927147388458252
iteration 300, loss 1.8207659721374512
iteration 400, loss 1.7785284519195557
iteration 500, loss 1.8320802450180054
iteration 600, loss 1.852042317390442
iteration 700, loss 1.854297399520874
iteration 800, loss 1.8125666379928589
iteration 0, loss 1.783795714378357
iteration 100, loss 1.7609939575195312
iteration 200, loss 1.8571465015411377
iteration 300, loss 1.7925223112106323
iteration 400, loss 1.810272455215454
iteration 500, loss 1.8700146675109863
iteration 600, loss 1.8525924682617188
iteration 700, loss 1.8315136432647705
iteration 800, loss 1.7865958213806152
iteration 0, loss 1.9019193649291992
iteration 100, loss 1.852750301361084
iteration 200, loss 1.8752880096435547
iteration 300, loss 1.7855126857757568
iteration 400, loss 1.9198660850524902
iteration 500, loss 1.7572836875915527
iteration 600, loss 1.9344323873519897
iteration 700, loss 1.7707135677337646
iteration 800, loss 1.8302974700927734
iteration 0, loss 1.8317923545837402
iteration 100, loss 1.7564345598220825
iteration 200, loss 1.7933783531188965
iteration 300, loss 1.8425641059875488
iteration 400, loss 1.8174890279769897
iteration 500, loss 1.8717021942138672
iteration 600, loss 1.8512080907821655
iteration 700, loss 1.7856007814407349
iteration 800, loss 1.8456169366836548
iteration 0, loss 1.869811773300171
iteration 100, loss 1.8921189308166504
iteration 200, loss 1.7872806787490845
iteration 300, loss 1.8777469396591187
iteration 400, loss 1.8580454587936401
iteration 500, loss 1.8544164896011353
iteration 600, loss 1.8419713973999023
iteration 700, loss 1.8013205528259277
iteration 800, loss 1.846615195274353
iteration 0, loss 1.8343064785003662
iteration 100, loss 1.7870054244995117
iteration 200, loss 1.8425565958023071
iteration 300, loss 1.781601071357727
iteration 400, loss 1.818832278251648
iteration 500, loss 1.8445279598236084
iteration 600, loss 1.8376890420913696
iteration 700, loss 1.9294244050979614
iteration 800, loss 1.8856240510940552
iteration 0, loss 1.8317441940307617
iteration 100, loss 1.864248275756836
iteration 200, loss 1.9067184925079346
iteration 300, loss 1.9028922319412231
iteration 400, loss 1.8177367448806763
iteration 500, loss 1.868219256401062
iteration 600, loss 1.8258628845214844
iteration 700, loss 1.8695125579833984
iteration 800, loss 1.867645025253296
iteration 0, loss 1.8484667539596558
iteration 100, loss 2.015502452850342
iteration 200, loss 1.9034266471862793
iteration 300, loss 1.7547647953033447
iteration 400, loss 1.7916291952133179
iteration 500, loss 1.8231563568115234
iteration 600, loss 1.8870960474014282
iteration 700, loss 1.8189678192138672
iteration 800, loss 1.8605012893676758
iteration 0, loss 1.7948209047317505
iteration 100, loss 1.8686630725860596
iteration 200, loss 1.8575841188430786
iteration 300, loss 1.8131263256072998
iteration 400, loss 1.7847298383712769
iteration 500, loss 1.9675254821777344
iteration 600, loss 1.7883827686309814
iteration 700, loss 1.8609682321548462
iteration 800, loss 1.8855582475662231
iteration 0, loss 1.7831395864486694
iteration 100, loss 1.8352081775665283
iteration 200, loss 1.835713505744934
iteration 300, loss 1.8310273885726929
iteration 400, loss 1.9283404350280762
iteration 500, loss 1.8603546619415283
iteration 600, loss 1.802651047706604
iteration 700, loss 1.8113226890563965
iteration 800, loss 1.8656740188598633
iteration 0, loss 1.789289116859436
iteration 100, loss 1.7954992055892944
iteration 200, loss 1.8683288097381592
iteration 300, loss 1.8376494646072388
iteration 400, loss 1.8393806219100952
iteration 500, loss 1.819023847579956
iteration 600, loss 1.8124970197677612
iteration 700, loss 1.9144783020019531
iteration 800, loss 1.7824161052703857
iteration 0, loss 1.8707712888717651
iteration 100, loss 1.8187874555587769
iteration 200, loss 1.8440910577774048
iteration 300, loss 1.815352439880371
iteration 400, loss 1.8584684133529663
iteration 500, loss 1.803861379623413
iteration 600, loss 1.8310867547988892
iteration 700, loss 1.7944672107696533
iteration 800, loss 1.8506819009780884
iteration 0, loss 1.801737904548645
iteration 100, loss 1.8582139015197754
iteration 200, loss 1.84272038936615
iteration 300, loss 1.8586918115615845
iteration 400, loss 1.9101066589355469
iteration 500, loss 1.822323203086853
iteration 600, loss 1.8515892028808594
iteration 700, loss 1.8999935388565063
iteration 800, loss 1.860961675643921
iteration 0, loss 1.8650333881378174
iteration 100, loss 1.8936609029769897
iteration 200, loss 1.8173185586929321
iteration 300, loss 1.8765735626220703
iteration 400, loss 1.8622652292251587
iteration 500, loss 1.8070811033248901
iteration 600, loss 1.8984490633010864
iteration 700, loss 1.877853274345398
iteration 800, loss 1.8392131328582764
iteration 0, loss 1.8337119817733765
iteration 100, loss 1.8891565799713135
iteration 200, loss 1.8346374034881592
iteration 300, loss 1.8009111881256104
iteration 400, loss 1.8643079996109009
iteration 500, loss 1.8806087970733643
iteration 600, loss 1.806365728378296
iteration 700, loss 1.8994184732437134
iteration 800, loss 1.850987434387207
iteration 0, loss 1.8129390478134155
iteration 100, loss 1.8320612907409668
iteration 200, loss 1.842244029045105
iteration 300, loss 1.8224948644638062
iteration 400, loss 1.9109505414962769
iteration 500, loss 1.894242525100708
iteration 600, loss 1.8397694826126099
iteration 700, loss 1.7805864810943604
iteration 800, loss 1.8809279203414917
iteration 0, loss 1.8389686346054077
iteration 100, loss 1.9320082664489746
iteration 200, loss 1.782963752746582
iteration 300, loss 1.8172228336334229
iteration 400, loss 1.8497411012649536
iteration 500, loss 1.8296467065811157
iteration 600, loss 1.768031120300293
iteration 700, loss 1.903887152671814
iteration 800, loss 1.7776352167129517
iteration 0, loss 1.7890632152557373
iteration 100, loss 1.8512030839920044
iteration 200, loss 1.8247171640396118
iteration 300, loss 1.8746886253356934
iteration 400, loss 1.7764649391174316
iteration 500, loss 1.874556064605713
iteration 600, loss 1.8228411674499512
iteration 700, loss 1.8458715677261353
iteration 800, loss 1.777571201324463
iteration 0, loss 1.817639708518982
iteration 100, loss 1.8815672397613525
iteration 200, loss 1.8941855430603027
iteration 300, loss 1.7935764789581299
iteration 400, loss 1.8804980516433716
iteration 500, loss 1.826189637184143
iteration 600, loss 1.8715646266937256
iteration 700, loss 1.8234665393829346
iteration 800, loss 1.844504952430725
iteration 0, loss 1.8604600429534912
iteration 100, loss 1.9065096378326416
iteration 200, loss 1.8614217042922974
iteration 300, loss 1.8060035705566406
iteration 400, loss 1.8267630338668823
iteration 500, loss 1.8500335216522217
iteration 600, loss 1.8359007835388184
iteration 700, loss 1.7876362800598145
iteration 800, loss 1.8761858940124512
iteration 0, loss 1.750423550605774
iteration 100, loss 1.9307582378387451
iteration 200, loss 1.8060615062713623
iteration 300, loss 1.9312801361083984
iteration 400, loss 1.8895807266235352
iteration 500, loss 1.8169456720352173
iteration 600, loss 1.8519667387008667
iteration 700, loss 1.8483545780181885
iteration 800, loss 1.8382174968719482
iteration 0, loss 1.750327229499817
iteration 100, loss 1.8082702159881592
iteration 200, loss 1.8443336486816406
iteration 300, loss 1.789802074432373
iteration 400, loss 1.8562589883804321
iteration 500, loss 1.8647065162658691
iteration 600, loss 1.887109398841858
iteration 700, loss 1.8641726970672607
iteration 800, loss 1.8709895610809326
iteration 0, loss 1.8446406126022339
iteration 100, loss 1.779380202293396
iteration 200, loss 1.8531856536865234
iteration 300, loss 1.8874212503433228
iteration 400, loss 1.8104976415634155
iteration 500, loss 1.7245477437973022
iteration 600, loss 1.8200538158416748
iteration 700, loss 1.8343039751052856
iteration 800, loss 1.8478596210479736
iteration 0, loss 1.9078409671783447
iteration 100, loss 1.9627711772918701
iteration 200, loss 1.7787659168243408
iteration 300, loss 1.7936182022094727
iteration 400, loss 1.8260594606399536
iteration 500, loss 1.8722150325775146
iteration 600, loss 1.8015497922897339
iteration 700, loss 1.803855299949646
iteration 800, loss 1.7783470153808594
iteration 0, loss 1.8304871320724487
iteration 100, loss 1.79988694190979
iteration 200, loss 1.8157764673233032
iteration 300, loss 1.8098795413970947
iteration 400, loss 1.7979590892791748
iteration 500, loss 1.8149774074554443
iteration 600, loss 1.7565715312957764
iteration 700, loss 1.823350191116333
iteration 800, loss 1.865309715270996
iteration 0, loss 1.8438400030136108
iteration 100, loss 1.8886617422103882
iteration 200, loss 1.790166974067688
iteration 300, loss 1.840409517288208
iteration 400, loss 1.836996078491211
iteration 500, loss 1.8346478939056396
iteration 600, loss 1.8627742528915405
iteration 700, loss 1.8473173379898071
iteration 800, loss 1.9132264852523804
iteration 0, loss 1.8869450092315674
iteration 100, loss 1.8064714670181274
iteration 200, loss 1.8755550384521484
iteration 300, loss 1.8341821432113647
iteration 400, loss 1.7965787649154663
iteration 500, loss 1.8021190166473389
iteration 600, loss 1.758630633354187
iteration 700, loss 1.7536300420761108
iteration 800, loss 1.8359229564666748
iteration 0, loss 1.8416751623153687
iteration 100, loss 1.8117618560791016
iteration 200, loss 1.8039233684539795
iteration 300, loss 1.799496054649353
iteration 400, loss 1.8507810831069946
iteration 500, loss 1.8079229593276978
iteration 600, loss 1.8690464496612549
iteration 700, loss 1.9354019165039062
iteration 800, loss 1.8251643180847168
iteration 0, loss 1.8207377195358276
iteration 100, loss 1.877619981765747
iteration 200, loss 1.871907353401184
iteration 300, loss 1.8108837604522705
iteration 400, loss 1.8267254829406738
iteration 500, loss 1.8655592203140259
iteration 600, loss 1.8730254173278809
iteration 700, loss 1.8038723468780518
iteration 800, loss 1.8877496719360352
iteration 0, loss 1.8843235969543457
iteration 100, loss 1.8454711437225342
iteration 200, loss 1.8550262451171875
iteration 300, loss 1.8883525133132935
iteration 400, loss 1.8412036895751953
iteration 500, loss 1.8712266683578491
iteration 600, loss 1.8799549341201782
iteration 700, loss 1.8973398208618164
iteration 800, loss 1.7396489381790161
iteration 0, loss 1.8696407079696655
iteration 100, loss 1.8017573356628418
iteration 200, loss 1.8757143020629883
iteration 300, loss 1.8395518064498901
iteration 400, loss 1.8685436248779297
iteration 500, loss 1.7850004434585571
iteration 600, loss 1.8766306638717651
iteration 700, loss 1.9193084239959717
iteration 800, loss 1.8204786777496338
iteration 0, loss 1.767137050628662
iteration 100, loss 1.7891536951065063
iteration 200, loss 1.7260006666183472
iteration 300, loss 1.7940834760665894
iteration 400, loss 1.8609802722930908
iteration 500, loss 1.8026574850082397
iteration 600, loss 1.9168100357055664
iteration 700, loss 1.839647650718689
iteration 800, loss 1.7603979110717773
iteration 0, loss 1.7735110521316528
iteration 100, loss 1.8760454654693604
iteration 200, loss 1.8534259796142578
iteration 300, loss 1.8239727020263672
iteration 400, loss 1.9527572393417358
iteration 500, loss 1.855216145515442
iteration 600, loss 1.8815529346466064
iteration 700, loss 1.8362854719161987
iteration 800, loss 1.8623133897781372
fold 2 accuracy: 0.4125714285714286
iteration 0, loss 1.7824848890304565
iteration 100, loss 1.7408252954483032
iteration 200, loss 1.8025639057159424
iteration 300, loss 1.8302066326141357
iteration 400, loss 1.7975972890853882
iteration 500, loss 1.8116047382354736
iteration 600, loss 1.8916445970535278
iteration 700, loss 1.8376514911651611
iteration 800, loss 1.8181489706039429
iteration 0, loss 1.8093544244766235
iteration 100, loss 1.8135089874267578
iteration 200, loss 1.882459044456482
iteration 300, loss 1.8712247610092163
iteration 400, loss 1.8151161670684814
iteration 500, loss 1.8327685594558716
iteration 600, loss 1.9380167722702026
iteration 700, loss 1.7786288261413574
iteration 800, loss 1.7722609043121338
iteration 0, loss 1.9323135614395142
iteration 100, loss 1.8580673933029175
iteration 200, loss 1.7993659973144531
iteration 300, loss 1.7506821155548096
iteration 400, loss 1.8267186880111694
iteration 500, loss 1.8894459009170532
iteration 600, loss 1.890912413597107
iteration 700, loss 1.8683006763458252
iteration 800, loss 1.7830792665481567
iteration 0, loss 1.9490641355514526
iteration 100, loss 1.8109608888626099
iteration 200, loss 1.870510458946228
iteration 300, loss 1.8109581470489502
iteration 400, loss 1.8436371088027954
iteration 500, loss 1.803529977798462
iteration 600, loss 1.879963755607605
iteration 700, loss 1.833817481994629
iteration 800, loss 1.8063087463378906
iteration 0, loss 1.7634180784225464
iteration 100, loss 1.8058406114578247
iteration 200, loss 1.8204797506332397
iteration 300, loss 1.9209046363830566
iteration 400, loss 1.8132442235946655
iteration 500, loss 1.811453938484192
iteration 600, loss 1.791619062423706
iteration 700, loss 1.8342174291610718
iteration 800, loss 1.8278659582138062
iteration 0, loss 1.7574121952056885
iteration 100, loss 1.9135594367980957
iteration 200, loss 1.803973913192749
iteration 300, loss 1.9319777488708496
iteration 400, loss 1.914534568786621
iteration 500, loss 1.8931083679199219
iteration 600, loss 1.8506146669387817
iteration 700, loss 1.846167802810669
iteration 800, loss 1.8708446025848389
iteration 0, loss 1.832558035850525
iteration 100, loss 1.9123164415359497
iteration 200, loss 1.8342972993850708
iteration 300, loss 1.8043677806854248
iteration 400, loss 1.8276358842849731
iteration 500, loss 1.8127830028533936
iteration 600, loss 1.8749927282333374
iteration 700, loss 1.8275055885314941
iteration 800, loss 1.8943480253219604
iteration 0, loss 1.923973798751831
iteration 100, loss 1.809311032295227
iteration 200, loss 1.8541619777679443
iteration 300, loss 1.8070855140686035
iteration 400, loss 1.779437780380249
iteration 500, loss 1.7782984972000122
iteration 600, loss 1.8319756984710693
iteration 700, loss 1.7995684146881104
iteration 800, loss 1.8797457218170166
iteration 0, loss 1.8500193357467651
iteration 100, loss 1.8111522197723389
iteration 200, loss 1.8659329414367676
iteration 300, loss 1.8345779180526733
iteration 400, loss 1.8151766061782837
iteration 500, loss 1.8401411771774292
iteration 600, loss 1.7926268577575684
iteration 700, loss 1.7939850091934204
iteration 800, loss 1.808356523513794
iteration 0, loss 1.7898880243301392
iteration 100, loss 1.8813016414642334
iteration 200, loss 1.8247685432434082
iteration 300, loss 1.892702579498291
iteration 400, loss 1.8298792839050293
iteration 500, loss 1.7637848854064941
iteration 600, loss 1.8370829820632935
iteration 700, loss 1.8082058429718018
iteration 800, loss 1.9215787649154663
iteration 0, loss 1.79924476146698
iteration 100, loss 1.8091070652008057
iteration 200, loss 1.8668253421783447
iteration 300, loss 1.803134560585022
iteration 400, loss 1.7953526973724365
iteration 500, loss 1.804877519607544
iteration 600, loss 1.809571623802185
iteration 700, loss 1.9151912927627563
iteration 800, loss 1.7475519180297852
iteration 0, loss 1.8853753805160522
iteration 100, loss 1.7549148797988892
iteration 200, loss 1.8160531520843506
iteration 300, loss 1.851887822151184
iteration 400, loss 1.8624000549316406
iteration 500, loss 1.8265011310577393
iteration 600, loss 1.8759424686431885
iteration 700, loss 1.7683717012405396
iteration 800, loss 1.8522363901138306
iteration 0, loss 1.7523229122161865
iteration 100, loss 1.8326770067214966
iteration 200, loss 1.809687852859497
iteration 300, loss 1.7844595909118652
iteration 400, loss 1.7415868043899536
iteration 500, loss 1.8885416984558105
iteration 600, loss 1.8113234043121338
iteration 700, loss 1.8029471635818481
iteration 800, loss 1.7575007677078247
iteration 0, loss 1.776527762413025
iteration 100, loss 1.81294846534729
iteration 200, loss 1.873847484588623
iteration 300, loss 1.8449928760528564
iteration 400, loss 1.8562726974487305
iteration 500, loss 1.8652443885803223
iteration 600, loss 1.792952299118042
iteration 700, loss 1.8277138471603394
iteration 800, loss 1.8585951328277588
iteration 0, loss 1.8868926763534546
iteration 100, loss 1.8045507669448853
iteration 200, loss 1.8256655931472778
iteration 300, loss 1.805689811706543
iteration 400, loss 1.8211065530776978
iteration 500, loss 1.843104600906372
iteration 600, loss 1.8298169374465942
iteration 700, loss 1.8491367101669312
iteration 800, loss 1.9106639623641968
iteration 0, loss 1.7810930013656616
iteration 100, loss 1.8284648656845093
iteration 200, loss 1.8694422245025635
iteration 300, loss 1.781662940979004
iteration 400, loss 1.8523914813995361
iteration 500, loss 1.8647828102111816
iteration 600, loss 1.90316903591156
iteration 700, loss 1.837477207183838
iteration 800, loss 1.786515235900879
iteration 0, loss 1.785892128944397
iteration 100, loss 1.820959210395813
iteration 200, loss 1.9177113771438599
iteration 300, loss 1.858292818069458
iteration 400, loss 1.8342175483703613
iteration 500, loss 1.9225184917449951
iteration 600, loss 1.8256986141204834
iteration 700, loss 1.7580136060714722
iteration 800, loss 1.8969995975494385
iteration 0, loss 1.8706156015396118
iteration 100, loss 1.8009438514709473
iteration 200, loss 1.9183639287948608
iteration 300, loss 1.8250802755355835
iteration 400, loss 1.8067997694015503
iteration 500, loss 1.8349318504333496
iteration 600, loss 1.9180952310562134
iteration 700, loss 1.833858609199524
iteration 800, loss 1.9046066999435425
iteration 0, loss 1.827562928199768
iteration 100, loss 1.8579800128936768
iteration 200, loss 1.8518034219741821
iteration 300, loss 1.8122477531433105
iteration 400, loss 1.8249332904815674
iteration 500, loss 1.7974580526351929
iteration 600, loss 1.812099575996399
iteration 700, loss 1.7178558111190796
iteration 800, loss 1.8039544820785522
iteration 0, loss 1.8241298198699951
iteration 100, loss 1.7666175365447998
iteration 200, loss 1.799750804901123
iteration 300, loss 1.8762569427490234
iteration 400, loss 1.755738377571106
iteration 500, loss 1.8320153951644897
iteration 600, loss 1.837364912033081
iteration 700, loss 1.8358122110366821
iteration 800, loss 1.8583983182907104
iteration 0, loss 1.8006178140640259
iteration 100, loss 1.853445053100586
iteration 200, loss 1.8597930669784546
iteration 300, loss 1.8129745721817017
iteration 400, loss 1.8286081552505493
iteration 500, loss 1.8374327421188354
iteration 600, loss 1.8511866331100464
iteration 700, loss 1.890845537185669
iteration 800, loss 1.9335057735443115
iteration 0, loss 1.8370915651321411
iteration 100, loss 1.7827706336975098
iteration 200, loss 1.9267503023147583
iteration 300, loss 1.9227930307388306
iteration 400, loss 1.9272650480270386
iteration 500, loss 1.7921727895736694
iteration 600, loss 1.818009853363037
iteration 700, loss 1.893612265586853
iteration 800, loss 1.8377726078033447
iteration 0, loss 1.8543970584869385
iteration 100, loss 1.8706189393997192
iteration 200, loss 1.822525978088379
iteration 300, loss 1.781151533126831
iteration 400, loss 1.8397014141082764
iteration 500, loss 1.8266794681549072
iteration 600, loss 1.796267032623291
iteration 700, loss 1.8353503942489624
iteration 800, loss 1.7911920547485352
iteration 0, loss 1.838109016418457
iteration 100, loss 1.8570795059204102
iteration 200, loss 1.8214112520217896
iteration 300, loss 1.8516443967819214
iteration 400, loss 1.8724251985549927
iteration 500, loss 1.7419254779815674
iteration 600, loss 1.834299087524414
iteration 700, loss 1.7934510707855225
iteration 800, loss 1.8485217094421387
iteration 0, loss 1.8292419910430908
iteration 100, loss 1.8563756942749023
iteration 200, loss 1.8726766109466553
iteration 300, loss 1.8346511125564575
iteration 400, loss 1.8299264907836914
iteration 500, loss 1.858152985572815
iteration 600, loss 1.8679914474487305
iteration 700, loss 1.814733862876892
iteration 800, loss 1.8674546480178833
iteration 0, loss 1.904396653175354
iteration 100, loss 1.9360147714614868
iteration 200, loss 1.8717278242111206
iteration 300, loss 1.8548997640609741
iteration 400, loss 1.8888236284255981
iteration 500, loss 1.8605163097381592
iteration 600, loss 1.8786052465438843
iteration 700, loss 1.8558313846588135
iteration 800, loss 1.8316398859024048
iteration 0, loss 1.8190922737121582
iteration 100, loss 1.8569954633712769
iteration 200, loss 1.8141099214553833
iteration 300, loss 1.8474600315093994
iteration 400, loss 1.803920030593872
iteration 500, loss 1.8428372144699097
iteration 600, loss 1.8430982828140259
iteration 700, loss 1.755081295967102
iteration 800, loss 1.806374192237854
iteration 0, loss 1.850799560546875
iteration 100, loss 1.81940495967865
iteration 200, loss 1.8072962760925293
iteration 300, loss 1.8294768333435059
iteration 400, loss 1.8410714864730835
iteration 500, loss 1.8064939975738525
iteration 600, loss 1.9633815288543701
iteration 700, loss 1.8167848587036133
iteration 800, loss 1.8266149759292603
iteration 0, loss 1.8963428735733032
iteration 100, loss 1.8733865022659302
iteration 200, loss 1.7814339399337769
iteration 300, loss 1.8962454795837402
iteration 400, loss 1.8509409427642822
iteration 500, loss 1.7519373893737793
iteration 600, loss 1.7578214406967163
iteration 700, loss 1.7995115518569946
iteration 800, loss 1.9245096445083618
iteration 0, loss 1.7406320571899414
iteration 100, loss 1.8636378049850464
iteration 200, loss 1.8097054958343506
iteration 300, loss 1.8382309675216675
iteration 400, loss 1.8448996543884277
iteration 500, loss 1.8143987655639648
iteration 600, loss 1.7894197702407837
iteration 700, loss 1.8497464656829834
iteration 800, loss 1.8485829830169678
iteration 0, loss 1.8842424154281616
iteration 100, loss 1.8927929401397705
iteration 200, loss 1.814402461051941
iteration 300, loss 1.7984414100646973
iteration 400, loss 1.802616834640503
iteration 500, loss 1.8536841869354248
iteration 600, loss 1.7658185958862305
iteration 700, loss 1.8686357736587524
iteration 800, loss 1.873283863067627
iteration 0, loss 1.8529787063598633
iteration 100, loss 1.8301563262939453
iteration 200, loss 1.8007556200027466
iteration 300, loss 1.885605812072754
iteration 400, loss 1.8675439357757568
iteration 500, loss 1.8728030920028687
iteration 600, loss 1.8498947620391846
iteration 700, loss 1.8444393873214722
iteration 800, loss 1.8209223747253418
iteration 0, loss 1.8241335153579712
iteration 100, loss 1.8739749193191528
iteration 200, loss 1.9288585186004639
iteration 300, loss 1.761786699295044
iteration 400, loss 1.8935489654541016
iteration 500, loss 1.8905279636383057
iteration 600, loss 1.797837495803833
iteration 700, loss 1.8438801765441895
iteration 800, loss 1.8248955011367798
iteration 0, loss 1.8270047903060913
iteration 100, loss 1.8204668760299683
iteration 200, loss 1.8719885349273682
iteration 300, loss 1.8440641164779663
iteration 400, loss 1.7682956457138062
iteration 500, loss 1.7601221799850464
iteration 600, loss 1.908493995666504
iteration 700, loss 1.8473342657089233
iteration 800, loss 1.877255916595459
iteration 0, loss 1.8241859674453735
iteration 100, loss 1.7916679382324219
iteration 200, loss 1.8490478992462158
iteration 300, loss 1.8467351198196411
iteration 400, loss 1.831549882888794
iteration 500, loss 1.8574707508087158
iteration 600, loss 1.842166781425476
iteration 700, loss 1.842033863067627
iteration 800, loss 1.791698694229126
iteration 0, loss 1.8093618154525757
iteration 100, loss 1.9123948812484741
iteration 200, loss 1.8832277059555054
iteration 300, loss 1.8242381811141968
iteration 400, loss 1.8458675146102905
iteration 500, loss 1.8458774089813232
iteration 600, loss 1.7728484869003296
iteration 700, loss 1.8025848865509033
iteration 800, loss 1.8313565254211426
iteration 0, loss 1.8538818359375
iteration 100, loss 1.8643659353256226
iteration 200, loss 1.7831672430038452
iteration 300, loss 1.8616341352462769
iteration 400, loss 1.7810287475585938
iteration 500, loss 1.7606254816055298
iteration 600, loss 1.8432623147964478
iteration 700, loss 1.8383055925369263
iteration 800, loss 1.7717186212539673
iteration 0, loss 1.8081676959991455
iteration 100, loss 1.8029288053512573
iteration 200, loss 1.8207226991653442
iteration 300, loss 1.7976058721542358
iteration 400, loss 1.7754124402999878
iteration 500, loss 1.7700576782226562
iteration 600, loss 1.8006179332733154
iteration 700, loss 1.8475639820098877
iteration 800, loss 1.8094192743301392
iteration 0, loss 1.9059839248657227
iteration 100, loss 1.9053584337234497
iteration 200, loss 1.819535732269287
iteration 300, loss 1.8447359800338745
iteration 400, loss 1.8096140623092651
iteration 500, loss 1.9497690200805664
iteration 600, loss 1.78778874874115
iteration 700, loss 1.9089858531951904
iteration 800, loss 1.796875238418579
iteration 0, loss 1.8790061473846436
iteration 100, loss 1.8569053411483765
iteration 200, loss 1.8402436971664429
iteration 300, loss 1.878101110458374
iteration 400, loss 1.8731846809387207
iteration 500, loss 1.797878623008728
iteration 600, loss 1.798754096031189
iteration 700, loss 1.7758879661560059
iteration 800, loss 1.8733006715774536
iteration 0, loss 1.8211041688919067
iteration 100, loss 1.9077918529510498
iteration 200, loss 1.7935104370117188
iteration 300, loss 1.8313889503479004
iteration 400, loss 1.8454643487930298
iteration 500, loss 1.8062403202056885
iteration 600, loss 1.8282946348190308
iteration 700, loss 1.8055083751678467
iteration 800, loss 1.8304722309112549
iteration 0, loss 1.8919849395751953
iteration 100, loss 1.8280912637710571
iteration 200, loss 1.8313517570495605
iteration 300, loss 1.7936286926269531
iteration 400, loss 1.7669484615325928
iteration 500, loss 1.835907220840454
iteration 600, loss 1.8615707159042358
iteration 700, loss 1.82792067527771
iteration 800, loss 1.7918556928634644
iteration 0, loss 1.8430893421173096
iteration 100, loss 1.7923911809921265
iteration 200, loss 1.8784544467926025
iteration 300, loss 1.8392952680587769
iteration 400, loss 1.8101379871368408
iteration 500, loss 1.7962435483932495
iteration 600, loss 1.8206195831298828
iteration 700, loss 1.8281939029693604
iteration 800, loss 1.77997624874115
iteration 0, loss 1.8516567945480347
iteration 100, loss 1.788989543914795
iteration 200, loss 1.84810209274292
iteration 300, loss 1.8613176345825195
iteration 400, loss 1.9184457063674927
iteration 500, loss 1.8628733158111572
iteration 600, loss 1.8514423370361328
iteration 700, loss 1.793131947517395
iteration 800, loss 1.9073594808578491
iteration 0, loss 1.840067744255066
iteration 100, loss 1.8252050876617432
iteration 200, loss 1.945345401763916
iteration 300, loss 1.8375762701034546
iteration 400, loss 1.903051495552063
iteration 500, loss 1.742143988609314
iteration 600, loss 1.850629210472107
iteration 700, loss 1.7439287900924683
iteration 800, loss 1.9192345142364502
iteration 0, loss 1.850003719329834
iteration 100, loss 1.9149483442306519
iteration 200, loss 1.8088223934173584
iteration 300, loss 1.7918808460235596
iteration 400, loss 1.7867070436477661
iteration 500, loss 1.8661086559295654
iteration 600, loss 1.8367043733596802
iteration 700, loss 1.8031562566757202
iteration 800, loss 1.8210594654083252
iteration 0, loss 1.8015413284301758
iteration 100, loss 1.871373176574707
iteration 200, loss 1.8198384046554565
iteration 300, loss 1.8056519031524658
iteration 400, loss 1.8662729263305664
iteration 500, loss 1.8040132522583008
iteration 600, loss 1.8288780450820923
iteration 700, loss 1.9173686504364014
iteration 800, loss 1.8492412567138672
iteration 0, loss 1.8143579959869385
iteration 100, loss 1.7747288942337036
iteration 200, loss 1.8438973426818848
iteration 300, loss 1.8007203340530396
iteration 400, loss 1.8540929555892944
iteration 500, loss 1.9021779298782349
iteration 600, loss 1.8531856536865234
iteration 700, loss 1.8282155990600586
iteration 800, loss 1.8554027080535889
iteration 0, loss 1.7699874639511108
iteration 100, loss 1.857904314994812
iteration 200, loss 1.8246114253997803
iteration 300, loss 1.7792078256607056
iteration 400, loss 1.8537847995758057
iteration 500, loss 1.8202123641967773
iteration 600, loss 1.7713563442230225
iteration 700, loss 1.786854863166809
iteration 800, loss 1.8430960178375244
iteration 0, loss 1.8410284519195557
iteration 100, loss 1.8905227184295654
iteration 200, loss 1.8255032300949097
iteration 300, loss 1.80434250831604
iteration 400, loss 1.838866114616394
iteration 500, loss 1.7515430450439453
iteration 600, loss 1.8623324632644653
iteration 700, loss 1.797409176826477
iteration 800, loss 1.8834394216537476
fold 3 accuracy: 0.4139285714285714
iteration 0, loss 1.8788681030273438
iteration 100, loss 1.8876287937164307
iteration 200, loss 1.8739007711410522
iteration 300, loss 1.8031946420669556
iteration 400, loss 1.8727473020553589
iteration 500, loss 1.7754967212677002
iteration 600, loss 1.820355772972107
iteration 700, loss 1.821446418762207
iteration 800, loss 1.8392622470855713
iteration 0, loss 1.7715256214141846
iteration 100, loss 1.8709310293197632
iteration 200, loss 1.8311110734939575
iteration 300, loss 1.7525475025177002
iteration 400, loss 1.8352166414260864
iteration 500, loss 1.789175271987915
iteration 600, loss 1.8528907299041748
iteration 700, loss 1.893007516860962
iteration 800, loss 1.8364745378494263
iteration 0, loss 1.7953850030899048
iteration 100, loss 1.8182450532913208
iteration 200, loss 1.8485037088394165
iteration 300, loss 1.8487586975097656
iteration 400, loss 1.7928173542022705
iteration 500, loss 1.8906219005584717
iteration 600, loss 1.79068922996521
iteration 700, loss 1.79672372341156
iteration 800, loss 1.8121342658996582
iteration 0, loss 1.8714903593063354
iteration 100, loss 1.7718088626861572
iteration 200, loss 1.9048528671264648
iteration 300, loss 1.8878517150878906
iteration 400, loss 1.920911192893982
iteration 500, loss 1.9059321880340576
iteration 600, loss 1.8511050939559937
iteration 700, loss 1.858814001083374
iteration 800, loss 1.8936680555343628
iteration 0, loss 1.829648733139038
iteration 100, loss 1.8581786155700684
iteration 200, loss 1.8365538120269775
iteration 300, loss 1.7838685512542725
iteration 400, loss 1.8994295597076416
iteration 500, loss 1.8832770586013794
iteration 600, loss 1.7887393236160278
iteration 700, loss 1.9272102117538452
iteration 800, loss 1.7519208192825317
iteration 0, loss 1.8305233716964722
iteration 100, loss 1.774389386177063
iteration 200, loss 1.8150311708450317
iteration 300, loss 1.9134145975112915
iteration 400, loss 1.844353437423706
iteration 500, loss 1.8022701740264893
iteration 600, loss 1.8676095008850098
iteration 700, loss 1.8313747644424438
iteration 800, loss 1.8854482173919678
iteration 0, loss 1.805875301361084
iteration 100, loss 1.8180838823318481
iteration 200, loss 1.7817343473434448
iteration 300, loss 1.7476129531860352
iteration 400, loss 1.8657039403915405
iteration 500, loss 1.818990707397461
iteration 600, loss 1.8979501724243164
iteration 700, loss 1.82673180103302
iteration 800, loss 1.8099309206008911
iteration 0, loss 1.753644585609436
iteration 100, loss 1.8608826398849487
iteration 200, loss 1.8168777227401733
iteration 300, loss 1.794895887374878
iteration 400, loss 1.8073722124099731
iteration 500, loss 1.7903350591659546
iteration 600, loss 1.8215776681900024
iteration 700, loss 1.8388558626174927
iteration 800, loss 1.791817545890808
iteration 0, loss 1.8471848964691162
iteration 100, loss 1.809426188468933
iteration 200, loss 1.786928415298462
iteration 300, loss 1.8641952276229858
iteration 400, loss 1.79440176486969
iteration 500, loss 1.856637716293335
iteration 600, loss 1.9328773021697998
iteration 700, loss 1.8328273296356201
iteration 800, loss 1.7921861410140991
iteration 0, loss 1.84019935131073
iteration 100, loss 1.8199394941329956
iteration 200, loss 1.7828953266143799
iteration 300, loss 1.9409079551696777
iteration 400, loss 1.8999311923980713
iteration 500, loss 1.8945131301879883
iteration 600, loss 1.9399968385696411
iteration 700, loss 1.8743714094161987
iteration 800, loss 1.8252230882644653
iteration 0, loss 1.8486570119857788
iteration 100, loss 1.8405461311340332
iteration 200, loss 1.802435278892517
iteration 300, loss 1.86238694190979
iteration 400, loss 1.779726266860962
iteration 500, loss 1.7873210906982422
iteration 600, loss 1.8777838945388794
iteration 700, loss 1.8406673669815063
iteration 800, loss 1.8437652587890625
iteration 0, loss 1.815349817276001
iteration 100, loss 1.8849735260009766
iteration 200, loss 1.7800827026367188
iteration 300, loss 1.8581632375717163
iteration 400, loss 1.808036208152771
iteration 500, loss 1.7955834865570068
iteration 600, loss 1.8551042079925537
iteration 700, loss 1.8464019298553467
iteration 800, loss 1.7376610040664673
iteration 0, loss 1.8111931085586548
iteration 100, loss 1.83578360080719
iteration 200, loss 1.7521027326583862
iteration 300, loss 1.892497181892395
iteration 400, loss 1.8549007177352905
iteration 500, loss 1.7643545866012573
iteration 600, loss 1.8139736652374268
iteration 700, loss 1.7959226369857788
iteration 800, loss 1.8655728101730347
iteration 0, loss 1.7950000762939453
iteration 100, loss 1.7938728332519531
iteration 200, loss 1.8448913097381592
iteration 300, loss 1.8378459215164185
iteration 400, loss 1.866652250289917
iteration 500, loss 1.846034049987793
iteration 600, loss 1.8045740127563477
iteration 700, loss 1.8462175130844116
iteration 800, loss 1.7904387712478638
iteration 0, loss 1.8303340673446655
iteration 100, loss 1.803122878074646
iteration 200, loss 1.7527439594268799
iteration 300, loss 1.8124775886535645
iteration 400, loss 1.8176854848861694
iteration 500, loss 1.8545957803726196
iteration 600, loss 1.8717122077941895
iteration 700, loss 1.818427324295044
iteration 800, loss 1.888685703277588
iteration 0, loss 1.8739343881607056
iteration 100, loss 1.7786153554916382
iteration 200, loss 1.9492268562316895
iteration 300, loss 1.8785948753356934
iteration 400, loss 1.8904838562011719
iteration 500, loss 1.8149045705795288
iteration 600, loss 1.8577359914779663
iteration 700, loss 1.7592841386795044
iteration 800, loss 1.8150051832199097
iteration 0, loss 1.8196312189102173
iteration 100, loss 1.8534142971038818
iteration 200, loss 1.846348524093628
iteration 300, loss 1.832170009613037
iteration 400, loss 1.894378662109375
iteration 500, loss 1.8611677885055542
iteration 600, loss 1.8319385051727295
iteration 700, loss 1.815463900566101
iteration 800, loss 1.8556764125823975
iteration 0, loss 1.8991034030914307
iteration 100, loss 1.913432002067566
iteration 200, loss 1.8151884078979492
iteration 300, loss 1.8422961235046387
iteration 400, loss 1.7499618530273438
iteration 500, loss 1.8857262134552002
iteration 600, loss 1.8154884576797485
iteration 700, loss 1.7521964311599731
iteration 800, loss 1.9164152145385742
iteration 0, loss 1.7926808595657349
iteration 100, loss 1.8180080652236938
iteration 200, loss 1.8875722885131836
iteration 300, loss 1.8132182359695435
iteration 400, loss 1.838363766670227
iteration 500, loss 1.7577449083328247
iteration 600, loss 1.8359153270721436
iteration 700, loss 1.9020206928253174
iteration 800, loss 1.875862956047058
iteration 0, loss 1.8705130815505981
iteration 100, loss 1.8004026412963867
iteration 200, loss 1.8154000043869019
iteration 300, loss 1.8859617710113525
iteration 400, loss 1.7736263275146484
iteration 500, loss 1.8081945180892944
iteration 600, loss 1.833838939666748
iteration 700, loss 1.7836596965789795
iteration 800, loss 1.8309409618377686
iteration 0, loss 1.8190696239471436
iteration 100, loss 1.7957286834716797
iteration 200, loss 1.7622179985046387
iteration 300, loss 1.805113673210144
iteration 400, loss 1.856261134147644
iteration 500, loss 1.8249669075012207
iteration 600, loss 1.7882659435272217
iteration 700, loss 1.8114521503448486
iteration 800, loss 1.837712287902832
iteration 0, loss 1.8893284797668457
iteration 100, loss 1.8088467121124268
iteration 200, loss 1.864365577697754
iteration 300, loss 1.8672869205474854
iteration 400, loss 1.7993279695510864
iteration 500, loss 1.773000717163086
iteration 600, loss 1.8314026594161987
iteration 700, loss 1.8013224601745605
iteration 800, loss 1.8190338611602783
iteration 0, loss 1.8034653663635254
iteration 100, loss 1.813850998878479
iteration 200, loss 1.8007965087890625
iteration 300, loss 1.8720003366470337
iteration 400, loss 1.8103435039520264
iteration 500, loss 1.8091223239898682
iteration 600, loss 1.977474570274353
iteration 700, loss 1.8449293375015259
iteration 800, loss 1.8502638339996338
iteration 0, loss 1.8782638311386108
iteration 100, loss 1.841224193572998
iteration 200, loss 1.8426928520202637
iteration 300, loss 1.8838112354278564
iteration 400, loss 1.804449200630188
iteration 500, loss 1.864273190498352
iteration 600, loss 1.8783464431762695
iteration 700, loss 1.9164572954177856
iteration 800, loss 1.8428624868392944
iteration 0, loss 1.7425010204315186
iteration 100, loss 1.8403226137161255
iteration 200, loss 1.8180344104766846
iteration 300, loss 1.8584232330322266
iteration 400, loss 1.8071167469024658
iteration 500, loss 1.8515815734863281
iteration 600, loss 1.816312313079834
iteration 700, loss 1.8538414239883423
iteration 800, loss 1.8534482717514038
iteration 0, loss 1.8294843435287476
iteration 100, loss 1.8302606344223022
iteration 200, loss 1.8825865983963013
iteration 300, loss 1.864323616027832
iteration 400, loss 1.8369415998458862
iteration 500, loss 1.857483148574829
iteration 600, loss 1.8663209676742554
iteration 700, loss 1.8251250982284546
iteration 800, loss 1.8879164457321167
iteration 0, loss 1.8530200719833374
iteration 100, loss 1.8511580228805542
iteration 200, loss 1.8296034336090088
iteration 300, loss 1.8319600820541382
iteration 400, loss 1.7869985103607178
iteration 500, loss 1.864877462387085
iteration 600, loss 1.7939634323120117
iteration 700, loss 1.8960663080215454
iteration 800, loss 1.774749517440796
iteration 0, loss 1.872509241104126
iteration 100, loss 1.820588231086731
iteration 200, loss 1.8637890815734863
iteration 300, loss 1.7810953855514526
iteration 400, loss 1.8650389909744263
iteration 500, loss 1.8058791160583496
iteration 600, loss 1.7980693578720093
iteration 700, loss 1.8677974939346313
iteration 800, loss 1.9103388786315918
iteration 0, loss 1.8330146074295044
iteration 100, loss 1.7891933917999268
iteration 200, loss 1.8048365116119385
iteration 300, loss 1.854536771774292
iteration 400, loss 1.7711564302444458
iteration 500, loss 1.9059842824935913
iteration 600, loss 1.84269118309021
iteration 700, loss 1.7780424356460571
iteration 800, loss 1.7950834035873413
iteration 0, loss 1.7953357696533203
iteration 100, loss 1.7929402589797974
iteration 200, loss 1.849048137664795
iteration 300, loss 1.8592369556427002
iteration 400, loss 1.8670494556427002
iteration 500, loss 1.8024910688400269
iteration 600, loss 1.894321322441101
iteration 700, loss 1.8467339277267456
iteration 800, loss 1.8436901569366455
iteration 0, loss 1.8814435005187988
iteration 100, loss 1.8831430673599243
iteration 200, loss 1.8380509614944458
iteration 300, loss 1.817580223083496
iteration 400, loss 1.8644963502883911
iteration 500, loss 1.8584229946136475
iteration 600, loss 1.8443704843521118
iteration 700, loss 1.828601598739624
iteration 800, loss 1.7725610733032227
iteration 0, loss 1.8060551881790161
iteration 100, loss 1.9424965381622314
iteration 200, loss 1.9382649660110474
iteration 300, loss 1.8414971828460693
iteration 400, loss 1.8380135297775269
iteration 500, loss 1.8192996978759766
iteration 600, loss 1.8347901105880737
iteration 700, loss 1.8807122707366943
iteration 800, loss 1.7899603843688965
iteration 0, loss 1.956836462020874
iteration 100, loss 1.7929461002349854
iteration 200, loss 1.8252720832824707
iteration 300, loss 1.8856874704360962
iteration 400, loss 1.85390305519104
iteration 500, loss 1.7992699146270752
iteration 600, loss 1.8518298864364624
iteration 700, loss 1.9283883571624756
iteration 800, loss 1.7860523462295532
iteration 0, loss 1.8616541624069214
iteration 100, loss 1.8704391717910767
iteration 200, loss 1.8303464651107788
iteration 300, loss 1.7849582433700562
iteration 400, loss 1.8489130735397339
iteration 500, loss 1.9433308839797974
iteration 600, loss 1.7852489948272705
iteration 700, loss 1.8882454633712769
iteration 800, loss 1.8794236183166504
iteration 0, loss 1.8100838661193848
iteration 100, loss 1.8270931243896484
iteration 200, loss 1.8783061504364014
iteration 300, loss 1.8372808694839478
iteration 400, loss 1.7855572700500488
iteration 500, loss 1.8131017684936523
iteration 600, loss 1.8994437456130981
iteration 700, loss 1.8953651189804077
iteration 800, loss 1.8150216341018677
iteration 0, loss 1.798362135887146
iteration 100, loss 1.944832444190979
iteration 200, loss 1.804362177848816
iteration 300, loss 1.8587697744369507
iteration 400, loss 1.8230090141296387
iteration 500, loss 1.8270771503448486
iteration 600, loss 1.7907735109329224
iteration 700, loss 1.814972996711731
iteration 800, loss 1.797874093055725
iteration 0, loss 1.7797101736068726
iteration 100, loss 1.8112249374389648
iteration 200, loss 1.8027944564819336
iteration 300, loss 1.7757283449172974
iteration 400, loss 1.7348555326461792
iteration 500, loss 1.890718936920166
iteration 600, loss 1.8968075513839722
iteration 700, loss 1.8122869729995728
iteration 800, loss 1.8179785013198853
iteration 0, loss 1.752256989479065
iteration 100, loss 1.8218966722488403
iteration 200, loss 1.8305641412734985
iteration 300, loss 1.8083820343017578
iteration 400, loss 1.787859559059143
iteration 500, loss 1.782783031463623
iteration 600, loss 1.7943971157073975
iteration 700, loss 1.8176908493041992
iteration 800, loss 1.8803234100341797
iteration 0, loss 1.8879852294921875
iteration 100, loss 1.7853093147277832
iteration 200, loss 1.8389043807983398
iteration 300, loss 1.7908142805099487
iteration 400, loss 1.7471877336502075
iteration 500, loss 1.92128586769104
iteration 600, loss 1.851897120475769
iteration 700, loss 1.8592528104782104
iteration 800, loss 1.8332806825637817
iteration 0, loss 1.8298102617263794
iteration 100, loss 1.7648102045059204
iteration 200, loss 1.9322807788848877
iteration 300, loss 1.846644401550293
iteration 400, loss 1.8598088026046753
iteration 500, loss 1.914339303970337
iteration 600, loss 1.8524357080459595
iteration 700, loss 1.8447096347808838
iteration 800, loss 1.9122028350830078
iteration 0, loss 1.7955654859542847
iteration 100, loss 1.8761610984802246
iteration 200, loss 1.8229174613952637
iteration 300, loss 1.8786247968673706
iteration 400, loss 1.8318873643875122
iteration 500, loss 1.8160344362258911
iteration 600, loss 1.7658072710037231
iteration 700, loss 1.8704640865325928
iteration 800, loss 1.864288568496704
iteration 0, loss 1.8244788646697998
iteration 100, loss 1.8652762174606323
iteration 200, loss 1.8796242475509644
iteration 300, loss 1.7953948974609375
iteration 400, loss 1.8877739906311035
iteration 500, loss 1.8789061307907104
iteration 600, loss 1.8355237245559692
iteration 700, loss 1.9133319854736328
iteration 800, loss 1.8778234720230103
iteration 0, loss 1.8034346103668213
iteration 100, loss 1.8066861629486084
iteration 200, loss 1.9326223134994507
iteration 300, loss 1.865661859512329
iteration 400, loss 1.859184741973877
iteration 500, loss 1.838199496269226
iteration 600, loss 1.909622311592102
iteration 700, loss 1.8324888944625854
iteration 800, loss 1.8391425609588623
iteration 0, loss 1.7974328994750977
iteration 100, loss 1.8100506067276
iteration 200, loss 1.885693073272705
iteration 300, loss 1.7924871444702148
iteration 400, loss 1.838971734046936
iteration 500, loss 1.8014514446258545
iteration 600, loss 1.8325996398925781
iteration 700, loss 1.8971720933914185
iteration 800, loss 1.8354727029800415
iteration 0, loss 1.8544281721115112
iteration 100, loss 1.8442614078521729
iteration 200, loss 1.7440884113311768
iteration 300, loss 1.8289953470230103
iteration 400, loss 1.8054195642471313
iteration 500, loss 1.8513405323028564
iteration 600, loss 1.7859869003295898
iteration 700, loss 1.835234522819519
iteration 800, loss 1.8647005558013916
iteration 0, loss 1.7782504558563232
iteration 100, loss 1.7941384315490723
iteration 200, loss 1.816121220588684
iteration 300, loss 1.8334892988204956
iteration 400, loss 1.8893455266952515
iteration 500, loss 1.8678386211395264
iteration 600, loss 1.7878340482711792
iteration 700, loss 1.8734815120697021
iteration 800, loss 1.8512505292892456
iteration 0, loss 1.8572094440460205
iteration 100, loss 1.8281348943710327
iteration 200, loss 1.834725022315979
iteration 300, loss 1.8597590923309326
iteration 400, loss 1.7884225845336914
iteration 500, loss 1.8372441530227661
iteration 600, loss 1.8281981945037842
iteration 700, loss 1.8167142868041992
iteration 800, loss 1.8250212669372559
iteration 0, loss 1.831565499305725
iteration 100, loss 1.856544017791748
iteration 200, loss 1.7962086200714111
iteration 300, loss 1.9036266803741455
iteration 400, loss 1.8067314624786377
iteration 500, loss 1.7886477708816528
iteration 600, loss 1.7290295362472534
iteration 700, loss 1.845881700515747
iteration 800, loss 1.8544466495513916
iteration 0, loss 1.8382117748260498
iteration 100, loss 1.8060729503631592
iteration 200, loss 1.9273096323013306
iteration 300, loss 1.7976323366165161
iteration 400, loss 1.8633428812026978
iteration 500, loss 1.8254191875457764
iteration 600, loss 1.8220535516738892
iteration 700, loss 1.861396312713623
iteration 800, loss 1.7840782403945923
iteration 0, loss 1.7631133794784546
iteration 100, loss 1.7819639444351196
iteration 200, loss 1.7827041149139404
iteration 300, loss 1.813500165939331
iteration 400, loss 1.797316312789917
iteration 500, loss 1.8450146913528442
iteration 600, loss 1.8796133995056152
iteration 700, loss 1.7874723672866821
iteration 800, loss 1.8178753852844238
fold 4 accuracy: 0.412
[2024-02-28 22:50:07,764] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-28 22:50:07,765] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            482.8 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.16 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '482.8 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 482.8 us = 100% latency, 1.16 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 384.09 us = 79.56% latency, 1.46 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 30.99 us = 6.42% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-28 22:50:07,767] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
iteration 0, loss 2.28141450881958
iteration 100, loss 2.287672281265259
iteration 200, loss 2.1560027599334717
iteration 300, loss 2.1000125408172607
iteration 400, loss 2.052806854248047
iteration 500, loss 2.019054889678955
iteration 600, loss 2.049173355102539
iteration 700, loss 1.9757647514343262
iteration 800, loss 2.013634204864502
iteration 0, loss 2.0071122646331787
iteration 100, loss 1.9304921627044678
iteration 200, loss 1.9900740385055542
iteration 300, loss 1.950116753578186
iteration 400, loss 1.9836585521697998
iteration 500, loss 1.9221723079681396
iteration 600, loss 1.9715858697891235
iteration 700, loss 1.9886900186538696
iteration 800, loss 2.0181727409362793
iteration 0, loss 1.9705414772033691
iteration 100, loss 1.9276282787322998
iteration 200, loss 1.9197907447814941
iteration 300, loss 1.9583096504211426
iteration 400, loss 1.983304500579834
iteration 500, loss 1.9917241334915161
iteration 600, loss 1.9188512563705444
iteration 700, loss 1.9356917142868042
iteration 800, loss 1.9067221879959106
iteration 0, loss 1.9242162704467773
iteration 100, loss 1.975765347480774
iteration 200, loss 1.8916735649108887
iteration 300, loss 1.959650993347168
iteration 400, loss 1.9422087669372559
iteration 500, loss 1.9070334434509277
iteration 600, loss 1.9047253131866455
iteration 700, loss 1.8891963958740234
iteration 800, loss 1.8752784729003906
iteration 0, loss 1.8723506927490234
iteration 100, loss 1.8617697954177856
iteration 200, loss 1.9251887798309326
iteration 300, loss 1.8658850193023682
iteration 400, loss 1.955859661102295
iteration 500, loss 1.8782386779785156
iteration 600, loss 1.9582099914550781
iteration 700, loss 1.881324052810669
iteration 800, loss 1.906004548072815
iteration 0, loss 1.8536611795425415
iteration 100, loss 1.9487028121948242
iteration 200, loss 1.9009190797805786
iteration 300, loss 1.8577966690063477
iteration 400, loss 1.8664638996124268
iteration 500, loss 1.8944367170333862
iteration 600, loss 1.839331030845642
iteration 700, loss 1.8659628629684448
iteration 800, loss 1.9220213890075684
iteration 0, loss 1.8837664127349854
iteration 100, loss 1.826422929763794
iteration 200, loss 1.8541557788848877
iteration 300, loss 1.8667982816696167
iteration 400, loss 1.9180850982666016
iteration 500, loss 1.9057406187057495
iteration 600, loss 1.8456567525863647
iteration 700, loss 1.8490517139434814
iteration 800, loss 1.878894329071045
iteration 0, loss 1.8493577241897583
iteration 100, loss 1.943948745727539
iteration 200, loss 1.8565574884414673
iteration 300, loss 1.8848670721054077
iteration 400, loss 1.8111826181411743
iteration 500, loss 1.9061609506607056
iteration 600, loss 1.8614977598190308
iteration 700, loss 1.7911369800567627
iteration 800, loss 1.874332308769226
iteration 0, loss 1.8831462860107422
iteration 100, loss 1.793359637260437
iteration 200, loss 1.8950525522232056
iteration 300, loss 1.9245959520339966
iteration 400, loss 1.8728222846984863
iteration 500, loss 1.8316371440887451
iteration 600, loss 1.8186181783676147
iteration 700, loss 1.8769203424453735
iteration 800, loss 1.8315105438232422
iteration 0, loss 1.8020445108413696
iteration 100, loss 1.8509238958358765
iteration 200, loss 1.8379483222961426
iteration 300, loss 1.8334171772003174
iteration 400, loss 1.9052979946136475
iteration 500, loss 1.8570834398269653
iteration 600, loss 1.8313570022583008
iteration 700, loss 1.879694938659668
iteration 800, loss 1.9005672931671143
iteration 0, loss 1.889231562614441
iteration 100, loss 1.860076665878296
iteration 200, loss 1.8663033246994019
iteration 300, loss 1.9592657089233398
iteration 400, loss 1.842553734779358
iteration 500, loss 1.8612614870071411
iteration 600, loss 1.884613275527954
iteration 700, loss 1.871719241142273
iteration 800, loss 1.858500599861145
iteration 0, loss 1.871735692024231
iteration 100, loss 1.852908968925476
iteration 200, loss 1.8356865644454956
iteration 300, loss 1.8286384344100952
iteration 400, loss 1.8742294311523438
iteration 500, loss 1.8683711290359497
iteration 600, loss 1.8628921508789062
iteration 700, loss 1.8159812688827515
iteration 800, loss 1.8416037559509277
iteration 0, loss 1.798914909362793
iteration 100, loss 1.879408359527588
iteration 200, loss 1.9364224672317505
iteration 300, loss 1.8389215469360352
iteration 400, loss 1.841051459312439
iteration 500, loss 1.8401565551757812
iteration 600, loss 1.845921516418457
iteration 700, loss 1.91383695602417
iteration 800, loss 1.8577697277069092
iteration 0, loss 1.8241068124771118
iteration 100, loss 1.8854681253433228
iteration 200, loss 1.8598912954330444
iteration 300, loss 1.79020357131958
iteration 400, loss 1.8617045879364014
iteration 500, loss 1.8535051345825195
iteration 600, loss 1.8804774284362793
iteration 700, loss 1.8333330154418945
iteration 800, loss 1.8899346590042114
iteration 0, loss 1.806175708770752
iteration 100, loss 1.8519634008407593
iteration 200, loss 1.8659757375717163
iteration 300, loss 1.921262264251709
iteration 400, loss 1.7962896823883057
iteration 500, loss 1.8946317434310913
iteration 600, loss 1.8586041927337646
iteration 700, loss 1.8120925426483154
iteration 800, loss 1.9299521446228027
iteration 0, loss 1.833137035369873
iteration 100, loss 1.7829874753952026
iteration 200, loss 1.855600357055664
iteration 300, loss 1.8834260702133179
iteration 400, loss 1.846379041671753
iteration 500, loss 1.8781704902648926
iteration 600, loss 1.8597984313964844
iteration 700, loss 1.8000545501708984
iteration 800, loss 1.8702945709228516
iteration 0, loss 1.9508353471755981
iteration 100, loss 1.8312461376190186
iteration 200, loss 1.8158235549926758
iteration 300, loss 1.7947431802749634
iteration 400, loss 1.8524723052978516
iteration 500, loss 1.791390061378479
iteration 600, loss 1.7967443466186523
iteration 700, loss 1.8766424655914307
iteration 800, loss 1.8769760131835938
iteration 0, loss 1.8712306022644043
iteration 100, loss 1.842176079750061
iteration 200, loss 1.866507887840271
iteration 300, loss 1.8513633012771606
iteration 400, loss 1.8887184858322144
iteration 500, loss 1.8481751680374146
iteration 600, loss 1.8146268129348755
iteration 700, loss 1.8415489196777344
iteration 800, loss 1.8475698232650757
iteration 0, loss 1.9192051887512207
iteration 100, loss 1.8983033895492554
iteration 200, loss 1.8037725687026978
iteration 300, loss 1.8023720979690552
iteration 400, loss 1.8732640743255615
iteration 500, loss 1.858953833580017
iteration 600, loss 1.855391502380371
iteration 700, loss 1.8018145561218262
iteration 800, loss 1.8027026653289795
iteration 0, loss 1.77483069896698
iteration 100, loss 1.9465478658676147
iteration 200, loss 1.7459862232208252
iteration 300, loss 1.8628066778182983
iteration 400, loss 1.8242347240447998
iteration 500, loss 1.8379671573638916
iteration 600, loss 1.8627290725708008
iteration 700, loss 1.8386569023132324
iteration 800, loss 1.8771247863769531
iteration 0, loss 1.8902006149291992
iteration 100, loss 1.8617130517959595
iteration 200, loss 1.892261028289795
iteration 300, loss 1.9206229448318481
iteration 400, loss 1.869069218635559
iteration 500, loss 1.7978992462158203
iteration 600, loss 1.8090559244155884
iteration 700, loss 1.805162787437439
iteration 800, loss 1.855889916419983
iteration 0, loss 1.8714959621429443
iteration 100, loss 1.8196542263031006
iteration 200, loss 1.851811408996582
iteration 300, loss 1.7957513332366943
iteration 400, loss 1.8652604818344116
iteration 500, loss 1.8123244047164917
iteration 600, loss 1.9055572748184204
iteration 700, loss 1.8181182146072388
iteration 800, loss 1.880994200706482
iteration 0, loss 1.797703742980957
iteration 100, loss 1.839411973953247
iteration 200, loss 1.8540107011795044
iteration 300, loss 1.8849014043807983
iteration 400, loss 1.848286747932434
iteration 500, loss 1.8202648162841797
iteration 600, loss 1.837252140045166
iteration 700, loss 1.7684307098388672
iteration 800, loss 1.868820309638977
iteration 0, loss 1.8471150398254395
iteration 100, loss 1.9165412187576294
iteration 200, loss 1.8519898653030396
iteration 300, loss 1.7806801795959473
iteration 400, loss 1.8131582736968994
iteration 500, loss 1.8337938785552979
iteration 600, loss 1.8296184539794922
iteration 700, loss 1.8753141164779663
iteration 800, loss 1.8246346712112427
iteration 0, loss 1.8001461029052734
iteration 100, loss 1.9168634414672852
iteration 200, loss 1.8243132829666138
iteration 300, loss 1.8313562870025635
iteration 400, loss 1.8520067930221558
iteration 500, loss 1.8390264511108398
iteration 600, loss 1.762112021446228
iteration 700, loss 1.855083703994751
iteration 800, loss 1.8542640209197998
iteration 0, loss 1.8614567518234253
iteration 100, loss 1.8156542778015137
iteration 200, loss 1.8059117794036865
iteration 300, loss 1.838582158088684
iteration 400, loss 1.896311640739441
iteration 500, loss 1.8277549743652344
iteration 600, loss 1.8300139904022217
iteration 700, loss 1.8538730144500732
iteration 800, loss 1.8536198139190674
iteration 0, loss 1.8576488494873047
iteration 100, loss 1.8769993782043457
iteration 200, loss 1.7642196416854858
iteration 300, loss 1.8099337816238403
iteration 400, loss 1.8629001379013062
iteration 500, loss 1.8406285047531128
iteration 600, loss 1.812718152999878
iteration 700, loss 1.8652839660644531
iteration 800, loss 1.8831039667129517
iteration 0, loss 1.8132953643798828
iteration 100, loss 1.853381633758545
iteration 200, loss 1.8411346673965454
iteration 300, loss 1.8175041675567627
iteration 400, loss 1.8747872114181519
iteration 500, loss 1.849422574043274
iteration 600, loss 1.8130614757537842
iteration 700, loss 1.7613685131072998
iteration 800, loss 1.756605863571167
iteration 0, loss 1.8141744136810303
iteration 100, loss 1.8044652938842773
iteration 200, loss 1.896409511566162
iteration 300, loss 1.8869681358337402
iteration 400, loss 1.8025027513504028
iteration 500, loss 1.7824010848999023
iteration 600, loss 1.8591035604476929
iteration 700, loss 1.8737562894821167
iteration 800, loss 1.8587461709976196
iteration 0, loss 1.7731342315673828
iteration 100, loss 1.795924425125122
iteration 200, loss 1.8197706937789917
iteration 300, loss 1.8231983184814453
iteration 400, loss 1.8005999326705933
iteration 500, loss 1.9012583494186401
iteration 600, loss 1.7918996810913086
iteration 700, loss 1.8114714622497559
iteration 800, loss 1.8782232999801636
iteration 0, loss 1.897711992263794
iteration 100, loss 1.882442831993103
iteration 200, loss 1.8382985591888428
iteration 300, loss 1.8256773948669434
iteration 400, loss 1.8029402494430542
iteration 500, loss 1.8627129793167114
iteration 600, loss 1.7570788860321045
iteration 700, loss 1.822756290435791
iteration 800, loss 1.853450894355774
iteration 0, loss 1.8457716703414917
iteration 100, loss 1.7555630207061768
iteration 200, loss 1.8550350666046143
iteration 300, loss 1.8550690412521362
iteration 400, loss 1.902043342590332
iteration 500, loss 1.8751130104064941
iteration 600, loss 1.7839503288269043
iteration 700, loss 1.8798936605453491
iteration 800, loss 1.8755888938903809
iteration 0, loss 1.88516366481781
iteration 100, loss 1.7633659839630127
iteration 200, loss 1.8360317945480347
iteration 300, loss 1.836506962776184
iteration 400, loss 1.8302801847457886
iteration 500, loss 1.8086111545562744
iteration 600, loss 1.8814806938171387
iteration 700, loss 1.8223035335540771
iteration 800, loss 1.8629361391067505
iteration 0, loss 1.8367701768875122
iteration 100, loss 1.8438588380813599
iteration 200, loss 1.832777738571167
iteration 300, loss 1.7888288497924805
iteration 400, loss 1.8055626153945923
iteration 500, loss 1.7782825231552124
iteration 600, loss 1.8588224649429321
iteration 700, loss 1.821008563041687
iteration 800, loss 1.8172527551651
iteration 0, loss 1.7955212593078613
iteration 100, loss 1.7871143817901611
iteration 200, loss 1.8517539501190186
iteration 300, loss 1.849843978881836
iteration 400, loss 1.8670461177825928
iteration 500, loss 1.9152528047561646
iteration 600, loss 1.79112708568573
iteration 700, loss 1.805566668510437
iteration 800, loss 1.8477728366851807
iteration 0, loss 1.8553345203399658
iteration 100, loss 1.807268500328064
iteration 200, loss 1.825629711151123
iteration 300, loss 1.824853539466858
iteration 400, loss 1.8406689167022705
iteration 500, loss 1.859146237373352
iteration 600, loss 1.7813447713851929
iteration 700, loss 1.8071609735488892
iteration 800, loss 1.75089430809021
iteration 0, loss 1.830869436264038
iteration 100, loss 1.8125872611999512
iteration 200, loss 1.8324588537216187
iteration 300, loss 1.794628381729126
iteration 400, loss 1.8527148962020874
iteration 500, loss 1.8619519472122192
iteration 600, loss 1.8665449619293213
iteration 700, loss 1.875483512878418
iteration 800, loss 1.8719109296798706
iteration 0, loss 1.780503273010254
iteration 100, loss 1.7032735347747803
iteration 200, loss 1.8539460897445679
iteration 300, loss 1.779152274131775
iteration 400, loss 1.8962539434432983
iteration 500, loss 1.8136205673217773
iteration 600, loss 1.8147112131118774
iteration 700, loss 1.8350439071655273
iteration 800, loss 1.7714717388153076
iteration 0, loss 1.836948037147522
iteration 100, loss 1.7806211709976196
iteration 200, loss 1.7933672666549683
iteration 300, loss 1.7907434701919556
iteration 400, loss 1.8171783685684204
iteration 500, loss 1.9211844205856323
iteration 600, loss 1.821650743484497
iteration 700, loss 1.8494081497192383
iteration 800, loss 1.748974084854126
iteration 0, loss 1.8299041986465454
iteration 100, loss 1.7550634145736694
iteration 200, loss 1.8381162881851196
iteration 300, loss 1.8625799417495728
iteration 400, loss 1.8709124326705933
iteration 500, loss 1.8192110061645508
iteration 600, loss 1.8669157028198242
iteration 700, loss 1.9061390161514282
iteration 800, loss 1.8663263320922852
iteration 0, loss 1.862461805343628
iteration 100, loss 1.8404678106307983
iteration 200, loss 1.8376563787460327
iteration 300, loss 1.856539249420166
iteration 400, loss 1.8299360275268555
iteration 500, loss 1.8163303136825562
iteration 600, loss 1.833790898323059
iteration 700, loss 1.883088231086731
iteration 800, loss 1.8430262804031372
iteration 0, loss 1.8255739212036133
iteration 100, loss 1.843156337738037
iteration 200, loss 1.814579725265503
iteration 300, loss 1.7573319673538208
iteration 400, loss 1.7582846879959106
iteration 500, loss 1.8661036491394043
iteration 600, loss 1.803330421447754
iteration 700, loss 1.8720241785049438
iteration 800, loss 1.7986280918121338
iteration 0, loss 1.731366515159607
iteration 100, loss 1.8460333347320557
iteration 200, loss 1.7853399515151978
iteration 300, loss 1.77574622631073
iteration 400, loss 1.8509111404418945
iteration 500, loss 1.8626469373703003
iteration 600, loss 1.834942102432251
iteration 700, loss 1.8380390405654907
iteration 800, loss 1.8658188581466675
iteration 0, loss 1.8151700496673584
iteration 100, loss 1.7861729860305786
iteration 200, loss 1.801711082458496
iteration 300, loss 1.822311520576477
iteration 400, loss 1.8602792024612427
iteration 500, loss 1.81532621383667
iteration 600, loss 1.902647614479065
iteration 700, loss 1.8091650009155273
iteration 800, loss 1.7202074527740479
iteration 0, loss 1.8217616081237793
iteration 100, loss 1.8577532768249512
iteration 200, loss 1.9173327684402466
iteration 300, loss 1.8661950826644897
iteration 400, loss 1.7970956563949585
iteration 500, loss 1.8119450807571411
iteration 600, loss 1.8225386142730713
iteration 700, loss 1.8446459770202637
iteration 800, loss 1.7749688625335693
iteration 0, loss 1.8210351467132568
iteration 100, loss 1.8087910413742065
iteration 200, loss 1.8603074550628662
iteration 300, loss 1.8738923072814941
iteration 400, loss 1.8095121383666992
iteration 500, loss 1.80479896068573
iteration 600, loss 1.816805124282837
iteration 700, loss 1.769303560256958
iteration 800, loss 1.8392703533172607
iteration 0, loss 1.8071447610855103
iteration 100, loss 1.9119329452514648
iteration 200, loss 1.800635576248169
iteration 300, loss 1.8109318017959595
iteration 400, loss 1.7723382711410522
iteration 500, loss 1.8596174716949463
iteration 600, loss 1.873300552368164
iteration 700, loss 1.8441640138626099
iteration 800, loss 1.7276405096054077
iteration 0, loss 1.809342622756958
iteration 100, loss 1.8756662607192993
iteration 200, loss 1.8585155010223389
iteration 300, loss 1.7355762720108032
iteration 400, loss 1.8639845848083496
iteration 500, loss 1.8646903038024902
iteration 600, loss 1.8454833030700684
iteration 700, loss 1.853514552116394
iteration 800, loss 1.7397124767303467
iteration 0, loss 1.8501508235931396
iteration 100, loss 1.8590787649154663
iteration 200, loss 1.8329763412475586
iteration 300, loss 1.763681173324585
iteration 400, loss 1.7371563911437988
iteration 500, loss 1.7972139120101929
iteration 600, loss 1.7555779218673706
iteration 700, loss 1.811680555343628
iteration 800, loss 1.8595870733261108
iteration 0, loss 1.843460202217102
iteration 100, loss 1.7417445182800293
iteration 200, loss 1.8308571577072144
iteration 300, loss 1.8304296731948853
iteration 400, loss 1.7525620460510254
iteration 500, loss 1.730058193206787
iteration 600, loss 1.7699472904205322
iteration 700, loss 1.8295732736587524
iteration 800, loss 1.7880803346633911
fold 0 accuracy: 0.4692857142857143
iteration 0, loss 1.8181208372116089
iteration 100, loss 1.756546139717102
iteration 200, loss 1.8196089267730713
iteration 300, loss 1.8172231912612915
iteration 400, loss 1.8267855644226074
iteration 500, loss 1.8202612400054932
iteration 600, loss 1.839343786239624
iteration 700, loss 1.8289031982421875
iteration 800, loss 1.7921292781829834
iteration 0, loss 1.8072457313537598
iteration 100, loss 1.8608981370925903
iteration 200, loss 1.8047982454299927
iteration 300, loss 1.9074257612228394
iteration 400, loss 1.8276605606079102
iteration 500, loss 1.8265925645828247
iteration 600, loss 1.8686237335205078
iteration 700, loss 1.8014246225357056
iteration 800, loss 1.8731426000595093
iteration 0, loss 1.8106619119644165
iteration 100, loss 1.8340929746627808
iteration 200, loss 1.7595593929290771
iteration 300, loss 1.85944402217865
iteration 400, loss 1.7755204439163208
iteration 500, loss 1.8307774066925049
iteration 600, loss 1.8084219694137573
iteration 700, loss 1.8160429000854492
iteration 800, loss 1.8249516487121582
iteration 0, loss 1.8507144451141357
iteration 100, loss 1.858605146408081
iteration 200, loss 1.8278003931045532
iteration 300, loss 1.809335708618164
iteration 400, loss 1.8056988716125488
iteration 500, loss 1.8601230382919312
iteration 600, loss 1.7657179832458496
iteration 700, loss 1.8896489143371582
iteration 800, loss 1.7812093496322632
iteration 0, loss 1.7903190851211548
iteration 100, loss 1.8544644117355347
iteration 200, loss 1.9284765720367432
iteration 300, loss 1.7676643133163452
iteration 400, loss 1.8979570865631104
iteration 500, loss 1.8152525424957275
iteration 600, loss 1.8174833059310913
iteration 700, loss 1.8112659454345703
iteration 800, loss 1.7768608331680298
iteration 0, loss 1.8204045295715332
iteration 100, loss 1.8050920963287354
iteration 200, loss 1.8997184038162231
iteration 300, loss 1.8907569646835327
iteration 400, loss 1.8125698566436768
iteration 500, loss 1.9201430082321167
iteration 600, loss 1.7703871726989746
iteration 700, loss 1.8241556882858276
iteration 800, loss 1.863052487373352
iteration 0, loss 1.851167917251587
iteration 100, loss 1.8223421573638916
iteration 200, loss 1.8042218685150146
iteration 300, loss 1.827423095703125
iteration 400, loss 1.7862083911895752
iteration 500, loss 1.8346153497695923
iteration 600, loss 1.861139178276062
iteration 700, loss 1.794219970703125
iteration 800, loss 1.8133187294006348
iteration 0, loss 1.8435592651367188
iteration 100, loss 1.809935212135315
iteration 200, loss 1.7488725185394287
iteration 300, loss 1.7742217779159546
iteration 400, loss 1.830372929573059
iteration 500, loss 1.8133182525634766
iteration 600, loss 1.7885644435882568
iteration 700, loss 1.808465600013733
iteration 800, loss 1.771986961364746
iteration 0, loss 1.8408724069595337
iteration 100, loss 1.8309364318847656
iteration 200, loss 1.8736841678619385
iteration 300, loss 1.7932488918304443
iteration 400, loss 1.8753010034561157
iteration 500, loss 1.776260495185852
iteration 600, loss 1.8362423181533813
iteration 700, loss 1.7612674236297607
iteration 800, loss 1.8344670534133911
iteration 0, loss 1.8569680452346802
iteration 100, loss 1.789271593093872
iteration 200, loss 1.8574048280715942
iteration 300, loss 1.8167238235473633
iteration 400, loss 1.8239777088165283
iteration 500, loss 1.842449426651001
iteration 600, loss 1.7915222644805908
iteration 700, loss 1.81406831741333
iteration 800, loss 1.8098517656326294
iteration 0, loss 1.8066747188568115
iteration 100, loss 1.7713404893875122
iteration 200, loss 1.8085527420043945
iteration 300, loss 1.7669438123703003
iteration 400, loss 1.7852064371109009
iteration 500, loss 1.7774686813354492
iteration 600, loss 1.8252122402191162
iteration 700, loss 1.852367877960205
iteration 800, loss 1.7781572341918945
iteration 0, loss 1.8536009788513184
iteration 100, loss 1.7576723098754883
iteration 200, loss 1.7443684339523315
iteration 300, loss 1.7955305576324463
iteration 400, loss 1.8750360012054443
iteration 500, loss 1.8851889371871948
iteration 600, loss 1.9016835689544678
iteration 700, loss 1.8373031616210938
iteration 800, loss 1.8180793523788452
iteration 0, loss 1.7934950590133667
iteration 100, loss 1.8451282978057861
iteration 200, loss 1.823136568069458
iteration 300, loss 1.838424563407898
iteration 400, loss 1.8532558679580688
iteration 500, loss 1.8441396951675415
iteration 600, loss 1.826923131942749
iteration 700, loss 1.73656165599823
iteration 800, loss 1.9324569702148438
iteration 0, loss 1.7964953184127808
iteration 100, loss 1.8263123035430908
iteration 200, loss 1.9226714372634888
iteration 300, loss 1.9263386726379395
iteration 400, loss 1.8090260028839111
iteration 500, loss 1.8067662715911865
iteration 600, loss 1.7912074327468872
iteration 700, loss 1.863106369972229
iteration 800, loss 1.77463698387146
iteration 0, loss 1.8467586040496826
iteration 100, loss 1.7521859407424927
iteration 200, loss 1.8555693626403809
iteration 300, loss 1.8152918815612793
iteration 400, loss 1.7893565893173218
iteration 500, loss 1.8243151903152466
iteration 600, loss 1.839064598083496
iteration 700, loss 1.8865176439285278
iteration 800, loss 1.8262897729873657
iteration 0, loss 1.853083848953247
iteration 100, loss 1.8306423425674438
iteration 200, loss 1.848210334777832
iteration 300, loss 1.8456220626831055
iteration 400, loss 1.8265044689178467
iteration 500, loss 1.8364442586898804
iteration 600, loss 1.8509631156921387
iteration 700, loss 1.8439137935638428
iteration 800, loss 1.8442628383636475
iteration 0, loss 1.8151471614837646
iteration 100, loss 1.8649641275405884
iteration 200, loss 1.844999074935913
iteration 300, loss 1.7855165004730225
iteration 400, loss 1.8779242038726807
iteration 500, loss 1.8192708492279053
iteration 600, loss 1.830095887184143
iteration 700, loss 1.8911720514297485
iteration 800, loss 1.79619562625885
iteration 0, loss 1.8388030529022217
iteration 100, loss 1.7671241760253906
iteration 200, loss 1.7232517004013062
iteration 300, loss 1.8197271823883057
iteration 400, loss 1.8576072454452515
iteration 500, loss 1.8127988576889038
iteration 600, loss 1.8588727712631226
iteration 700, loss 1.785309076309204
iteration 800, loss 1.752892255783081
iteration 0, loss 1.763918161392212
iteration 100, loss 1.864678978919983
iteration 200, loss 1.7904666662216187
iteration 300, loss 1.8636701107025146
iteration 400, loss 1.7401663064956665
iteration 500, loss 1.7994961738586426
iteration 600, loss 1.7683517932891846
iteration 700, loss 1.7457736730575562
iteration 800, loss 1.8651950359344482
iteration 0, loss 1.7756602764129639
iteration 100, loss 1.7626020908355713
iteration 200, loss 1.806232213973999
iteration 300, loss 1.8472071886062622
iteration 400, loss 1.7886258363723755
iteration 500, loss 1.84391188621521
iteration 600, loss 1.8003618717193604
iteration 700, loss 1.7726515531539917
iteration 800, loss 1.7644274234771729
iteration 0, loss 1.7587203979492188
iteration 100, loss 1.840895414352417
iteration 200, loss 1.7959662675857544
iteration 300, loss 1.845353126525879
iteration 400, loss 1.900341510772705
iteration 500, loss 1.9014467000961304
iteration 600, loss 1.8381768465042114
iteration 700, loss 1.8058018684387207
iteration 800, loss 1.7922264337539673
iteration 0, loss 1.8886533975601196
iteration 100, loss 1.837261438369751
iteration 200, loss 1.8631188869476318
iteration 300, loss 1.8382524251937866
iteration 400, loss 1.885222315788269
iteration 500, loss 1.731440782546997
iteration 600, loss 1.8450274467468262
iteration 700, loss 1.7753225564956665
iteration 800, loss 1.8710991144180298
iteration 0, loss 1.8710684776306152
iteration 100, loss 1.8976094722747803
iteration 200, loss 1.8717595338821411
iteration 300, loss 1.7854527235031128
iteration 400, loss 1.8601953983306885
iteration 500, loss 1.779920220375061
iteration 600, loss 1.8390365839004517
iteration 700, loss 1.7532576322555542
iteration 800, loss 1.766098976135254
iteration 0, loss 1.8696056604385376
iteration 100, loss 1.847110390663147
iteration 200, loss 1.868263602256775
iteration 300, loss 1.846847414970398
iteration 400, loss 1.7270644903182983
iteration 500, loss 1.7911731004714966
iteration 600, loss 1.8555618524551392
iteration 700, loss 1.8484277725219727
iteration 800, loss 1.7673643827438354
iteration 0, loss 1.8131146430969238
iteration 100, loss 1.8201196193695068
iteration 200, loss 1.8157870769500732
iteration 300, loss 1.807233214378357
iteration 400, loss 1.8154293298721313
iteration 500, loss 1.8992599248886108
iteration 600, loss 1.77064049243927
iteration 700, loss 1.824096441268921
iteration 800, loss 1.7595140933990479
iteration 0, loss 1.8012217283248901
iteration 100, loss 1.822177529335022
iteration 200, loss 1.859022617340088
iteration 300, loss 1.806473970413208
iteration 400, loss 1.7744910717010498
iteration 500, loss 1.7808820009231567
iteration 600, loss 1.7975114583969116
iteration 700, loss 1.8488783836364746
iteration 800, loss 1.8484193086624146
iteration 0, loss 1.8580543994903564
iteration 100, loss 1.843982458114624
iteration 200, loss 1.7765387296676636
iteration 300, loss 1.8509637117385864
iteration 400, loss 1.808766484260559
iteration 500, loss 1.8252002000808716
iteration 600, loss 1.8527567386627197
iteration 700, loss 1.828210473060608
iteration 800, loss 1.8473600149154663
iteration 0, loss 1.8128968477249146
iteration 100, loss 1.781613826751709
iteration 200, loss 1.8437652587890625
iteration 300, loss 1.8246673345565796
iteration 400, loss 1.853985071182251
iteration 500, loss 1.8876370191574097
iteration 600, loss 1.788727045059204
iteration 700, loss 1.8149144649505615
iteration 800, loss 1.7774429321289062
iteration 0, loss 1.784764289855957
iteration 100, loss 1.8297350406646729
iteration 200, loss 1.8321985006332397
iteration 300, loss 1.848952054977417
iteration 400, loss 1.9675629138946533
iteration 500, loss 1.8522173166275024
iteration 600, loss 1.9422681331634521
iteration 700, loss 1.8283339738845825
iteration 800, loss 1.8539763689041138
iteration 0, loss 1.905502200126648
iteration 100, loss 1.8271358013153076
iteration 200, loss 1.796999216079712
iteration 300, loss 1.7635128498077393
iteration 400, loss 1.8397513628005981
iteration 500, loss 1.8074485063552856
iteration 600, loss 1.843977928161621
iteration 700, loss 1.807722568511963
iteration 800, loss 1.878629446029663
iteration 0, loss 1.8601758480072021
iteration 100, loss 1.770463228225708
iteration 200, loss 1.7762023210525513
iteration 300, loss 1.88730788230896
iteration 400, loss 1.7978425025939941
iteration 500, loss 1.7701772451400757
iteration 600, loss 1.821839690208435
iteration 700, loss 1.8281384706497192
iteration 800, loss 1.8199785947799683
iteration 0, loss 1.8314158916473389
iteration 100, loss 1.8900337219238281
iteration 200, loss 1.7759779691696167
iteration 300, loss 1.805454969406128
iteration 400, loss 1.7955513000488281
iteration 500, loss 1.7747085094451904
iteration 600, loss 1.8703666925430298
iteration 700, loss 1.7440530061721802
iteration 800, loss 1.8604789972305298
iteration 0, loss 1.76515531539917
iteration 100, loss 1.7830017805099487
iteration 200, loss 1.8677141666412354
iteration 300, loss 1.8672178983688354
iteration 400, loss 1.7817792892456055
iteration 500, loss 1.8062705993652344
iteration 600, loss 1.81296968460083
iteration 700, loss 1.8652106523513794
iteration 800, loss 1.900119662284851
iteration 0, loss 1.8276835680007935
iteration 100, loss 1.856400489807129
iteration 200, loss 1.7947098016738892
iteration 300, loss 1.791617751121521
iteration 400, loss 1.839249610900879
iteration 500, loss 1.7876288890838623
iteration 600, loss 1.876092791557312
iteration 700, loss 1.8233413696289062
iteration 800, loss 1.8262277841567993
iteration 0, loss 1.87763249874115
iteration 100, loss 1.8372970819473267
iteration 200, loss 1.7987335920333862
iteration 300, loss 1.8502589464187622
iteration 400, loss 1.8337703943252563
iteration 500, loss 1.8100556135177612
iteration 600, loss 1.8424696922302246
iteration 700, loss 1.829247236251831
iteration 800, loss 1.8906936645507812
iteration 0, loss 1.7764472961425781
iteration 100, loss 1.7697337865829468
iteration 200, loss 1.8444726467132568
iteration 300, loss 1.8552472591400146
iteration 400, loss 1.8034158945083618
iteration 500, loss 1.762574315071106
iteration 600, loss 1.7653400897979736
iteration 700, loss 1.8771430253982544
iteration 800, loss 1.824903130531311
iteration 0, loss 1.8169217109680176
iteration 100, loss 1.8099303245544434
iteration 200, loss 1.877607822418213
iteration 300, loss 1.832920789718628
iteration 400, loss 1.8322161436080933
iteration 500, loss 1.8220014572143555
iteration 600, loss 1.8023992776870728
iteration 700, loss 1.8270936012268066
iteration 800, loss 1.7762882709503174
iteration 0, loss 1.7822577953338623
iteration 100, loss 1.7759915590286255
iteration 200, loss 1.7811802625656128
iteration 300, loss 1.7534747123718262
iteration 400, loss 1.759313702583313
iteration 500, loss 1.8932433128356934
iteration 600, loss 1.780573844909668
iteration 700, loss 1.9076488018035889
iteration 800, loss 1.8633238077163696
iteration 0, loss 1.8269561529159546
iteration 100, loss 1.7781078815460205
iteration 200, loss 1.9076554775238037
iteration 300, loss 1.841248631477356
iteration 400, loss 1.8301188945770264
iteration 500, loss 1.7699673175811768
iteration 600, loss 1.857342004776001
iteration 700, loss 1.8049057722091675
iteration 800, loss 1.8339561223983765
iteration 0, loss 1.824434757232666
iteration 100, loss 1.9513870477676392
iteration 200, loss 1.8221780061721802
iteration 300, loss 1.8560638427734375
iteration 400, loss 1.87584388256073
iteration 500, loss 1.8386507034301758
iteration 600, loss 1.8971765041351318
iteration 700, loss 1.8152291774749756
iteration 800, loss 1.7882325649261475
iteration 0, loss 1.78461754322052
iteration 100, loss 1.7849501371383667
iteration 200, loss 1.827399492263794
iteration 300, loss 1.704820990562439
iteration 400, loss 1.7973823547363281
iteration 500, loss 1.8525437116622925
iteration 600, loss 1.8190743923187256
iteration 700, loss 1.7770498991012573
iteration 800, loss 1.7658568620681763
iteration 0, loss 1.7740542888641357
iteration 100, loss 1.8203359842300415
iteration 200, loss 1.75230872631073
iteration 300, loss 1.8259999752044678
iteration 400, loss 1.8203710317611694
iteration 500, loss 1.8422534465789795
iteration 600, loss 1.8896636962890625
iteration 700, loss 1.8220548629760742
iteration 800, loss 1.8733327388763428
iteration 0, loss 1.8514784574508667
iteration 100, loss 1.8068015575408936
iteration 200, loss 1.8318381309509277
iteration 300, loss 1.8198890686035156
iteration 400, loss 1.8245140314102173
iteration 500, loss 1.7975969314575195
iteration 600, loss 1.8273818492889404
iteration 700, loss 1.8922711610794067
iteration 800, loss 1.8505855798721313
iteration 0, loss 1.8452638387680054
iteration 100, loss 1.8565504550933838
iteration 200, loss 1.859028935432434
iteration 300, loss 1.7975472211837769
iteration 400, loss 1.828020691871643
iteration 500, loss 1.7676737308502197
iteration 600, loss 1.7988852262496948
iteration 700, loss 1.785205602645874
iteration 800, loss 1.85873544216156
iteration 0, loss 1.8546946048736572
iteration 100, loss 1.7868094444274902
iteration 200, loss 1.7671682834625244
iteration 300, loss 1.8618263006210327
iteration 400, loss 1.8340654373168945
iteration 500, loss 1.85431706905365
iteration 600, loss 1.871202826499939
iteration 700, loss 1.8127609491348267
iteration 800, loss 1.8255178928375244
iteration 0, loss 1.7834635972976685
iteration 100, loss 1.8433430194854736
iteration 200, loss 1.7797826528549194
iteration 300, loss 1.8153263330459595
iteration 400, loss 1.759867548942566
iteration 500, loss 1.8634694814682007
iteration 600, loss 1.8173291683197021
iteration 700, loss 1.8709005117416382
iteration 800, loss 1.7872718572616577
iteration 0, loss 1.7924364805221558
iteration 100, loss 1.841766595840454
iteration 200, loss 1.8179256916046143
iteration 300, loss 1.7587485313415527
iteration 400, loss 1.8765761852264404
iteration 500, loss 1.8298983573913574
iteration 600, loss 1.8285346031188965
iteration 700, loss 1.7547566890716553
iteration 800, loss 1.8497943878173828
iteration 0, loss 1.7785619497299194
iteration 100, loss 1.8279666900634766
iteration 200, loss 1.8255360126495361
iteration 300, loss 1.7707463502883911
iteration 400, loss 1.790095329284668
iteration 500, loss 1.7820830345153809
iteration 600, loss 1.8182423114776611
iteration 700, loss 1.7550368309020996
iteration 800, loss 1.8887724876403809
iteration 0, loss 1.8080216646194458
iteration 100, loss 1.7992699146270752
iteration 200, loss 1.794641137123108
iteration 300, loss 1.8331676721572876
iteration 400, loss 1.8073582649230957
iteration 500, loss 1.7542294263839722
iteration 600, loss 1.8055435419082642
iteration 700, loss 1.7335001230239868
iteration 800, loss 1.8259531259536743
iteration 0, loss 1.9097557067871094
iteration 100, loss 1.8322622776031494
iteration 200, loss 1.8500275611877441
iteration 300, loss 1.8487895727157593
iteration 400, loss 1.8263486623764038
iteration 500, loss 1.853721022605896
iteration 600, loss 1.8360124826431274
iteration 700, loss 1.809714436531067
iteration 800, loss 1.8347440958023071
fold 1 accuracy: 0.47635714285714287
iteration 0, loss 1.816918134689331
iteration 100, loss 1.8691340684890747
iteration 200, loss 1.8484892845153809
iteration 300, loss 1.8228929042816162
iteration 400, loss 1.7133076190948486
iteration 500, loss 1.8255687952041626
iteration 600, loss 1.7401715517044067
iteration 700, loss 1.7672483921051025
iteration 800, loss 1.7566933631896973
iteration 0, loss 1.7963731288909912
iteration 100, loss 1.7985899448394775
iteration 200, loss 1.821651816368103
iteration 300, loss 1.784668207168579
iteration 400, loss 1.8245253562927246
iteration 500, loss 1.6958035230636597
iteration 600, loss 1.8114049434661865
iteration 700, loss 1.8257516622543335
iteration 800, loss 1.7966006994247437
iteration 0, loss 1.8028947114944458
iteration 100, loss 1.7896661758422852
iteration 200, loss 1.8360660076141357
iteration 300, loss 1.7918717861175537
iteration 400, loss 1.853331208229065
iteration 500, loss 1.7800272703170776
iteration 600, loss 1.7050538063049316
iteration 700, loss 1.823846459388733
iteration 800, loss 1.856619954109192
iteration 0, loss 1.7475380897521973
iteration 100, loss 1.7738049030303955
iteration 200, loss 1.8901429176330566
iteration 300, loss 1.8251460790634155
iteration 400, loss 1.852975606918335
iteration 500, loss 1.7794866561889648
iteration 600, loss 1.8501839637756348
iteration 700, loss 1.8577842712402344
iteration 800, loss 1.8273030519485474
iteration 0, loss 1.7795624732971191
iteration 100, loss 1.8505017757415771
iteration 200, loss 1.8193995952606201
iteration 300, loss 1.771647572517395
iteration 400, loss 1.879568099975586
iteration 500, loss 1.7904020547866821
iteration 600, loss 1.7111940383911133
iteration 700, loss 1.8235700130462646
iteration 800, loss 1.7397385835647583
iteration 0, loss 1.8535125255584717
iteration 100, loss 1.7908625602722168
iteration 200, loss 1.8462903499603271
iteration 300, loss 1.8136308193206787
iteration 400, loss 1.85930597782135
iteration 500, loss 1.8925148248672485
iteration 600, loss 1.8887580633163452
iteration 700, loss 1.7683777809143066
iteration 800, loss 1.774080753326416
iteration 0, loss 1.8058327436447144
iteration 100, loss 1.7292605638504028
iteration 200, loss 1.8311625719070435
iteration 300, loss 1.8055917024612427
iteration 400, loss 1.7892221212387085
iteration 500, loss 1.8401216268539429
iteration 600, loss 1.8254804611206055
iteration 700, loss 1.8193761110305786
iteration 800, loss 1.7940822839736938
iteration 0, loss 1.8206982612609863
iteration 100, loss 1.76479172706604
iteration 200, loss 1.8538141250610352
iteration 300, loss 1.8206920623779297
iteration 400, loss 1.8018444776535034
iteration 500, loss 1.7807631492614746
iteration 600, loss 1.773739218711853
iteration 700, loss 1.8501614332199097
iteration 800, loss 1.7744134664535522
iteration 0, loss 1.7901581525802612
iteration 100, loss 1.7481619119644165
iteration 200, loss 1.8377865552902222
iteration 300, loss 1.8745461702346802
iteration 400, loss 1.8643748760223389
iteration 500, loss 1.8260269165039062
iteration 600, loss 1.8029439449310303
iteration 700, loss 1.8511371612548828
iteration 800, loss 1.7783747911453247
iteration 0, loss 1.8336204290390015
iteration 100, loss 1.9555972814559937
iteration 200, loss 1.8721407651901245
iteration 300, loss 1.7531723976135254
iteration 400, loss 1.7802338600158691
iteration 500, loss 1.8525919914245605
iteration 600, loss 1.8205939531326294
iteration 700, loss 1.8653241395950317
iteration 800, loss 1.7335381507873535
iteration 0, loss 1.8351935148239136
iteration 100, loss 1.8273813724517822
iteration 200, loss 1.8044840097427368
iteration 300, loss 1.7838847637176514
iteration 400, loss 1.8927298784255981
iteration 500, loss 1.8910088539123535
iteration 600, loss 1.7600476741790771
iteration 700, loss 1.7205567359924316
iteration 800, loss 1.7833963632583618
iteration 0, loss 1.9190106391906738
iteration 100, loss 1.8744356632232666
iteration 200, loss 1.7677479982376099
iteration 300, loss 1.7735998630523682
iteration 400, loss 1.8514126539230347
iteration 500, loss 1.811511754989624
iteration 600, loss 1.8143287897109985
iteration 700, loss 1.8207441568374634
iteration 800, loss 1.806839942932129
iteration 0, loss 1.8476295471191406
iteration 100, loss 1.7620272636413574
iteration 200, loss 1.7787158489227295
iteration 300, loss 1.8392603397369385
iteration 400, loss 1.7904936075210571
iteration 500, loss 1.8264541625976562
iteration 600, loss 1.8214207887649536
iteration 700, loss 1.8858160972595215
iteration 800, loss 1.7969504594802856
iteration 0, loss 1.7343913316726685
iteration 100, loss 1.7976727485656738
iteration 200, loss 1.808018684387207
iteration 300, loss 1.773491382598877
iteration 400, loss 1.8194172382354736
iteration 500, loss 1.8593766689300537
iteration 600, loss 1.8487746715545654
iteration 700, loss 1.8838146924972534
iteration 800, loss 1.818356990814209
iteration 0, loss 1.7831900119781494
iteration 100, loss 1.8248541355133057
iteration 200, loss 1.7844542264938354
iteration 300, loss 1.8234968185424805
iteration 400, loss 1.8416081666946411
iteration 500, loss 1.8273825645446777
iteration 600, loss 1.7759084701538086
iteration 700, loss 1.8134496212005615
iteration 800, loss 1.8251256942749023
iteration 0, loss 1.9267574548721313
iteration 100, loss 1.8564908504486084
iteration 200, loss 1.841383457183838
iteration 300, loss 1.8387764692306519
iteration 400, loss 1.8080185651779175
iteration 500, loss 1.8216078281402588
iteration 600, loss 1.743750810623169
iteration 700, loss 1.7883477210998535
iteration 800, loss 1.7672069072723389
iteration 0, loss 1.8143930435180664
iteration 100, loss 1.8672147989273071
iteration 200, loss 1.839712142944336
iteration 300, loss 1.7837809324264526
iteration 400, loss 1.7588872909545898
iteration 500, loss 1.813468337059021
iteration 600, loss 1.8704452514648438
iteration 700, loss 1.7244348526000977
iteration 800, loss 1.7645891904830933
iteration 0, loss 1.748761773109436
iteration 100, loss 1.7728979587554932
iteration 200, loss 1.7865544557571411
iteration 300, loss 1.8317164182662964
iteration 400, loss 1.8389892578125
iteration 500, loss 1.8231890201568604
iteration 600, loss 1.799083948135376
iteration 700, loss 1.8660451173782349
iteration 800, loss 1.7979367971420288
iteration 0, loss 1.8015588521957397
iteration 100, loss 1.8074809312820435
iteration 200, loss 1.864353060722351
iteration 300, loss 1.8213938474655151
iteration 400, loss 1.8380340337753296
iteration 500, loss 1.7934000492095947
iteration 600, loss 1.8254162073135376
iteration 700, loss 1.734818458557129
iteration 800, loss 1.7427411079406738
iteration 0, loss 1.8041821718215942
iteration 100, loss 1.7753835916519165
iteration 200, loss 1.777402400970459
iteration 300, loss 1.8445422649383545
iteration 400, loss 1.8923860788345337
iteration 500, loss 1.8617860078811646
iteration 600, loss 1.8558443784713745
iteration 700, loss 1.7526915073394775
iteration 800, loss 1.8171695470809937
iteration 0, loss 1.8511766195297241
iteration 100, loss 1.8622848987579346
iteration 200, loss 1.781437635421753
iteration 300, loss 1.8505306243896484
iteration 400, loss 1.8406003713607788
iteration 500, loss 1.7851072549819946
iteration 600, loss 1.788691520690918
iteration 700, loss 1.8122934103012085
iteration 800, loss 1.8361860513687134
iteration 0, loss 1.8957542181015015
iteration 100, loss 1.906191110610962
iteration 200, loss 1.8266870975494385
iteration 300, loss 1.898937463760376
iteration 400, loss 1.7875475883483887
iteration 500, loss 1.8391053676605225
iteration 600, loss 1.8238780498504639
iteration 700, loss 1.8048949241638184
iteration 800, loss 1.7932665348052979
iteration 0, loss 1.8531239032745361
iteration 100, loss 1.7541720867156982
iteration 200, loss 1.8198429346084595
iteration 300, loss 1.822257161140442
iteration 400, loss 1.8449766635894775
iteration 500, loss 1.8762933015823364
iteration 600, loss 1.848638892173767
iteration 700, loss 1.8094074726104736
iteration 800, loss 1.832051157951355
iteration 0, loss 1.8114153146743774
iteration 100, loss 1.8743079900741577
iteration 200, loss 1.8807752132415771
iteration 300, loss 1.8242422342300415
iteration 400, loss 1.785184383392334
iteration 500, loss 1.8213177919387817
iteration 600, loss 1.883374571800232
iteration 700, loss 1.8130849599838257
iteration 800, loss 1.7890757322311401
iteration 0, loss 1.7817633152008057
iteration 100, loss 1.822441816329956
iteration 200, loss 1.789794921875
iteration 300, loss 1.7640336751937866
iteration 400, loss 1.8707778453826904
iteration 500, loss 1.8559266328811646
iteration 600, loss 1.7536365985870361
iteration 700, loss 1.84524405002594
iteration 800, loss 1.798921823501587
iteration 0, loss 1.8562949895858765
iteration 100, loss 1.7750978469848633
iteration 200, loss 1.7770984172821045
iteration 300, loss 1.8079824447631836
iteration 400, loss 1.7465999126434326
iteration 500, loss 1.8428751230239868
iteration 600, loss 1.8205833435058594
iteration 700, loss 1.7816022634506226
iteration 800, loss 1.853922963142395
iteration 0, loss 1.7797465324401855
iteration 100, loss 1.794152855873108
iteration 200, loss 1.832247257232666
iteration 300, loss 1.901010274887085
iteration 400, loss 1.823542594909668
iteration 500, loss 1.851934790611267
iteration 600, loss 1.8619252443313599
iteration 700, loss 1.688018798828125
iteration 800, loss 1.824159860610962
iteration 0, loss 1.7967156171798706
iteration 100, loss 1.7949553728103638
iteration 200, loss 1.8122644424438477
iteration 300, loss 1.7599201202392578
iteration 400, loss 1.919279932975769
iteration 500, loss 1.7773966789245605
iteration 600, loss 1.8607091903686523
iteration 700, loss 1.8304301500320435
iteration 800, loss 1.7969160079956055
iteration 0, loss 1.8526965379714966
iteration 100, loss 1.8505127429962158
iteration 200, loss 1.8117464780807495
iteration 300, loss 1.8004156351089478
iteration 400, loss 1.7646722793579102
iteration 500, loss 1.8517659902572632
iteration 600, loss 1.7333779335021973
iteration 700, loss 1.7675594091415405
iteration 800, loss 1.838895559310913
iteration 0, loss 1.78935968875885
iteration 100, loss 1.855606198310852
iteration 200, loss 1.7967591285705566
iteration 300, loss 1.890127182006836
iteration 400, loss 1.7563296556472778
iteration 500, loss 1.8350626230239868
iteration 600, loss 1.8496005535125732
iteration 700, loss 1.7585753202438354
iteration 800, loss 1.8234622478485107
iteration 0, loss 1.8439221382141113
iteration 100, loss 1.822507619857788
iteration 200, loss 1.8601832389831543
iteration 300, loss 1.87042236328125
iteration 400, loss 1.851087212562561
iteration 500, loss 1.7912321090698242
iteration 600, loss 1.7995035648345947
iteration 700, loss 1.7448889017105103
iteration 800, loss 1.806509256362915
iteration 0, loss 1.8007009029388428
iteration 100, loss 1.8578033447265625
iteration 200, loss 1.8275175094604492
iteration 300, loss 1.7923970222473145
iteration 400, loss 1.8707151412963867
iteration 500, loss 1.8372700214385986
iteration 600, loss 1.8087847232818604
iteration 700, loss 1.8635718822479248
iteration 800, loss 1.7614620923995972
iteration 0, loss 1.7339833974838257
iteration 100, loss 1.838165283203125
iteration 200, loss 1.8547263145446777
iteration 300, loss 1.7785977125167847
iteration 400, loss 1.8307554721832275
iteration 500, loss 1.8133490085601807
iteration 600, loss 1.793160319328308
iteration 700, loss 1.7808992862701416
iteration 800, loss 1.8455700874328613
iteration 0, loss 1.7671395540237427
iteration 100, loss 1.7543141841888428
iteration 200, loss 1.7505247592926025
iteration 300, loss 1.847546100616455
iteration 400, loss 1.8441505432128906
iteration 500, loss 1.7458093166351318
iteration 600, loss 1.844894528388977
iteration 700, loss 1.8013663291931152
iteration 800, loss 1.79695463180542
iteration 0, loss 1.775200605392456
iteration 100, loss 1.8186097145080566
iteration 200, loss 1.8487920761108398
iteration 300, loss 1.8327972888946533
iteration 400, loss 1.810421109199524
iteration 500, loss 1.8380388021469116
iteration 600, loss 1.810003638267517
iteration 700, loss 1.7690856456756592
iteration 800, loss 1.7470194101333618
iteration 0, loss 1.8601528406143188
iteration 100, loss 1.783535122871399
iteration 200, loss 1.809627652168274
iteration 300, loss 1.8416328430175781
iteration 400, loss 1.824479341506958
iteration 500, loss 1.818604588508606
iteration 600, loss 1.8427284955978394
iteration 700, loss 1.7783687114715576
iteration 800, loss 1.766100287437439
iteration 0, loss 1.7890313863754272
iteration 100, loss 1.8390955924987793
iteration 200, loss 1.789705753326416
iteration 300, loss 1.8804796934127808
iteration 400, loss 1.7803887128829956
iteration 500, loss 1.8005667924880981
iteration 600, loss 1.797231674194336
iteration 700, loss 1.8160126209259033
iteration 800, loss 1.7790378332138062
iteration 0, loss 1.7946938276290894
iteration 100, loss 1.8545020818710327
iteration 200, loss 1.7877233028411865
iteration 300, loss 1.8327738046646118
iteration 400, loss 1.7878497838974
iteration 500, loss 1.765744686126709
iteration 600, loss 1.8036348819732666
iteration 700, loss 1.8274016380310059
iteration 800, loss 1.7355530261993408
iteration 0, loss 1.8125619888305664
iteration 100, loss 1.834714412689209
iteration 200, loss 1.7959067821502686
iteration 300, loss 1.8652883768081665
iteration 400, loss 1.8326736688613892
iteration 500, loss 1.8974939584732056
iteration 600, loss 1.79909086227417
iteration 700, loss 1.7768915891647339
iteration 800, loss 1.834306240081787
iteration 0, loss 1.8056726455688477
iteration 100, loss 1.8513606786727905
iteration 200, loss 1.750180721282959
iteration 300, loss 1.8132864236831665
iteration 400, loss 1.8094278573989868
iteration 500, loss 1.8466274738311768
iteration 600, loss 1.7462879419326782
iteration 700, loss 1.8429481983184814
iteration 800, loss 1.8118103742599487
iteration 0, loss 1.8416814804077148
iteration 100, loss 1.8559600114822388
iteration 200, loss 1.8930014371871948
iteration 300, loss 1.8301665782928467
iteration 400, loss 1.7901002168655396
iteration 500, loss 1.7819234132766724
iteration 600, loss 1.7499727010726929
iteration 700, loss 1.8481109142303467
iteration 800, loss 1.8170713186264038
iteration 0, loss 1.838226079940796
iteration 100, loss 1.8165491819381714
iteration 200, loss 1.7886009216308594
iteration 300, loss 1.7906420230865479
iteration 400, loss 1.7693767547607422
iteration 500, loss 1.8662023544311523
iteration 600, loss 1.805437445640564
iteration 700, loss 1.8175033330917358
iteration 800, loss 1.7801687717437744
iteration 0, loss 1.7592618465423584
iteration 100, loss 1.8467105627059937
iteration 200, loss 1.7625274658203125
iteration 300, loss 1.8062235116958618
iteration 400, loss 1.8915003538131714
iteration 500, loss 1.8235399723052979
iteration 600, loss 1.8213645219802856
iteration 700, loss 1.8299022912979126
iteration 800, loss 1.8009413480758667
iteration 0, loss 1.7627832889556885
iteration 100, loss 1.8094053268432617
iteration 200, loss 1.8810275793075562
iteration 300, loss 1.718667984008789
iteration 400, loss 1.7769482135772705
iteration 500, loss 1.778256893157959
iteration 600, loss 1.8059566020965576
iteration 700, loss 1.7769664525985718
iteration 800, loss 1.8716644048690796
iteration 0, loss 1.8737438917160034
iteration 100, loss 1.7960577011108398
iteration 200, loss 1.8138275146484375
iteration 300, loss 1.7664262056350708
iteration 400, loss 1.8233562707901
iteration 500, loss 1.8649978637695312
iteration 600, loss 1.8001809120178223
iteration 700, loss 1.7953569889068604
iteration 800, loss 1.7765918970108032
iteration 0, loss 1.8947725296020508
iteration 100, loss 1.8285088539123535
iteration 200, loss 1.805683970451355
iteration 300, loss 1.8224866390228271
iteration 400, loss 1.8662654161453247
iteration 500, loss 1.8312230110168457
iteration 600, loss 1.7819488048553467
iteration 700, loss 1.8732619285583496
iteration 800, loss 1.829047679901123
iteration 0, loss 1.8563721179962158
iteration 100, loss 1.837337851524353
iteration 200, loss 1.7630497217178345
iteration 300, loss 1.8444910049438477
iteration 400, loss 1.8128280639648438
iteration 500, loss 1.7838753461837769
iteration 600, loss 1.7929965257644653
iteration 700, loss 1.7894567251205444
iteration 800, loss 1.8446329832077026
iteration 0, loss 1.7645686864852905
iteration 100, loss 1.8629050254821777
iteration 200, loss 1.818294882774353
iteration 300, loss 1.7822074890136719
iteration 400, loss 1.925034761428833
iteration 500, loss 1.7473137378692627
iteration 600, loss 1.8768879175186157
iteration 700, loss 1.7993555068969727
iteration 800, loss 1.7859307527542114
iteration 0, loss 1.8115122318267822
iteration 100, loss 1.8039956092834473
iteration 200, loss 1.7602670192718506
iteration 300, loss 1.8708410263061523
iteration 400, loss 1.7630926370620728
iteration 500, loss 1.8370108604431152
iteration 600, loss 1.8247066736221313
iteration 700, loss 1.696947693824768
iteration 800, loss 1.8355168104171753
iteration 0, loss 1.8528380393981934
iteration 100, loss 1.8311666250228882
iteration 200, loss 1.8725080490112305
iteration 300, loss 1.8478320837020874
iteration 400, loss 1.8676159381866455
iteration 500, loss 1.8631702661514282
iteration 600, loss 1.7667531967163086
iteration 700, loss 1.7775261402130127
iteration 800, loss 1.8542062044143677
fold 2 accuracy: 0.46435714285714286
iteration 0, loss 1.8515162467956543
iteration 100, loss 1.7940980195999146
iteration 200, loss 1.7804207801818848
iteration 300, loss 1.8039426803588867
iteration 400, loss 1.8861745595932007
iteration 500, loss 1.8416322469711304
iteration 600, loss 1.797242522239685
iteration 700, loss 1.8238645792007446
iteration 800, loss 1.8959894180297852
iteration 0, loss 1.789371371269226
iteration 100, loss 1.7915425300598145
iteration 200, loss 1.7792755365371704
iteration 300, loss 1.857310175895691
iteration 400, loss 1.7599583864212036
iteration 500, loss 1.7770512104034424
iteration 600, loss 1.810227632522583
iteration 700, loss 1.8479502201080322
iteration 800, loss 1.8447145223617554
iteration 0, loss 1.7915937900543213
iteration 100, loss 1.8128509521484375
iteration 200, loss 1.8222413063049316
iteration 300, loss 1.8105067014694214
iteration 400, loss 1.761475920677185
iteration 500, loss 1.7396286725997925
iteration 600, loss 1.8690648078918457
iteration 700, loss 1.7929573059082031
iteration 800, loss 1.750553846359253
iteration 0, loss 1.818619966506958
iteration 100, loss 1.7403311729431152
iteration 200, loss 1.7658848762512207
iteration 300, loss 1.810161828994751
iteration 400, loss 1.806193232536316
iteration 500, loss 1.7880932092666626
iteration 600, loss 1.8683003187179565
iteration 700, loss 1.7530478239059448
iteration 800, loss 1.7771739959716797
iteration 0, loss 1.7864651679992676
iteration 100, loss 1.8194911479949951
iteration 200, loss 1.7845628261566162
iteration 300, loss 1.815054178237915
iteration 400, loss 1.7872121334075928
iteration 500, loss 1.8366799354553223
iteration 600, loss 1.8194328546524048
iteration 700, loss 1.8208742141723633
iteration 800, loss 1.8271095752716064
iteration 0, loss 1.8371515274047852
iteration 100, loss 1.8164303302764893
iteration 200, loss 1.772369623184204
iteration 300, loss 1.7876111268997192
iteration 400, loss 1.8314893245697021
iteration 500, loss 1.7905184030532837
iteration 600, loss 1.8525817394256592
iteration 700, loss 1.7758302688598633
iteration 800, loss 1.9038180112838745
iteration 0, loss 1.8359463214874268
iteration 100, loss 1.8149393796920776
iteration 200, loss 1.8286117315292358
iteration 300, loss 1.8515703678131104
iteration 400, loss 1.8213814496994019
iteration 500, loss 1.8185920715332031
iteration 600, loss 1.7605887651443481
iteration 700, loss 1.8365148305892944
iteration 800, loss 1.805908441543579
iteration 0, loss 1.8159377574920654
iteration 100, loss 1.8177632093429565
iteration 200, loss 1.7801635265350342
iteration 300, loss 1.8300262689590454
iteration 400, loss 1.7855716943740845
iteration 500, loss 1.776388168334961
iteration 600, loss 1.852009892463684
iteration 700, loss 1.837685465812683
iteration 800, loss 1.8363337516784668
iteration 0, loss 1.8099420070648193
iteration 100, loss 1.7940798997879028
iteration 200, loss 1.8271214962005615
iteration 300, loss 1.7388895750045776
iteration 400, loss 1.8409322500228882
iteration 500, loss 1.839562177658081
iteration 600, loss 1.7798271179199219
iteration 700, loss 1.8184179067611694
iteration 800, loss 1.8113853931427002
iteration 0, loss 1.763472080230713
iteration 100, loss 1.8843472003936768
iteration 200, loss 1.8726383447647095
iteration 300, loss 1.7934143543243408
iteration 400, loss 1.852097988128662
iteration 500, loss 1.7770949602127075
iteration 600, loss 1.7777061462402344
iteration 700, loss 1.8974967002868652
iteration 800, loss 1.8513966798782349
iteration 0, loss 1.918793797492981
iteration 100, loss 1.7559516429901123
iteration 200, loss 1.7870134115219116
iteration 300, loss 1.8783701658248901
iteration 400, loss 1.825842022895813
iteration 500, loss 1.8390074968338013
iteration 600, loss 1.8000026941299438
iteration 700, loss 1.826405644416809
iteration 800, loss 1.8089797496795654
iteration 0, loss 1.8116587400436401
iteration 100, loss 1.814540982246399
iteration 200, loss 1.821883201599121
iteration 300, loss 1.8486250638961792
iteration 400, loss 1.8198256492614746
iteration 500, loss 1.7819886207580566
iteration 600, loss 1.8332533836364746
iteration 700, loss 1.8973983526229858
iteration 800, loss 1.8294650316238403
iteration 0, loss 1.845688819885254
iteration 100, loss 1.7823125123977661
iteration 200, loss 1.8490626811981201
iteration 300, loss 1.856296181678772
iteration 400, loss 1.8186076879501343
iteration 500, loss 1.7521246671676636
iteration 600, loss 1.845616102218628
iteration 700, loss 1.8554699420928955
iteration 800, loss 1.8290491104125977
iteration 0, loss 1.8513414859771729
iteration 100, loss 1.7466955184936523
iteration 200, loss 1.8688344955444336
iteration 300, loss 1.832943081855774
iteration 400, loss 1.8029026985168457
iteration 500, loss 1.801798939704895
iteration 600, loss 1.8652057647705078
iteration 700, loss 1.8981423377990723
iteration 800, loss 1.8106179237365723
iteration 0, loss 1.804334044456482
iteration 100, loss 1.8201349973678589
iteration 200, loss 1.8267927169799805
iteration 300, loss 1.8017916679382324
iteration 400, loss 1.9006775617599487
iteration 500, loss 1.8286620378494263
iteration 600, loss 1.7130330801010132
iteration 700, loss 1.8718398809432983
iteration 800, loss 1.8198864459991455
iteration 0, loss 1.7987698316574097
iteration 100, loss 1.8020575046539307
iteration 200, loss 1.7934644222259521
iteration 300, loss 1.8535221815109253
iteration 400, loss 1.822008490562439
iteration 500, loss 1.8468619585037231
iteration 600, loss 1.9058953523635864
iteration 700, loss 1.7938635349273682
iteration 800, loss 1.881489872932434
iteration 0, loss 1.7836261987686157
iteration 100, loss 1.8001480102539062
iteration 200, loss 1.8131345510482788
iteration 300, loss 1.8088988065719604
iteration 400, loss 1.7676818370819092
iteration 500, loss 1.8483915328979492
iteration 600, loss 1.8892366886138916
iteration 700, loss 1.7829176187515259
iteration 800, loss 1.8878073692321777
iteration 0, loss 1.8432611227035522
iteration 100, loss 1.7556575536727905
iteration 200, loss 1.801399827003479
iteration 300, loss 1.8368964195251465
iteration 400, loss 1.8517322540283203
iteration 500, loss 1.878209114074707
iteration 600, loss 1.791682481765747
iteration 700, loss 1.8139066696166992
iteration 800, loss 1.775378942489624
iteration 0, loss 1.754376769065857
iteration 100, loss 1.8772878646850586
iteration 200, loss 1.7774832248687744
iteration 300, loss 1.7987916469573975
iteration 400, loss 1.8096117973327637
iteration 500, loss 1.807460904121399
iteration 600, loss 1.8132617473602295
iteration 700, loss 1.9200342893600464
iteration 800, loss 1.8438236713409424
iteration 0, loss 1.8428775072097778
iteration 100, loss 1.9050472974777222
iteration 200, loss 1.7806603908538818
iteration 300, loss 1.7442739009857178
iteration 400, loss 1.806389331817627
iteration 500, loss 1.7240325212478638
iteration 600, loss 1.8826522827148438
iteration 700, loss 1.8308290243148804
iteration 800, loss 1.82174813747406
iteration 0, loss 1.7793375253677368
iteration 100, loss 1.7700793743133545
iteration 200, loss 1.7608824968338013
iteration 300, loss 1.8206168413162231
iteration 400, loss 1.7634799480438232
iteration 500, loss 1.871024250984192
iteration 600, loss 1.8190479278564453
iteration 700, loss 1.8690043687820435
iteration 800, loss 1.9228101968765259
iteration 0, loss 1.8050570487976074
iteration 100, loss 1.8253686428070068
iteration 200, loss 1.7764450311660767
iteration 300, loss 1.890013575553894
iteration 400, loss 1.8294600248336792
iteration 500, loss 1.8319427967071533
iteration 600, loss 1.7734832763671875
iteration 700, loss 1.7993519306182861
iteration 800, loss 1.8213226795196533
iteration 0, loss 1.85532546043396
iteration 100, loss 1.8653082847595215
iteration 200, loss 1.726536512374878
iteration 300, loss 1.8539317846298218
iteration 400, loss 1.8122974634170532
iteration 500, loss 1.7542834281921387
iteration 600, loss 1.8443913459777832
iteration 700, loss 1.934584617614746
iteration 800, loss 1.7944639921188354
iteration 0, loss 1.812413215637207
iteration 100, loss 1.7583061456680298
iteration 200, loss 1.8439644575119019
iteration 300, loss 1.7889564037322998
iteration 400, loss 1.8409479856491089
iteration 500, loss 1.779952049255371
iteration 600, loss 1.805492639541626
iteration 700, loss 1.7977497577667236
iteration 800, loss 1.8019341230392456
iteration 0, loss 1.8395153284072876
iteration 100, loss 1.7553339004516602
iteration 200, loss 1.9262995719909668
iteration 300, loss 1.713440179824829
iteration 400, loss 1.7866921424865723
iteration 500, loss 1.8023924827575684
iteration 600, loss 1.884665846824646
iteration 700, loss 1.7531678676605225
iteration 800, loss 1.885196566581726
iteration 0, loss 1.8677550554275513
iteration 100, loss 1.7944318056106567
iteration 200, loss 1.8290836811065674
iteration 300, loss 1.8227980136871338
iteration 400, loss 1.8034963607788086
iteration 500, loss 1.7994682788848877
iteration 600, loss 1.7696352005004883
iteration 700, loss 1.8455688953399658
iteration 800, loss 1.8388534784317017
iteration 0, loss 1.806483268737793
iteration 100, loss 1.8019871711730957
iteration 200, loss 1.772972583770752
iteration 300, loss 1.7501777410507202
iteration 400, loss 1.8300069570541382
iteration 500, loss 1.8585704565048218
iteration 600, loss 1.8086994886398315
iteration 700, loss 1.8490214347839355
iteration 800, loss 1.7651491165161133
iteration 0, loss 1.7780406475067139
iteration 100, loss 1.857956886291504
iteration 200, loss 1.8218029737472534
iteration 300, loss 1.8037632703781128
iteration 400, loss 1.7713359594345093
iteration 500, loss 1.7820645570755005
iteration 600, loss 1.8505666255950928
iteration 700, loss 1.7709355354309082
iteration 800, loss 1.7541065216064453
iteration 0, loss 1.785596251487732
iteration 100, loss 1.8127187490463257
iteration 200, loss 1.7549222707748413
iteration 300, loss 1.7456974983215332
iteration 400, loss 1.7622336149215698
iteration 500, loss 1.837213397026062
iteration 600, loss 1.7450002431869507
iteration 700, loss 1.755192518234253
iteration 800, loss 1.795117735862732
iteration 0, loss 1.7660638093948364
iteration 100, loss 1.7677756547927856
iteration 200, loss 1.9223558902740479
iteration 300, loss 1.8869835138320923
iteration 400, loss 1.7897989749908447
iteration 500, loss 1.7547922134399414
iteration 600, loss 1.8439583778381348
iteration 700, loss 1.74240243434906
iteration 800, loss 1.8091596364974976
iteration 0, loss 1.7904361486434937
iteration 100, loss 1.8193116188049316
iteration 200, loss 1.7999613285064697
iteration 300, loss 1.809370994567871
iteration 400, loss 1.7857468128204346
iteration 500, loss 1.7698428630828857
iteration 600, loss 1.7519148588180542
iteration 700, loss 1.7507482767105103
iteration 800, loss 1.7430676221847534
iteration 0, loss 1.834086298942566
iteration 100, loss 1.795720100402832
iteration 200, loss 1.7673366069793701
iteration 300, loss 1.8736988306045532
iteration 400, loss 1.7847895622253418
iteration 500, loss 1.8254308700561523
iteration 600, loss 1.7877429723739624
iteration 700, loss 1.8124136924743652
iteration 800, loss 1.8153924942016602
iteration 0, loss 1.8486148118972778
iteration 100, loss 1.8398187160491943
iteration 200, loss 1.8271186351776123
iteration 300, loss 1.7581881284713745
iteration 400, loss 1.8662432432174683
iteration 500, loss 1.821218729019165
iteration 600, loss 1.8583251237869263
iteration 700, loss 1.8817023038864136
iteration 800, loss 1.8436322212219238
iteration 0, loss 1.8342945575714111
iteration 100, loss 1.9065086841583252
iteration 200, loss 1.8515104055404663
iteration 300, loss 1.7292207479476929
iteration 400, loss 1.8602402210235596
iteration 500, loss 1.7770466804504395
iteration 600, loss 1.8362656831741333
iteration 700, loss 1.7957634925842285
iteration 800, loss 1.8269712924957275
iteration 0, loss 1.7918391227722168
iteration 100, loss 1.8081899881362915
iteration 200, loss 1.7389365434646606
iteration 300, loss 1.7500758171081543
iteration 400, loss 1.8523520231246948
iteration 500, loss 1.823026418685913
iteration 600, loss 1.811047911643982
iteration 700, loss 1.798719882965088
iteration 800, loss 1.715698003768921
iteration 0, loss 1.8116474151611328
iteration 100, loss 1.812982201576233
iteration 200, loss 1.7897186279296875
iteration 300, loss 1.8291178941726685
iteration 400, loss 1.8137729167938232
iteration 500, loss 1.7724815607070923
iteration 600, loss 1.7630900144577026
iteration 700, loss 1.8124068975448608
iteration 800, loss 1.736977458000183
iteration 0, loss 1.8126469850540161
iteration 100, loss 1.8132717609405518
iteration 200, loss 1.7948728799819946
iteration 300, loss 1.8176120519638062
iteration 400, loss 1.8140878677368164
iteration 500, loss 1.8744077682495117
iteration 600, loss 1.7687288522720337
iteration 700, loss 1.7687984704971313
iteration 800, loss 1.8269147872924805
iteration 0, loss 1.780298113822937
iteration 100, loss 1.8293612003326416
iteration 200, loss 1.752204418182373
iteration 300, loss 1.771472454071045
iteration 400, loss 1.8385541439056396
iteration 500, loss 1.8391621112823486
iteration 600, loss 1.8738234043121338
iteration 700, loss 1.829751968383789
iteration 800, loss 1.8294280767440796
iteration 0, loss 1.8321971893310547
iteration 100, loss 1.8440641164779663
iteration 200, loss 1.7869892120361328
iteration 300, loss 1.7414329051971436
iteration 400, loss 1.8023386001586914
iteration 500, loss 1.8454333543777466
iteration 600, loss 1.8385460376739502
iteration 700, loss 1.7572107315063477
iteration 800, loss 1.8152192831039429
iteration 0, loss 1.8502839803695679
iteration 100, loss 1.8250969648361206
iteration 200, loss 1.8422945737838745
iteration 300, loss 1.8255870342254639
iteration 400, loss 1.767508864402771
iteration 500, loss 1.7815139293670654
iteration 600, loss 1.850560188293457
iteration 700, loss 1.8317058086395264
iteration 800, loss 1.8529589176177979
iteration 0, loss 1.8100333213806152
iteration 100, loss 1.759826421737671
iteration 200, loss 1.8075082302093506
iteration 300, loss 1.8550498485565186
iteration 400, loss 1.7892943620681763
iteration 500, loss 1.8570337295532227
iteration 600, loss 1.7689063549041748
iteration 700, loss 1.8254159688949585
iteration 800, loss 1.8092615604400635
iteration 0, loss 1.7950105667114258
iteration 100, loss 1.7359997034072876
iteration 200, loss 1.8245424032211304
iteration 300, loss 1.9038715362548828
iteration 400, loss 1.8776462078094482
iteration 500, loss 1.8364161252975464
iteration 600, loss 1.8483755588531494
iteration 700, loss 1.7823076248168945
iteration 800, loss 1.798983097076416
iteration 0, loss 1.8231974840164185
iteration 100, loss 1.8572196960449219
iteration 200, loss 1.7869033813476562
iteration 300, loss 1.7990821599960327
iteration 400, loss 1.8700395822525024
iteration 500, loss 1.7893930673599243
iteration 600, loss 1.835313081741333
iteration 700, loss 1.8243454694747925
iteration 800, loss 1.8524686098098755
iteration 0, loss 1.8104145526885986
iteration 100, loss 1.7779955863952637
iteration 200, loss 1.7634750604629517
iteration 300, loss 1.7846256494522095
iteration 400, loss 1.7579785585403442
iteration 500, loss 1.8101778030395508
iteration 600, loss 1.7981547117233276
iteration 700, loss 1.7989883422851562
iteration 800, loss 1.9357414245605469
iteration 0, loss 1.8638955354690552
iteration 100, loss 1.8322105407714844
iteration 200, loss 1.7938387393951416
iteration 300, loss 1.7712905406951904
iteration 400, loss 1.79598867893219
iteration 500, loss 1.8149749040603638
iteration 600, loss 1.866361141204834
iteration 700, loss 1.775146245956421
iteration 800, loss 1.7456111907958984
iteration 0, loss 1.820454478263855
iteration 100, loss 1.8197556734085083
iteration 200, loss 1.8634817600250244
iteration 300, loss 1.8175874948501587
iteration 400, loss 1.7551242113113403
iteration 500, loss 1.8229023218154907
iteration 600, loss 1.7874287366867065
iteration 700, loss 1.8576006889343262
iteration 800, loss 1.79948091506958
iteration 0, loss 1.84477961063385
iteration 100, loss 1.8861727714538574
iteration 200, loss 1.8790069818496704
iteration 300, loss 1.8137471675872803
iteration 400, loss 1.789716124534607
iteration 500, loss 1.791473150253296
iteration 600, loss 1.835892677307129
iteration 700, loss 1.7614214420318604
iteration 800, loss 1.772758960723877
iteration 0, loss 1.7698951959609985
iteration 100, loss 1.8622702360153198
iteration 200, loss 1.796738624572754
iteration 300, loss 1.7618465423583984
iteration 400, loss 1.8020050525665283
iteration 500, loss 1.8302069902420044
iteration 600, loss 1.8115159273147583
iteration 700, loss 1.8229788541793823
iteration 800, loss 1.797101616859436
iteration 0, loss 1.8769389390945435
iteration 100, loss 1.819852352142334
iteration 200, loss 1.820793628692627
iteration 300, loss 1.7580246925354004
iteration 400, loss 1.7999119758605957
iteration 500, loss 1.8196178674697876
iteration 600, loss 1.8473577499389648
iteration 700, loss 1.7493741512298584
iteration 800, loss 1.8971866369247437
iteration 0, loss 1.8277194499969482
iteration 100, loss 1.8293235301971436
iteration 200, loss 1.8100721836090088
iteration 300, loss 1.7790944576263428
iteration 400, loss 1.8264319896697998
iteration 500, loss 1.846415638923645
iteration 600, loss 1.8426223993301392
iteration 700, loss 1.8408637046813965
iteration 800, loss 1.8235085010528564
fold 3 accuracy: 0.4835
iteration 0, loss 1.824611783027649
iteration 100, loss 1.800000786781311
iteration 200, loss 1.9131107330322266
iteration 300, loss 1.8090524673461914
iteration 400, loss 1.80441415309906
iteration 500, loss 1.8413242101669312
iteration 600, loss 1.8456703424453735
iteration 700, loss 1.8150279521942139
iteration 800, loss 1.735671043395996
iteration 0, loss 1.7723747491836548
iteration 100, loss 1.803030014038086
iteration 200, loss 1.845887303352356
iteration 300, loss 1.8260515928268433
iteration 400, loss 1.8237183094024658
iteration 500, loss 1.8752341270446777
iteration 600, loss 1.830825924873352
iteration 700, loss 1.7739558219909668
iteration 800, loss 1.8353956937789917
iteration 0, loss 1.8914787769317627
iteration 100, loss 1.8172235488891602
iteration 200, loss 1.8062899112701416
iteration 300, loss 1.8046058416366577
iteration 400, loss 1.8454314470291138
iteration 500, loss 1.874716877937317
iteration 600, loss 1.8433146476745605
iteration 700, loss 1.7372910976409912
iteration 800, loss 1.7380353212356567
iteration 0, loss 1.8205782175064087
iteration 100, loss 1.8403078317642212
iteration 200, loss 1.8783559799194336
iteration 300, loss 1.8340528011322021
iteration 400, loss 1.8932793140411377
iteration 500, loss 1.7985336780548096
iteration 600, loss 1.8047151565551758
iteration 700, loss 1.7931501865386963
iteration 800, loss 1.7894693613052368
iteration 0, loss 1.7787690162658691
iteration 100, loss 1.829616665840149
iteration 200, loss 1.8012536764144897
iteration 300, loss 1.8174753189086914
iteration 400, loss 1.8047012090682983
iteration 500, loss 1.78605318069458
iteration 600, loss 1.8041809797286987
iteration 700, loss 1.7956980466842651
iteration 800, loss 1.910347819328308
iteration 0, loss 1.8283202648162842
iteration 100, loss 1.8109195232391357
iteration 200, loss 1.7835478782653809
iteration 300, loss 1.7392089366912842
iteration 400, loss 1.8324650526046753
iteration 500, loss 1.873192310333252
iteration 600, loss 1.7867319583892822
iteration 700, loss 1.8152856826782227
iteration 800, loss 1.8565902709960938
iteration 0, loss 1.8075268268585205
iteration 100, loss 1.8337129354476929
iteration 200, loss 1.816755771636963
iteration 300, loss 1.7293435335159302
iteration 400, loss 1.847177267074585
iteration 500, loss 1.8466850519180298
iteration 600, loss 1.7824978828430176
iteration 700, loss 1.7910972833633423
iteration 800, loss 1.8348197937011719
iteration 0, loss 1.9167274236679077
iteration 100, loss 1.762446641921997
iteration 200, loss 1.7810553312301636
iteration 300, loss 1.7701724767684937
iteration 400, loss 1.758475661277771
iteration 500, loss 1.8509457111358643
iteration 600, loss 1.8347876071929932
iteration 700, loss 1.7543721199035645
iteration 800, loss 1.8577326536178589
iteration 0, loss 1.8007116317749023
iteration 100, loss 1.8434269428253174
iteration 200, loss 1.8048512935638428
iteration 300, loss 1.88204026222229
iteration 400, loss 1.803979754447937
iteration 500, loss 1.9162287712097168
iteration 600, loss 1.8630354404449463
iteration 700, loss 1.8491005897521973
iteration 800, loss 1.8690937757492065
iteration 0, loss 1.7852247953414917
iteration 100, loss 1.7832304239273071
iteration 200, loss 1.9054429531097412
iteration 300, loss 1.8341573476791382
iteration 400, loss 1.727299451828003
iteration 500, loss 1.8588758707046509
iteration 600, loss 1.9039702415466309
iteration 700, loss 1.81780207157135
iteration 800, loss 1.7856640815734863
iteration 0, loss 1.7785288095474243
iteration 100, loss 1.8637206554412842
iteration 200, loss 1.839921474456787
iteration 300, loss 1.7625117301940918
iteration 400, loss 1.7531318664550781
iteration 500, loss 1.8097862005233765
iteration 600, loss 1.805888056755066
iteration 700, loss 1.8380674123764038
iteration 800, loss 1.8153916597366333
iteration 0, loss 1.7826517820358276
iteration 100, loss 1.8481066226959229
iteration 200, loss 1.8499542474746704
iteration 300, loss 1.793620228767395
iteration 400, loss 1.7858314514160156
iteration 500, loss 1.7973225116729736
iteration 600, loss 1.8857319355010986
iteration 700, loss 1.8397655487060547
iteration 800, loss 1.7840454578399658
iteration 0, loss 1.7995538711547852
iteration 100, loss 1.8247087001800537
iteration 200, loss 1.8200476169586182
iteration 300, loss 1.852708101272583
iteration 400, loss 1.8058404922485352
iteration 500, loss 1.7759647369384766
iteration 600, loss 1.9042901992797852
iteration 700, loss 1.7493478059768677
iteration 800, loss 1.7880560159683228
iteration 0, loss 1.8526012897491455
iteration 100, loss 1.7836012840270996
iteration 200, loss 1.811478853225708
iteration 300, loss 1.791643738746643
iteration 400, loss 1.7964400053024292
iteration 500, loss 1.806280255317688
iteration 600, loss 1.881372332572937
iteration 700, loss 1.8182932138442993
iteration 800, loss 1.8080567121505737
iteration 0, loss 1.8168281316757202
iteration 100, loss 1.8007965087890625
iteration 200, loss 1.746093511581421
iteration 300, loss 1.8277941942214966
iteration 400, loss 1.7519898414611816
iteration 500, loss 1.812927007675171
iteration 600, loss 1.8169316053390503
iteration 700, loss 1.8329907655715942
iteration 800, loss 1.8117518424987793
iteration 0, loss 1.8102203607559204
iteration 100, loss 1.8300197124481201
iteration 200, loss 1.8274974822998047
iteration 300, loss 1.8024442195892334
iteration 400, loss 1.8053631782531738
iteration 500, loss 1.878815770149231
iteration 600, loss 1.7602009773254395
iteration 700, loss 1.7973041534423828
iteration 800, loss 1.865761160850525
iteration 0, loss 1.8099263906478882
iteration 100, loss 1.7619329690933228
iteration 200, loss 1.8191425800323486
iteration 300, loss 1.8144601583480835
iteration 400, loss 1.7384535074234009
iteration 500, loss 1.7948585748672485
iteration 600, loss 1.8871625661849976
iteration 700, loss 1.886304497718811
iteration 800, loss 1.9167001247406006
iteration 0, loss 1.7819582223892212
iteration 100, loss 1.821382999420166
iteration 200, loss 1.8188109397888184
iteration 300, loss 1.8131011724472046
iteration 400, loss 1.8309507369995117
iteration 500, loss 1.8767597675323486
iteration 600, loss 1.8619823455810547
iteration 700, loss 1.9036818742752075
iteration 800, loss 1.8693971633911133
iteration 0, loss 1.7606335878372192
iteration 100, loss 1.7638771533966064
iteration 200, loss 1.7648342847824097
iteration 300, loss 1.7532076835632324
iteration 400, loss 1.8631675243377686
iteration 500, loss 1.784867525100708
iteration 600, loss 1.848766803741455
iteration 700, loss 1.8458425998687744
iteration 800, loss 1.8197516202926636
iteration 0, loss 1.8234078884124756
iteration 100, loss 1.7455967664718628
iteration 200, loss 1.8763121366500854
iteration 300, loss 1.8294122219085693
iteration 400, loss 1.7979236841201782
iteration 500, loss 1.801906943321228
iteration 600, loss 1.8534914255142212
iteration 700, loss 1.7812225818634033
iteration 800, loss 1.8616443872451782
iteration 0, loss 1.720849871635437
iteration 100, loss 1.8961241245269775
iteration 200, loss 1.7681676149368286
iteration 300, loss 1.7674131393432617
iteration 400, loss 1.8479254245758057
iteration 500, loss 1.7921369075775146
iteration 600, loss 1.749529242515564
iteration 700, loss 1.8784950971603394
iteration 800, loss 1.8324192762374878
iteration 0, loss 1.8012869358062744
iteration 100, loss 1.7454289197921753
iteration 200, loss 1.7731093168258667
iteration 300, loss 1.8482376337051392
iteration 400, loss 1.9219698905944824
iteration 500, loss 1.7887141704559326
iteration 600, loss 1.7764432430267334
iteration 700, loss 1.8191858530044556
iteration 800, loss 1.7784757614135742
iteration 0, loss 1.829124927520752
iteration 100, loss 1.7329766750335693
iteration 200, loss 1.799658179283142
iteration 300, loss 1.7586661577224731
iteration 400, loss 1.8256553411483765
iteration 500, loss 1.7738244533538818
iteration 600, loss 1.94755220413208
iteration 700, loss 1.7503468990325928
iteration 800, loss 1.7833137512207031
iteration 0, loss 1.7985700368881226
iteration 100, loss 1.8006991147994995
iteration 200, loss 1.8743454217910767
iteration 300, loss 1.7852734327316284
iteration 400, loss 1.8656314611434937
iteration 500, loss 1.7785992622375488
iteration 600, loss 1.8590257167816162
iteration 700, loss 1.8417576551437378
iteration 800, loss 1.7908580303192139
iteration 0, loss 1.745331048965454
iteration 100, loss 1.8051201105117798
iteration 200, loss 1.76431405544281
iteration 300, loss 1.7984015941619873
iteration 400, loss 1.8919447660446167
iteration 500, loss 1.7422688007354736
iteration 600, loss 1.8231676816940308
iteration 700, loss 1.8503048419952393
iteration 800, loss 1.7893784046173096
iteration 0, loss 1.7779797315597534
iteration 100, loss 1.847900152206421
iteration 200, loss 1.7452878952026367
iteration 300, loss 1.7999820709228516
iteration 400, loss 1.8409266471862793
iteration 500, loss 1.8173978328704834
iteration 600, loss 1.8440290689468384
iteration 700, loss 1.8247075080871582
iteration 800, loss 1.7661144733428955
iteration 0, loss 1.8871639966964722
iteration 100, loss 1.758726954460144
iteration 200, loss 1.8073837757110596
iteration 300, loss 1.826683759689331
iteration 400, loss 1.7806434631347656
iteration 500, loss 1.7904605865478516
iteration 600, loss 1.8268500566482544
iteration 700, loss 1.8106420040130615
iteration 800, loss 1.818185567855835
iteration 0, loss 1.8116133213043213
iteration 100, loss 1.8209303617477417
iteration 200, loss 1.8243086338043213
iteration 300, loss 1.8047468662261963
iteration 400, loss 1.7729918956756592
iteration 500, loss 1.780808448791504
iteration 600, loss 1.9026952981948853
iteration 700, loss 1.8227972984313965
iteration 800, loss 1.7879258394241333
iteration 0, loss 1.893561601638794
iteration 100, loss 1.7960177659988403
iteration 200, loss 1.7768938541412354
iteration 300, loss 1.9214515686035156
iteration 400, loss 1.8334318399429321
iteration 500, loss 1.8852506875991821
iteration 600, loss 1.8761365413665771
iteration 700, loss 1.80193030834198
iteration 800, loss 1.8451004028320312
iteration 0, loss 1.8375322818756104
iteration 100, loss 1.8452826738357544
iteration 200, loss 1.8170591592788696
iteration 300, loss 1.8309459686279297
iteration 400, loss 1.830139398574829
iteration 500, loss 1.7920382022857666
iteration 600, loss 1.8637462854385376
iteration 700, loss 1.8026477098464966
iteration 800, loss 1.8164006471633911
iteration 0, loss 1.855926752090454
iteration 100, loss 1.8020535707473755
iteration 200, loss 1.8207638263702393
iteration 300, loss 1.877591848373413
iteration 400, loss 1.7519501447677612
iteration 500, loss 1.7250375747680664
iteration 600, loss 1.8189138174057007
iteration 700, loss 1.937939167022705
iteration 800, loss 1.8387796878814697
iteration 0, loss 1.75291907787323
iteration 100, loss 1.8362116813659668
iteration 200, loss 1.774845838546753
iteration 300, loss 1.7695372104644775
iteration 400, loss 1.889039397239685
iteration 500, loss 1.8197953701019287
iteration 600, loss 1.813704013824463
iteration 700, loss 1.8131637573242188
iteration 800, loss 1.757448673248291
iteration 0, loss 1.7906694412231445
iteration 100, loss 1.8610680103302002
iteration 200, loss 1.8299447298049927
iteration 300, loss 1.8189043998718262
iteration 400, loss 1.7884047031402588
iteration 500, loss 1.829250693321228
iteration 600, loss 1.8491597175598145
iteration 700, loss 1.8638373613357544
iteration 800, loss 1.768402338027954
iteration 0, loss 1.8207989931106567
iteration 100, loss 1.7871835231781006
iteration 200, loss 1.7204563617706299
iteration 300, loss 1.9025654792785645
iteration 400, loss 1.8937515020370483
iteration 500, loss 1.8510704040527344
iteration 600, loss 1.849371075630188
iteration 700, loss 1.8470207452774048
iteration 800, loss 1.9267218112945557
iteration 0, loss 1.8613349199295044
iteration 100, loss 1.8041318655014038
iteration 200, loss 1.8466237783432007
iteration 300, loss 1.8444855213165283
iteration 400, loss 1.74033784866333
iteration 500, loss 1.8056209087371826
iteration 600, loss 1.8079352378845215
iteration 700, loss 1.7315069437026978
iteration 800, loss 1.8021609783172607
iteration 0, loss 1.8472775220870972
iteration 100, loss 1.8369770050048828
iteration 200, loss 1.7917675971984863
iteration 300, loss 1.8220282793045044
iteration 400, loss 1.7459752559661865
iteration 500, loss 1.851468563079834
iteration 600, loss 1.8645997047424316
iteration 700, loss 1.7777429819107056
iteration 800, loss 1.8654752969741821
iteration 0, loss 1.7505881786346436
iteration 100, loss 1.8239954710006714
iteration 200, loss 1.8470165729522705
iteration 300, loss 1.7784850597381592
iteration 400, loss 1.7565395832061768
iteration 500, loss 1.8570958375930786
iteration 600, loss 1.842767596244812
iteration 700, loss 1.7668477296829224
iteration 800, loss 1.7903943061828613
iteration 0, loss 1.751881718635559
iteration 100, loss 1.8406099081039429
iteration 200, loss 1.799551248550415
iteration 300, loss 1.8213034868240356
iteration 400, loss 1.8474165201187134
iteration 500, loss 1.8180630207061768
iteration 600, loss 1.8009636402130127
iteration 700, loss 1.833449125289917
iteration 800, loss 1.8188246488571167
iteration 0, loss 1.7661800384521484
iteration 100, loss 1.7752089500427246
iteration 200, loss 1.8174067735671997
iteration 300, loss 1.8343489170074463
iteration 400, loss 1.8090873956680298
iteration 500, loss 1.755306363105774
iteration 600, loss 1.9321033954620361
iteration 700, loss 1.8782389163970947
iteration 800, loss 1.8077656030654907
iteration 0, loss 1.7868117094039917
iteration 100, loss 1.7801904678344727
iteration 200, loss 1.8464480638504028
iteration 300, loss 1.8151698112487793
iteration 400, loss 1.8371357917785645
iteration 500, loss 1.7504241466522217
iteration 600, loss 1.8106683492660522
iteration 700, loss 1.8108490705490112
iteration 800, loss 1.860493779182434
iteration 0, loss 1.8012131452560425
iteration 100, loss 1.8442353010177612
iteration 200, loss 1.8056795597076416
iteration 300, loss 1.8656717538833618
iteration 400, loss 1.8253278732299805
iteration 500, loss 1.8441181182861328
iteration 600, loss 1.7301243543624878
iteration 700, loss 1.7465953826904297
iteration 800, loss 1.7485343217849731
iteration 0, loss 1.8216140270233154
iteration 100, loss 1.747510552406311
iteration 200, loss 1.8122416734695435
iteration 300, loss 1.8301379680633545
iteration 400, loss 1.8604686260223389
iteration 500, loss 1.8582262992858887
iteration 600, loss 1.798016905784607
iteration 700, loss 1.8023695945739746
iteration 800, loss 1.7454524040222168
iteration 0, loss 1.7901966571807861
iteration 100, loss 1.7940444946289062
iteration 200, loss 1.7673437595367432
iteration 300, loss 1.734980583190918
iteration 400, loss 1.82119882106781
iteration 500, loss 1.8472293615341187
iteration 600, loss 1.7781208753585815
iteration 700, loss 1.8155335187911987
iteration 800, loss 1.7844464778900146
iteration 0, loss 1.8328756093978882
iteration 100, loss 1.8453298807144165
iteration 200, loss 1.8146250247955322
iteration 300, loss 1.752618432044983
iteration 400, loss 1.8376320600509644
iteration 500, loss 1.8865046501159668
iteration 600, loss 1.817445993423462
iteration 700, loss 1.9027600288391113
iteration 800, loss 1.8564969301223755
iteration 0, loss 1.8212897777557373
iteration 100, loss 1.8771929740905762
iteration 200, loss 1.8212971687316895
iteration 300, loss 1.7572113275527954
iteration 400, loss 1.7641571760177612
iteration 500, loss 1.796065092086792
iteration 600, loss 1.7271533012390137
iteration 700, loss 1.9047907590866089
iteration 800, loss 1.783632755279541
iteration 0, loss 1.8923901319503784
iteration 100, loss 1.8004049062728882
iteration 200, loss 1.7916525602340698
iteration 300, loss 1.8071057796478271
iteration 400, loss 1.854710340499878
iteration 500, loss 1.8280408382415771
iteration 600, loss 1.8601210117340088
iteration 700, loss 1.791443109512329
iteration 800, loss 1.858094573020935
iteration 0, loss 1.7834486961364746
iteration 100, loss 1.7628545761108398
iteration 200, loss 1.7577999830245972
iteration 300, loss 1.7480361461639404
iteration 400, loss 1.9147454500198364
iteration 500, loss 1.7791866064071655
iteration 600, loss 1.8307932615280151
iteration 700, loss 1.8619463443756104
iteration 800, loss 1.8054523468017578
iteration 0, loss 1.7822601795196533
iteration 100, loss 1.780037760734558
iteration 200, loss 1.8480151891708374
iteration 300, loss 1.8226902484893799
iteration 400, loss 1.892122507095337
iteration 500, loss 1.8302185535430908
iteration 600, loss 1.7492440938949585
iteration 700, loss 1.8681104183197021
iteration 800, loss 1.83981454372406
iteration 0, loss 1.7958215475082397
iteration 100, loss 1.7961746454238892
iteration 200, loss 1.803078293800354
iteration 300, loss 1.705572485923767
iteration 400, loss 1.8506875038146973
iteration 500, loss 1.8444422483444214
iteration 600, loss 1.7852829694747925
iteration 700, loss 1.8140169382095337
iteration 800, loss 1.9646235704421997
iteration 0, loss 1.7789558172225952
iteration 100, loss 1.8596000671386719
iteration 200, loss 1.8528525829315186
iteration 300, loss 1.8930336236953735
iteration 400, loss 1.8220361471176147
iteration 500, loss 1.842817783355713
iteration 600, loss 1.8177618980407715
iteration 700, loss 1.8286516666412354
iteration 800, loss 1.852783441543579
fold 4 accuracy: 0.49078571428571427
[2024-02-28 23:09:45,435] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-28 23:09:45,436] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            323.53 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.73 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '323.53 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 323.53 us = 100% latency, 1.73 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 229.84 us = 71.04% latency, 2.44 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 28.37 us = 8.77% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-28 23:09:45,438] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
iteration 0, loss 2.323072910308838
iteration 100, loss 2.27544903755188
iteration 200, loss 2.180891752243042
iteration 300, loss 2.1781301498413086
iteration 400, loss 2.120178699493408
iteration 500, loss 2.0975847244262695
iteration 600, loss 1.9694987535476685
iteration 700, loss 2.0594542026519775
iteration 800, loss 1.9978307485580444
iteration 0, loss 1.94722580909729
iteration 100, loss 1.9732729196548462
iteration 200, loss 1.9646605253219604
iteration 300, loss 2.0144894123077393
iteration 400, loss 1.9343611001968384
iteration 500, loss 1.9497919082641602
iteration 600, loss 1.941217303276062
iteration 700, loss 1.9595011472702026
iteration 800, loss 2.0196261405944824
iteration 0, loss 1.929996371269226
iteration 100, loss 1.895424246788025
iteration 200, loss 1.9687793254852295
iteration 300, loss 1.9500048160552979
iteration 400, loss 1.940411925315857
iteration 500, loss 1.9332008361816406
iteration 600, loss 1.9172300100326538
iteration 700, loss 1.9608263969421387
iteration 800, loss 1.9287571907043457
iteration 0, loss 1.9542983770370483
iteration 100, loss 1.9224071502685547
iteration 200, loss 1.9588072299957275
iteration 300, loss 1.938899278640747
iteration 400, loss 1.8666433095932007
iteration 500, loss 1.9079995155334473
iteration 600, loss 1.8730801343917847
iteration 700, loss 1.9198622703552246
iteration 800, loss 1.9327830076217651
iteration 0, loss 1.8884079456329346
iteration 100, loss 1.9195154905319214
iteration 200, loss 1.935280680656433
iteration 300, loss 1.985828161239624
iteration 400, loss 1.8849273920059204
iteration 500, loss 1.9785770177841187
iteration 600, loss 1.8693946599960327
iteration 700, loss 1.9691786766052246
iteration 800, loss 1.9464765787124634
iteration 0, loss 1.8956732749938965
iteration 100, loss 1.9878268241882324
iteration 200, loss 1.866749882698059
iteration 300, loss 1.981209635734558
iteration 400, loss 1.8619401454925537
iteration 500, loss 2.001830577850342
iteration 600, loss 1.8873270750045776
iteration 700, loss 1.9733021259307861
iteration 800, loss 1.9297956228256226
iteration 0, loss 1.933347463607788
iteration 100, loss 1.8757572174072266
iteration 200, loss 1.8931134939193726
iteration 300, loss 1.9046847820281982
iteration 400, loss 1.8644061088562012
iteration 500, loss 1.8813769817352295
iteration 600, loss 1.906540870666504
iteration 700, loss 1.914394736289978
iteration 800, loss 1.9586414098739624
iteration 0, loss 1.8757116794586182
iteration 100, loss 1.8967640399932861
iteration 200, loss 1.9412517547607422
iteration 300, loss 1.8428035974502563
iteration 400, loss 1.9967172145843506
iteration 500, loss 1.838643193244934
iteration 600, loss 1.9019064903259277
iteration 700, loss 1.9894546270370483
iteration 800, loss 1.9577760696411133
iteration 0, loss 1.9868947267532349
iteration 100, loss 1.8791069984436035
iteration 200, loss 1.8807364702224731
iteration 300, loss 1.8818860054016113
iteration 400, loss 1.8865381479263306
iteration 500, loss 1.853818416595459
iteration 600, loss 1.9101632833480835
iteration 700, loss 2.0562753677368164
iteration 800, loss 1.8693925142288208
iteration 0, loss 1.841284155845642
iteration 100, loss 1.8805017471313477
iteration 200, loss 1.935530662536621
iteration 300, loss 1.899566411972046
iteration 400, loss 1.8312411308288574
iteration 500, loss 1.9536219835281372
iteration 600, loss 1.9092402458190918
iteration 700, loss 1.88668954372406
iteration 800, loss 1.9369804859161377
iteration 0, loss 1.914095401763916
iteration 100, loss 1.8921526670455933
iteration 200, loss 1.9076107740402222
iteration 300, loss 1.8649990558624268
iteration 400, loss 1.913355827331543
iteration 500, loss 1.9439207315444946
iteration 600, loss 1.9618206024169922
iteration 700, loss 1.8866513967514038
iteration 800, loss 1.9271204471588135
iteration 0, loss 1.8265280723571777
iteration 100, loss 1.8401567935943604
iteration 200, loss 1.8781174421310425
iteration 300, loss 1.9192934036254883
iteration 400, loss 1.9592525959014893
iteration 500, loss 1.925620675086975
iteration 600, loss 1.8894939422607422
iteration 700, loss 1.8917734622955322
iteration 800, loss 1.9550973176956177
iteration 0, loss 1.8981132507324219
iteration 100, loss 1.956452488899231
iteration 200, loss 1.9100524187088013
iteration 300, loss 1.853797197341919
iteration 400, loss 1.9234001636505127
iteration 500, loss 1.8608362674713135
iteration 600, loss 1.8806147575378418
iteration 700, loss 1.8504304885864258
iteration 800, loss 1.9533982276916504
iteration 0, loss 1.8127033710479736
iteration 100, loss 1.9267423152923584
iteration 200, loss 1.848375916481018
iteration 300, loss 1.878218173980713
iteration 400, loss 1.9450061321258545
iteration 500, loss 1.9098018407821655
iteration 600, loss 1.9105699062347412
iteration 700, loss 1.8873512744903564
iteration 800, loss 1.8495616912841797
iteration 0, loss 1.8860325813293457
iteration 100, loss 1.894362449645996
iteration 200, loss 1.8301687240600586
iteration 300, loss 1.819446086883545
iteration 400, loss 1.7917484045028687
iteration 500, loss 1.9358501434326172
iteration 600, loss 1.8436625003814697
iteration 700, loss 1.8567575216293335
iteration 800, loss 1.9142409563064575
iteration 0, loss 1.7933462858200073
iteration 100, loss 1.9569283723831177
iteration 200, loss 1.9155161380767822
iteration 300, loss 1.8577592372894287
iteration 400, loss 1.8946171998977661
iteration 500, loss 1.8351188898086548
iteration 600, loss 1.9024064540863037
iteration 700, loss 2.0044569969177246
iteration 800, loss 1.8523437976837158
iteration 0, loss 1.9202046394348145
iteration 100, loss 1.8150062561035156
iteration 200, loss 1.9127414226531982
iteration 300, loss 2.0288608074188232
iteration 400, loss 1.902620553970337
iteration 500, loss 1.8822264671325684
iteration 600, loss 1.8633800745010376
iteration 700, loss 1.9093228578567505
iteration 800, loss 1.8806475400924683
iteration 0, loss 1.909528374671936
iteration 100, loss 1.8626312017440796
iteration 200, loss 1.8493099212646484
iteration 300, loss 1.878317952156067
iteration 400, loss 1.8962758779525757
iteration 500, loss 1.870916724205017
iteration 600, loss 1.8541234731674194
iteration 700, loss 1.7953414916992188
iteration 800, loss 1.8059972524642944
iteration 0, loss 1.8086215257644653
iteration 100, loss 1.804471731185913
iteration 200, loss 1.857799768447876
iteration 300, loss 1.807854413986206
iteration 400, loss 1.7740521430969238
iteration 500, loss 1.8346376419067383
iteration 600, loss 1.8270618915557861
iteration 700, loss 1.8516894578933716
iteration 800, loss 1.8157782554626465
iteration 0, loss 1.8060872554779053
iteration 100, loss 1.8583365678787231
iteration 200, loss 1.8522542715072632
iteration 300, loss 1.8559532165527344
iteration 400, loss 1.8657584190368652
iteration 500, loss 1.8450061082839966
iteration 600, loss 1.9191257953643799
iteration 700, loss 1.861120581626892
iteration 800, loss 1.8362435102462769
iteration 0, loss 1.8297607898712158
iteration 100, loss 1.8596175909042358
iteration 200, loss 1.8103952407836914
iteration 300, loss 1.9303494691848755
iteration 400, loss 2.0026044845581055
iteration 500, loss 1.8178495168685913
iteration 600, loss 1.8675689697265625
iteration 700, loss 1.953796625137329
iteration 800, loss 1.9089354276657104
iteration 0, loss 1.9068224430084229
iteration 100, loss 1.87125563621521
iteration 200, loss 1.8239517211914062
iteration 300, loss 1.9312394857406616
iteration 400, loss 1.8901206254959106
iteration 500, loss 1.8820953369140625
iteration 600, loss 1.87361478805542
iteration 700, loss 1.927074909210205
iteration 800, loss 1.8985551595687866
iteration 0, loss 1.8299548625946045
iteration 100, loss 1.8397276401519775
iteration 200, loss 1.797464370727539
iteration 300, loss 1.8457893133163452
iteration 400, loss 1.8659718036651611
iteration 500, loss 1.8997979164123535
iteration 600, loss 1.957698941230774
iteration 700, loss 1.92582368850708
iteration 800, loss 1.835763931274414
iteration 0, loss 1.9088249206542969
iteration 100, loss 1.8303018808364868
iteration 200, loss 1.8626136779785156
iteration 300, loss 1.9425530433654785
iteration 400, loss 1.9455863237380981
iteration 500, loss 1.8631832599639893
iteration 600, loss 1.8505219221115112
iteration 700, loss 1.8538551330566406
iteration 800, loss 1.9005262851715088
iteration 0, loss 1.964768409729004
iteration 100, loss 1.9090481996536255
iteration 200, loss 1.8643959760665894
iteration 300, loss 1.8741519451141357
iteration 400, loss 1.8470779657363892
iteration 500, loss 1.8879586458206177
iteration 600, loss 1.922013521194458
iteration 700, loss 1.8555063009262085
iteration 800, loss 1.8616520166397095
iteration 0, loss 1.900679349899292
iteration 100, loss 1.8305810689926147
iteration 200, loss 1.875888705253601
iteration 300, loss 1.892858862876892
iteration 400, loss 1.7941025495529175
iteration 500, loss 1.833358883857727
iteration 600, loss 1.9076241254806519
iteration 700, loss 1.8751065731048584
iteration 800, loss 1.9269964694976807
iteration 0, loss 1.9210368394851685
iteration 100, loss 1.8370683193206787
iteration 200, loss 1.8533098697662354
iteration 300, loss 1.8953592777252197
iteration 400, loss 1.7998201847076416
iteration 500, loss 1.8814116716384888
iteration 600, loss 1.8969513177871704
iteration 700, loss 1.8880820274353027
iteration 800, loss 1.897929310798645
iteration 0, loss 1.862329363822937
iteration 100, loss 1.8188385963439941
iteration 200, loss 1.9135849475860596
iteration 300, loss 1.9029117822647095
iteration 400, loss 1.8661125898361206
iteration 500, loss 1.868797779083252
iteration 600, loss 1.797316312789917
iteration 700, loss 1.870936632156372
iteration 800, loss 1.906353235244751
iteration 0, loss 1.8739107847213745
iteration 100, loss 1.9162898063659668
iteration 200, loss 1.8624935150146484
iteration 300, loss 1.822453260421753
iteration 400, loss 1.8430609703063965
iteration 500, loss 1.8590418100357056
iteration 600, loss 1.8252636194229126
iteration 700, loss 1.9298944473266602
iteration 800, loss 1.8388296365737915
iteration 0, loss 1.8141634464263916
iteration 100, loss 1.8994983434677124
iteration 200, loss 1.8300026655197144
iteration 300, loss 1.8564884662628174
iteration 400, loss 1.878233551979065
iteration 500, loss 1.871272087097168
iteration 600, loss 1.8371281623840332
iteration 700, loss 1.8934152126312256
iteration 800, loss 1.9379112720489502
iteration 0, loss 1.8965063095092773
iteration 100, loss 1.8577897548675537
iteration 200, loss 1.9520241022109985
iteration 300, loss 1.9195380210876465
iteration 400, loss 1.8726152181625366
iteration 500, loss 1.8658546209335327
iteration 600, loss 1.8333194255828857
iteration 700, loss 1.8830540180206299
iteration 800, loss 1.9051635265350342
iteration 0, loss 1.861117959022522
iteration 100, loss 1.7997201681137085
iteration 200, loss 1.8841785192489624
iteration 300, loss 1.9340310096740723
iteration 400, loss 1.8395272493362427
iteration 500, loss 1.8836454153060913
iteration 600, loss 1.9162797927856445
iteration 700, loss 1.8069108724594116
iteration 800, loss 1.8719810247421265
iteration 0, loss 1.8963825702667236
iteration 100, loss 1.8140521049499512
iteration 200, loss 1.851215124130249
iteration 300, loss 1.8323031663894653
iteration 400, loss 1.8340107202529907
iteration 500, loss 1.8653855323791504
iteration 600, loss 1.9232397079467773
iteration 700, loss 1.8630280494689941
iteration 800, loss 1.7894396781921387
iteration 0, loss 1.806634545326233
iteration 100, loss 1.953145980834961
iteration 200, loss 1.9173694849014282
iteration 300, loss 1.8885565996170044
iteration 400, loss 1.8238803148269653
iteration 500, loss 1.7642461061477661
iteration 600, loss 1.8876538276672363
iteration 700, loss 1.9100004434585571
iteration 800, loss 1.8216921091079712
iteration 0, loss 1.847687840461731
iteration 100, loss 1.864251971244812
iteration 200, loss 1.8974746465682983
iteration 300, loss 1.8479459285736084
iteration 400, loss 1.8269785642623901
iteration 500, loss 1.8046852350234985
iteration 600, loss 1.7966806888580322
iteration 700, loss 1.8739248514175415
iteration 800, loss 1.7656068801879883
iteration 0, loss 1.902026653289795
iteration 100, loss 1.837705135345459
iteration 200, loss 1.8514902591705322
iteration 300, loss 1.9552409648895264
iteration 400, loss 1.8588614463806152
iteration 500, loss 1.7948111295700073
iteration 600, loss 1.889600157737732
iteration 700, loss 1.905895471572876
iteration 800, loss 1.9188144207000732
iteration 0, loss 1.902483582496643
iteration 100, loss 1.8216257095336914
iteration 200, loss 1.8354504108428955
iteration 300, loss 1.8561232089996338
iteration 400, loss 1.8196420669555664
iteration 500, loss 1.874143362045288
iteration 600, loss 1.8059512376785278
iteration 700, loss 1.8433419466018677
iteration 800, loss 1.8493119478225708
iteration 0, loss 1.8490060567855835
iteration 100, loss 1.8771389722824097
iteration 200, loss 1.806801676750183
iteration 300, loss 1.8982434272766113
iteration 400, loss 1.7805843353271484
iteration 500, loss 1.8583558797836304
iteration 600, loss 1.8189760446548462
iteration 700, loss 1.8442423343658447
iteration 800, loss 1.940306544303894
iteration 0, loss 1.8030595779418945
iteration 100, loss 1.9069596529006958
iteration 200, loss 1.8999218940734863
iteration 300, loss 1.8468211889266968
iteration 400, loss 1.9286785125732422
iteration 500, loss 1.939875602722168
iteration 600, loss 1.8536351919174194
iteration 700, loss 1.9736984968185425
iteration 800, loss 1.8429961204528809
iteration 0, loss 1.842847228050232
iteration 100, loss 1.848734974861145
iteration 200, loss 1.8955200910568237
iteration 300, loss 1.8919256925582886
iteration 400, loss 1.8605629205703735
iteration 500, loss 1.848577618598938
iteration 600, loss 1.8804585933685303
iteration 700, loss 1.882786512374878
iteration 800, loss 1.8992334604263306
iteration 0, loss 1.813340663909912
iteration 100, loss 1.8087821006774902
iteration 200, loss 1.8651896715164185
iteration 300, loss 1.8452386856079102
iteration 400, loss 1.8549691438674927
iteration 500, loss 1.8106048107147217
iteration 600, loss 1.8579145669937134
iteration 700, loss 1.9383556842803955
iteration 800, loss 1.8220515251159668
iteration 0, loss 1.8966517448425293
iteration 100, loss 1.8012689352035522
iteration 200, loss 1.8717787265777588
iteration 300, loss 1.8650987148284912
iteration 400, loss 1.8703194856643677
iteration 500, loss 1.809178352355957
iteration 600, loss 1.8615013360977173
iteration 700, loss 1.8748219013214111
iteration 800, loss 1.7875745296478271
iteration 0, loss 1.9321125745773315
iteration 100, loss 1.907480239868164
iteration 200, loss 1.8960449695587158
iteration 300, loss 1.870336890220642
iteration 400, loss 1.9186018705368042
iteration 500, loss 1.8356634378433228
iteration 600, loss 1.8392961025238037
iteration 700, loss 1.867843508720398
iteration 800, loss 1.8879657983779907
iteration 0, loss 1.8558462858200073
iteration 100, loss 1.8670029640197754
iteration 200, loss 1.9175999164581299
iteration 300, loss 1.8816097974777222
iteration 400, loss 1.8442764282226562
iteration 500, loss 1.8107231855392456
iteration 600, loss 1.8888115882873535
iteration 700, loss 1.9028347730636597
iteration 800, loss 1.893813967704773
iteration 0, loss 1.7755439281463623
iteration 100, loss 1.87952721118927
iteration 200, loss 1.8304030895233154
iteration 300, loss 1.886232614517212
iteration 400, loss 1.8544691801071167
iteration 500, loss 1.849098801612854
iteration 600, loss 1.8325868844985962
iteration 700, loss 1.7858954668045044
iteration 800, loss 1.813485860824585
iteration 0, loss 1.8656611442565918
iteration 100, loss 1.8873212337493896
iteration 200, loss 1.8304740190505981
iteration 300, loss 1.7686432600021362
iteration 400, loss 1.8787094354629517
iteration 500, loss 1.8678300380706787
iteration 600, loss 1.828071117401123
iteration 700, loss 1.972399115562439
iteration 800, loss 1.9078090190887451
iteration 0, loss 1.9393820762634277
iteration 100, loss 1.789955496788025
iteration 200, loss 1.7662931680679321
iteration 300, loss 1.856544017791748
iteration 400, loss 1.919227123260498
iteration 500, loss 1.9290984869003296
iteration 600, loss 1.8541924953460693
iteration 700, loss 1.9257491827011108
iteration 800, loss 1.8334897756576538
iteration 0, loss 1.9523783922195435
iteration 100, loss 1.8531692028045654
iteration 200, loss 1.9165382385253906
iteration 300, loss 1.8315460681915283
iteration 400, loss 1.829585313796997
iteration 500, loss 1.919308066368103
iteration 600, loss 1.9099775552749634
iteration 700, loss 1.8457093238830566
iteration 800, loss 1.8323546648025513
iteration 0, loss 1.878056287765503
iteration 100, loss 1.8442363739013672
iteration 200, loss 1.8596715927124023
iteration 300, loss 1.90517258644104
iteration 400, loss 1.8270529508590698
iteration 500, loss 1.8078081607818604
iteration 600, loss 1.8079360723495483
iteration 700, loss 1.865286111831665
iteration 800, loss 1.8068695068359375
iteration 0, loss 1.8089842796325684
iteration 100, loss 1.8204001188278198
iteration 200, loss 1.8658993244171143
iteration 300, loss 1.850233793258667
iteration 400, loss 1.8725639581680298
iteration 500, loss 1.8520413637161255
iteration 600, loss 1.8465114831924438
iteration 700, loss 1.8509796857833862
iteration 800, loss 1.8705713748931885
fold 0 accuracy: 0.42907142857142855
iteration 0, loss 1.8268980979919434
iteration 100, loss 1.931437373161316
iteration 200, loss 1.858582615852356
iteration 300, loss 1.8717941045761108
iteration 400, loss 1.8768256902694702
iteration 500, loss 1.8769363164901733
iteration 600, loss 1.7878612279891968
iteration 700, loss 1.841395378112793
iteration 800, loss 1.8640657663345337
iteration 0, loss 1.879239797592163
iteration 100, loss 1.8160114288330078
iteration 200, loss 1.9646400213241577
iteration 300, loss 1.8337777853012085
iteration 400, loss 1.8630397319793701
iteration 500, loss 1.795556902885437
iteration 600, loss 1.7824652194976807
iteration 700, loss 1.8185070753097534
iteration 800, loss 1.86872398853302
iteration 0, loss 1.865695595741272
iteration 100, loss 1.7785831689834595
iteration 200, loss 1.810351014137268
iteration 300, loss 1.9164401292800903
iteration 400, loss 1.7903985977172852
iteration 500, loss 1.8840669393539429
iteration 600, loss 1.9083136320114136
iteration 700, loss 1.7962859869003296
iteration 800, loss 1.854388952255249
iteration 0, loss 1.8358066082000732
iteration 100, loss 1.888043999671936
iteration 200, loss 1.8254956007003784
iteration 300, loss 1.8490911722183228
iteration 400, loss 1.8314011096954346
iteration 500, loss 1.7645083665847778
iteration 600, loss 1.8601360321044922
iteration 700, loss 1.8580342531204224
iteration 800, loss 1.8806591033935547
iteration 0, loss 1.8005683422088623
iteration 100, loss 1.8974136114120483
iteration 200, loss 1.8197126388549805
iteration 300, loss 1.8101906776428223
iteration 400, loss 1.836248755455017
iteration 500, loss 1.885765552520752
iteration 600, loss 1.8544347286224365
iteration 700, loss 1.8312093019485474
iteration 800, loss 1.9479522705078125
iteration 0, loss 1.8254200220108032
iteration 100, loss 1.8976998329162598
iteration 200, loss 1.8127198219299316
iteration 300, loss 1.8220040798187256
iteration 400, loss 1.877673625946045
iteration 500, loss 1.8780674934387207
iteration 600, loss 1.846388339996338
iteration 700, loss 1.8558189868927002
iteration 800, loss 1.8727129697799683
iteration 0, loss 1.8441306352615356
iteration 100, loss 1.854472041130066
iteration 200, loss 1.8687411546707153
iteration 300, loss 1.8613299131393433
iteration 400, loss 1.890878438949585
iteration 500, loss 1.821592926979065
iteration 600, loss 1.7995611429214478
iteration 700, loss 1.754785418510437
iteration 800, loss 1.8079571723937988
iteration 0, loss 1.867031216621399
iteration 100, loss 1.8197113275527954
iteration 200, loss 1.7949309349060059
iteration 300, loss 1.8261451721191406
iteration 400, loss 1.8311586380004883
iteration 500, loss 1.828655481338501
iteration 600, loss 1.8050799369812012
iteration 700, loss 1.9382357597351074
iteration 800, loss 1.8283729553222656
iteration 0, loss 1.8769327402114868
iteration 100, loss 1.8826853036880493
iteration 200, loss 1.8079674243927002
iteration 300, loss 1.930380940437317
iteration 400, loss 1.869978904724121
iteration 500, loss 1.8470441102981567
iteration 600, loss 1.797139048576355
iteration 700, loss 1.832821011543274
iteration 800, loss 1.852980375289917
iteration 0, loss 1.8863219022750854
iteration 100, loss 1.86183762550354
iteration 200, loss 1.8576481342315674
iteration 300, loss 1.7739620208740234
iteration 400, loss 1.8166468143463135
iteration 500, loss 1.9073172807693481
iteration 600, loss 1.9072442054748535
iteration 700, loss 1.8495241403579712
iteration 800, loss 1.8823162317276
iteration 0, loss 1.8776129484176636
iteration 100, loss 1.855002999305725
iteration 200, loss 1.8594540357589722
iteration 300, loss 1.837927222251892
iteration 400, loss 1.8034712076187134
iteration 500, loss 1.9062162637710571
iteration 600, loss 1.8314367532730103
iteration 700, loss 1.9117685556411743
iteration 800, loss 1.796639084815979
iteration 0, loss 1.8349025249481201
iteration 100, loss 1.862673282623291
iteration 200, loss 1.8770438432693481
iteration 300, loss 1.8063932657241821
iteration 400, loss 1.8468116521835327
iteration 500, loss 1.828475832939148
iteration 600, loss 1.8879730701446533
iteration 700, loss 1.8699827194213867
iteration 800, loss 1.863097906112671
iteration 0, loss 1.9132304191589355
iteration 100, loss 1.8238530158996582
iteration 200, loss 1.7939597368240356
iteration 300, loss 1.8448705673217773
iteration 400, loss 1.805873155593872
iteration 500, loss 1.8607923984527588
iteration 600, loss 1.8732686042785645
iteration 700, loss 1.878839135169983
iteration 800, loss 1.856149435043335
iteration 0, loss 1.88827383518219
iteration 100, loss 1.8524374961853027
iteration 200, loss 1.7900357246398926
iteration 300, loss 1.7625879049301147
iteration 400, loss 1.8873469829559326
iteration 500, loss 1.8316097259521484
iteration 600, loss 1.8241246938705444
iteration 700, loss 1.7547645568847656
iteration 800, loss 1.84992253780365
iteration 0, loss 1.8464540243148804
iteration 100, loss 1.839404821395874
iteration 200, loss 1.9313424825668335
iteration 300, loss 1.877547264099121
iteration 400, loss 1.8194199800491333
iteration 500, loss 1.8631280660629272
iteration 600, loss 1.8611087799072266
iteration 700, loss 1.8139616250991821
iteration 800, loss 1.8801329135894775
iteration 0, loss 1.8680639266967773
iteration 100, loss 1.8693270683288574
iteration 200, loss 1.878585934638977
iteration 300, loss 1.8495192527770996
iteration 400, loss 1.7989816665649414
iteration 500, loss 1.7822331190109253
iteration 600, loss 1.9644197225570679
iteration 700, loss 1.7850165367126465
iteration 800, loss 1.866393804550171
iteration 0, loss 1.7443110942840576
iteration 100, loss 1.8673228025436401
iteration 200, loss 1.8194366693496704
iteration 300, loss 1.8076225519180298
iteration 400, loss 1.8400365114212036
iteration 500, loss 1.8484259843826294
iteration 600, loss 1.814536213874817
iteration 700, loss 1.8369594812393188
iteration 800, loss 1.8800146579742432
iteration 0, loss 1.8546476364135742
iteration 100, loss 1.8629783391952515
iteration 200, loss 1.915831446647644
iteration 300, loss 1.8639692068099976
iteration 400, loss 1.8979337215423584
iteration 500, loss 1.8737605810165405
iteration 600, loss 1.8296921253204346
iteration 700, loss 1.8226741552352905
iteration 800, loss 1.8436685800552368
iteration 0, loss 1.852140188217163
iteration 100, loss 1.82081139087677
iteration 200, loss 1.9308048486709595
iteration 300, loss 1.7300190925598145
iteration 400, loss 1.8618882894515991
iteration 500, loss 1.82821786403656
iteration 600, loss 1.913017988204956
iteration 700, loss 1.7856476306915283
iteration 800, loss 1.8254567384719849
iteration 0, loss 1.8537253141403198
iteration 100, loss 1.7836350202560425
iteration 200, loss 1.8711073398590088
iteration 300, loss 1.9127591848373413
iteration 400, loss 1.8600361347198486
iteration 500, loss 1.8355034589767456
iteration 600, loss 1.867211103439331
iteration 700, loss 1.7752195596694946
iteration 800, loss 1.8517132997512817
iteration 0, loss 1.8824936151504517
iteration 100, loss 1.742552399635315
iteration 200, loss 1.8726093769073486
iteration 300, loss 1.8096479177474976
iteration 400, loss 1.8900641202926636
iteration 500, loss 1.8230066299438477
iteration 600, loss 1.8594565391540527
iteration 700, loss 1.8368113040924072
iteration 800, loss 1.848355770111084
iteration 0, loss 1.845507025718689
iteration 100, loss 1.8286749124526978
iteration 200, loss 1.8589316606521606
iteration 300, loss 1.8414307832717896
iteration 400, loss 1.84869384765625
iteration 500, loss 1.7934026718139648
iteration 600, loss 1.7601280212402344
iteration 700, loss 1.8367595672607422
iteration 800, loss 1.8781837224960327
iteration 0, loss 1.8189895153045654
iteration 100, loss 1.819678783416748
iteration 200, loss 1.7922369241714478
iteration 300, loss 1.8776473999023438
iteration 400, loss 1.8228058815002441
iteration 500, loss 1.8015894889831543
iteration 600, loss 1.8608407974243164
iteration 700, loss 1.9142879247665405
iteration 800, loss 1.8191806077957153
iteration 0, loss 1.832375168800354
iteration 100, loss 1.7942508459091187
iteration 200, loss 1.9007724523544312
iteration 300, loss 1.7496280670166016
iteration 400, loss 1.8461841344833374
iteration 500, loss 1.839594841003418
iteration 600, loss 1.7906068563461304
iteration 700, loss 1.8559213876724243
iteration 800, loss 1.872260570526123
iteration 0, loss 1.8417730331420898
iteration 100, loss 1.8474794626235962
iteration 200, loss 1.8704805374145508
iteration 300, loss 1.8509737253189087
iteration 400, loss 1.8314154148101807
iteration 500, loss 1.8835049867630005
iteration 600, loss 1.8645061254501343
iteration 700, loss 1.8696037530899048
iteration 800, loss 1.8654553890228271
iteration 0, loss 1.8718738555908203
iteration 100, loss 1.8513113260269165
iteration 200, loss 1.7899802923202515
iteration 300, loss 1.9261469841003418
iteration 400, loss 1.8441030979156494
iteration 500, loss 1.8418238162994385
iteration 600, loss 1.8828258514404297
iteration 700, loss 1.790423035621643
iteration 800, loss 1.9491163492202759
iteration 0, loss 1.8507184982299805
iteration 100, loss 1.8239943981170654
iteration 200, loss 1.8237533569335938
iteration 300, loss 1.831081748008728
iteration 400, loss 1.844386100769043
iteration 500, loss 1.8127671480178833
iteration 600, loss 1.819064736366272
iteration 700, loss 1.8448829650878906
iteration 800, loss 1.8256434202194214
iteration 0, loss 1.8365283012390137
iteration 100, loss 1.8572192192077637
iteration 200, loss 1.8620330095291138
iteration 300, loss 1.91148042678833
iteration 400, loss 1.8439342975616455
iteration 500, loss 1.7487366199493408
iteration 600, loss 1.808732509613037
iteration 700, loss 1.8869446516036987
iteration 800, loss 1.8662235736846924
iteration 0, loss 1.7914223670959473
iteration 100, loss 1.7688044309616089
iteration 200, loss 1.8363479375839233
iteration 300, loss 1.8147884607315063
iteration 400, loss 1.7593977451324463
iteration 500, loss 1.9162850379943848
iteration 600, loss 1.900894284248352
iteration 700, loss 1.8383803367614746
iteration 800, loss 1.8484289646148682
iteration 0, loss 1.7512710094451904
iteration 100, loss 1.8516288995742798
iteration 200, loss 1.8677579164505005
iteration 300, loss 1.8939824104309082
iteration 400, loss 1.8599773645401
iteration 500, loss 1.8707553148269653
iteration 600, loss 1.8465352058410645
iteration 700, loss 1.8645942211151123
iteration 800, loss 1.848068118095398
iteration 0, loss 1.9088292121887207
iteration 100, loss 1.801694631576538
iteration 200, loss 1.855952501296997
iteration 300, loss 1.7965363264083862
iteration 400, loss 1.8476768732070923
iteration 500, loss 1.793472409248352
iteration 600, loss 1.867781400680542
iteration 700, loss 1.8316634893417358
iteration 800, loss 1.8212336301803589
iteration 0, loss 1.9213128089904785
iteration 100, loss 1.862370252609253
iteration 200, loss 1.7834745645523071
iteration 300, loss 1.9081387519836426
iteration 400, loss 1.7835607528686523
iteration 500, loss 1.8974233865737915
iteration 600, loss 1.884522557258606
iteration 700, loss 1.843043565750122
iteration 800, loss 1.7984697818756104
iteration 0, loss 1.8615630865097046
iteration 100, loss 1.8573639392852783
iteration 200, loss 1.820177435874939
iteration 300, loss 1.7601101398468018
iteration 400, loss 1.9525827169418335
iteration 500, loss 1.8352216482162476
iteration 600, loss 1.9393771886825562
iteration 700, loss 1.7859028577804565
iteration 800, loss 1.876635193824768
iteration 0, loss 1.8122179508209229
iteration 100, loss 1.8271386623382568
iteration 200, loss 1.84238600730896
iteration 300, loss 1.9363670349121094
iteration 400, loss 1.7833240032196045
iteration 500, loss 1.8937422037124634
iteration 600, loss 1.8973276615142822
iteration 700, loss 1.7662445306777954
iteration 800, loss 1.815024495124817
iteration 0, loss 1.850201964378357
iteration 100, loss 1.8213245868682861
iteration 200, loss 1.9244204759597778
iteration 300, loss 1.8344250917434692
iteration 400, loss 1.801181435585022
iteration 500, loss 1.8694299459457397
iteration 600, loss 1.9006012678146362
iteration 700, loss 1.8398747444152832
iteration 800, loss 1.8580352067947388
iteration 0, loss 1.8574755191802979
iteration 100, loss 1.8558741807937622
iteration 200, loss 1.8448907136917114
iteration 300, loss 1.8158423900604248
iteration 400, loss 1.9263781309127808
iteration 500, loss 1.911505937576294
iteration 600, loss 1.7426820993423462
iteration 700, loss 1.8608968257904053
iteration 800, loss 1.7668962478637695
iteration 0, loss 1.8015587329864502
iteration 100, loss 1.8534555435180664
iteration 200, loss 1.8139688968658447
iteration 300, loss 1.7917397022247314
iteration 400, loss 1.8564612865447998
iteration 500, loss 1.8708202838897705
iteration 600, loss 1.8980134725570679
iteration 700, loss 1.8905671834945679
iteration 800, loss 1.7917683124542236
iteration 0, loss 1.8342424631118774
iteration 100, loss 1.8366279602050781
iteration 200, loss 1.8600733280181885
iteration 300, loss 1.794168472290039
iteration 400, loss 1.8046505451202393
iteration 500, loss 1.8744697570800781
iteration 600, loss 1.8212482929229736
iteration 700, loss 1.7629337310791016
iteration 800, loss 1.8291456699371338
iteration 0, loss 1.7797701358795166
iteration 100, loss 1.778635859489441
iteration 200, loss 1.8046256303787231
iteration 300, loss 1.8839162588119507
iteration 400, loss 1.8694089651107788
iteration 500, loss 1.829585075378418
iteration 600, loss 1.8149590492248535
iteration 700, loss 1.8000715970993042
iteration 800, loss 1.827805519104004
iteration 0, loss 1.7675838470458984
iteration 100, loss 1.8080525398254395
iteration 200, loss 1.839755892753601
iteration 300, loss 1.834883213043213
iteration 400, loss 1.905720591545105
iteration 500, loss 1.808836817741394
iteration 600, loss 1.8482182025909424
iteration 700, loss 1.9935499429702759
iteration 800, loss 1.8240735530853271
iteration 0, loss 1.8062344789505005
iteration 100, loss 1.8184294700622559
iteration 200, loss 1.8797979354858398
iteration 300, loss 1.819710612297058
iteration 400, loss 1.8396801948547363
iteration 500, loss 1.8601996898651123
iteration 600, loss 1.7666285037994385
iteration 700, loss 1.7410975694656372
iteration 800, loss 1.8265901803970337
iteration 0, loss 1.8395785093307495
iteration 100, loss 1.8270357847213745
iteration 200, loss 1.7949318885803223
iteration 300, loss 1.8643661737442017
iteration 400, loss 1.899131417274475
iteration 500, loss 1.8173683881759644
iteration 600, loss 1.9616469144821167
iteration 700, loss 1.8474757671356201
iteration 800, loss 1.790094256401062
iteration 0, loss 1.8510701656341553
iteration 100, loss 1.8598237037658691
iteration 200, loss 1.8809328079223633
iteration 300, loss 1.7904322147369385
iteration 400, loss 1.8279533386230469
iteration 500, loss 1.9053012132644653
iteration 600, loss 1.7707878351211548
iteration 700, loss 1.7847567796707153
iteration 800, loss 1.882394552230835
iteration 0, loss 1.8018827438354492
iteration 100, loss 1.79167902469635
iteration 200, loss 1.8910548686981201
iteration 300, loss 1.8160983324050903
iteration 400, loss 1.8657736778259277
iteration 500, loss 1.8245753049850464
iteration 600, loss 1.8682054281234741
iteration 700, loss 1.8449969291687012
iteration 800, loss 1.7990267276763916
iteration 0, loss 1.7870938777923584
iteration 100, loss 1.887128233909607
iteration 200, loss 1.7985877990722656
iteration 300, loss 1.8024476766586304
iteration 400, loss 1.7566051483154297
iteration 500, loss 1.8829654455184937
iteration 600, loss 1.702150821685791
iteration 700, loss 1.8377352952957153
iteration 800, loss 1.8048231601715088
iteration 0, loss 1.8920695781707764
iteration 100, loss 1.7807462215423584
iteration 200, loss 1.8329434394836426
iteration 300, loss 1.817391037940979
iteration 400, loss 1.8157883882522583
iteration 500, loss 1.8792388439178467
iteration 600, loss 1.8303561210632324
iteration 700, loss 1.7997287511825562
iteration 800, loss 1.7788505554199219
iteration 0, loss 1.9209873676300049
iteration 100, loss 1.8315500020980835
iteration 200, loss 1.8082388639450073
iteration 300, loss 1.7757186889648438
iteration 400, loss 1.8597478866577148
iteration 500, loss 1.8648463487625122
iteration 600, loss 1.8080570697784424
iteration 700, loss 1.8889954090118408
iteration 800, loss 1.7861065864562988
iteration 0, loss 1.8762609958648682
iteration 100, loss 1.7993749380111694
iteration 200, loss 1.902535319328308
iteration 300, loss 1.771685242652893
iteration 400, loss 1.8077940940856934
iteration 500, loss 1.8536931276321411
iteration 600, loss 1.822466254234314
iteration 700, loss 1.856060266494751
iteration 800, loss 1.8755793571472168
iteration 0, loss 1.8582103252410889
iteration 100, loss 1.7808631658554077
iteration 200, loss 1.8681637048721313
iteration 300, loss 1.8285112380981445
iteration 400, loss 1.9200153350830078
iteration 500, loss 1.8363910913467407
iteration 600, loss 1.7535200119018555
iteration 700, loss 1.8182190656661987
iteration 800, loss 1.799687147140503
iteration 0, loss 1.8776193857192993
iteration 100, loss 1.8304343223571777
iteration 200, loss 1.8237180709838867
iteration 300, loss 1.8381348848342896
iteration 400, loss 1.8939013481140137
iteration 500, loss 1.8356566429138184
iteration 600, loss 1.9203906059265137
iteration 700, loss 1.7938913106918335
iteration 800, loss 1.8755043745040894
fold 1 accuracy: 0.4019285714285714
iteration 0, loss 1.8479199409484863
iteration 100, loss 1.7799532413482666
iteration 200, loss 1.831506371498108
iteration 300, loss 1.808647632598877
iteration 400, loss 1.802106261253357
iteration 500, loss 1.8520177602767944
iteration 600, loss 1.9348516464233398
iteration 700, loss 1.8125603199005127
iteration 800, loss 1.896264672279358
iteration 0, loss 1.8470680713653564
iteration 100, loss 1.888991117477417
iteration 200, loss 1.8155313730239868
iteration 300, loss 1.846671462059021
iteration 400, loss 1.799128532409668
iteration 500, loss 1.8277908563613892
iteration 600, loss 1.791045069694519
iteration 700, loss 1.8194130659103394
iteration 800, loss 1.8442296981811523
iteration 0, loss 1.8788962364196777
iteration 100, loss 1.8172972202301025
iteration 200, loss 1.86628258228302
iteration 300, loss 1.7887309789657593
iteration 400, loss 1.7983349561691284
iteration 500, loss 1.899132251739502
iteration 600, loss 1.8257650136947632
iteration 700, loss 1.8314058780670166
iteration 800, loss 1.8379013538360596
iteration 0, loss 1.7853655815124512
iteration 100, loss 1.8921335935592651
iteration 200, loss 1.8547182083129883
iteration 300, loss 1.8070420026779175
iteration 400, loss 1.8466417789459229
iteration 500, loss 1.927438497543335
iteration 600, loss 1.8834165334701538
iteration 700, loss 1.8343256711959839
iteration 800, loss 1.7904434204101562
iteration 0, loss 1.91902756690979
iteration 100, loss 1.799446940422058
iteration 200, loss 1.8575589656829834
iteration 300, loss 1.8289097547531128
iteration 400, loss 1.8105450868606567
iteration 500, loss 1.8244985342025757
iteration 600, loss 1.879834532737732
iteration 700, loss 1.8719927072525024
iteration 800, loss 1.8159199953079224
iteration 0, loss 1.8466012477874756
iteration 100, loss 1.9212287664413452
iteration 200, loss 1.8872216939926147
iteration 300, loss 1.8220795392990112
iteration 400, loss 1.8485169410705566
iteration 500, loss 1.8100852966308594
iteration 600, loss 1.8397502899169922
iteration 700, loss 1.8480184078216553
iteration 800, loss 1.7977403402328491
iteration 0, loss 1.864506721496582
iteration 100, loss 1.823860764503479
iteration 200, loss 1.7871391773223877
iteration 300, loss 1.8039600849151611
iteration 400, loss 1.7971669435501099
iteration 500, loss 1.7862485647201538
iteration 600, loss 1.9007775783538818
iteration 700, loss 1.8748831748962402
iteration 800, loss 1.8705517053604126
iteration 0, loss 1.802256464958191
iteration 100, loss 1.8876783847808838
iteration 200, loss 1.7990206480026245
iteration 300, loss 1.8362756967544556
iteration 400, loss 1.8328423500061035
iteration 500, loss 1.827831506729126
iteration 600, loss 1.795851230621338
iteration 700, loss 1.7779638767242432
iteration 800, loss 1.8157271146774292
iteration 0, loss 1.8730888366699219
iteration 100, loss 1.897196650505066
iteration 200, loss 1.8137145042419434
iteration 300, loss 1.8328187465667725
iteration 400, loss 1.8127158880233765
iteration 500, loss 1.8830538988113403
iteration 600, loss 1.8350008726119995
iteration 700, loss 1.896246075630188
iteration 800, loss 1.8337879180908203
iteration 0, loss 1.764435887336731
iteration 100, loss 1.8956623077392578
iteration 200, loss 1.8632385730743408
iteration 300, loss 1.874387502670288
iteration 400, loss 1.7682539224624634
iteration 500, loss 1.860538125038147
iteration 600, loss 1.8041329383850098
iteration 700, loss 1.8598945140838623
iteration 800, loss 1.8162329196929932
iteration 0, loss 1.8833445310592651
iteration 100, loss 1.8597978353500366
iteration 200, loss 1.8389469385147095
iteration 300, loss 1.7490341663360596
iteration 400, loss 1.8313175439834595
iteration 500, loss 1.8167630434036255
iteration 600, loss 1.8254868984222412
iteration 700, loss 1.8417503833770752
iteration 800, loss 1.7499423027038574
iteration 0, loss 1.802021861076355
iteration 100, loss 1.846912145614624
iteration 200, loss 1.830862045288086
iteration 300, loss 1.838494896888733
iteration 400, loss 1.871267557144165
iteration 500, loss 1.7584197521209717
iteration 600, loss 1.8157799243927002
iteration 700, loss 1.787941575050354
iteration 800, loss 1.7998374700546265
iteration 0, loss 1.8100813627243042
iteration 100, loss 1.8233213424682617
iteration 200, loss 1.8207489252090454
iteration 300, loss 1.8940110206604004
iteration 400, loss 1.825186848640442
iteration 500, loss 1.837868094444275
iteration 600, loss 1.8711767196655273
iteration 700, loss 1.824318528175354
iteration 800, loss 1.78981614112854
iteration 0, loss 1.8395943641662598
iteration 100, loss 1.8258826732635498
iteration 200, loss 1.8313822746276855
iteration 300, loss 1.8424509763717651
iteration 400, loss 1.8739639520645142
iteration 500, loss 1.8814928531646729
iteration 600, loss 1.8625973463058472
iteration 700, loss 1.8293524980545044
iteration 800, loss 1.8805935382843018
iteration 0, loss 1.782097339630127
iteration 100, loss 1.7783584594726562
iteration 200, loss 1.8245619535446167
iteration 300, loss 1.8312218189239502
iteration 400, loss 1.8391287326812744
iteration 500, loss 1.8047118186950684
iteration 600, loss 1.819276213645935
iteration 700, loss 1.7563776969909668
iteration 800, loss 1.792170524597168
iteration 0, loss 1.7566245794296265
iteration 100, loss 1.7974371910095215
iteration 200, loss 1.8953262567520142
iteration 300, loss 1.848151683807373
iteration 400, loss 1.7367010116577148
iteration 500, loss 1.8878129720687866
iteration 600, loss 1.840368628501892
iteration 700, loss 1.9212547540664673
iteration 800, loss 1.9535969495773315
iteration 0, loss 1.8409626483917236
iteration 100, loss 1.907395601272583
iteration 200, loss 1.8151050806045532
iteration 300, loss 1.7730203866958618
iteration 400, loss 1.8093323707580566
iteration 500, loss 1.8877997398376465
iteration 600, loss 1.8820258378982544
iteration 700, loss 1.852004885673523
iteration 800, loss 1.829797625541687
iteration 0, loss 1.8692865371704102
iteration 100, loss 1.8413094282150269
iteration 200, loss 1.843793272972107
iteration 300, loss 1.8628040552139282
iteration 400, loss 1.8456450700759888
iteration 500, loss 1.9228076934814453
iteration 600, loss 1.8552663326263428
iteration 700, loss 1.862618327140808
iteration 800, loss 1.8183094263076782
iteration 0, loss 1.8237249851226807
iteration 100, loss 1.82073974609375
iteration 200, loss 1.9044395685195923
iteration 300, loss 1.885436773300171
iteration 400, loss 1.8268694877624512
iteration 500, loss 1.8118979930877686
iteration 600, loss 1.8684589862823486
iteration 700, loss 1.8550808429718018
iteration 800, loss 1.8433585166931152
iteration 0, loss 1.8610594272613525
iteration 100, loss 1.773235559463501
iteration 200, loss 1.8603407144546509
iteration 300, loss 1.8558050394058228
iteration 400, loss 1.78751802444458
iteration 500, loss 1.78847336769104
iteration 600, loss 1.7834453582763672
iteration 700, loss 1.863869547843933
iteration 800, loss 1.8567525148391724
iteration 0, loss 1.7689192295074463
iteration 100, loss 1.9717508554458618
iteration 200, loss 1.831511378288269
iteration 300, loss 1.9075180292129517
iteration 400, loss 1.7675808668136597
iteration 500, loss 1.9436463117599487
iteration 600, loss 1.8376997709274292
iteration 700, loss 1.846637487411499
iteration 800, loss 1.8420746326446533
iteration 0, loss 1.901911973953247
iteration 100, loss 1.8149375915527344
iteration 200, loss 1.7620229721069336
iteration 300, loss 1.8472578525543213
iteration 400, loss 1.7898740768432617
iteration 500, loss 1.818272352218628
iteration 600, loss 1.7637251615524292
iteration 700, loss 1.8306958675384521
iteration 800, loss 1.7913405895233154
iteration 0, loss 1.8720381259918213
iteration 100, loss 1.806320309638977
iteration 200, loss 1.7780917882919312
iteration 300, loss 1.8017257452011108
iteration 400, loss 1.934141755104065
iteration 500, loss 1.8337782621383667
iteration 600, loss 1.8265236616134644
iteration 700, loss 1.89054274559021
iteration 800, loss 1.8249423503875732
iteration 0, loss 1.8619896173477173
iteration 100, loss 1.8074791431427002
iteration 200, loss 1.887777328491211
iteration 300, loss 1.8606566190719604
iteration 400, loss 1.7291916608810425
iteration 500, loss 1.8441760540008545
iteration 600, loss 1.8079389333724976
iteration 700, loss 1.7683908939361572
iteration 800, loss 1.8327662944793701
iteration 0, loss 1.8319978713989258
iteration 100, loss 1.8563052415847778
iteration 200, loss 1.8442349433898926
iteration 300, loss 1.8245203495025635
iteration 400, loss 1.8364696502685547
iteration 500, loss 1.8440519571304321
iteration 600, loss 1.8519260883331299
iteration 700, loss 1.826015830039978
iteration 800, loss 1.7755078077316284
iteration 0, loss 1.8653450012207031
iteration 100, loss 1.793801188468933
iteration 200, loss 1.8410544395446777
iteration 300, loss 1.7928638458251953
iteration 400, loss 1.8273615837097168
iteration 500, loss 1.850820779800415
iteration 600, loss 1.857853651046753
iteration 700, loss 1.8218810558319092
iteration 800, loss 1.833660364151001
iteration 0, loss 1.900411605834961
iteration 100, loss 1.7680546045303345
iteration 200, loss 1.8873579502105713
iteration 300, loss 1.8437408208847046
iteration 400, loss 1.9298597574234009
iteration 500, loss 1.8428983688354492
iteration 600, loss 1.841151237487793
iteration 700, loss 1.8502639532089233
iteration 800, loss 1.8612629175186157
iteration 0, loss 1.8865469694137573
iteration 100, loss 1.869571328163147
iteration 200, loss 1.8121387958526611
iteration 300, loss 1.846306324005127
iteration 400, loss 1.855071783065796
iteration 500, loss 1.9257736206054688
iteration 600, loss 1.8559350967407227
iteration 700, loss 1.8478519916534424
iteration 800, loss 1.831268548965454
iteration 0, loss 1.7582588195800781
iteration 100, loss 1.8962997198104858
iteration 200, loss 1.8133199214935303
iteration 300, loss 1.8087705373764038
iteration 400, loss 1.785728096961975
iteration 500, loss 1.8329049348831177
iteration 600, loss 1.8427180051803589
iteration 700, loss 1.890108585357666
iteration 800, loss 1.8489011526107788
iteration 0, loss 1.8428705930709839
iteration 100, loss 1.846682071685791
iteration 200, loss 1.822377324104309
iteration 300, loss 1.779327392578125
iteration 400, loss 1.791192889213562
iteration 500, loss 1.7930511236190796
iteration 600, loss 1.8951154947280884
iteration 700, loss 1.873350739479065
iteration 800, loss 1.903922438621521
iteration 0, loss 1.840135931968689
iteration 100, loss 1.84332275390625
iteration 200, loss 1.7884137630462646
iteration 300, loss 1.7816027402877808
iteration 400, loss 1.8373734951019287
iteration 500, loss 1.8304927349090576
iteration 600, loss 1.7865984439849854
iteration 700, loss 1.8687666654586792
iteration 800, loss 1.8917187452316284
iteration 0, loss 1.777645468711853
iteration 100, loss 1.8726974725723267
iteration 200, loss 1.8684256076812744
iteration 300, loss 1.869889736175537
iteration 400, loss 1.8582111597061157
iteration 500, loss 1.8056108951568604
iteration 600, loss 1.8093814849853516
iteration 700, loss 1.7527450323104858
iteration 800, loss 1.8506309986114502
iteration 0, loss 1.8614675998687744
iteration 100, loss 1.8189802169799805
iteration 200, loss 1.848759412765503
iteration 300, loss 1.8796042203903198
iteration 400, loss 1.8127144575119019
iteration 500, loss 1.8102525472640991
iteration 600, loss 1.895963191986084
iteration 700, loss 1.9046730995178223
iteration 800, loss 1.7945513725280762
iteration 0, loss 1.8051360845565796
iteration 100, loss 1.8235490322113037
iteration 200, loss 1.776944637298584
iteration 300, loss 1.8808327913284302
iteration 400, loss 1.8757250308990479
iteration 500, loss 1.802961826324463
iteration 600, loss 1.774543285369873
iteration 700, loss 1.8233585357666016
iteration 800, loss 1.89665687084198
iteration 0, loss 1.8737084865570068
iteration 100, loss 1.7972919940948486
iteration 200, loss 1.8396135568618774
iteration 300, loss 1.9432976245880127
iteration 400, loss 1.8065062761306763
iteration 500, loss 1.7442089319229126
iteration 600, loss 1.7921382188796997
iteration 700, loss 1.7506626844406128
iteration 800, loss 1.8299567699432373
iteration 0, loss 1.8193175792694092
iteration 100, loss 1.8450736999511719
iteration 200, loss 1.8604247570037842
iteration 300, loss 1.8887768983840942
iteration 400, loss 1.8203600645065308
iteration 500, loss 1.840722918510437
iteration 600, loss 1.8836517333984375
iteration 700, loss 1.8174786567687988
iteration 800, loss 1.809114694595337
iteration 0, loss 1.8904333114624023
iteration 100, loss 1.8597869873046875
iteration 200, loss 1.7698129415512085
iteration 300, loss 1.8594948053359985
iteration 400, loss 1.8070930242538452
iteration 500, loss 1.8773984909057617
iteration 600, loss 1.8415765762329102
iteration 700, loss 1.7873175144195557
iteration 800, loss 1.8262732028961182
iteration 0, loss 1.8965758085250854
iteration 100, loss 1.799898386001587
iteration 200, loss 1.8346325159072876
iteration 300, loss 1.8138790130615234
iteration 400, loss 1.8526531457901
iteration 500, loss 1.8763600587844849
iteration 600, loss 1.9112093448638916
iteration 700, loss 1.851267695426941
iteration 800, loss 1.9138025045394897
iteration 0, loss 1.8687160015106201
iteration 100, loss 1.8678838014602661
iteration 200, loss 1.8315852880477905
iteration 300, loss 1.796028733253479
iteration 400, loss 1.9017367362976074
iteration 500, loss 1.9086841344833374
iteration 600, loss 1.8180134296417236
iteration 700, loss 1.8722161054611206
iteration 800, loss 1.7643651962280273
iteration 0, loss 1.8345861434936523
iteration 100, loss 1.803621768951416
iteration 200, loss 1.800736904144287
iteration 300, loss 1.8643373250961304
iteration 400, loss 1.8205924034118652
iteration 500, loss 1.8487917184829712
iteration 600, loss 1.8627254962921143
iteration 700, loss 1.811142921447754
iteration 800, loss 1.805008888244629
iteration 0, loss 1.906409502029419
iteration 100, loss 1.8545969724655151
iteration 200, loss 1.8651772737503052
iteration 300, loss 1.8579732179641724
iteration 400, loss 1.8739469051361084
iteration 500, loss 1.916150450706482
iteration 600, loss 1.8660838603973389
iteration 700, loss 1.8413951396942139
iteration 800, loss 1.8395960330963135
iteration 0, loss 1.8065605163574219
iteration 100, loss 1.834610939025879
iteration 200, loss 1.8040539026260376
iteration 300, loss 1.8956348896026611
iteration 400, loss 1.782867670059204
iteration 500, loss 1.9290884733200073
iteration 600, loss 1.874661922454834
iteration 700, loss 1.8127853870391846
iteration 800, loss 1.8739182949066162
iteration 0, loss 1.8609837293624878
iteration 100, loss 1.8838942050933838
iteration 200, loss 1.7948521375656128
iteration 300, loss 1.783953070640564
iteration 400, loss 1.8475992679595947
iteration 500, loss 1.915574073791504
iteration 600, loss 1.9036766290664673
iteration 700, loss 1.8692899942398071
iteration 800, loss 1.8366411924362183
iteration 0, loss 1.7862802743911743
iteration 100, loss 1.8031904697418213
iteration 200, loss 1.7658238410949707
iteration 300, loss 1.9085216522216797
iteration 400, loss 1.911420226097107
iteration 500, loss 1.8735225200653076
iteration 600, loss 1.8511426448822021
iteration 700, loss 1.7635003328323364
iteration 800, loss 1.8486073017120361
iteration 0, loss 1.7742705345153809
iteration 100, loss 1.8028607368469238
iteration 200, loss 1.8100792169570923
iteration 300, loss 1.810951828956604
iteration 400, loss 1.7914501428604126
iteration 500, loss 1.8591728210449219
iteration 600, loss 1.80147123336792
iteration 700, loss 1.914425253868103
iteration 800, loss 1.9115235805511475
iteration 0, loss 1.7751449346542358
iteration 100, loss 1.816644310951233
iteration 200, loss 1.840569019317627
iteration 300, loss 1.7954264879226685
iteration 400, loss 1.9268838167190552
iteration 500, loss 1.8107020854949951
iteration 600, loss 1.8811649084091187
iteration 700, loss 1.859747052192688
iteration 800, loss 1.7322521209716797
iteration 0, loss 1.7814706563949585
iteration 100, loss 1.8289813995361328
iteration 200, loss 1.827200174331665
iteration 300, loss 1.945440411567688
iteration 400, loss 1.8071775436401367
iteration 500, loss 1.8376106023788452
iteration 600, loss 1.8769434690475464
iteration 700, loss 1.831068992614746
iteration 800, loss 1.768884539604187
iteration 0, loss 1.7910202741622925
iteration 100, loss 1.8273937702178955
iteration 200, loss 1.7799030542373657
iteration 300, loss 1.7815722227096558
iteration 400, loss 1.7647621631622314
iteration 500, loss 1.8508721590042114
iteration 600, loss 1.8058381080627441
iteration 700, loss 1.8489267826080322
iteration 800, loss 1.936795711517334
iteration 0, loss 1.8415796756744385
iteration 100, loss 1.807936429977417
iteration 200, loss 1.753183126449585
iteration 300, loss 1.8217532634735107
iteration 400, loss 1.7744437456130981
iteration 500, loss 1.8611032962799072
iteration 600, loss 1.8701255321502686
iteration 700, loss 1.746404767036438
iteration 800, loss 1.8185452222824097
iteration 0, loss 1.8306103944778442
iteration 100, loss 1.8180304765701294
iteration 200, loss 1.8301706314086914
iteration 300, loss 1.9232280254364014
iteration 400, loss 1.837664246559143
iteration 500, loss 1.8830747604370117
iteration 600, loss 1.8587926626205444
iteration 700, loss 1.8582186698913574
iteration 800, loss 1.8065903186798096
fold 2 accuracy: 0.4075
iteration 0, loss 1.8544882535934448
iteration 100, loss 1.8694725036621094
iteration 200, loss 1.8549928665161133
iteration 300, loss 1.7893892526626587
iteration 400, loss 1.7841447591781616
iteration 500, loss 1.7963011264801025
iteration 600, loss 1.8030229806900024
iteration 700, loss 1.9604064226150513
iteration 800, loss 1.7562984228134155
iteration 0, loss 1.8440983295440674
iteration 100, loss 1.8004673719406128
iteration 200, loss 1.8567134141921997
iteration 300, loss 1.9113473892211914
iteration 400, loss 1.7843668460845947
iteration 500, loss 1.8852180242538452
iteration 600, loss 1.8101638555526733
iteration 700, loss 1.8742269277572632
iteration 800, loss 1.8284540176391602
iteration 0, loss 1.7863941192626953
iteration 100, loss 1.8274673223495483
iteration 200, loss 1.8141417503356934
iteration 300, loss 1.81583833694458
iteration 400, loss 1.8186712265014648
iteration 500, loss 1.767073392868042
iteration 600, loss 1.807629108428955
iteration 700, loss 1.8546110391616821
iteration 800, loss 1.8466479778289795
iteration 0, loss 1.8543193340301514
iteration 100, loss 1.8247478008270264
iteration 200, loss 1.8288952112197876
iteration 300, loss 1.8528962135314941
iteration 400, loss 1.8179470300674438
iteration 500, loss 1.828922152519226
iteration 600, loss 1.8423852920532227
iteration 700, loss 1.8828130960464478
iteration 800, loss 1.836953043937683
iteration 0, loss 1.8037726879119873
iteration 100, loss 1.7871171236038208
iteration 200, loss 1.8154041767120361
iteration 300, loss 1.856587290763855
iteration 400, loss 1.8676230907440186
iteration 500, loss 1.8246806859970093
iteration 600, loss 1.83574640750885
iteration 700, loss 1.8415210247039795
iteration 800, loss 1.8997673988342285
iteration 0, loss 1.7330886125564575
iteration 100, loss 1.828061819076538
iteration 200, loss 1.7700965404510498
iteration 300, loss 1.756984829902649
iteration 400, loss 1.8206707239151
iteration 500, loss 1.8712607622146606
iteration 600, loss 1.8676408529281616
iteration 700, loss 1.9200550317764282
iteration 800, loss 1.7873737812042236
iteration 0, loss 1.8008562326431274
iteration 100, loss 1.9568595886230469
iteration 200, loss 1.7949273586273193
iteration 300, loss 1.8495899438858032
iteration 400, loss 1.8340603113174438
iteration 500, loss 1.7403126955032349
iteration 600, loss 1.787673830986023
iteration 700, loss 1.8155502080917358
iteration 800, loss 1.8261737823486328
iteration 0, loss 1.7334809303283691
iteration 100, loss 1.7827587127685547
iteration 200, loss 1.8440475463867188
iteration 300, loss 1.9203120470046997
iteration 400, loss 1.75565505027771
iteration 500, loss 1.7985609769821167
iteration 600, loss 1.8512850999832153
iteration 700, loss 1.859853982925415
iteration 800, loss 1.8586522340774536
iteration 0, loss 1.841502070426941
iteration 100, loss 1.8242063522338867
iteration 200, loss 1.8695039749145508
iteration 300, loss 1.8402351140975952
iteration 400, loss 1.8524093627929688
iteration 500, loss 1.8361527919769287
iteration 600, loss 1.9109941720962524
iteration 700, loss 1.7951008081436157
iteration 800, loss 1.8179863691329956
iteration 0, loss 1.8937177658081055
iteration 100, loss 1.7864704132080078
iteration 200, loss 1.788818359375
iteration 300, loss 1.896370530128479
iteration 400, loss 1.8790110349655151
iteration 500, loss 1.8264336585998535
iteration 600, loss 1.8056151866912842
iteration 700, loss 1.8517457246780396
iteration 800, loss 1.8061712980270386
iteration 0, loss 1.8791840076446533
iteration 100, loss 1.78236722946167
iteration 200, loss 1.7752864360809326
iteration 300, loss 1.8160181045532227
iteration 400, loss 1.831094741821289
iteration 500, loss 1.7870417833328247
iteration 600, loss 1.8051527738571167
iteration 700, loss 1.8506718873977661
iteration 800, loss 1.8635873794555664
iteration 0, loss 1.7432717084884644
iteration 100, loss 1.862830638885498
iteration 200, loss 1.8535140752792358
iteration 300, loss 1.80540132522583
iteration 400, loss 1.8694435358047485
iteration 500, loss 1.8355313539505005
iteration 600, loss 1.8091336488723755
iteration 700, loss 1.8101332187652588
iteration 800, loss 1.8645843267440796
iteration 0, loss 1.8150461912155151
iteration 100, loss 1.7907702922821045
iteration 200, loss 1.8470525741577148
iteration 300, loss 1.764539122581482
iteration 400, loss 1.8810513019561768
iteration 500, loss 1.7800343036651611
iteration 600, loss 1.8476498126983643
iteration 700, loss 1.836476445198059
iteration 800, loss 1.878196120262146
iteration 0, loss 1.8074897527694702
iteration 100, loss 1.849286437034607
iteration 200, loss 1.8102508783340454
iteration 300, loss 1.9076125621795654
iteration 400, loss 1.901046633720398
iteration 500, loss 1.8876696825027466
iteration 600, loss 1.830222725868225
iteration 700, loss 1.8308509588241577
iteration 800, loss 1.8433401584625244
iteration 0, loss 1.792387843132019
iteration 100, loss 1.9174505472183228
iteration 200, loss 1.8602015972137451
iteration 300, loss 1.8095682859420776
iteration 400, loss 1.8770763874053955
iteration 500, loss 1.7790693044662476
iteration 600, loss 1.8370078802108765
iteration 700, loss 1.8932477235794067
iteration 800, loss 1.764391303062439
iteration 0, loss 1.8557493686676025
iteration 100, loss 1.896798014640808
iteration 200, loss 1.7675139904022217
iteration 300, loss 1.7997052669525146
iteration 400, loss 1.7849304676055908
iteration 500, loss 1.7671698331832886
iteration 600, loss 1.8249996900558472
iteration 700, loss 1.857080340385437
iteration 800, loss 1.80624520778656
iteration 0, loss 1.8887892961502075
iteration 100, loss 1.8460357189178467
iteration 200, loss 1.7899850606918335
iteration 300, loss 1.9072271585464478
iteration 400, loss 1.819163203239441
iteration 500, loss 1.7254053354263306
iteration 600, loss 1.799403190612793
iteration 700, loss 1.8713831901550293
iteration 800, loss 1.8607182502746582
iteration 0, loss 1.8526698350906372
iteration 100, loss 1.8514952659606934
iteration 200, loss 1.7372819185256958
iteration 300, loss 1.8401057720184326
iteration 400, loss 1.8197011947631836
iteration 500, loss 1.790393352508545
iteration 600, loss 1.8055928945541382
iteration 700, loss 1.797810435295105
iteration 800, loss 1.7898873090744019
iteration 0, loss 1.8018662929534912
iteration 100, loss 1.9250295162200928
iteration 200, loss 1.8286432027816772
iteration 300, loss 1.8328654766082764
iteration 400, loss 1.771294116973877
iteration 500, loss 1.873091459274292
iteration 600, loss 1.761722445487976
iteration 700, loss 1.7668243646621704
iteration 800, loss 1.7932345867156982
iteration 0, loss 1.886118769645691
iteration 100, loss 1.8097213506698608
iteration 200, loss 1.8244497776031494
iteration 300, loss 1.8795239925384521
iteration 400, loss 1.9151891469955444
iteration 500, loss 1.8340743780136108
iteration 600, loss 1.7995890378952026
iteration 700, loss 1.9315170049667358
iteration 800, loss 1.7951197624206543
iteration 0, loss 1.8077253103256226
iteration 100, loss 1.8520655632019043
iteration 200, loss 1.8391960859298706
iteration 300, loss 1.80208420753479
iteration 400, loss 1.8580939769744873
iteration 500, loss 1.8274726867675781
iteration 600, loss 1.8156388998031616
iteration 700, loss 1.8539972305297852
iteration 800, loss 1.8397468328475952
iteration 0, loss 1.832848072052002
iteration 100, loss 1.8363101482391357
iteration 200, loss 1.8493058681488037
iteration 300, loss 1.8377760648727417
iteration 400, loss 1.7827423810958862
iteration 500, loss 1.8480288982391357
iteration 600, loss 1.8625645637512207
iteration 700, loss 1.8455352783203125
iteration 800, loss 1.8230615854263306
iteration 0, loss 1.8146268129348755
iteration 100, loss 1.7928862571716309
iteration 200, loss 1.819096326828003
iteration 300, loss 1.7990328073501587
iteration 400, loss 1.9173979759216309
iteration 500, loss 1.7615889310836792
iteration 600, loss 1.7857848405838013
iteration 700, loss 1.7738864421844482
iteration 800, loss 1.8013406991958618
iteration 0, loss 1.8404958248138428
iteration 100, loss 1.8439373970031738
iteration 200, loss 1.9148550033569336
iteration 300, loss 1.8069148063659668
iteration 400, loss 1.763495922088623
iteration 500, loss 1.8050639629364014
iteration 600, loss 1.8361632823944092
iteration 700, loss 1.8080222606658936
iteration 800, loss 1.8116576671600342
iteration 0, loss 1.8146709203720093
iteration 100, loss 1.804666519165039
iteration 200, loss 1.9165352582931519
iteration 300, loss 1.8117722272872925
iteration 400, loss 1.7987602949142456
iteration 500, loss 1.8411898612976074
iteration 600, loss 1.849316954612732
iteration 700, loss 1.7552249431610107
iteration 800, loss 1.8221652507781982
iteration 0, loss 1.8202157020568848
iteration 100, loss 1.820428729057312
iteration 200, loss 1.8116222620010376
iteration 300, loss 1.8147636651992798
iteration 400, loss 1.838438630104065
iteration 500, loss 1.8867285251617432
iteration 600, loss 1.7736355066299438
iteration 700, loss 1.8982679843902588
iteration 800, loss 1.7985045909881592
iteration 0, loss 1.8707462549209595
iteration 100, loss 1.8183518648147583
iteration 200, loss 1.77863609790802
iteration 300, loss 1.8629945516586304
iteration 400, loss 1.8858015537261963
iteration 500, loss 1.8686730861663818
iteration 600, loss 1.805464506149292
iteration 700, loss 1.8117411136627197
iteration 800, loss 1.8265016078948975
iteration 0, loss 1.866970181465149
iteration 100, loss 1.8204219341278076
iteration 200, loss 1.816462755203247
iteration 300, loss 1.8477476835250854
iteration 400, loss 1.8359090089797974
iteration 500, loss 1.83079993724823
iteration 600, loss 1.8472890853881836
iteration 700, loss 1.825297474861145
iteration 800, loss 1.9324615001678467
iteration 0, loss 1.8320419788360596
iteration 100, loss 1.8515868186950684
iteration 200, loss 1.7945854663848877
iteration 300, loss 1.8531360626220703
iteration 400, loss 1.9195693731307983
iteration 500, loss 1.79012131690979
iteration 600, loss 1.870758295059204
iteration 700, loss 1.8475215435028076
iteration 800, loss 1.8923369646072388
iteration 0, loss 1.8349543809890747
iteration 100, loss 1.750243902206421
iteration 200, loss 1.8334933519363403
iteration 300, loss 1.8541591167449951
iteration 400, loss 1.82437002658844
iteration 500, loss 1.8464444875717163
iteration 600, loss 1.7569024562835693
iteration 700, loss 1.8801518678665161
iteration 800, loss 1.8725327253341675
iteration 0, loss 1.848866581916809
iteration 100, loss 1.823845624923706
iteration 200, loss 1.8083925247192383
iteration 300, loss 1.8202577829360962
iteration 400, loss 1.8724114894866943
iteration 500, loss 1.8977302312850952
iteration 600, loss 1.780310869216919
iteration 700, loss 1.8284896612167358
iteration 800, loss 1.8349329233169556
iteration 0, loss 1.8278570175170898
iteration 100, loss 1.7754428386688232
iteration 200, loss 1.8763940334320068
iteration 300, loss 1.92021906375885
iteration 400, loss 1.868788480758667
iteration 500, loss 1.7904412746429443
iteration 600, loss 1.8926341533660889
iteration 700, loss 1.788859248161316
iteration 800, loss 1.7616654634475708
iteration 0, loss 1.8662029504776
iteration 100, loss 1.9002386331558228
iteration 200, loss 1.8899482488632202
iteration 300, loss 1.8378868103027344
iteration 400, loss 1.8516180515289307
iteration 500, loss 1.7956613302230835
iteration 600, loss 1.7978025674819946
iteration 700, loss 1.8505511283874512
iteration 800, loss 1.81951904296875
iteration 0, loss 1.812511682510376
iteration 100, loss 1.9039828777313232
iteration 200, loss 1.9198311567306519
iteration 300, loss 1.7803964614868164
iteration 400, loss 1.8564811944961548
iteration 500, loss 1.784898281097412
iteration 600, loss 1.8564616441726685
iteration 700, loss 1.8187274932861328
iteration 800, loss 1.8548640012741089
iteration 0, loss 1.8249331712722778
iteration 100, loss 1.7922345399856567
iteration 200, loss 1.9101566076278687
iteration 300, loss 1.8150087594985962
iteration 400, loss 1.8348708152770996
iteration 500, loss 1.8613380193710327
iteration 600, loss 1.7908332347869873
iteration 700, loss 1.9042644500732422
iteration 800, loss 1.7800711393356323
iteration 0, loss 1.87166428565979
iteration 100, loss 1.8318127393722534
iteration 200, loss 1.7638450860977173
iteration 300, loss 1.86137855052948
iteration 400, loss 1.8374123573303223
iteration 500, loss 1.7964391708374023
iteration 600, loss 1.817535161972046
iteration 700, loss 1.8329849243164062
iteration 800, loss 1.8151054382324219
iteration 0, loss 1.9068922996520996
iteration 100, loss 1.9076406955718994
iteration 200, loss 1.788679838180542
iteration 300, loss 1.795212745666504
iteration 400, loss 1.8932510614395142
iteration 500, loss 1.8516554832458496
iteration 600, loss 1.8306299448013306
iteration 700, loss 1.8896278142929077
iteration 800, loss 1.8973416090011597
iteration 0, loss 1.7827978134155273
iteration 100, loss 1.8422905206680298
iteration 200, loss 1.8011754751205444
iteration 300, loss 1.774853229522705
iteration 400, loss 1.8262939453125
iteration 500, loss 1.8881434202194214
iteration 600, loss 1.7890509366989136
iteration 700, loss 1.791961669921875
iteration 800, loss 1.9069331884384155
iteration 0, loss 1.7805347442626953
iteration 100, loss 1.819801926612854
iteration 200, loss 1.7762020826339722
iteration 300, loss 1.8075231313705444
iteration 400, loss 1.817177176475525
iteration 500, loss 1.8223671913146973
iteration 600, loss 1.8618714809417725
iteration 700, loss 1.7926586866378784
iteration 800, loss 1.810418963432312
iteration 0, loss 1.8186267614364624
iteration 100, loss 1.8114757537841797
iteration 200, loss 1.8386162519454956
iteration 300, loss 1.8263212442398071
iteration 400, loss 1.8616973161697388
iteration 500, loss 1.7717225551605225
iteration 600, loss 1.9457076787948608
iteration 700, loss 1.8202897310256958
iteration 800, loss 1.8450117111206055
iteration 0, loss 1.7468743324279785
iteration 100, loss 1.8886284828186035
iteration 200, loss 1.8218371868133545
iteration 300, loss 1.826254963874817
iteration 400, loss 1.840451955795288
iteration 500, loss 1.8777884244918823
iteration 600, loss 1.8208674192428589
iteration 700, loss 1.7965871095657349
iteration 800, loss 1.8289120197296143
iteration 0, loss 1.8924407958984375
iteration 100, loss 1.8887544870376587
iteration 200, loss 1.7725766897201538
iteration 300, loss 1.8009366989135742
iteration 400, loss 1.8963027000427246
iteration 500, loss 1.8797016143798828
iteration 600, loss 1.8922059535980225
iteration 700, loss 1.7696995735168457
iteration 800, loss 1.79448401927948
iteration 0, loss 1.8465750217437744
iteration 100, loss 1.7958158254623413
iteration 200, loss 1.8366830348968506
iteration 300, loss 1.7689030170440674
iteration 400, loss 1.8419303894042969
iteration 500, loss 1.7419098615646362
iteration 600, loss 1.8246536254882812
iteration 700, loss 1.7498029470443726
iteration 800, loss 1.8421485424041748
iteration 0, loss 1.8544927835464478
iteration 100, loss 1.7980364561080933
iteration 200, loss 1.8204020261764526
iteration 300, loss 1.8111847639083862
iteration 400, loss 1.7897274494171143
iteration 500, loss 1.817913293838501
iteration 600, loss 1.8775768280029297
iteration 700, loss 1.8139318227767944
iteration 800, loss 1.8049705028533936
iteration 0, loss 1.8927435874938965
iteration 100, loss 1.8220751285552979
iteration 200, loss 1.7751446962356567
iteration 300, loss 1.8623441457748413
iteration 400, loss 1.8080685138702393
iteration 500, loss 1.7606251239776611
iteration 600, loss 1.841973900794983
iteration 700, loss 1.8585903644561768
iteration 800, loss 1.856954574584961
iteration 0, loss 1.8917251825332642
iteration 100, loss 1.7996314764022827
iteration 200, loss 1.8020743131637573
iteration 300, loss 1.824836254119873
iteration 400, loss 1.8958662748336792
iteration 500, loss 1.869739055633545
iteration 600, loss 1.7885366678237915
iteration 700, loss 1.8454809188842773
iteration 800, loss 1.8703467845916748
iteration 0, loss 1.8921170234680176
iteration 100, loss 1.7843468189239502
iteration 200, loss 1.8784759044647217
iteration 300, loss 1.844625473022461
iteration 400, loss 1.8581434488296509
iteration 500, loss 1.7971067428588867
iteration 600, loss 1.8569635152816772
iteration 700, loss 1.8995565176010132
iteration 800, loss 1.9096980094909668
iteration 0, loss 1.799868106842041
iteration 100, loss 1.8566045761108398
iteration 200, loss 1.867387056350708
iteration 300, loss 1.8571022748947144
iteration 400, loss 1.8686615228652954
iteration 500, loss 1.8274106979370117
iteration 600, loss 1.9099328517913818
iteration 700, loss 1.8595412969589233
iteration 800, loss 1.8136184215545654
iteration 0, loss 1.8787921667099
iteration 100, loss 1.8520338535308838
iteration 200, loss 1.783070683479309
iteration 300, loss 1.8017369508743286
iteration 400, loss 1.816312313079834
iteration 500, loss 1.779362678527832
iteration 600, loss 1.8572157621383667
iteration 700, loss 1.8462756872177124
iteration 800, loss 1.8709555864334106
iteration 0, loss 1.8000704050064087
iteration 100, loss 1.7421255111694336
iteration 200, loss 1.8006277084350586
iteration 300, loss 1.827784538269043
iteration 400, loss 1.8429213762283325
iteration 500, loss 1.9122761487960815
iteration 600, loss 1.831960678100586
iteration 700, loss 1.8168586492538452
iteration 800, loss 1.801814079284668
fold 3 accuracy: 0.40485714285714286
iteration 0, loss 1.8147350549697876
iteration 100, loss 1.846174955368042
iteration 200, loss 1.8864469528198242
iteration 300, loss 1.8027881383895874
iteration 400, loss 1.809715986251831
iteration 500, loss 1.9300442934036255
iteration 600, loss 1.8532015085220337
iteration 700, loss 1.803892731666565
iteration 800, loss 1.8037883043289185
iteration 0, loss 1.8190844058990479
iteration 100, loss 1.8565571308135986
iteration 200, loss 1.7603864669799805
iteration 300, loss 1.7936521768569946
iteration 400, loss 1.8261340856552124
iteration 500, loss 1.9338685274124146
iteration 600, loss 1.87130606174469
iteration 700, loss 1.8349405527114868
iteration 800, loss 1.7846684455871582
iteration 0, loss 1.7908971309661865
iteration 100, loss 1.8048204183578491
iteration 200, loss 1.8860740661621094
iteration 300, loss 1.8302644491195679
iteration 400, loss 1.833445429801941
iteration 500, loss 1.8629429340362549
iteration 600, loss 1.8121896982192993
iteration 700, loss 1.821905255317688
iteration 800, loss 1.803722858428955
iteration 0, loss 1.788685917854309
iteration 100, loss 1.8165992498397827
iteration 200, loss 1.8088738918304443
iteration 300, loss 1.86911141872406
iteration 400, loss 1.8424174785614014
iteration 500, loss 1.8209766149520874
iteration 600, loss 1.8450604677200317
iteration 700, loss 1.9335049390792847
iteration 800, loss 1.7842228412628174
iteration 0, loss 1.7864526510238647
iteration 100, loss 1.9075578451156616
iteration 200, loss 1.7256745100021362
iteration 300, loss 1.7991623878479004
iteration 400, loss 1.801069736480713
iteration 500, loss 1.9313217401504517
iteration 600, loss 1.8100429773330688
iteration 700, loss 1.8393791913986206
iteration 800, loss 1.8585267066955566
iteration 0, loss 1.734847068786621
iteration 100, loss 1.8438197374343872
iteration 200, loss 1.863234519958496
iteration 300, loss 1.8007179498672485
iteration 400, loss 1.879238486289978
iteration 500, loss 1.7592036724090576
iteration 600, loss 1.8224034309387207
iteration 700, loss 1.8191977739334106
iteration 800, loss 1.8170244693756104
iteration 0, loss 1.7851475477218628
iteration 100, loss 1.7835981845855713
iteration 200, loss 1.8852049112319946
iteration 300, loss 1.8282437324523926
iteration 400, loss 1.7938271760940552
iteration 500, loss 1.7750424146652222
iteration 600, loss 1.8163683414459229
iteration 700, loss 1.7489455938339233
iteration 800, loss 1.8388447761535645
iteration 0, loss 1.8292431831359863
iteration 100, loss 1.8135250806808472
iteration 200, loss 1.9677969217300415
iteration 300, loss 1.8696913719177246
iteration 400, loss 1.8194071054458618
iteration 500, loss 1.8786840438842773
iteration 600, loss 1.8490725755691528
iteration 700, loss 1.8368860483169556
iteration 800, loss 1.8411204814910889
iteration 0, loss 1.8308968544006348
iteration 100, loss 1.9373527765274048
iteration 200, loss 1.8013554811477661
iteration 300, loss 1.8044459819793701
iteration 400, loss 1.8572990894317627
iteration 500, loss 1.8948885202407837
iteration 600, loss 1.791815161705017
iteration 700, loss 1.8426880836486816
iteration 800, loss 1.896146297454834
iteration 0, loss 1.8501806259155273
iteration 100, loss 1.8867652416229248
iteration 200, loss 1.7861638069152832
iteration 300, loss 1.8282670974731445
iteration 400, loss 1.7883944511413574
iteration 500, loss 1.8157610893249512
iteration 600, loss 1.7830712795257568
iteration 700, loss 1.913163661956787
iteration 800, loss 1.8146623373031616
iteration 0, loss 1.7707030773162842
iteration 100, loss 1.8698163032531738
iteration 200, loss 1.8633900880813599
iteration 300, loss 1.782187581062317
iteration 400, loss 1.8118232488632202
iteration 500, loss 1.7790682315826416
iteration 600, loss 1.8200322389602661
iteration 700, loss 1.8850609064102173
iteration 800, loss 1.8309050798416138
iteration 0, loss 1.8288697004318237
iteration 100, loss 1.8493988513946533
iteration 200, loss 1.796273946762085
iteration 300, loss 1.8148193359375
iteration 400, loss 1.8013049364089966
iteration 500, loss 1.7900409698486328
iteration 600, loss 1.7959169149398804
iteration 700, loss 1.755156397819519
iteration 800, loss 1.8108298778533936
iteration 0, loss 1.8459813594818115
iteration 100, loss 1.8390008211135864
iteration 200, loss 1.8471990823745728
iteration 300, loss 1.79530930519104
iteration 400, loss 1.8202062845230103
iteration 500, loss 1.8604133129119873
iteration 600, loss 1.8840934038162231
iteration 700, loss 1.7860133647918701
iteration 800, loss 1.8041963577270508
iteration 0, loss 1.7833001613616943
iteration 100, loss 1.8689906597137451
iteration 200, loss 1.7905406951904297
iteration 300, loss 1.8375452756881714
iteration 400, loss 1.7769750356674194
iteration 500, loss 1.8815937042236328
iteration 600, loss 1.808959722518921
iteration 700, loss 1.7836410999298096
iteration 800, loss 1.8934987783432007
iteration 0, loss 1.844819188117981
iteration 100, loss 1.7992985248565674
iteration 200, loss 1.8340939283370972
iteration 300, loss 1.813660979270935
iteration 400, loss 1.8206592798233032
iteration 500, loss 1.821550726890564
iteration 600, loss 1.801012396812439
iteration 700, loss 1.7968369722366333
iteration 800, loss 1.7721275091171265
iteration 0, loss 1.8555355072021484
iteration 100, loss 1.8954790830612183
iteration 200, loss 1.808096170425415
iteration 300, loss 1.8279258012771606
iteration 400, loss 1.7920664548873901
iteration 500, loss 1.7931381464004517
iteration 600, loss 1.7748979330062866
iteration 700, loss 1.8587181568145752
iteration 800, loss 1.8113996982574463
iteration 0, loss 1.7819918394088745
iteration 100, loss 1.8951067924499512
iteration 200, loss 1.8077843189239502
iteration 300, loss 1.7940853834152222
iteration 400, loss 1.8345270156860352
iteration 500, loss 1.8803870677947998
iteration 600, loss 1.969366431236267
iteration 700, loss 1.8199352025985718
iteration 800, loss 1.7255069017410278
iteration 0, loss 1.8850129842758179
iteration 100, loss 1.7540702819824219
iteration 200, loss 1.8511345386505127
iteration 300, loss 1.7889914512634277
iteration 400, loss 1.8075776100158691
iteration 500, loss 1.844377875328064
iteration 600, loss 1.8400225639343262
iteration 700, loss 1.8294256925582886
iteration 800, loss 1.7848323583602905
iteration 0, loss 1.8495374917984009
iteration 100, loss 1.8598213195800781
iteration 200, loss 1.8245574235916138
iteration 300, loss 1.8111172914505005
iteration 400, loss 1.8296254873275757
iteration 500, loss 1.853453278541565
iteration 600, loss 1.79188871383667
iteration 700, loss 1.8149288892745972
iteration 800, loss 1.7805999517440796
iteration 0, loss 1.8405884504318237
iteration 100, loss 1.8369429111480713
iteration 200, loss 1.8905640840530396
iteration 300, loss 1.7572379112243652
iteration 400, loss 1.8153718709945679
iteration 500, loss 1.8485610485076904
iteration 600, loss 1.843241810798645
iteration 700, loss 1.77161705493927
iteration 800, loss 1.8292884826660156
iteration 0, loss 1.7880709171295166
iteration 100, loss 1.8897963762283325
iteration 200, loss 1.8412352800369263
iteration 300, loss 1.8778045177459717
iteration 400, loss 1.8851368427276611
iteration 500, loss 1.7643507719039917
iteration 600, loss 1.8495632410049438
iteration 700, loss 1.8122402429580688
iteration 800, loss 1.8816405534744263
iteration 0, loss 1.880712866783142
iteration 100, loss 1.8857264518737793
iteration 200, loss 1.8254339694976807
iteration 300, loss 1.853190302848816
iteration 400, loss 1.8717875480651855
iteration 500, loss 1.8760015964508057
iteration 600, loss 1.7975109815597534
iteration 700, loss 1.7851485013961792
iteration 800, loss 1.8897979259490967
iteration 0, loss 1.7901043891906738
iteration 100, loss 1.8456281423568726
iteration 200, loss 1.7363711595535278
iteration 300, loss 1.899398684501648
iteration 400, loss 1.7891135215759277
iteration 500, loss 1.8187309503555298
iteration 600, loss 1.8742790222167969
iteration 700, loss 1.8796523809432983
iteration 800, loss 1.8819613456726074
iteration 0, loss 1.8217687606811523
iteration 100, loss 1.898728370666504
iteration 200, loss 1.838173270225525
iteration 300, loss 1.8282978534698486
iteration 400, loss 1.8145486116409302
iteration 500, loss 1.8446592092514038
iteration 600, loss 1.8131893873214722
iteration 700, loss 1.891278862953186
iteration 800, loss 1.8321373462677002
iteration 0, loss 1.7879440784454346
iteration 100, loss 1.8250107765197754
iteration 200, loss 1.8007676601409912
iteration 300, loss 1.7879215478897095
iteration 400, loss 1.8394221067428589
iteration 500, loss 1.8725181818008423
iteration 600, loss 1.7463191747665405
iteration 700, loss 1.7560065984725952
iteration 800, loss 1.830293893814087
iteration 0, loss 1.8324705362319946
iteration 100, loss 1.8795243501663208
iteration 200, loss 1.8354840278625488
iteration 300, loss 1.7840536832809448
iteration 400, loss 1.8647066354751587
iteration 500, loss 1.8503694534301758
iteration 600, loss 1.8773915767669678
iteration 700, loss 1.8281773328781128
iteration 800, loss 1.830176830291748
iteration 0, loss 1.8733611106872559
iteration 100, loss 1.79347562789917
iteration 200, loss 1.8432012796401978
iteration 300, loss 1.901718258857727
iteration 400, loss 1.844812273979187
iteration 500, loss 1.8183629512786865
iteration 600, loss 1.8089600801467896
iteration 700, loss 1.786627173423767
iteration 800, loss 1.8264477252960205
iteration 0, loss 1.8719286918640137
iteration 100, loss 1.849649429321289
iteration 200, loss 1.8608627319335938
iteration 300, loss 1.8838258981704712
iteration 400, loss 1.859220266342163
iteration 500, loss 1.7969865798950195
iteration 600, loss 1.741615891456604
iteration 700, loss 1.917688012123108
iteration 800, loss 1.8344049453735352
iteration 0, loss 1.8068057298660278
iteration 100, loss 1.8311727046966553
iteration 200, loss 1.7904537916183472
iteration 300, loss 1.8985352516174316
iteration 400, loss 1.8396282196044922
iteration 500, loss 1.8212645053863525
iteration 600, loss 1.8282631635665894
iteration 700, loss 1.7721389532089233
iteration 800, loss 1.8191472291946411
iteration 0, loss 1.9164679050445557
iteration 100, loss 1.8634072542190552
iteration 200, loss 1.8635883331298828
iteration 300, loss 1.8494229316711426
iteration 400, loss 1.8305972814559937
iteration 500, loss 1.8417928218841553
iteration 600, loss 1.7992656230926514
iteration 700, loss 1.7703486680984497
iteration 800, loss 1.8257168531417847
iteration 0, loss 1.9277135133743286
iteration 100, loss 1.8531733751296997
iteration 200, loss 1.8826184272766113
iteration 300, loss 1.8590141534805298
iteration 400, loss 1.8935976028442383
iteration 500, loss 1.7591617107391357
iteration 600, loss 1.8607932329177856
iteration 700, loss 1.7664235830307007
iteration 800, loss 1.7471065521240234
iteration 0, loss 1.7779244184494019
iteration 100, loss 1.7959610223770142
iteration 200, loss 1.7987773418426514
iteration 300, loss 1.846813440322876
iteration 400, loss 1.8513245582580566
iteration 500, loss 1.8025377988815308
iteration 600, loss 1.7437105178833008
iteration 700, loss 1.8145760297775269
iteration 800, loss 1.9012459516525269
iteration 0, loss 1.8844138383865356
iteration 100, loss 1.8158882856369019
iteration 200, loss 1.7938770055770874
iteration 300, loss 1.9607182741165161
iteration 400, loss 1.8982313871383667
iteration 500, loss 1.8457484245300293
iteration 600, loss 1.807320237159729
iteration 700, loss 1.887830138206482
iteration 800, loss 1.8594670295715332
iteration 0, loss 1.8230310678482056
iteration 100, loss 1.8066545724868774
iteration 200, loss 1.8178348541259766
iteration 300, loss 1.9021409749984741
iteration 400, loss 1.828300952911377
iteration 500, loss 1.835015058517456
iteration 600, loss 1.8100075721740723
iteration 700, loss 1.8672116994857788
iteration 800, loss 1.7866101264953613
iteration 0, loss 1.8805807828903198
iteration 100, loss 1.8890972137451172
iteration 200, loss 1.845647931098938
iteration 300, loss 1.8804212808609009
iteration 400, loss 1.7863613367080688
iteration 500, loss 1.8593765497207642
iteration 600, loss 1.8493828773498535
iteration 700, loss 1.8487194776535034
iteration 800, loss 1.8608516454696655
iteration 0, loss 1.7873995304107666
iteration 100, loss 1.806687593460083
iteration 200, loss 1.8755292892456055
iteration 300, loss 1.7544540166854858
iteration 400, loss 1.9123315811157227
iteration 500, loss 1.8076047897338867
iteration 600, loss 1.857162594795227
iteration 700, loss 1.8009405136108398
iteration 800, loss 1.8307312726974487
iteration 0, loss 1.8200125694274902
iteration 100, loss 1.7924014329910278
iteration 200, loss 1.885573387145996
iteration 300, loss 1.788915991783142
iteration 400, loss 1.8340779542922974
iteration 500, loss 1.8007533550262451
iteration 600, loss 1.839166522026062
iteration 700, loss 1.8028535842895508
iteration 800, loss 1.8327713012695312
iteration 0, loss 1.8353517055511475
iteration 100, loss 1.8146439790725708
iteration 200, loss 1.8171175718307495
iteration 300, loss 1.7777726650238037
iteration 400, loss 1.839928388595581
iteration 500, loss 1.7967767715454102
iteration 600, loss 1.7894248962402344
iteration 700, loss 1.8092011213302612
iteration 800, loss 1.8249552249908447
iteration 0, loss 1.82753324508667
iteration 100, loss 1.831960678100586
iteration 200, loss 1.7651634216308594
iteration 300, loss 1.7906484603881836
iteration 400, loss 1.8414732217788696
iteration 500, loss 1.8220012187957764
iteration 600, loss 1.8169244527816772
iteration 700, loss 1.8776544332504272
iteration 800, loss 1.8091809749603271
iteration 0, loss 1.8027350902557373
iteration 100, loss 1.8255150318145752
iteration 200, loss 1.8287146091461182
iteration 300, loss 1.7856922149658203
iteration 400, loss 1.873037576675415
iteration 500, loss 1.8869543075561523
iteration 600, loss 1.8585096597671509
iteration 700, loss 1.8444981575012207
iteration 800, loss 1.7990491390228271
iteration 0, loss 1.868194580078125
iteration 100, loss 1.8691582679748535
iteration 200, loss 1.8210138082504272
iteration 300, loss 1.8311351537704468
iteration 400, loss 1.882101058959961
iteration 500, loss 1.822400450706482
iteration 600, loss 1.8612595796585083
iteration 700, loss 1.7867971658706665
iteration 800, loss 1.8046174049377441
iteration 0, loss 1.8609665632247925
iteration 100, loss 1.8188084363937378
iteration 200, loss 1.8618799448013306
iteration 300, loss 1.772681474685669
iteration 400, loss 1.7725776433944702
iteration 500, loss 1.8107448816299438
iteration 600, loss 1.8484597206115723
iteration 700, loss 1.827349305152893
iteration 800, loss 1.8073160648345947
iteration 0, loss 1.8314940929412842
iteration 100, loss 1.7250362634658813
iteration 200, loss 1.8466514348983765
iteration 300, loss 1.8281506299972534
iteration 400, loss 1.7825955152511597
iteration 500, loss 1.7940489053726196
iteration 600, loss 1.793682336807251
iteration 700, loss 1.8630516529083252
iteration 800, loss 1.7779643535614014
iteration 0, loss 1.8681360483169556
iteration 100, loss 1.8647507429122925
iteration 200, loss 1.8055769205093384
iteration 300, loss 1.7929364442825317
iteration 400, loss 1.816025972366333
iteration 500, loss 1.8481225967407227
iteration 600, loss 1.8287713527679443
iteration 700, loss 1.8613566160202026
iteration 800, loss 1.8677644729614258
iteration 0, loss 1.7947664260864258
iteration 100, loss 1.8090802431106567
iteration 200, loss 1.7598761320114136
iteration 300, loss 1.7545098066329956
iteration 400, loss 1.812515377998352
iteration 500, loss 1.7674612998962402
iteration 600, loss 1.8051342964172363
iteration 700, loss 1.8362517356872559
iteration 800, loss 1.8124425411224365
iteration 0, loss 1.913331389427185
iteration 100, loss 1.7797715663909912
iteration 200, loss 1.8504624366760254
iteration 300, loss 1.7679259777069092
iteration 400, loss 1.7580618858337402
iteration 500, loss 1.9375505447387695
iteration 600, loss 1.895899772644043
iteration 700, loss 1.817340612411499
iteration 800, loss 1.7174800634384155
iteration 0, loss 1.837212085723877
iteration 100, loss 1.8503363132476807
iteration 200, loss 1.8807740211486816
iteration 300, loss 1.8007900714874268
iteration 400, loss 1.8436819314956665
iteration 500, loss 1.838291883468628
iteration 600, loss 1.8669226169586182
iteration 700, loss 1.9638667106628418
iteration 800, loss 1.8233329057693481
iteration 0, loss 1.75712251663208
iteration 100, loss 1.8302028179168701
iteration 200, loss 1.8625489473342896
iteration 300, loss 1.8692225217819214
iteration 400, loss 1.8528081178665161
iteration 500, loss 1.8797087669372559
iteration 600, loss 1.8497838973999023
iteration 700, loss 1.7429081201553345
iteration 800, loss 1.8427423238754272
iteration 0, loss 1.762033224105835
iteration 100, loss 1.8130273818969727
iteration 200, loss 1.80367112159729
iteration 300, loss 1.7787981033325195
iteration 400, loss 1.7651326656341553
iteration 500, loss 1.8278508186340332
iteration 600, loss 1.8155914545059204
iteration 700, loss 1.8025227785110474
iteration 800, loss 1.8926284313201904
iteration 0, loss 1.891133427619934
iteration 100, loss 1.7772808074951172
iteration 200, loss 1.8058297634124756
iteration 300, loss 1.9016189575195312
iteration 400, loss 1.8136613368988037
iteration 500, loss 1.7688237428665161
iteration 600, loss 1.8000799417495728
iteration 700, loss 1.8323485851287842
iteration 800, loss 1.843376636505127
fold 4 accuracy: 0.40414285714285714
[2024-02-28 23:31:06,396] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-28 23:31:06,402] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            331.88 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.69 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '331.88 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 331.88 us = 100% latency, 1.69 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 236.27 us = 71.19% latency, 2.37 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 27.89 us = 8.41% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-28 23:31:06,403] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
iteration 0, loss 2.288407802581787
iteration 100, loss 2.3573954105377197
iteration 200, loss 2.208800792694092
iteration 300, loss 2.0930817127227783
iteration 400, loss 2.1001367568969727
iteration 500, loss 2.0435547828674316
iteration 600, loss 2.040829658508301
iteration 700, loss 2.0454142093658447
iteration 800, loss 2.0047223567962646
iteration 0, loss 2.0470738410949707
iteration 100, loss 2.001178741455078
iteration 200, loss 1.9893066883087158
iteration 300, loss 2.055169105529785
iteration 400, loss 2.006472110748291
iteration 500, loss 2.000229835510254
iteration 600, loss 2.003356456756592
iteration 700, loss 2.03292179107666
iteration 800, loss 1.965436339378357
iteration 0, loss 2.0209715366363525
iteration 100, loss 2.017829418182373
iteration 200, loss 2.0122921466827393
iteration 300, loss 1.8933502435684204
iteration 400, loss 2.0380637645721436
iteration 500, loss 1.945114254951477
iteration 600, loss 1.9572381973266602
iteration 700, loss 1.9036951065063477
iteration 800, loss 1.9159562587738037
iteration 0, loss 1.9694745540618896
iteration 100, loss 1.9478535652160645
iteration 200, loss 1.9512217044830322
iteration 300, loss 1.9661660194396973
iteration 400, loss 1.8899565935134888
iteration 500, loss 1.976392388343811
iteration 600, loss 1.9250051975250244
iteration 700, loss 1.9272875785827637
iteration 800, loss 1.8957858085632324
iteration 0, loss 1.9417656660079956
iteration 100, loss 1.950933814048767
iteration 200, loss 1.959197759628296
iteration 300, loss 1.9078471660614014
iteration 400, loss 1.913766860961914
iteration 500, loss 1.9027049541473389
iteration 600, loss 1.9740899801254272
iteration 700, loss 1.940011739730835
iteration 800, loss 1.8206593990325928
iteration 0, loss 1.9577258825302124
iteration 100, loss 1.880051851272583
iteration 200, loss 1.9062843322753906
iteration 300, loss 1.8853510618209839
iteration 400, loss 1.8892902135849
iteration 500, loss 1.9558175802230835
iteration 600, loss 1.9663857221603394
iteration 700, loss 1.9519068002700806
iteration 800, loss 1.9604626893997192
iteration 0, loss 1.9282511472702026
iteration 100, loss 1.944780945777893
iteration 200, loss 1.8962993621826172
iteration 300, loss 1.876383900642395
iteration 400, loss 1.9310588836669922
iteration 500, loss 1.9508814811706543
iteration 600, loss 1.8487932682037354
iteration 700, loss 1.8531074523925781
iteration 800, loss 1.9099740982055664
iteration 0, loss 1.8782050609588623
iteration 100, loss 1.949621558189392
iteration 200, loss 1.9332592487335205
iteration 300, loss 1.9032776355743408
iteration 400, loss 1.9208019971847534
iteration 500, loss 1.895525336265564
iteration 600, loss 2.010019063949585
iteration 700, loss 1.9205026626586914
iteration 800, loss 1.8896287679672241
iteration 0, loss 1.9137359857559204
iteration 100, loss 1.9245631694793701
iteration 200, loss 1.8190680742263794
iteration 300, loss 1.9791231155395508
iteration 400, loss 1.8097020387649536
iteration 500, loss 1.8901798725128174
iteration 600, loss 1.8338675498962402
iteration 700, loss 1.8340345621109009
iteration 800, loss 1.875788688659668
iteration 0, loss 1.8550138473510742
iteration 100, loss 1.9220507144927979
iteration 200, loss 1.9140995740890503
iteration 300, loss 1.7965933084487915
iteration 400, loss 1.898112177848816
iteration 500, loss 1.9190541505813599
iteration 600, loss 1.858909010887146
iteration 700, loss 1.8558835983276367
iteration 800, loss 1.9404761791229248
iteration 0, loss 1.8539063930511475
iteration 100, loss 1.86709463596344
iteration 200, loss 1.9274541139602661
iteration 300, loss 1.8691514730453491
iteration 400, loss 1.8777717351913452
iteration 500, loss 1.8862543106079102
iteration 600, loss 1.860763669013977
iteration 700, loss 1.9287834167480469
iteration 800, loss 1.8310400247573853
iteration 0, loss 1.8978914022445679
iteration 100, loss 1.9247502088546753
iteration 200, loss 1.8963446617126465
iteration 300, loss 1.921078085899353
iteration 400, loss 1.8582009077072144
iteration 500, loss 1.9224803447723389
iteration 600, loss 1.8796451091766357
iteration 700, loss 1.9260038137435913
iteration 800, loss 1.8880141973495483
iteration 0, loss 1.9120874404907227
iteration 100, loss 1.8716084957122803
iteration 200, loss 1.881381630897522
iteration 300, loss 1.8479399681091309
iteration 400, loss 1.950883150100708
iteration 500, loss 1.8156815767288208
iteration 600, loss 1.835604190826416
iteration 700, loss 1.938380479812622
iteration 800, loss 1.9458396434783936
iteration 0, loss 1.7776293754577637
iteration 100, loss 1.8666592836380005
iteration 200, loss 1.893172025680542
iteration 300, loss 1.9436618089675903
iteration 400, loss 1.8957587480545044
iteration 500, loss 1.8096117973327637
iteration 600, loss 1.8272333145141602
iteration 700, loss 1.8927401304244995
iteration 800, loss 1.9296873807907104
iteration 0, loss 1.8455764055252075
iteration 100, loss 1.8824386596679688
iteration 200, loss 1.841684341430664
iteration 300, loss 1.860107660293579
iteration 400, loss 1.8301151990890503
iteration 500, loss 1.9440720081329346
iteration 600, loss 1.8729307651519775
iteration 700, loss 1.8088910579681396
iteration 800, loss 1.8117210865020752
iteration 0, loss 1.980919361114502
iteration 100, loss 1.8384523391723633
iteration 200, loss 1.8963713645935059
iteration 300, loss 1.8205927610397339
iteration 400, loss 1.9119502305984497
iteration 500, loss 1.8756452798843384
iteration 600, loss 1.8563357591629028
iteration 700, loss 1.834219217300415
iteration 800, loss 1.8734865188598633
iteration 0, loss 1.8248577117919922
iteration 100, loss 1.9074532985687256
iteration 200, loss 1.8932479619979858
iteration 300, loss 1.8452945947647095
iteration 400, loss 1.9342222213745117
iteration 500, loss 1.8719947338104248
iteration 600, loss 1.8491737842559814
iteration 700, loss 1.9467005729675293
iteration 800, loss 1.8564629554748535
iteration 0, loss 1.8888344764709473
iteration 100, loss 1.8184844255447388
iteration 200, loss 1.872103214263916
iteration 300, loss 1.8818284273147583
iteration 400, loss 1.9453164339065552
iteration 500, loss 1.8103314638137817
iteration 600, loss 1.7517054080963135
iteration 700, loss 1.8560311794281006
iteration 800, loss 1.8943712711334229
iteration 0, loss 1.8612014055252075
iteration 100, loss 1.8327716588974
iteration 200, loss 1.8467768430709839
iteration 300, loss 1.8140943050384521
iteration 400, loss 1.835415005683899
iteration 500, loss 1.8940651416778564
iteration 600, loss 1.788823127746582
iteration 700, loss 1.804611086845398
iteration 800, loss 1.8888508081436157
iteration 0, loss 1.8119838237762451
iteration 100, loss 1.8300914764404297
iteration 200, loss 1.8987969160079956
iteration 300, loss 1.8016149997711182
iteration 400, loss 1.8516277074813843
iteration 500, loss 1.876415491104126
iteration 600, loss 1.8356244564056396
iteration 700, loss 1.799557089805603
iteration 800, loss 1.8633261919021606
iteration 0, loss 1.8027209043502808
iteration 100, loss 1.8679089546203613
iteration 200, loss 1.8284497261047363
iteration 300, loss 1.816200613975525
iteration 400, loss 1.9017890691757202
iteration 500, loss 1.808396339416504
iteration 600, loss 1.8688260316848755
iteration 700, loss 1.8880535364151
iteration 800, loss 1.8642572164535522
iteration 0, loss 1.8213030099868774
iteration 100, loss 1.841198444366455
iteration 200, loss 1.8408232927322388
iteration 300, loss 1.8376504182815552
iteration 400, loss 1.7983328104019165
iteration 500, loss 1.902289867401123
iteration 600, loss 1.8641891479492188
iteration 700, loss 1.8532648086547852
iteration 800, loss 1.857583999633789
iteration 0, loss 1.8539228439331055
iteration 100, loss 1.8765833377838135
iteration 200, loss 1.8411178588867188
iteration 300, loss 1.8362222909927368
iteration 400, loss 1.7992924451828003
iteration 500, loss 1.8419028520584106
iteration 600, loss 1.8163816928863525
iteration 700, loss 1.8156334161758423
iteration 800, loss 1.8223493099212646
iteration 0, loss 1.8668522834777832
iteration 100, loss 1.828734278678894
iteration 200, loss 1.8465337753295898
iteration 300, loss 1.8342385292053223
iteration 400, loss 1.8818682432174683
iteration 500, loss 1.7990870475769043
iteration 600, loss 1.825495719909668
iteration 700, loss 1.8024905920028687
iteration 800, loss 1.840517520904541
iteration 0, loss 1.8103852272033691
iteration 100, loss 1.8609871864318848
iteration 200, loss 1.862306833267212
iteration 300, loss 1.7938885688781738
iteration 400, loss 1.8480615615844727
iteration 500, loss 1.8168305158615112
iteration 600, loss 1.7961853742599487
iteration 700, loss 1.8433562517166138
iteration 800, loss 1.802786111831665
iteration 0, loss 1.8085432052612305
iteration 100, loss 1.7566490173339844
iteration 200, loss 1.8916809558868408
iteration 300, loss 1.8615429401397705
iteration 400, loss 1.7882076501846313
iteration 500, loss 1.8731201887130737
iteration 600, loss 1.8536527156829834
iteration 700, loss 1.8958468437194824
iteration 800, loss 1.8871307373046875
iteration 0, loss 1.803593397140503
iteration 100, loss 1.8488712310791016
iteration 200, loss 1.828348994255066
iteration 300, loss 1.8998730182647705
iteration 400, loss 1.87875497341156
iteration 500, loss 1.8558484315872192
iteration 600, loss 1.8508801460266113
iteration 700, loss 1.9022724628448486
iteration 800, loss 1.8602429628372192
iteration 0, loss 1.9149714708328247
iteration 100, loss 1.8664557933807373
iteration 200, loss 1.8501272201538086
iteration 300, loss 1.8892732858657837
iteration 400, loss 1.816806674003601
iteration 500, loss 1.80745267868042
iteration 600, loss 1.7735763788223267
iteration 700, loss 1.840086579322815
iteration 800, loss 1.8689144849777222
iteration 0, loss 1.8836272954940796
iteration 100, loss 1.8536925315856934
iteration 200, loss 1.8859868049621582
iteration 300, loss 1.7663778066635132
iteration 400, loss 1.7906101942062378
iteration 500, loss 1.8831851482391357
iteration 600, loss 1.8662017583847046
iteration 700, loss 1.8161470890045166
iteration 800, loss 1.852103352546692
iteration 0, loss 1.8723186254501343
iteration 100, loss 1.8404998779296875
iteration 200, loss 1.7681171894073486
iteration 300, loss 1.8607609272003174
iteration 400, loss 1.8092010021209717
iteration 500, loss 1.7995136976242065
iteration 600, loss 1.7895383834838867
iteration 700, loss 1.786357045173645
iteration 800, loss 1.8247050046920776
iteration 0, loss 1.8208307027816772
iteration 100, loss 1.83916175365448
iteration 200, loss 1.84512460231781
iteration 300, loss 1.838746190071106
iteration 400, loss 1.8480401039123535
iteration 500, loss 1.8749918937683105
iteration 600, loss 1.8189103603363037
iteration 700, loss 1.908402919769287
iteration 800, loss 1.8087682723999023
iteration 0, loss 1.8395895957946777
iteration 100, loss 1.820002555847168
iteration 200, loss 1.8730627298355103
iteration 300, loss 1.85624361038208
iteration 400, loss 1.8676925897598267
iteration 500, loss 1.8325037956237793
iteration 600, loss 1.8226169347763062
iteration 700, loss 1.8384230136871338
iteration 800, loss 1.804911494255066
iteration 0, loss 1.9101574420928955
iteration 100, loss 1.8094252347946167
iteration 200, loss 1.8136844635009766
iteration 300, loss 1.8178586959838867
iteration 400, loss 1.848549485206604
iteration 500, loss 1.7889896631240845
iteration 600, loss 1.8260382413864136
iteration 700, loss 1.8481160402297974
iteration 800, loss 1.8676594495773315
iteration 0, loss 1.856677770614624
iteration 100, loss 1.7592438459396362
iteration 200, loss 1.8094981908798218
iteration 300, loss 1.778954267501831
iteration 400, loss 1.8153146505355835
iteration 500, loss 1.8700015544891357
iteration 600, loss 1.882493019104004
iteration 700, loss 1.8319146633148193
iteration 800, loss 1.812718152999878
iteration 0, loss 1.7721867561340332
iteration 100, loss 1.8249813318252563
iteration 200, loss 1.8696932792663574
iteration 300, loss 1.7931745052337646
iteration 400, loss 1.7928850650787354
iteration 500, loss 1.837317705154419
iteration 600, loss 1.816142201423645
iteration 700, loss 1.853420376777649
iteration 800, loss 1.7694026231765747
iteration 0, loss 1.8596599102020264
iteration 100, loss 1.784618854522705
iteration 200, loss 1.8808013200759888
iteration 300, loss 1.8502947092056274
iteration 400, loss 1.7937238216400146
iteration 500, loss 1.8597326278686523
iteration 600, loss 1.8972985744476318
iteration 700, loss 1.8271515369415283
iteration 800, loss 1.8031394481658936
iteration 0, loss 1.881253957748413
iteration 100, loss 1.8274707794189453
iteration 200, loss 1.764380693435669
iteration 300, loss 1.8097944259643555
iteration 400, loss 1.8024946451187134
iteration 500, loss 1.825805902481079
iteration 600, loss 1.8497958183288574
iteration 700, loss 1.8868520259857178
iteration 800, loss 1.8597209453582764
iteration 0, loss 1.8295109272003174
iteration 100, loss 1.8621227741241455
iteration 200, loss 1.9054924249649048
iteration 300, loss 1.7994900941848755
iteration 400, loss 1.8251227140426636
iteration 500, loss 1.812231421470642
iteration 600, loss 1.7687077522277832
iteration 700, loss 1.7823213338851929
iteration 800, loss 1.85914945602417
iteration 0, loss 1.853610634803772
iteration 100, loss 1.7887465953826904
iteration 200, loss 1.8405178785324097
iteration 300, loss 1.8211485147476196
iteration 400, loss 1.8349761962890625
iteration 500, loss 1.8068490028381348
iteration 600, loss 1.854419469833374
iteration 700, loss 1.8055638074874878
iteration 800, loss 1.8207008838653564
iteration 0, loss 1.790266990661621
iteration 100, loss 1.827809453010559
iteration 200, loss 1.8712055683135986
iteration 300, loss 1.851339340209961
iteration 400, loss 1.8216195106506348
iteration 500, loss 1.807254433631897
iteration 600, loss 1.8104130029678345
iteration 700, loss 1.9485005140304565
iteration 800, loss 1.8374967575073242
iteration 0, loss 1.8480501174926758
iteration 100, loss 1.7759023904800415
iteration 200, loss 1.7927789688110352
iteration 300, loss 1.8026716709136963
iteration 400, loss 1.8384830951690674
iteration 500, loss 1.810452938079834
iteration 600, loss 1.8660774230957031
iteration 700, loss 1.86207115650177
iteration 800, loss 1.883653163909912
iteration 0, loss 1.8023104667663574
iteration 100, loss 1.7731914520263672
iteration 200, loss 1.8326343297958374
iteration 300, loss 1.8213815689086914
iteration 400, loss 1.8433058261871338
iteration 500, loss 1.9197698831558228
iteration 600, loss 1.8457145690917969
iteration 700, loss 1.8686996698379517
iteration 800, loss 1.8652020692825317
iteration 0, loss 1.8740471601486206
iteration 100, loss 1.8098514080047607
iteration 200, loss 1.7730660438537598
iteration 300, loss 1.7921956777572632
iteration 400, loss 1.7930370569229126
iteration 500, loss 1.8302580118179321
iteration 600, loss 1.7628315687179565
iteration 700, loss 1.8021236658096313
iteration 800, loss 1.849083423614502
iteration 0, loss 1.837927222251892
iteration 100, loss 1.8225890398025513
iteration 200, loss 1.8553229570388794
iteration 300, loss 1.8186168670654297
iteration 400, loss 1.8086477518081665
iteration 500, loss 1.8343076705932617
iteration 600, loss 1.7918344736099243
iteration 700, loss 1.8274457454681396
iteration 800, loss 1.869711995124817
iteration 0, loss 1.7648322582244873
iteration 100, loss 1.8300421237945557
iteration 200, loss 1.8340030908584595
iteration 300, loss 1.834385871887207
iteration 400, loss 1.8445326089859009
iteration 500, loss 1.7722399234771729
iteration 600, loss 1.848846673965454
iteration 700, loss 1.788253903388977
iteration 800, loss 1.8244104385375977
iteration 0, loss 1.8068556785583496
iteration 100, loss 1.8140524625778198
iteration 200, loss 1.8365228176116943
iteration 300, loss 1.8157553672790527
iteration 400, loss 1.8264844417572021
iteration 500, loss 1.8672339916229248
iteration 600, loss 1.8168387413024902
iteration 700, loss 1.8531776666641235
iteration 800, loss 1.8208363056182861
iteration 0, loss 1.8373357057571411
iteration 100, loss 1.8488121032714844
iteration 200, loss 1.9264572858810425
iteration 300, loss 1.842419981956482
iteration 400, loss 1.8000773191452026
iteration 500, loss 1.8342697620391846
iteration 600, loss 1.843827486038208
iteration 700, loss 1.8455582857131958
iteration 800, loss 1.7467241287231445
iteration 0, loss 1.8633419275283813
iteration 100, loss 1.854364275932312
iteration 200, loss 1.7824002504348755
iteration 300, loss 1.8713706731796265
iteration 400, loss 1.8586031198501587
iteration 500, loss 1.7993440628051758
iteration 600, loss 1.8327816724777222
iteration 700, loss 1.77560555934906
iteration 800, loss 1.7475461959838867
iteration 0, loss 1.918411374092102
iteration 100, loss 1.7514086961746216
iteration 200, loss 1.8733634948730469
iteration 300, loss 1.9594295024871826
iteration 400, loss 1.797723412513733
iteration 500, loss 1.8252216577529907
iteration 600, loss 1.885282039642334
iteration 700, loss 1.7368968725204468
iteration 800, loss 1.7884783744812012
iteration 0, loss 1.831205129623413
iteration 100, loss 1.883070468902588
iteration 200, loss 1.9112365245819092
iteration 300, loss 1.8118338584899902
iteration 400, loss 1.8186125755310059
iteration 500, loss 1.7467190027236938
iteration 600, loss 1.8999056816101074
iteration 700, loss 1.819139838218689
iteration 800, loss 1.8552989959716797
fold 0 accuracy: 0.4714285714285714
iteration 0, loss 1.8711769580841064
iteration 100, loss 1.7997015714645386
iteration 200, loss 1.8139604330062866
iteration 300, loss 1.9082103967666626
iteration 400, loss 1.8261549472808838
iteration 500, loss 1.7542797327041626
iteration 600, loss 1.886277675628662
iteration 700, loss 1.876065731048584
iteration 800, loss 1.8267946243286133
iteration 0, loss 1.811282992362976
iteration 100, loss 1.849385380744934
iteration 200, loss 1.839091420173645
iteration 300, loss 1.7700271606445312
iteration 400, loss 1.877063512802124
iteration 500, loss 1.7434179782867432
iteration 600, loss 1.9276124238967896
iteration 700, loss 1.809269905090332
iteration 800, loss 1.844454288482666
iteration 0, loss 1.8680957555770874
iteration 100, loss 1.8273944854736328
iteration 200, loss 1.8238425254821777
iteration 300, loss 1.8230669498443604
iteration 400, loss 1.7810944318771362
iteration 500, loss 1.8241922855377197
iteration 600, loss 1.7913001775741577
iteration 700, loss 1.843299388885498
iteration 800, loss 1.7437455654144287
iteration 0, loss 1.8170174360275269
iteration 100, loss 1.8468937873840332
iteration 200, loss 1.7163230180740356
iteration 300, loss 1.7981868982315063
iteration 400, loss 1.7706881761550903
iteration 500, loss 1.756232500076294
iteration 600, loss 1.8403629064559937
iteration 700, loss 1.8178696632385254
iteration 800, loss 1.8479676246643066
iteration 0, loss 1.8444275856018066
iteration 100, loss 1.831525444984436
iteration 200, loss 1.8470650911331177
iteration 300, loss 1.8678897619247437
iteration 400, loss 1.8090258836746216
iteration 500, loss 1.8344703912734985
iteration 600, loss 1.8301842212677002
iteration 700, loss 1.8356876373291016
iteration 800, loss 1.8157600164413452
iteration 0, loss 1.8517358303070068
iteration 100, loss 1.9151699542999268
iteration 200, loss 1.7892272472381592
iteration 300, loss 1.764780879020691
iteration 400, loss 1.7986093759536743
iteration 500, loss 1.8564122915267944
iteration 600, loss 1.9511851072311401
iteration 700, loss 1.8431267738342285
iteration 800, loss 1.8402937650680542
iteration 0, loss 1.887583613395691
iteration 100, loss 1.8306701183319092
iteration 200, loss 1.872848391532898
iteration 300, loss 1.7574890851974487
iteration 400, loss 1.870430827140808
iteration 500, loss 1.8415312767028809
iteration 600, loss 1.7580206394195557
iteration 700, loss 1.7922545671463013
iteration 800, loss 1.8659995794296265
iteration 0, loss 1.8147488832473755
iteration 100, loss 1.8580433130264282
iteration 200, loss 1.8050580024719238
iteration 300, loss 1.7917308807373047
iteration 400, loss 1.8779906034469604
iteration 500, loss 1.8390202522277832
iteration 600, loss 1.8089321851730347
iteration 700, loss 1.7715675830841064
iteration 800, loss 1.8167392015457153
iteration 0, loss 1.8236823081970215
iteration 100, loss 1.9089765548706055
iteration 200, loss 1.8009841442108154
iteration 300, loss 1.8802180290222168
iteration 400, loss 1.8328322172164917
iteration 500, loss 1.8200359344482422
iteration 600, loss 1.8302192687988281
iteration 700, loss 1.7406330108642578
iteration 800, loss 1.8741436004638672
iteration 0, loss 1.8423923254013062
iteration 100, loss 1.8131697177886963
iteration 200, loss 1.836864709854126
iteration 300, loss 1.8227382898330688
iteration 400, loss 1.7625714540481567
iteration 500, loss 1.9641298055648804
iteration 600, loss 1.8089128732681274
iteration 700, loss 1.8621212244033813
iteration 800, loss 1.8376024961471558
iteration 0, loss 1.8970136642456055
iteration 100, loss 1.8384265899658203
iteration 200, loss 1.7807272672653198
iteration 300, loss 1.7250617742538452
iteration 400, loss 1.8696210384368896
iteration 500, loss 1.8143311738967896
iteration 600, loss 1.7769880294799805
iteration 700, loss 1.8705123662948608
iteration 800, loss 1.7979919910430908
iteration 0, loss 1.7512749433517456
iteration 100, loss 1.7482523918151855
iteration 200, loss 1.7904796600341797
iteration 300, loss 1.819066047668457
iteration 400, loss 1.8405334949493408
iteration 500, loss 1.8048956394195557
iteration 600, loss 1.7716056108474731
iteration 700, loss 1.8746027946472168
iteration 800, loss 1.8551039695739746
iteration 0, loss 1.7501766681671143
iteration 100, loss 1.864302635192871
iteration 200, loss 1.931671380996704
iteration 300, loss 1.7972333431243896
iteration 400, loss 1.8456926345825195
iteration 500, loss 1.8666892051696777
iteration 600, loss 1.8742681741714478
iteration 700, loss 1.8127321004867554
iteration 800, loss 1.7733632326126099
iteration 0, loss 1.797698736190796
iteration 100, loss 1.7879871129989624
iteration 200, loss 1.8173401355743408
iteration 300, loss 1.856643557548523
iteration 400, loss 1.7979155778884888
iteration 500, loss 1.7861672639846802
iteration 600, loss 1.751334547996521
iteration 700, loss 1.842535138130188
iteration 800, loss 1.817071557044983
iteration 0, loss 1.7686612606048584
iteration 100, loss 1.7493723630905151
iteration 200, loss 1.7445098161697388
iteration 300, loss 1.7914960384368896
iteration 400, loss 1.820523977279663
iteration 500, loss 1.864609718322754
iteration 600, loss 1.842371940612793
iteration 700, loss 1.801782250404358
iteration 800, loss 1.7698017358779907
iteration 0, loss 1.8053489923477173
iteration 100, loss 1.8389338254928589
iteration 200, loss 1.7729074954986572
iteration 300, loss 1.8340296745300293
iteration 400, loss 1.773639440536499
iteration 500, loss 1.8446736335754395
iteration 600, loss 1.8285061120986938
iteration 700, loss 1.7883367538452148
iteration 800, loss 1.8448573350906372
iteration 0, loss 1.8706399202346802
iteration 100, loss 1.8358961343765259
iteration 200, loss 1.8160289525985718
iteration 300, loss 1.8241339921951294
iteration 400, loss 1.8019315004348755
iteration 500, loss 1.7738069295883179
iteration 600, loss 1.862870216369629
iteration 700, loss 1.9006619453430176
iteration 800, loss 1.8482954502105713
iteration 0, loss 1.8002731800079346
iteration 100, loss 1.8404523134231567
iteration 200, loss 1.8750174045562744
iteration 300, loss 1.8526294231414795
iteration 400, loss 1.7829222679138184
iteration 500, loss 1.8748652935028076
iteration 600, loss 1.7634094953536987
iteration 700, loss 1.8325296640396118
iteration 800, loss 1.8506431579589844
iteration 0, loss 1.7952220439910889
iteration 100, loss 1.8047300577163696
iteration 200, loss 1.7959368228912354
iteration 300, loss 1.8300588130950928
iteration 400, loss 1.8502346277236938
iteration 500, loss 1.843660593032837
iteration 600, loss 1.752063512802124
iteration 700, loss 1.859100103378296
iteration 800, loss 1.8198283910751343
iteration 0, loss 1.7671706676483154
iteration 100, loss 1.8022433519363403
iteration 200, loss 1.8359333276748657
iteration 300, loss 1.801123023033142
iteration 400, loss 1.8131980895996094
iteration 500, loss 1.8184353113174438
iteration 600, loss 1.8061450719833374
iteration 700, loss 1.8556228876113892
iteration 800, loss 1.8380486965179443
iteration 0, loss 1.8034433126449585
iteration 100, loss 1.854559063911438
iteration 200, loss 1.8423324823379517
iteration 300, loss 1.8417229652404785
iteration 400, loss 1.8052809238433838
iteration 500, loss 1.7782903909683228
iteration 600, loss 1.8523170948028564
iteration 700, loss 1.854012370109558
iteration 800, loss 1.829704761505127
iteration 0, loss 1.754150629043579
iteration 100, loss 1.919201374053955
iteration 200, loss 1.7856180667877197
iteration 300, loss 1.8791613578796387
iteration 400, loss 1.8372325897216797
iteration 500, loss 1.7358711957931519
iteration 600, loss 1.7893412113189697
iteration 700, loss 1.8385450839996338
iteration 800, loss 1.7657948732376099
iteration 0, loss 1.7683953046798706
iteration 100, loss 1.8423504829406738
iteration 200, loss 1.8635400533676147
iteration 300, loss 1.799821138381958
iteration 400, loss 1.8548916578292847
iteration 500, loss 1.817359209060669
iteration 600, loss 1.8452391624450684
iteration 700, loss 1.7941783666610718
iteration 800, loss 1.8081257343292236
iteration 0, loss 1.8571220636367798
iteration 100, loss 1.803952932357788
iteration 200, loss 1.7534369230270386
iteration 300, loss 1.8303533792495728
iteration 400, loss 1.8583904504776
iteration 500, loss 1.849509358406067
iteration 600, loss 1.745538353919983
iteration 700, loss 1.821661353111267
iteration 800, loss 1.8639531135559082
iteration 0, loss 1.8567852973937988
iteration 100, loss 1.8184654712677002
iteration 200, loss 1.793829083442688
iteration 300, loss 1.756044864654541
iteration 400, loss 1.8524250984191895
iteration 500, loss 1.8247815370559692
iteration 600, loss 1.831396222114563
iteration 700, loss 1.7886462211608887
iteration 800, loss 1.8270366191864014
iteration 0, loss 1.9061707258224487
iteration 100, loss 1.7768625020980835
iteration 200, loss 1.8435558080673218
iteration 300, loss 1.8983912467956543
iteration 400, loss 1.9048041105270386
iteration 500, loss 1.780017614364624
iteration 600, loss 1.8132022619247437
iteration 700, loss 1.8040980100631714
iteration 800, loss 1.7953298091888428
iteration 0, loss 1.8392657041549683
iteration 100, loss 1.7965272665023804
iteration 200, loss 1.8749572038650513
iteration 300, loss 1.8578336238861084
iteration 400, loss 1.7889314889907837
iteration 500, loss 1.8761323690414429
iteration 600, loss 1.7329152822494507
iteration 700, loss 1.8212226629257202
iteration 800, loss 1.7853333950042725
iteration 0, loss 1.7894914150238037
iteration 100, loss 1.8031103610992432
iteration 200, loss 1.8092730045318604
iteration 300, loss 1.8339850902557373
iteration 400, loss 1.8917083740234375
iteration 500, loss 1.810044288635254
iteration 600, loss 1.7970352172851562
iteration 700, loss 1.8069894313812256
iteration 800, loss 1.732090711593628
iteration 0, loss 1.8116055727005005
iteration 100, loss 1.933076024055481
iteration 200, loss 1.799872636795044
iteration 300, loss 1.8129470348358154
iteration 400, loss 1.804299235343933
iteration 500, loss 1.8682528734207153
iteration 600, loss 1.8434237241744995
iteration 700, loss 1.8231968879699707
iteration 800, loss 1.7959587574005127
iteration 0, loss 1.802374005317688
iteration 100, loss 1.8515197038650513
iteration 200, loss 1.843837022781372
iteration 300, loss 1.9215378761291504
iteration 400, loss 1.8089137077331543
iteration 500, loss 1.7921816110610962
iteration 600, loss 1.7801315784454346
iteration 700, loss 1.8307653665542603
iteration 800, loss 1.7876079082489014
iteration 0, loss 1.852015495300293
iteration 100, loss 1.8927656412124634
iteration 200, loss 1.8692744970321655
iteration 300, loss 1.8348047733306885
iteration 400, loss 1.8666101694107056
iteration 500, loss 1.8201475143432617
iteration 600, loss 1.7954944372177124
iteration 700, loss 1.8317506313323975
iteration 800, loss 1.9160339832305908
iteration 0, loss 1.7675998210906982
iteration 100, loss 1.7591464519500732
iteration 200, loss 1.8198316097259521
iteration 300, loss 1.8318297863006592
iteration 400, loss 1.8259869813919067
iteration 500, loss 1.8312039375305176
iteration 600, loss 1.7697622776031494
iteration 700, loss 1.8865423202514648
iteration 800, loss 1.758617639541626
iteration 0, loss 1.8036532402038574
iteration 100, loss 1.772735834121704
iteration 200, loss 1.8713858127593994
iteration 300, loss 1.8484036922454834
iteration 400, loss 1.7842442989349365
iteration 500, loss 1.8590967655181885
iteration 600, loss 1.8329814672470093
iteration 700, loss 1.9137784242630005
iteration 800, loss 1.8119285106658936
iteration 0, loss 1.8237378597259521
iteration 100, loss 1.882726788520813
iteration 200, loss 1.799363136291504
iteration 300, loss 1.84572172164917
iteration 400, loss 1.8100104331970215
iteration 500, loss 1.8205218315124512
iteration 600, loss 1.8303232192993164
iteration 700, loss 1.8230845928192139
iteration 800, loss 1.8464828729629517
iteration 0, loss 1.7618701457977295
iteration 100, loss 1.920797348022461
iteration 200, loss 1.7951464653015137
iteration 300, loss 1.8906351327896118
iteration 400, loss 1.8840316534042358
iteration 500, loss 1.8361893892288208
iteration 600, loss 1.7962355613708496
iteration 700, loss 1.8380612134933472
iteration 800, loss 1.7485992908477783
iteration 0, loss 1.864978313446045
iteration 100, loss 1.893617033958435
iteration 200, loss 1.7644882202148438
iteration 300, loss 1.7930963039398193
iteration 400, loss 1.774247407913208
iteration 500, loss 1.8941208124160767
iteration 600, loss 1.740932583808899
iteration 700, loss 1.8510760068893433
iteration 800, loss 1.8389445543289185
iteration 0, loss 1.7679752111434937
iteration 100, loss 1.8223156929016113
iteration 200, loss 1.8038630485534668
iteration 300, loss 1.8899521827697754
iteration 400, loss 1.8370201587677002
iteration 500, loss 1.8705602884292603
iteration 600, loss 1.8372728824615479
iteration 700, loss 1.7825878858566284
iteration 800, loss 1.8342227935791016
iteration 0, loss 1.7649482488632202
iteration 100, loss 1.8329980373382568
iteration 200, loss 1.7987613677978516
iteration 300, loss 1.7988836765289307
iteration 400, loss 1.7920523881912231
iteration 500, loss 1.8259336948394775
iteration 600, loss 1.8457108736038208
iteration 700, loss 1.7539610862731934
iteration 800, loss 1.7635600566864014
iteration 0, loss 1.7747340202331543
iteration 100, loss 1.8445322513580322
iteration 200, loss 1.8273950815200806
iteration 300, loss 1.8088057041168213
iteration 400, loss 1.8090264797210693
iteration 500, loss 1.8460955619812012
iteration 600, loss 1.842634916305542
iteration 700, loss 1.849452018737793
iteration 800, loss 1.801095962524414
iteration 0, loss 1.781090259552002
iteration 100, loss 1.8150510787963867
iteration 200, loss 1.8101513385772705
iteration 300, loss 1.79017972946167
iteration 400, loss 1.800829529762268
iteration 500, loss 1.7617050409317017
iteration 600, loss 1.807502031326294
iteration 700, loss 1.7956959009170532
iteration 800, loss 1.897979974746704
iteration 0, loss 1.8310635089874268
iteration 100, loss 1.8325772285461426
iteration 200, loss 1.803209662437439
iteration 300, loss 1.8278467655181885
iteration 400, loss 1.8087117671966553
iteration 500, loss 1.8197474479675293
iteration 600, loss 1.8190910816192627
iteration 700, loss 1.8412240743637085
iteration 800, loss 1.8433356285095215
iteration 0, loss 1.853516936302185
iteration 100, loss 1.853277325630188
iteration 200, loss 1.9305312633514404
iteration 300, loss 1.8338414430618286
iteration 400, loss 1.7853200435638428
iteration 500, loss 1.7819169759750366
iteration 600, loss 1.77659273147583
iteration 700, loss 1.8394359350204468
iteration 800, loss 1.8979886770248413
iteration 0, loss 1.8241462707519531
iteration 100, loss 1.786598563194275
iteration 200, loss 1.84036386013031
iteration 300, loss 1.7948226928710938
iteration 400, loss 1.8080393075942993
iteration 500, loss 1.756853699684143
iteration 600, loss 1.8440402746200562
iteration 700, loss 1.8904510736465454
iteration 800, loss 1.7930386066436768
iteration 0, loss 1.803817868232727
iteration 100, loss 1.8437743186950684
iteration 200, loss 1.851895809173584
iteration 300, loss 1.8230160474777222
iteration 400, loss 1.7795741558074951
iteration 500, loss 1.8192691802978516
iteration 600, loss 1.740678071975708
iteration 700, loss 1.9123773574829102
iteration 800, loss 1.8229949474334717
iteration 0, loss 1.793285608291626
iteration 100, loss 1.827430009841919
iteration 200, loss 1.7785234451293945
iteration 300, loss 1.866837978363037
iteration 400, loss 1.7986819744110107
iteration 500, loss 1.8039534091949463
iteration 600, loss 1.8776953220367432
iteration 700, loss 1.80649733543396
iteration 800, loss 1.8800129890441895
iteration 0, loss 1.786478042602539
iteration 100, loss 1.7893575429916382
iteration 200, loss 1.7991310358047485
iteration 300, loss 1.8046576976776123
iteration 400, loss 1.762362003326416
iteration 500, loss 1.7484670877456665
iteration 600, loss 1.8536171913146973
iteration 700, loss 1.8364648818969727
iteration 800, loss 1.8515526056289673
iteration 0, loss 1.793253779411316
iteration 100, loss 1.80398690700531
iteration 200, loss 1.797296404838562
iteration 300, loss 1.7865713834762573
iteration 400, loss 1.7468178272247314
iteration 500, loss 1.8039060831069946
iteration 600, loss 1.7799259424209595
iteration 700, loss 1.8143571615219116
iteration 800, loss 1.7763397693634033
iteration 0, loss 1.783135175704956
iteration 100, loss 1.8103917837142944
iteration 200, loss 1.8038758039474487
iteration 300, loss 1.8571914434432983
iteration 400, loss 1.7737013101577759
iteration 500, loss 1.8510602712631226
iteration 600, loss 1.8033831119537354
iteration 700, loss 1.84638512134552
iteration 800, loss 1.7986193895339966
iteration 0, loss 1.7153128385543823
iteration 100, loss 1.8647024631500244
iteration 200, loss 1.7555485963821411
iteration 300, loss 1.8799201250076294
iteration 400, loss 1.7848998308181763
iteration 500, loss 1.7290215492248535
iteration 600, loss 1.811697244644165
iteration 700, loss 1.7763296365737915
iteration 800, loss 1.7814762592315674
iteration 0, loss 1.80779230594635
iteration 100, loss 1.8468750715255737
iteration 200, loss 1.7590913772583008
iteration 300, loss 1.811098337173462
iteration 400, loss 1.8374738693237305
iteration 500, loss 1.7643163204193115
iteration 600, loss 1.7928165197372437
iteration 700, loss 1.7684199810028076
iteration 800, loss 1.8705869913101196
fold 1 accuracy: 0.477
iteration 0, loss 1.8334861993789673
iteration 100, loss 1.7737830877304077
iteration 200, loss 1.78902268409729
iteration 300, loss 1.7337026596069336
iteration 400, loss 1.8031516075134277
iteration 500, loss 1.821242332458496
iteration 600, loss 1.9172687530517578
iteration 700, loss 1.8343087434768677
iteration 800, loss 1.8550279140472412
iteration 0, loss 1.7917317152023315
iteration 100, loss 1.7881271839141846
iteration 200, loss 1.8352792263031006
iteration 300, loss 1.8949220180511475
iteration 400, loss 1.798085331916809
iteration 500, loss 1.8652470111846924
iteration 600, loss 1.8320221900939941
iteration 700, loss 1.8133069276809692
iteration 800, loss 1.780663013458252
iteration 0, loss 1.8299140930175781
iteration 100, loss 1.8661062717437744
iteration 200, loss 1.843515157699585
iteration 300, loss 1.8092845678329468
iteration 400, loss 1.8033194541931152
iteration 500, loss 1.7919049263000488
iteration 600, loss 1.9639503955841064
iteration 700, loss 1.8423254489898682
iteration 800, loss 1.772358775138855
iteration 0, loss 1.753163456916809
iteration 100, loss 1.8116470575332642
iteration 200, loss 1.821958065032959
iteration 300, loss 1.815733790397644
iteration 400, loss 1.9132506847381592
iteration 500, loss 1.8596129417419434
iteration 600, loss 1.8634872436523438
iteration 700, loss 1.7986810207366943
iteration 800, loss 1.8092107772827148
iteration 0, loss 1.7908897399902344
iteration 100, loss 1.7901279926300049
iteration 200, loss 1.8472509384155273
iteration 300, loss 1.8452121019363403
iteration 400, loss 1.8050051927566528
iteration 500, loss 1.7524040937423706
iteration 600, loss 1.7701982259750366
iteration 700, loss 1.8589338064193726
iteration 800, loss 1.8674904108047485
iteration 0, loss 1.8113199472427368
iteration 100, loss 1.8961997032165527
iteration 200, loss 1.7877775430679321
iteration 300, loss 1.7577428817749023
iteration 400, loss 1.782288670539856
iteration 500, loss 1.7855952978134155
iteration 600, loss 1.8537397384643555
iteration 700, loss 1.8255904912948608
iteration 800, loss 1.8000788688659668
iteration 0, loss 1.8243680000305176
iteration 100, loss 1.8654474020004272
iteration 200, loss 1.7897506952285767
iteration 300, loss 1.7415497303009033
iteration 400, loss 1.8623777627944946
iteration 500, loss 1.8265020847320557
iteration 600, loss 1.776737928390503
iteration 700, loss 1.8157923221588135
iteration 800, loss 1.7694209814071655
iteration 0, loss 1.7675632238388062
iteration 100, loss 1.7096774578094482
iteration 200, loss 1.8491334915161133
iteration 300, loss 1.8585715293884277
iteration 400, loss 1.834053874015808
iteration 500, loss 1.8549116849899292
iteration 600, loss 1.8613338470458984
iteration 700, loss 1.7818975448608398
iteration 800, loss 1.781018853187561
iteration 0, loss 1.8871735334396362
iteration 100, loss 1.831587791442871
iteration 200, loss 1.7943443059921265
iteration 300, loss 1.8803938627243042
iteration 400, loss 1.802331805229187
iteration 500, loss 1.7305855751037598
iteration 600, loss 1.8586395978927612
iteration 700, loss 1.8493883609771729
iteration 800, loss 1.9335827827453613
iteration 0, loss 1.8158423900604248
iteration 100, loss 1.816480040550232
iteration 200, loss 1.8553200960159302
iteration 300, loss 1.8299002647399902
iteration 400, loss 1.8904086351394653
iteration 500, loss 1.8307287693023682
iteration 600, loss 1.7893941402435303
iteration 700, loss 1.802753210067749
iteration 800, loss 1.7792984247207642
iteration 0, loss 1.8526908159255981
iteration 100, loss 1.8814444541931152
iteration 200, loss 1.795738935470581
iteration 300, loss 1.7569143772125244
iteration 400, loss 1.8613547086715698
iteration 500, loss 1.8382097482681274
iteration 600, loss 1.8104981184005737
iteration 700, loss 1.8080061674118042
iteration 800, loss 1.8043500185012817
iteration 0, loss 1.8259166479110718
iteration 100, loss 1.759303331375122
iteration 200, loss 1.8340528011322021
iteration 300, loss 1.7811293601989746
iteration 400, loss 1.8385047912597656
iteration 500, loss 1.785140037536621
iteration 600, loss 1.822272539138794
iteration 700, loss 1.930713415145874
iteration 800, loss 1.8020159006118774
iteration 0, loss 1.8497369289398193
iteration 100, loss 1.8783167600631714
iteration 200, loss 1.8556711673736572
iteration 300, loss 1.8372209072113037
iteration 400, loss 1.7798118591308594
iteration 500, loss 1.7258223295211792
iteration 600, loss 1.8478517532348633
iteration 700, loss 1.7898826599121094
iteration 800, loss 1.795515537261963
iteration 0, loss 1.8029546737670898
iteration 100, loss 1.8153777122497559
iteration 200, loss 1.8659738302230835
iteration 300, loss 1.764028549194336
iteration 400, loss 1.847136378288269
iteration 500, loss 1.7736239433288574
iteration 600, loss 1.86888587474823
iteration 700, loss 1.8372268676757812
iteration 800, loss 1.8240500688552856
iteration 0, loss 1.8569893836975098
iteration 100, loss 1.8606001138687134
iteration 200, loss 1.8104712963104248
iteration 300, loss 1.7829018831253052
iteration 400, loss 1.7279433012008667
iteration 500, loss 1.8238849639892578
iteration 600, loss 1.8276872634887695
iteration 700, loss 1.7863188982009888
iteration 800, loss 1.8406908512115479
iteration 0, loss 1.7815907001495361
iteration 100, loss 1.842491626739502
iteration 200, loss 1.881296157836914
iteration 300, loss 1.8397095203399658
iteration 400, loss 1.7715510129928589
iteration 500, loss 1.7319917678833008
iteration 600, loss 1.782745361328125
iteration 700, loss 1.8181637525558472
iteration 800, loss 1.7533849477767944
iteration 0, loss 1.7930225133895874
iteration 100, loss 1.7994853258132935
iteration 200, loss 1.8344645500183105
iteration 300, loss 1.7783058881759644
iteration 400, loss 1.8174299001693726
iteration 500, loss 1.918587327003479
iteration 600, loss 1.8657561540603638
iteration 700, loss 1.8197544813156128
iteration 800, loss 1.8108011484146118
iteration 0, loss 1.8525097370147705
iteration 100, loss 1.8725696802139282
iteration 200, loss 1.8215709924697876
iteration 300, loss 1.7864770889282227
iteration 400, loss 1.8754360675811768
iteration 500, loss 1.8489681482315063
iteration 600, loss 1.747129201889038
iteration 700, loss 1.785763144493103
iteration 800, loss 1.7789641618728638
iteration 0, loss 1.8135335445404053
iteration 100, loss 1.77176833152771
iteration 200, loss 1.781577229499817
iteration 300, loss 1.837141752243042
iteration 400, loss 1.828066349029541
iteration 500, loss 1.7675451040267944
iteration 600, loss 1.792221188545227
iteration 700, loss 1.8794121742248535
iteration 800, loss 1.7635313272476196
iteration 0, loss 1.7858965396881104
iteration 100, loss 1.770637035369873
iteration 200, loss 1.9017255306243896
iteration 300, loss 1.8827214241027832
iteration 400, loss 1.8320907354354858
iteration 500, loss 1.8788516521453857
iteration 600, loss 1.8063567876815796
iteration 700, loss 1.8560737371444702
iteration 800, loss 1.7842313051223755
iteration 0, loss 1.846526026725769
iteration 100, loss 1.8894116878509521
iteration 200, loss 1.8344621658325195
iteration 300, loss 1.7945269346237183
iteration 400, loss 1.7951420545578003
iteration 500, loss 1.7926682233810425
iteration 600, loss 1.8562113046646118
iteration 700, loss 1.8316433429718018
iteration 800, loss 1.7303369045257568
iteration 0, loss 1.8211488723754883
iteration 100, loss 1.7921738624572754
iteration 200, loss 1.8688591718673706
iteration 300, loss 1.78541898727417
iteration 400, loss 1.747698426246643
iteration 500, loss 1.7770309448242188
iteration 600, loss 1.8276013135910034
iteration 700, loss 1.8224985599517822
iteration 800, loss 1.802614688873291
iteration 0, loss 1.8480576276779175
iteration 100, loss 1.7901883125305176
iteration 200, loss 1.8254704475402832
iteration 300, loss 1.799699306488037
iteration 400, loss 1.7689462900161743
iteration 500, loss 1.7681188583374023
iteration 600, loss 1.8378937244415283
iteration 700, loss 1.7816345691680908
iteration 800, loss 1.8956421613693237
iteration 0, loss 1.79059636592865
iteration 100, loss 1.7964746952056885
iteration 200, loss 1.8694281578063965
iteration 300, loss 1.7736268043518066
iteration 400, loss 1.8115088939666748
iteration 500, loss 1.7835049629211426
iteration 600, loss 1.8131319284439087
iteration 700, loss 1.857884407043457
iteration 800, loss 1.8562310934066772
iteration 0, loss 1.7713958024978638
iteration 100, loss 1.8369837999343872
iteration 200, loss 1.859104871749878
iteration 300, loss 1.8205162286758423
iteration 400, loss 1.8816920518875122
iteration 500, loss 1.764002799987793
iteration 600, loss 1.882411003112793
iteration 700, loss 1.7732161283493042
iteration 800, loss 1.8264853954315186
iteration 0, loss 1.8233957290649414
iteration 100, loss 1.8409055471420288
iteration 200, loss 1.823740839958191
iteration 300, loss 1.8386807441711426
iteration 400, loss 1.779608130455017
iteration 500, loss 1.9178894758224487
iteration 600, loss 1.8172188997268677
iteration 700, loss 1.7417070865631104
iteration 800, loss 1.847192645072937
iteration 0, loss 1.8611023426055908
iteration 100, loss 1.7915328741073608
iteration 200, loss 1.8586995601654053
iteration 300, loss 1.7923808097839355
iteration 400, loss 1.7545173168182373
iteration 500, loss 1.782230257987976
iteration 600, loss 1.876110553741455
iteration 700, loss 1.8028476238250732
iteration 800, loss 1.8451504707336426
iteration 0, loss 1.8054914474487305
iteration 100, loss 1.7989370822906494
iteration 200, loss 1.7658944129943848
iteration 300, loss 1.7784720659255981
iteration 400, loss 1.7578669786453247
iteration 500, loss 1.78550386428833
iteration 600, loss 1.776100993156433
iteration 700, loss 1.793290615081787
iteration 800, loss 1.804182529449463
iteration 0, loss 1.8270148038864136
iteration 100, loss 1.8490148782730103
iteration 200, loss 1.8652613162994385
iteration 300, loss 1.8042430877685547
iteration 400, loss 1.8536149263381958
iteration 500, loss 1.7722715139389038
iteration 600, loss 1.9342585802078247
iteration 700, loss 1.8721694946289062
iteration 800, loss 1.8188868761062622
iteration 0, loss 1.823610782623291
iteration 100, loss 1.8244502544403076
iteration 200, loss 1.8528183698654175
iteration 300, loss 1.8714642524719238
iteration 400, loss 1.8104493618011475
iteration 500, loss 1.808363437652588
iteration 600, loss 1.8190371990203857
iteration 700, loss 1.7968716621398926
iteration 800, loss 1.8055428266525269
iteration 0, loss 1.7513552904129028
iteration 100, loss 1.8433983325958252
iteration 200, loss 1.80937922000885
iteration 300, loss 1.820552110671997
iteration 400, loss 1.7690643072128296
iteration 500, loss 1.800904393196106
iteration 600, loss 1.784319519996643
iteration 700, loss 1.8981317281723022
iteration 800, loss 1.7200716733932495
iteration 0, loss 1.8045308589935303
iteration 100, loss 1.807024598121643
iteration 200, loss 1.8453853130340576
iteration 300, loss 1.788862943649292
iteration 400, loss 1.781545877456665
iteration 500, loss 1.8618004322052002
iteration 600, loss 1.857460379600525
iteration 700, loss 1.7965781688690186
iteration 800, loss 1.8078091144561768
iteration 0, loss 1.7999187707901
iteration 100, loss 1.8919317722320557
iteration 200, loss 1.8305118083953857
iteration 300, loss 1.8342580795288086
iteration 400, loss 1.7638545036315918
iteration 500, loss 1.7696322202682495
iteration 600, loss 1.7657809257507324
iteration 700, loss 1.8182988166809082
iteration 800, loss 1.8486698865890503
iteration 0, loss 1.7978543043136597
iteration 100, loss 1.8801177740097046
iteration 200, loss 1.789397954940796
iteration 300, loss 1.7779057025909424
iteration 400, loss 1.81007719039917
iteration 500, loss 1.832822322845459
iteration 600, loss 1.8435578346252441
iteration 700, loss 1.8573784828186035
iteration 800, loss 1.845308542251587
iteration 0, loss 1.8499395847320557
iteration 100, loss 1.7532837390899658
iteration 200, loss 1.8899192810058594
iteration 300, loss 1.8356605768203735
iteration 400, loss 1.8041599988937378
iteration 500, loss 1.8296958208084106
iteration 600, loss 1.7875093221664429
iteration 700, loss 1.8147056102752686
iteration 800, loss 1.791982650756836
iteration 0, loss 1.8302477598190308
iteration 100, loss 1.9042428731918335
iteration 200, loss 1.7831659317016602
iteration 300, loss 1.9043731689453125
iteration 400, loss 1.8485020399093628
iteration 500, loss 1.8293545246124268
iteration 600, loss 1.892530918121338
iteration 700, loss 1.7906494140625
iteration 800, loss 1.7886409759521484
iteration 0, loss 1.8222962617874146
iteration 100, loss 1.771157145500183
iteration 200, loss 1.7822192907333374
iteration 300, loss 1.8931093215942383
iteration 400, loss 1.8039746284484863
iteration 500, loss 1.7468235492706299
iteration 600, loss 1.8136054277420044
iteration 700, loss 1.865541696548462
iteration 800, loss 1.8547183275222778
iteration 0, loss 1.817757487297058
iteration 100, loss 1.7836401462554932
iteration 200, loss 1.796656608581543
iteration 300, loss 1.9190292358398438
iteration 400, loss 1.8292841911315918
iteration 500, loss 1.7840958833694458
iteration 600, loss 1.8005998134613037
iteration 700, loss 1.8076598644256592
iteration 800, loss 1.8295940160751343
iteration 0, loss 1.8112759590148926
iteration 100, loss 1.8045017719268799
iteration 200, loss 1.8153698444366455
iteration 300, loss 1.8072513341903687
iteration 400, loss 1.839967966079712
iteration 500, loss 1.8549249172210693
iteration 600, loss 1.7795381546020508
iteration 700, loss 1.7836376428604126
iteration 800, loss 1.7901662588119507
iteration 0, loss 1.785906434059143
iteration 100, loss 1.8484588861465454
iteration 200, loss 1.818674921989441
iteration 300, loss 1.816227912902832
iteration 400, loss 1.7791416645050049
iteration 500, loss 1.8143129348754883
iteration 600, loss 1.8377856016159058
iteration 700, loss 1.8139876127243042
iteration 800, loss 1.8144522905349731
iteration 0, loss 1.7605592012405396
iteration 100, loss 1.8355140686035156
iteration 200, loss 1.8984371423721313
iteration 300, loss 1.807458758354187
iteration 400, loss 1.8638726472854614
iteration 500, loss 1.793277382850647
iteration 600, loss 1.7365530729293823
iteration 700, loss 1.769172191619873
iteration 800, loss 1.8276959657669067
iteration 0, loss 1.8207013607025146
iteration 100, loss 1.7850449085235596
iteration 200, loss 1.8790335655212402
iteration 300, loss 1.8501249551773071
iteration 400, loss 1.8130841255187988
iteration 500, loss 1.8479235172271729
iteration 600, loss 1.814455270767212
iteration 700, loss 1.7425401210784912
iteration 800, loss 1.797644853591919
iteration 0, loss 1.7955015897750854
iteration 100, loss 1.7948850393295288
iteration 200, loss 1.8009966611862183
iteration 300, loss 1.7338638305664062
iteration 400, loss 1.8112092018127441
iteration 500, loss 1.8121819496154785
iteration 600, loss 1.8350534439086914
iteration 700, loss 1.8155217170715332
iteration 800, loss 1.8017746210098267
iteration 0, loss 1.747022271156311
iteration 100, loss 1.8335795402526855
iteration 200, loss 1.8037762641906738
iteration 300, loss 1.737518548965454
iteration 400, loss 1.8177050352096558
iteration 500, loss 1.846447467803955
iteration 600, loss 1.8116282224655151
iteration 700, loss 1.8339414596557617
iteration 800, loss 1.800904393196106
iteration 0, loss 1.8225587606430054
iteration 100, loss 1.8404757976531982
iteration 200, loss 1.8165103197097778
iteration 300, loss 1.80390465259552
iteration 400, loss 1.807881236076355
iteration 500, loss 1.7364283800125122
iteration 600, loss 1.8397027254104614
iteration 700, loss 1.8313661813735962
iteration 800, loss 1.782631278038025
iteration 0, loss 1.753385305404663
iteration 100, loss 1.795143961906433
iteration 200, loss 1.8817484378814697
iteration 300, loss 1.8108090162277222
iteration 400, loss 1.8208764791488647
iteration 500, loss 1.8387949466705322
iteration 600, loss 1.811398983001709
iteration 700, loss 1.8534650802612305
iteration 800, loss 1.7697436809539795
iteration 0, loss 1.8420720100402832
iteration 100, loss 1.8286665678024292
iteration 200, loss 1.8031578063964844
iteration 300, loss 1.8690563440322876
iteration 400, loss 1.7834141254425049
iteration 500, loss 1.9080086946487427
iteration 600, loss 1.8493858575820923
iteration 700, loss 1.8205374479293823
iteration 800, loss 1.7590320110321045
iteration 0, loss 1.8327722549438477
iteration 100, loss 1.8145480155944824
iteration 200, loss 1.8630807399749756
iteration 300, loss 1.8567005395889282
iteration 400, loss 1.806227684020996
iteration 500, loss 1.8838672637939453
iteration 600, loss 1.804489254951477
iteration 700, loss 1.7843925952911377
iteration 800, loss 1.8560327291488647
iteration 0, loss 1.7689857482910156
iteration 100, loss 1.869648814201355
iteration 200, loss 1.84706711769104
iteration 300, loss 1.824294924736023
iteration 400, loss 1.7424348592758179
iteration 500, loss 1.88225519657135
iteration 600, loss 1.8020710945129395
iteration 700, loss 1.7649426460266113
iteration 800, loss 1.8546829223632812
iteration 0, loss 1.7970317602157593
iteration 100, loss 1.7783863544464111
iteration 200, loss 1.8074886798858643
iteration 300, loss 1.7816756963729858
iteration 400, loss 1.7703133821487427
iteration 500, loss 1.9121962785720825
iteration 600, loss 1.840477705001831
iteration 700, loss 1.844516634941101
iteration 800, loss 1.7687900066375732
fold 2 accuracy: 0.4794285714285714
iteration 0, loss 1.8113468885421753
iteration 100, loss 1.852029800415039
iteration 200, loss 1.852612018585205
iteration 300, loss 1.8981431722640991
iteration 400, loss 1.845880389213562
iteration 500, loss 1.7628004550933838
iteration 600, loss 1.8710371255874634
iteration 700, loss 1.8490480184555054
iteration 800, loss 1.866784691810608
iteration 0, loss 1.889939785003662
iteration 100, loss 1.818222999572754
iteration 200, loss 1.802383303642273
iteration 300, loss 1.8102998733520508
iteration 400, loss 1.9082481861114502
iteration 500, loss 1.846422791481018
iteration 600, loss 1.7284502983093262
iteration 700, loss 1.7468535900115967
iteration 800, loss 1.7574090957641602
iteration 0, loss 1.7788811922073364
iteration 100, loss 1.7592846155166626
iteration 200, loss 1.7200888395309448
iteration 300, loss 1.772528886795044
iteration 400, loss 1.8112552165985107
iteration 500, loss 1.7930265665054321
iteration 600, loss 1.8160521984100342
iteration 700, loss 1.7636165618896484
iteration 800, loss 1.7789043188095093
iteration 0, loss 1.8201627731323242
iteration 100, loss 1.839861512184143
iteration 200, loss 1.7785208225250244
iteration 300, loss 1.8185468912124634
iteration 400, loss 1.819226861000061
iteration 500, loss 1.8437168598175049
iteration 600, loss 1.8894476890563965
iteration 700, loss 1.853930115699768
iteration 800, loss 1.7656822204589844
iteration 0, loss 1.782989501953125
iteration 100, loss 1.8330323696136475
iteration 200, loss 1.846490740776062
iteration 300, loss 1.8138467073440552
iteration 400, loss 1.74037766456604
iteration 500, loss 1.8083709478378296
iteration 600, loss 1.7880592346191406
iteration 700, loss 1.7551368474960327
iteration 800, loss 1.8413931131362915
iteration 0, loss 1.8024401664733887
iteration 100, loss 1.8174951076507568
iteration 200, loss 1.7583013772964478
iteration 300, loss 1.7904613018035889
iteration 400, loss 1.808356761932373
iteration 500, loss 1.861113429069519
iteration 600, loss 1.7705007791519165
iteration 700, loss 1.8174878358840942
iteration 800, loss 1.8502851724624634
iteration 0, loss 1.854910969734192
iteration 100, loss 1.7316573858261108
iteration 200, loss 1.8118834495544434
iteration 300, loss 1.7359896898269653
iteration 400, loss 1.764025092124939
iteration 500, loss 1.8167693614959717
iteration 600, loss 1.752549648284912
iteration 700, loss 1.822574257850647
iteration 800, loss 1.7269690036773682
iteration 0, loss 1.780274748802185
iteration 100, loss 1.77729070186615
iteration 200, loss 1.8072251081466675
iteration 300, loss 1.8519726991653442
iteration 400, loss 1.8046202659606934
iteration 500, loss 1.7806445360183716
iteration 600, loss 1.8149265050888062
iteration 700, loss 1.6780292987823486
iteration 800, loss 1.860772967338562
iteration 0, loss 1.7963945865631104
iteration 100, loss 1.8061919212341309
iteration 200, loss 1.804399847984314
iteration 300, loss 1.8541150093078613
iteration 400, loss 1.7984226942062378
iteration 500, loss 1.814427137374878
iteration 600, loss 1.7573069334030151
iteration 700, loss 1.8543623685836792
iteration 800, loss 1.8177646398544312
iteration 0, loss 1.7866110801696777
iteration 100, loss 1.7821897268295288
iteration 200, loss 1.8202385902404785
iteration 300, loss 1.8757846355438232
iteration 400, loss 1.7387359142303467
iteration 500, loss 1.77570378780365
iteration 600, loss 1.833895206451416
iteration 700, loss 1.8353183269500732
iteration 800, loss 1.8269988298416138
iteration 0, loss 1.7986871004104614
iteration 100, loss 1.7237441539764404
iteration 200, loss 1.8215529918670654
iteration 300, loss 1.8900246620178223
iteration 400, loss 1.7858760356903076
iteration 500, loss 1.766777753829956
iteration 600, loss 1.8473765850067139
iteration 700, loss 1.7353438138961792
iteration 800, loss 1.7983063459396362
iteration 0, loss 1.8221299648284912
iteration 100, loss 1.797739863395691
iteration 200, loss 1.8267163038253784
iteration 300, loss 1.8669978380203247
iteration 400, loss 1.8114745616912842
iteration 500, loss 1.7993600368499756
iteration 600, loss 1.813659429550171
iteration 700, loss 1.8378846645355225
iteration 800, loss 1.8766894340515137
iteration 0, loss 1.8288992643356323
iteration 100, loss 1.867843508720398
iteration 200, loss 1.8222204446792603
iteration 300, loss 1.8397141695022583
iteration 400, loss 1.7816075086593628
iteration 500, loss 1.8213733434677124
iteration 600, loss 1.7467955350875854
iteration 700, loss 1.793157935142517
iteration 800, loss 1.8508373498916626
iteration 0, loss 1.810117483139038
iteration 100, loss 1.8550090789794922
iteration 200, loss 1.6868295669555664
iteration 300, loss 1.7899237871170044
iteration 400, loss 1.8875937461853027
iteration 500, loss 1.7999000549316406
iteration 600, loss 1.8109227418899536
iteration 700, loss 1.8225808143615723
iteration 800, loss 1.908430576324463
iteration 0, loss 1.7706888914108276
iteration 100, loss 1.851436972618103
iteration 200, loss 1.840763807296753
iteration 300, loss 1.8192025423049927
iteration 400, loss 1.812234878540039
iteration 500, loss 1.8709280490875244
iteration 600, loss 1.7977269887924194
iteration 700, loss 1.7469685077667236
iteration 800, loss 1.8252151012420654
iteration 0, loss 1.821864366531372
iteration 100, loss 1.9011954069137573
iteration 200, loss 1.8173532485961914
iteration 300, loss 1.8378973007202148
iteration 400, loss 1.8975633382797241
iteration 500, loss 1.7895978689193726
iteration 600, loss 1.8445019721984863
iteration 700, loss 1.7998766899108887
iteration 800, loss 1.8834766149520874
iteration 0, loss 1.8306620121002197
iteration 100, loss 1.7650851011276245
iteration 200, loss 1.8850131034851074
iteration 300, loss 1.9172756671905518
iteration 400, loss 1.9015132188796997
iteration 500, loss 1.7863295078277588
iteration 600, loss 1.8137084245681763
iteration 700, loss 1.714141607284546
iteration 800, loss 1.8462494611740112
iteration 0, loss 1.8499630689620972
iteration 100, loss 1.8095085620880127
iteration 200, loss 1.7678122520446777
iteration 300, loss 1.8605364561080933
iteration 400, loss 1.812021017074585
iteration 500, loss 1.833765983581543
iteration 600, loss 1.813692331314087
iteration 700, loss 1.8069581985473633
iteration 800, loss 1.7744497060775757
iteration 0, loss 1.7829179763793945
iteration 100, loss 1.874942421913147
iteration 200, loss 1.7899134159088135
iteration 300, loss 1.8167802095413208
iteration 400, loss 1.797533392906189
iteration 500, loss 1.7790237665176392
iteration 600, loss 1.785556674003601
iteration 700, loss 1.838871955871582
iteration 800, loss 1.7850507497787476
iteration 0, loss 1.782515525817871
iteration 100, loss 1.8615087270736694
iteration 200, loss 1.7985857725143433
iteration 300, loss 1.811151146888733
iteration 400, loss 1.7449932098388672
iteration 500, loss 1.7652325630187988
iteration 600, loss 1.8375591039657593
iteration 700, loss 1.7829835414886475
iteration 800, loss 1.8253777027130127
iteration 0, loss 1.7933741807937622
iteration 100, loss 1.74335777759552
iteration 200, loss 1.7745015621185303
iteration 300, loss 1.7463879585266113
iteration 400, loss 1.8747750520706177
iteration 500, loss 1.9057989120483398
iteration 600, loss 1.7802966833114624
iteration 700, loss 1.7953561544418335
iteration 800, loss 1.773006796836853
iteration 0, loss 1.8841689825057983
iteration 100, loss 1.7846522331237793
iteration 200, loss 1.8591792583465576
iteration 300, loss 1.7941946983337402
iteration 400, loss 1.8601888418197632
iteration 500, loss 1.8464535474777222
iteration 600, loss 1.7832387685775757
iteration 700, loss 1.7308794260025024
iteration 800, loss 1.8577003479003906
iteration 0, loss 1.844717264175415
iteration 100, loss 1.7812767028808594
iteration 200, loss 1.805008053779602
iteration 300, loss 1.7917596101760864
iteration 400, loss 1.8363378047943115
iteration 500, loss 1.8963773250579834
iteration 600, loss 1.8409630060195923
iteration 700, loss 1.788006067276001
iteration 800, loss 1.7430790662765503
iteration 0, loss 1.8447518348693848
iteration 100, loss 1.8149030208587646
iteration 200, loss 1.8269728422164917
iteration 300, loss 1.6999671459197998
iteration 400, loss 1.8344093561172485
iteration 500, loss 1.8665454387664795
iteration 600, loss 1.8629889488220215
iteration 700, loss 1.7891545295715332
iteration 800, loss 1.7791603803634644
iteration 0, loss 1.83124577999115
iteration 100, loss 1.7939927577972412
iteration 200, loss 1.7565569877624512
iteration 300, loss 1.791210412979126
iteration 400, loss 1.8204416036605835
iteration 500, loss 1.8494693040847778
iteration 600, loss 1.8599623441696167
iteration 700, loss 1.8384079933166504
iteration 800, loss 1.8098620176315308
iteration 0, loss 1.8043158054351807
iteration 100, loss 1.7657983303070068
iteration 200, loss 1.8042453527450562
iteration 300, loss 1.9781557321548462
iteration 400, loss 1.7922581434249878
iteration 500, loss 1.7955963611602783
iteration 600, loss 1.7585393190383911
iteration 700, loss 1.8365118503570557
iteration 800, loss 1.8373165130615234
iteration 0, loss 1.726499319076538
iteration 100, loss 1.860824465751648
iteration 200, loss 1.7696932554244995
iteration 300, loss 1.823905348777771
iteration 400, loss 1.8563071489334106
iteration 500, loss 1.8069583177566528
iteration 600, loss 1.8230431079864502
iteration 700, loss 1.8475489616394043
iteration 800, loss 1.8507214784622192
iteration 0, loss 1.7761372327804565
iteration 100, loss 1.7991939783096313
iteration 200, loss 1.9078246355056763
iteration 300, loss 1.831383466720581
iteration 400, loss 1.795419454574585
iteration 500, loss 1.743909478187561
iteration 600, loss 1.7768802642822266
iteration 700, loss 1.8617737293243408
iteration 800, loss 1.7728649377822876
iteration 0, loss 1.7941056489944458
iteration 100, loss 1.8259899616241455
iteration 200, loss 1.8451968431472778
iteration 300, loss 1.800993800163269
iteration 400, loss 1.7465416193008423
iteration 500, loss 1.7409992218017578
iteration 600, loss 1.7658320665359497
iteration 700, loss 1.8011882305145264
iteration 800, loss 1.7711807489395142
iteration 0, loss 1.8654861450195312
iteration 100, loss 1.8161046504974365
iteration 200, loss 1.8241318464279175
iteration 300, loss 1.7884554862976074
iteration 400, loss 1.8344330787658691
iteration 500, loss 1.805458903312683
iteration 600, loss 1.7841750383377075
iteration 700, loss 1.7784183025360107
iteration 800, loss 1.833534598350525
iteration 0, loss 1.85767662525177
iteration 100, loss 1.8005807399749756
iteration 200, loss 1.9011191129684448
iteration 300, loss 1.8301845788955688
iteration 400, loss 1.7651686668395996
iteration 500, loss 1.8356026411056519
iteration 600, loss 1.755767583847046
iteration 700, loss 1.75993013381958
iteration 800, loss 1.7408478260040283
iteration 0, loss 1.8138631582260132
iteration 100, loss 1.7518197298049927
iteration 200, loss 1.7391088008880615
iteration 300, loss 1.8487807512283325
iteration 400, loss 1.7940809726715088
iteration 500, loss 1.8404024839401245
iteration 600, loss 1.8001489639282227
iteration 700, loss 1.8090221881866455
iteration 800, loss 1.7893317937850952
iteration 0, loss 1.8959604501724243
iteration 100, loss 1.8285057544708252
iteration 200, loss 1.8303797245025635
iteration 300, loss 1.9104382991790771
iteration 400, loss 1.8509858846664429
iteration 500, loss 1.8325165510177612
iteration 600, loss 1.8446543216705322
iteration 700, loss 1.8502120971679688
iteration 800, loss 1.8266005516052246
iteration 0, loss 1.75179123878479
iteration 100, loss 1.7231632471084595
iteration 200, loss 1.8154337406158447
iteration 300, loss 1.815064549446106
iteration 400, loss 1.8799517154693604
iteration 500, loss 1.7972180843353271
iteration 600, loss 1.7738980054855347
iteration 700, loss 1.8617950677871704
iteration 800, loss 1.8513737916946411
iteration 0, loss 1.8059104681015015
iteration 100, loss 1.8041789531707764
iteration 200, loss 1.8086243867874146
iteration 300, loss 1.775599479675293
iteration 400, loss 1.7777776718139648
iteration 500, loss 1.7998021841049194
iteration 600, loss 1.7984644174575806
iteration 700, loss 1.8496686220169067
iteration 800, loss 1.7490125894546509
iteration 0, loss 1.733394742012024
iteration 100, loss 1.7953753471374512
iteration 200, loss 1.791785717010498
iteration 300, loss 1.8302674293518066
iteration 400, loss 1.7927383184432983
iteration 500, loss 1.7897459268569946
iteration 600, loss 1.8067833185195923
iteration 700, loss 1.8537944555282593
iteration 800, loss 1.8195075988769531
iteration 0, loss 1.8164572715759277
iteration 100, loss 1.8485769033432007
iteration 200, loss 1.829493522644043
iteration 300, loss 1.828978419303894
iteration 400, loss 1.8155605792999268
iteration 500, loss 1.8517600297927856
iteration 600, loss 1.8302485942840576
iteration 700, loss 1.8509657382965088
iteration 800, loss 1.7921010255813599
iteration 0, loss 1.7991795539855957
iteration 100, loss 1.8263871669769287
iteration 200, loss 1.7669037580490112
iteration 300, loss 1.8457441329956055
iteration 400, loss 1.8056292533874512
iteration 500, loss 1.8507142066955566
iteration 600, loss 1.7789322137832642
iteration 700, loss 1.7749404907226562
iteration 800, loss 1.8690823316574097
iteration 0, loss 1.8637089729309082
iteration 100, loss 1.8134061098098755
iteration 200, loss 1.8364964723587036
iteration 300, loss 1.7215481996536255
iteration 400, loss 1.7478710412979126
iteration 500, loss 1.7702349424362183
iteration 600, loss 1.7737529277801514
iteration 700, loss 1.8689649105072021
iteration 800, loss 1.785082459449768
iteration 0, loss 1.892507791519165
iteration 100, loss 1.823277473449707
iteration 200, loss 1.8355138301849365
iteration 300, loss 1.753579020500183
iteration 400, loss 1.8712377548217773
iteration 500, loss 1.8525986671447754
iteration 600, loss 1.8054273128509521
iteration 700, loss 1.8753445148468018
iteration 800, loss 1.7824615240097046
iteration 0, loss 1.7671133279800415
iteration 100, loss 1.7464325428009033
iteration 200, loss 1.7691031694412231
iteration 300, loss 1.7915475368499756
iteration 400, loss 1.7962603569030762
iteration 500, loss 1.7917879819869995
iteration 600, loss 1.858566164970398
iteration 700, loss 1.7888104915618896
iteration 800, loss 1.854500651359558
iteration 0, loss 1.8569227457046509
iteration 100, loss 1.8805160522460938
iteration 200, loss 1.8720619678497314
iteration 300, loss 1.7821046113967896
iteration 400, loss 1.772429347038269
iteration 500, loss 1.8491129875183105
iteration 600, loss 1.8812335729599
iteration 700, loss 1.8468319177627563
iteration 800, loss 1.8232203722000122
iteration 0, loss 1.825321912765503
iteration 100, loss 1.8667668104171753
iteration 200, loss 1.7974586486816406
iteration 300, loss 1.8374425172805786
iteration 400, loss 1.8041960000991821
iteration 500, loss 1.7764064073562622
iteration 600, loss 1.7409594058990479
iteration 700, loss 1.806610345840454
iteration 800, loss 1.8207266330718994
iteration 0, loss 1.8662750720977783
iteration 100, loss 1.8348921537399292
iteration 200, loss 1.7854440212249756
iteration 300, loss 1.7515923976898193
iteration 400, loss 1.8573431968688965
iteration 500, loss 1.8331395387649536
iteration 600, loss 1.7621876001358032
iteration 700, loss 1.7721309661865234
iteration 800, loss 1.850359320640564
iteration 0, loss 1.8734180927276611
iteration 100, loss 1.8087806701660156
iteration 200, loss 1.8140840530395508
iteration 300, loss 1.7885628938674927
iteration 400, loss 1.8155128955841064
iteration 500, loss 1.8345040082931519
iteration 600, loss 1.8092281818389893
iteration 700, loss 1.8742059469223022
iteration 800, loss 1.7743152379989624
iteration 0, loss 1.8130062818527222
iteration 100, loss 1.7653998136520386
iteration 200, loss 1.8517924547195435
iteration 300, loss 1.8562504053115845
iteration 400, loss 1.8566769361495972
iteration 500, loss 1.8285205364227295
iteration 600, loss 1.778254508972168
iteration 700, loss 1.797988772392273
iteration 800, loss 1.8667237758636475
iteration 0, loss 1.808509349822998
iteration 100, loss 1.824833869934082
iteration 200, loss 1.7691115140914917
iteration 300, loss 1.895734190940857
iteration 400, loss 1.7515016794204712
iteration 500, loss 1.7976139783859253
iteration 600, loss 1.8026765584945679
iteration 700, loss 1.8754401206970215
iteration 800, loss 1.7909574508666992
iteration 0, loss 1.869770884513855
iteration 100, loss 1.788296103477478
iteration 200, loss 1.8576390743255615
iteration 300, loss 1.8122626543045044
iteration 400, loss 1.835703730583191
iteration 500, loss 1.8500198125839233
iteration 600, loss 1.7955312728881836
iteration 700, loss 1.7683838605880737
iteration 800, loss 1.935784101486206
iteration 0, loss 1.8473918437957764
iteration 100, loss 1.8622554540634155
iteration 200, loss 1.806312084197998
iteration 300, loss 1.7877357006072998
iteration 400, loss 1.784541130065918
iteration 500, loss 1.828640341758728
iteration 600, loss 1.805702567100525
iteration 700, loss 1.8488165140151978
iteration 800, loss 1.8078911304473877
iteration 0, loss 1.8155186176300049
iteration 100, loss 1.812095284461975
iteration 200, loss 1.7589869499206543
iteration 300, loss 1.8840467929840088
iteration 400, loss 1.8805210590362549
iteration 500, loss 1.7851091623306274
iteration 600, loss 1.7784432172775269
iteration 700, loss 1.72967529296875
iteration 800, loss 1.8541836738586426
fold 3 accuracy: 0.4759285714285714
iteration 0, loss 1.7936521768569946
iteration 100, loss 1.8702175617218018
iteration 200, loss 1.8782142400741577
iteration 300, loss 1.7889491319656372
iteration 400, loss 1.8408102989196777
iteration 500, loss 1.8689637184143066
iteration 600, loss 1.828867793083191
iteration 700, loss 1.8150306940078735
iteration 800, loss 1.7620083093643188
iteration 0, loss 1.7783232927322388
iteration 100, loss 1.8546643257141113
iteration 200, loss 1.7081528902053833
iteration 300, loss 1.8571795225143433
iteration 400, loss 1.904270052909851
iteration 500, loss 1.9276018142700195
iteration 600, loss 1.8280056715011597
iteration 700, loss 1.7677921056747437
iteration 800, loss 1.7210681438446045
iteration 0, loss 1.7902758121490479
iteration 100, loss 1.8101593255996704
iteration 200, loss 1.8143811225891113
iteration 300, loss 1.727344274520874
iteration 400, loss 1.8196159601211548
iteration 500, loss 1.7891290187835693
iteration 600, loss 1.8246791362762451
iteration 700, loss 1.9096213579177856
iteration 800, loss 1.8275035619735718
iteration 0, loss 1.7783756256103516
iteration 100, loss 1.8161492347717285
iteration 200, loss 1.8395707607269287
iteration 300, loss 1.8468513488769531
iteration 400, loss 1.8935856819152832
iteration 500, loss 1.7842997312545776
iteration 600, loss 1.7896995544433594
iteration 700, loss 1.793734073638916
iteration 800, loss 1.724192500114441
iteration 0, loss 1.8264880180358887
iteration 100, loss 1.7195334434509277
iteration 200, loss 1.8118727207183838
iteration 300, loss 1.7974097728729248
iteration 400, loss 1.8729116916656494
iteration 500, loss 1.7537332773208618
iteration 600, loss 1.8520854711532593
iteration 700, loss 1.80581796169281
iteration 800, loss 1.7840224504470825
iteration 0, loss 1.8083560466766357
iteration 100, loss 1.800838589668274
iteration 200, loss 1.8283179998397827
iteration 300, loss 1.783445119857788
iteration 400, loss 1.7874587774276733
iteration 500, loss 1.7739708423614502
iteration 600, loss 1.8563783168792725
iteration 700, loss 1.7776691913604736
iteration 800, loss 1.8864015340805054
iteration 0, loss 1.8195775747299194
iteration 100, loss 1.8280354738235474
iteration 200, loss 1.816044807434082
iteration 300, loss 1.7465497255325317
iteration 400, loss 1.866802453994751
iteration 500, loss 1.8783785104751587
iteration 600, loss 1.8147236108779907
iteration 700, loss 1.8324910402297974
iteration 800, loss 1.8593647480010986
iteration 0, loss 1.7797069549560547
iteration 100, loss 1.842649221420288
iteration 200, loss 1.8264031410217285
iteration 300, loss 1.7632105350494385
iteration 400, loss 1.7930450439453125
iteration 500, loss 1.8104252815246582
iteration 600, loss 1.8052771091461182
iteration 700, loss 1.8121548891067505
iteration 800, loss 1.8390954732894897
iteration 0, loss 1.6850732564926147
iteration 100, loss 1.8418247699737549
iteration 200, loss 1.8118199110031128
iteration 300, loss 1.794170618057251
iteration 400, loss 1.7951620817184448
iteration 500, loss 1.7767051458358765
iteration 600, loss 1.89711332321167
iteration 700, loss 1.794656753540039
iteration 800, loss 1.8514976501464844
iteration 0, loss 1.7604082822799683
iteration 100, loss 1.8239343166351318
iteration 200, loss 1.846951961517334
iteration 300, loss 1.870137095451355
iteration 400, loss 1.8340492248535156
iteration 500, loss 1.7994176149368286
iteration 600, loss 1.7787816524505615
iteration 700, loss 1.8380708694458008
iteration 800, loss 1.8294583559036255
iteration 0, loss 1.7935388088226318
iteration 100, loss 1.8400022983551025
iteration 200, loss 1.8296432495117188
iteration 300, loss 1.836230993270874
iteration 400, loss 1.7675788402557373
iteration 500, loss 1.829861044883728
iteration 600, loss 1.8713326454162598
iteration 700, loss 1.8414945602416992
iteration 800, loss 1.9026896953582764
iteration 0, loss 1.784003734588623
iteration 100, loss 1.7956217527389526
iteration 200, loss 1.7731409072875977
iteration 300, loss 1.8233189582824707
iteration 400, loss 1.7951891422271729
iteration 500, loss 1.8929308652877808
iteration 600, loss 1.7856873273849487
iteration 700, loss 1.767739176750183
iteration 800, loss 1.7932283878326416
iteration 0, loss 1.7838715314865112
iteration 100, loss 1.8147571086883545
iteration 200, loss 1.789259433746338
iteration 300, loss 1.7884689569473267
iteration 400, loss 1.7925008535385132
iteration 500, loss 1.8002796173095703
iteration 600, loss 1.8191046714782715
iteration 700, loss 1.753193974494934
iteration 800, loss 1.8274379968643188
iteration 0, loss 1.801042914390564
iteration 100, loss 1.839718222618103
iteration 200, loss 1.792880892753601
iteration 300, loss 1.7479286193847656
iteration 400, loss 1.8259196281433105
iteration 500, loss 1.8711177110671997
iteration 600, loss 1.769100546836853
iteration 700, loss 1.8387563228607178
iteration 800, loss 1.781443476676941
iteration 0, loss 1.8898195028305054
iteration 100, loss 1.8541326522827148
iteration 200, loss 1.8301140069961548
iteration 300, loss 1.7415494918823242
iteration 400, loss 1.7788580656051636
iteration 500, loss 1.9697315692901611
iteration 600, loss 1.7861181497573853
iteration 700, loss 1.8337842226028442
iteration 800, loss 1.7301074266433716
iteration 0, loss 1.8200329542160034
iteration 100, loss 1.7795430421829224
iteration 200, loss 1.8782167434692383
iteration 300, loss 1.8133199214935303
iteration 400, loss 1.7795449495315552
iteration 500, loss 1.7348142862319946
iteration 600, loss 1.858146071434021
iteration 700, loss 1.7905548810958862
iteration 800, loss 1.8321175575256348
iteration 0, loss 1.764307975769043
iteration 100, loss 1.749271273612976
iteration 200, loss 1.8470301628112793
iteration 300, loss 1.8482171297073364
iteration 400, loss 1.808748722076416
iteration 500, loss 1.8351082801818848
iteration 600, loss 1.8240830898284912
iteration 700, loss 1.816346287727356
iteration 800, loss 1.7812209129333496
iteration 0, loss 1.7418686151504517
iteration 100, loss 1.8220237493515015
iteration 200, loss 1.7505871057510376
iteration 300, loss 1.7907896041870117
iteration 400, loss 1.843379259109497
iteration 500, loss 1.849553108215332
iteration 600, loss 1.8648054599761963
iteration 700, loss 1.8100327253341675
iteration 800, loss 1.853625774383545
iteration 0, loss 1.837783932685852
iteration 100, loss 1.8314415216445923
iteration 200, loss 1.798685908317566
iteration 300, loss 1.8859078884124756
iteration 400, loss 1.8433332443237305
iteration 500, loss 1.768532156944275
iteration 600, loss 1.8770735263824463
iteration 700, loss 1.7896170616149902
iteration 800, loss 1.8723171949386597
iteration 0, loss 1.854161024093628
iteration 100, loss 1.75702702999115
iteration 200, loss 1.7779123783111572
iteration 300, loss 1.831551432609558
iteration 400, loss 1.8730618953704834
iteration 500, loss 1.7958457469940186
iteration 600, loss 1.7606751918792725
iteration 700, loss 1.8808972835540771
iteration 800, loss 1.8340109586715698
iteration 0, loss 1.7757513523101807
iteration 100, loss 1.8107786178588867
iteration 200, loss 1.8617631196975708
iteration 300, loss 1.82353937625885
iteration 400, loss 1.8607741594314575
iteration 500, loss 1.8649177551269531
iteration 600, loss 1.8778589963912964
iteration 700, loss 1.8534070253372192
iteration 800, loss 1.7182726860046387
iteration 0, loss 1.8153671026229858
iteration 100, loss 1.8217233419418335
iteration 200, loss 1.770659327507019
iteration 300, loss 1.846267819404602
iteration 400, loss 1.7677392959594727
iteration 500, loss 1.8187710046768188
iteration 600, loss 1.770753026008606
iteration 700, loss 1.8954277038574219
iteration 800, loss 1.8479214906692505
iteration 0, loss 1.8324275016784668
iteration 100, loss 1.77422034740448
iteration 200, loss 1.7958989143371582
iteration 300, loss 1.8197052478790283
iteration 400, loss 1.7764248847961426
iteration 500, loss 1.8005552291870117
iteration 600, loss 1.7981462478637695
iteration 700, loss 1.7821532487869263
iteration 800, loss 1.8740136623382568
iteration 0, loss 1.8578312397003174
iteration 100, loss 1.8535717725753784
iteration 200, loss 1.8217554092407227
iteration 300, loss 1.778249740600586
iteration 400, loss 1.869396448135376
iteration 500, loss 1.8555999994277954
iteration 600, loss 1.770857810974121
iteration 700, loss 1.8679012060165405
iteration 800, loss 1.8445039987564087
iteration 0, loss 1.7634947299957275
iteration 100, loss 1.7535498142242432
iteration 200, loss 1.8160697221755981
iteration 300, loss 1.887681245803833
iteration 400, loss 1.852023720741272
iteration 500, loss 1.8129854202270508
iteration 600, loss 1.802340030670166
iteration 700, loss 1.8014057874679565
iteration 800, loss 1.8473783731460571
iteration 0, loss 1.88028085231781
iteration 100, loss 1.7900748252868652
iteration 200, loss 1.8363207578659058
iteration 300, loss 1.8128254413604736
iteration 400, loss 1.7708606719970703
iteration 500, loss 1.7654922008514404
iteration 600, loss 1.821894884109497
iteration 700, loss 1.824652075767517
iteration 800, loss 1.8494009971618652
iteration 0, loss 1.8387117385864258
iteration 100, loss 1.741588830947876
iteration 200, loss 1.8778759241104126
iteration 300, loss 1.8370578289031982
iteration 400, loss 1.79867684841156
iteration 500, loss 1.8756505250930786
iteration 600, loss 1.8313164710998535
iteration 700, loss 1.7770222425460815
iteration 800, loss 1.7162895202636719
iteration 0, loss 1.8160887956619263
iteration 100, loss 1.8296860456466675
iteration 200, loss 1.7537490129470825
iteration 300, loss 1.8632678985595703
iteration 400, loss 1.7853161096572876
iteration 500, loss 1.80181884765625
iteration 600, loss 1.7866790294647217
iteration 700, loss 1.7823492288589478
iteration 800, loss 1.8448314666748047
iteration 0, loss 1.751382827758789
iteration 100, loss 1.8664555549621582
iteration 200, loss 1.8346267938613892
iteration 300, loss 1.808221697807312
iteration 400, loss 1.837912917137146
iteration 500, loss 1.8046824932098389
iteration 600, loss 1.8679932355880737
iteration 700, loss 1.7750439643859863
iteration 800, loss 1.748718023300171
iteration 0, loss 1.7875161170959473
iteration 100, loss 1.865146279335022
iteration 200, loss 1.8007349967956543
iteration 300, loss 1.8080285787582397
iteration 400, loss 1.8437111377716064
iteration 500, loss 1.762061357498169
iteration 600, loss 1.8671905994415283
iteration 700, loss 1.9114044904708862
iteration 800, loss 1.7215757369995117
iteration 0, loss 1.8118489980697632
iteration 100, loss 1.7479770183563232
iteration 200, loss 1.8971673250198364
iteration 300, loss 1.8154116868972778
iteration 400, loss 1.8753087520599365
iteration 500, loss 1.7735786437988281
iteration 600, loss 1.6967483758926392
iteration 700, loss 1.830473780632019
iteration 800, loss 1.857285499572754
iteration 0, loss 1.8595046997070312
iteration 100, loss 1.7912063598632812
iteration 200, loss 1.7867555618286133
iteration 300, loss 1.8307209014892578
iteration 400, loss 1.8009620904922485
iteration 500, loss 1.7210760116577148
iteration 600, loss 1.8011128902435303
iteration 700, loss 1.882724404335022
iteration 800, loss 1.782480239868164
iteration 0, loss 1.7706263065338135
iteration 100, loss 1.7762451171875
iteration 200, loss 1.8390549421310425
iteration 300, loss 1.7971227169036865
iteration 400, loss 1.8758368492126465
iteration 500, loss 1.73496675491333
iteration 600, loss 1.8344643115997314
iteration 700, loss 1.8429926633834839
iteration 800, loss 1.8392876386642456
iteration 0, loss 1.774449348449707
iteration 100, loss 1.7112740278244019
iteration 200, loss 1.7742993831634521
iteration 300, loss 1.7411236763000488
iteration 400, loss 1.8141154050827026
iteration 500, loss 1.8616750240325928
iteration 600, loss 1.7934964895248413
iteration 700, loss 1.8368210792541504
iteration 800, loss 1.7962175607681274
iteration 0, loss 1.810039758682251
iteration 100, loss 1.7919131517410278
iteration 200, loss 1.8290718793869019
iteration 300, loss 1.777880311012268
iteration 400, loss 1.7799804210662842
iteration 500, loss 1.8686010837554932
iteration 600, loss 1.81162428855896
iteration 700, loss 1.8205158710479736
iteration 800, loss 1.7513282299041748
iteration 0, loss 1.8399676084518433
iteration 100, loss 1.7484263181686401
iteration 200, loss 1.8446849584579468
iteration 300, loss 1.8276216983795166
iteration 400, loss 1.8980461359024048
iteration 500, loss 1.7669037580490112
iteration 600, loss 1.7852460145950317
iteration 700, loss 1.7395893335342407
iteration 800, loss 1.867266058921814
iteration 0, loss 1.8158429861068726
iteration 100, loss 1.8266966342926025
iteration 200, loss 1.8323825597763062
iteration 300, loss 1.8365118503570557
iteration 400, loss 1.8011640310287476
iteration 500, loss 1.8416305780410767
iteration 600, loss 1.7503095865249634
iteration 700, loss 1.7212204933166504
iteration 800, loss 1.8462170362472534
iteration 0, loss 1.8240211009979248
iteration 100, loss 1.7476061582565308
iteration 200, loss 1.8612122535705566
iteration 300, loss 1.8153331279754639
iteration 400, loss 1.8438612222671509
iteration 500, loss 1.7747052907943726
iteration 600, loss 1.8220934867858887
iteration 700, loss 1.8010482788085938
iteration 800, loss 1.7940973043441772
iteration 0, loss 1.8217355012893677
iteration 100, loss 1.7565088272094727
iteration 200, loss 1.80930757522583
iteration 300, loss 1.8350435495376587
iteration 400, loss 1.8162800073623657
iteration 500, loss 1.8597122430801392
iteration 600, loss 1.875261902809143
iteration 700, loss 1.7974328994750977
iteration 800, loss 1.8049317598342896
iteration 0, loss 1.7933471202850342
iteration 100, loss 1.7994054555892944
iteration 200, loss 1.760072112083435
iteration 300, loss 1.7910676002502441
iteration 400, loss 1.8232744932174683
iteration 500, loss 1.8998371362686157
iteration 600, loss 1.8052823543548584
iteration 700, loss 1.8227319717407227
iteration 800, loss 1.8799742460250854
iteration 0, loss 1.8089672327041626
iteration 100, loss 1.8148934841156006
iteration 200, loss 1.7715502977371216
iteration 300, loss 1.8299968242645264
iteration 400, loss 1.843093991279602
iteration 500, loss 1.8317011594772339
iteration 600, loss 1.7959444522857666
iteration 700, loss 1.9162178039550781
iteration 800, loss 1.855074405670166
iteration 0, loss 1.7899295091629028
iteration 100, loss 1.8180615901947021
iteration 200, loss 1.7871167659759521
iteration 300, loss 1.8262304067611694
iteration 400, loss 1.8041789531707764
iteration 500, loss 1.8601971864700317
iteration 600, loss 1.8304896354675293
iteration 700, loss 1.8049519062042236
iteration 800, loss 1.7591437101364136
iteration 0, loss 1.8060150146484375
iteration 100, loss 1.9161344766616821
iteration 200, loss 1.7976114749908447
iteration 300, loss 1.8432073593139648
iteration 400, loss 1.7950727939605713
iteration 500, loss 1.751124382019043
iteration 600, loss 1.8728528022766113
iteration 700, loss 1.8491230010986328
iteration 800, loss 1.7991690635681152
iteration 0, loss 1.801870584487915
iteration 100, loss 1.8103854656219482
iteration 200, loss 1.8558663129806519
iteration 300, loss 1.7514445781707764
iteration 400, loss 1.860087513923645
iteration 500, loss 1.8252623081207275
iteration 600, loss 1.8212696313858032
iteration 700, loss 1.7998501062393188
iteration 800, loss 1.832905650138855
iteration 0, loss 1.7798784971237183
iteration 100, loss 1.9261653423309326
iteration 200, loss 1.8846170902252197
iteration 300, loss 1.723309874534607
iteration 400, loss 1.8460149765014648
iteration 500, loss 1.8096237182617188
iteration 600, loss 1.764164686203003
iteration 700, loss 1.8170759677886963
iteration 800, loss 1.8043756484985352
iteration 0, loss 1.7125914096832275
iteration 100, loss 1.7975659370422363
iteration 200, loss 1.840779423713684
iteration 300, loss 1.8323028087615967
iteration 400, loss 1.8096848726272583
iteration 500, loss 1.7578024864196777
iteration 600, loss 1.7924858331680298
iteration 700, loss 1.8162604570388794
iteration 800, loss 1.7708463668823242
iteration 0, loss 1.7517900466918945
iteration 100, loss 1.804007887840271
iteration 200, loss 1.7727115154266357
iteration 300, loss 1.7279908657073975
iteration 400, loss 1.8335113525390625
iteration 500, loss 1.839934229850769
iteration 600, loss 1.8371704816818237
iteration 700, loss 1.8331327438354492
iteration 800, loss 1.707538366317749
iteration 0, loss 1.7908120155334473
iteration 100, loss 1.8421739339828491
iteration 200, loss 1.8198076486587524
iteration 300, loss 1.8031482696533203
iteration 400, loss 1.786251425743103
iteration 500, loss 1.8515150547027588
iteration 600, loss 1.8639633655548096
iteration 700, loss 1.8190248012542725
iteration 800, loss 1.8419665098190308
iteration 0, loss 1.825465202331543
iteration 100, loss 1.8482904434204102
iteration 200, loss 1.7907837629318237
iteration 300, loss 1.7380071878433228
iteration 400, loss 1.791118860244751
iteration 500, loss 1.8256220817565918
iteration 600, loss 1.8369827270507812
iteration 700, loss 1.8008930683135986
iteration 800, loss 1.8050827980041504
iteration 0, loss 1.7713589668273926
iteration 100, loss 1.855669617652893
iteration 200, loss 1.7613043785095215
iteration 300, loss 1.8159136772155762
iteration 400, loss 1.770958662033081
iteration 500, loss 1.7359131574630737
iteration 600, loss 1.7939780950546265
iteration 700, loss 1.884739637374878
iteration 800, loss 1.8292169570922852
fold 4 accuracy: 0.4867142857142857
[2024-02-28 23:53:16,092] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-28 23:53:16,097] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            328.78 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.7 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '328.78 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 328.78 us = 100% latency, 1.7 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 237.46 us = 72.23% latency, 2.36 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 27.66 us = 8.41% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-28 23:53:16,098] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
iteration 0, loss 2.3067588806152344
iteration 100, loss 2.2887609004974365
iteration 200, loss 2.2247912883758545
iteration 300, loss 2.1127548217773438
iteration 400, loss 2.126210927963257
iteration 500, loss 1.9734984636306763
iteration 600, loss 1.9694175720214844
iteration 700, loss 1.9787930250167847
iteration 800, loss 1.9723708629608154
iteration 0, loss 1.9600940942764282
iteration 100, loss 1.9474631547927856
iteration 200, loss 2.0096981525421143
iteration 300, loss 1.948288083076477
iteration 400, loss 1.9884071350097656
iteration 500, loss 1.9621219635009766
iteration 600, loss 1.9189318418502808
iteration 700, loss 1.9071065187454224
iteration 800, loss 1.9702988862991333
iteration 0, loss 1.916135549545288
iteration 100, loss 1.9202194213867188
iteration 200, loss 1.858630895614624
iteration 300, loss 1.8888869285583496
iteration 400, loss 1.9323346614837646
iteration 500, loss 1.9576761722564697
iteration 600, loss 1.8586492538452148
iteration 700, loss 1.978366732597351
iteration 800, loss 2.046347141265869
iteration 0, loss 1.9454292058944702
iteration 100, loss 1.9464669227600098
iteration 200, loss 1.9026691913604736
iteration 300, loss 1.8726558685302734
iteration 400, loss 1.9454289674758911
iteration 500, loss 1.8063963651657104
iteration 600, loss 1.8673899173736572
iteration 700, loss 1.9415756464004517
iteration 800, loss 1.9154962301254272
iteration 0, loss 1.990182876586914
iteration 100, loss 1.8705878257751465
iteration 200, loss 1.9983075857162476
iteration 300, loss 1.940460443496704
iteration 400, loss 1.9485057592391968
iteration 500, loss 1.8889795541763306
iteration 600, loss 1.9934049844741821
iteration 700, loss 1.8844860792160034
iteration 800, loss 1.8625471591949463
iteration 0, loss 1.9648579359054565
iteration 100, loss 1.824204921722412
iteration 200, loss 1.9453507661819458
iteration 300, loss 1.8841474056243896
iteration 400, loss 1.936619758605957
iteration 500, loss 1.91283118724823
iteration 600, loss 1.8811652660369873
iteration 700, loss 1.9706549644470215
iteration 800, loss 1.8260581493377686
iteration 0, loss 1.8380461931228638
iteration 100, loss 1.914719581604004
iteration 200, loss 1.8945473432540894
iteration 300, loss 1.9683969020843506
iteration 400, loss 1.8441553115844727
iteration 500, loss 1.8763933181762695
iteration 600, loss 1.9695404767990112
iteration 700, loss 1.8514671325683594
iteration 800, loss 1.897856593132019
iteration 0, loss 1.9363019466400146
iteration 100, loss 1.9165754318237305
iteration 200, loss 1.8712106943130493
iteration 300, loss 1.9208711385726929
iteration 400, loss 1.952488899230957
iteration 500, loss 1.9154819250106812
iteration 600, loss 2.000784158706665
iteration 700, loss 1.8508867025375366
iteration 800, loss 1.9228278398513794
iteration 0, loss 1.8989777565002441
iteration 100, loss 1.884321928024292
iteration 200, loss 1.8781366348266602
iteration 300, loss 1.8873677253723145
iteration 400, loss 1.935300350189209
iteration 500, loss 1.8666805028915405
iteration 600, loss 1.919071078300476
iteration 700, loss 1.8554946184158325
iteration 800, loss 1.8816827535629272
iteration 0, loss 1.894484519958496
iteration 100, loss 1.89607834815979
iteration 200, loss 1.881696343421936
iteration 300, loss 1.935579776763916
iteration 400, loss 1.92450749874115
iteration 500, loss 1.855107069015503
iteration 600, loss 1.9496227502822876
iteration 700, loss 1.9687315225601196
iteration 800, loss 1.9040889739990234
iteration 0, loss 1.88241708278656
iteration 100, loss 1.8935039043426514
iteration 200, loss 1.8630322217941284
iteration 300, loss 1.8442977666854858
iteration 400, loss 1.8782994747161865
iteration 500, loss 1.8580586910247803
iteration 600, loss 1.9154930114746094
iteration 700, loss 2.0025382041931152
iteration 800, loss 1.761889934539795
iteration 0, loss 1.8384075164794922
iteration 100, loss 1.9913878440856934
iteration 200, loss 1.9146531820297241
iteration 300, loss 1.8560664653778076
iteration 400, loss 1.8172855377197266
iteration 500, loss 1.8863343000411987
iteration 600, loss 1.9551523923873901
iteration 700, loss 1.9931827783584595
iteration 800, loss 1.940729022026062
iteration 0, loss 1.9349753856658936
iteration 100, loss 1.8978458642959595
iteration 200, loss 1.8986926078796387
iteration 300, loss 1.7986394166946411
iteration 400, loss 1.8438791036605835
iteration 500, loss 1.9199373722076416
iteration 600, loss 1.8175139427185059
iteration 700, loss 1.9912006855010986
iteration 800, loss 1.8601008653640747
iteration 0, loss 1.9110149145126343
iteration 100, loss 1.898256540298462
iteration 200, loss 1.811038613319397
iteration 300, loss 1.8676573038101196
iteration 400, loss 1.9180974960327148
iteration 500, loss 1.88406503200531
iteration 600, loss 1.8960366249084473
iteration 700, loss 1.849370002746582
iteration 800, loss 1.9100195169448853
iteration 0, loss 1.8507879972457886
iteration 100, loss 1.8642014265060425
iteration 200, loss 1.9571958780288696
iteration 300, loss 1.8213058710098267
iteration 400, loss 1.803310751914978
iteration 500, loss 1.7813560962677002
iteration 600, loss 1.9719951152801514
iteration 700, loss 1.9173327684402466
iteration 800, loss 1.8578730821609497
iteration 0, loss 1.8579052686691284
iteration 100, loss 1.9623966217041016
iteration 200, loss 1.923896312713623
iteration 300, loss 1.8031609058380127
iteration 400, loss 1.8838889598846436
iteration 500, loss 1.9351779222488403
iteration 600, loss 1.8362139463424683
iteration 700, loss 1.9415158033370972
iteration 800, loss 1.842816948890686
iteration 0, loss 1.8449333906173706
iteration 100, loss 1.8747565746307373
iteration 200, loss 1.9208396673202515
iteration 300, loss 1.8924391269683838
iteration 400, loss 1.832201600074768
iteration 500, loss 1.853365182876587
iteration 600, loss 1.840592861175537
iteration 700, loss 1.9681044816970825
iteration 800, loss 1.8944182395935059
iteration 0, loss 1.8554917573928833
iteration 100, loss 1.8390501737594604
iteration 200, loss 1.9340630769729614
iteration 300, loss 1.7773404121398926
iteration 400, loss 1.8111075162887573
iteration 500, loss 1.9016205072402954
iteration 600, loss 1.826968789100647
iteration 700, loss 1.880312204360962
iteration 800, loss 1.8434441089630127
iteration 0, loss 1.8121834993362427
iteration 100, loss 1.8887927532196045
iteration 200, loss 1.8604134321212769
iteration 300, loss 1.7685747146606445
iteration 400, loss 1.8781399726867676
iteration 500, loss 1.8536052703857422
iteration 600, loss 1.8643007278442383
iteration 700, loss 1.9227038621902466
iteration 800, loss 1.8390586376190186
iteration 0, loss 1.8714842796325684
iteration 100, loss 1.7841017246246338
iteration 200, loss 1.9163475036621094
iteration 300, loss 1.9373104572296143
iteration 400, loss 1.8593615293502808
iteration 500, loss 1.916978359222412
iteration 600, loss 1.901843786239624
iteration 700, loss 1.8665515184402466
iteration 800, loss 1.8344939947128296
iteration 0, loss 1.8970363140106201
iteration 100, loss 1.788033366203308
iteration 200, loss 1.904918909072876
iteration 300, loss 1.9132002592086792
iteration 400, loss 1.8554235696792603
iteration 500, loss 1.9916996955871582
iteration 600, loss 1.8599597215652466
iteration 700, loss 1.926077961921692
iteration 800, loss 1.8693264722824097
iteration 0, loss 1.8835394382476807
iteration 100, loss 1.8981863260269165
iteration 200, loss 1.7419756650924683
iteration 300, loss 1.888895034790039
iteration 400, loss 1.9229316711425781
iteration 500, loss 1.971300721168518
iteration 600, loss 1.8713232278823853
iteration 700, loss 1.8142282962799072
iteration 800, loss 1.8782347440719604
iteration 0, loss 1.864920973777771
iteration 100, loss 1.8412598371505737
iteration 200, loss 1.8461672067642212
iteration 300, loss 1.8635742664337158
iteration 400, loss 1.963437795639038
iteration 500, loss 1.9424151182174683
iteration 600, loss 1.9332882165908813
iteration 700, loss 1.9094136953353882
iteration 800, loss 1.91376531124115
iteration 0, loss 1.8528907299041748
iteration 100, loss 1.8775172233581543
iteration 200, loss 1.8496973514556885
iteration 300, loss 1.8217251300811768
iteration 400, loss 1.964643120765686
iteration 500, loss 1.9040025472640991
iteration 600, loss 1.8876469135284424
iteration 700, loss 1.915436029434204
iteration 800, loss 1.8212488889694214
iteration 0, loss 1.8229962587356567
iteration 100, loss 1.8761322498321533
iteration 200, loss 1.8548635244369507
iteration 300, loss 1.8687489032745361
iteration 400, loss 1.9064286947250366
iteration 500, loss 1.9056438207626343
iteration 600, loss 1.9251259565353394
iteration 700, loss 1.886915922164917
iteration 800, loss 1.860212802886963
iteration 0, loss 1.8668153285980225
iteration 100, loss 1.895731806755066
iteration 200, loss 1.8320634365081787
iteration 300, loss 1.8945846557617188
iteration 400, loss 1.9171152114868164
iteration 500, loss 1.927053689956665
iteration 600, loss 1.8528097867965698
iteration 700, loss 1.9279414415359497
iteration 800, loss 1.8841146230697632
iteration 0, loss 1.9015791416168213
iteration 100, loss 1.9405261278152466
iteration 200, loss 1.8527839183807373
iteration 300, loss 1.9454658031463623
iteration 400, loss 1.9039454460144043
iteration 500, loss 1.888183355331421
iteration 600, loss 1.875414490699768
iteration 700, loss 1.8795703649520874
iteration 800, loss 1.830519199371338
iteration 0, loss 1.8210923671722412
iteration 100, loss 1.8936131000518799
iteration 200, loss 1.8087728023529053
iteration 300, loss 1.907548189163208
iteration 400, loss 1.889129400253296
iteration 500, loss 1.8134559392929077
iteration 600, loss 1.8807337284088135
iteration 700, loss 1.8883010149002075
iteration 800, loss 1.7989073991775513
iteration 0, loss 1.9100589752197266
iteration 100, loss 1.8166546821594238
iteration 200, loss 1.8729774951934814
iteration 300, loss 1.892430305480957
iteration 400, loss 1.896000862121582
iteration 500, loss 1.9060617685317993
iteration 600, loss 1.8963518142700195
iteration 700, loss 1.859222650527954
iteration 800, loss 1.8558028936386108
iteration 0, loss 1.8498643636703491
iteration 100, loss 1.943160057067871
iteration 200, loss 1.8121850490570068
iteration 300, loss 1.7923328876495361
iteration 400, loss 1.8227001428604126
iteration 500, loss 1.8062046766281128
iteration 600, loss 1.84263014793396
iteration 700, loss 1.9082310199737549
iteration 800, loss 1.8347584009170532
iteration 0, loss 1.8844454288482666
iteration 100, loss 1.8578321933746338
iteration 200, loss 1.816358208656311
iteration 300, loss 1.9389286041259766
iteration 400, loss 1.9424742460250854
iteration 500, loss 1.9902857542037964
iteration 600, loss 1.8754125833511353
iteration 700, loss 1.853036880493164
iteration 800, loss 1.7977707386016846
iteration 0, loss 1.843359351158142
iteration 100, loss 1.892502784729004
iteration 200, loss 1.9034686088562012
iteration 300, loss 1.9151318073272705
iteration 400, loss 1.873252272605896
iteration 500, loss 1.8393981456756592
iteration 600, loss 1.8712624311447144
iteration 700, loss 1.886480450630188
iteration 800, loss 1.8465838432312012
iteration 0, loss 1.8222754001617432
iteration 100, loss 1.967774748802185
iteration 200, loss 1.9200339317321777
iteration 300, loss 1.8288768529891968
iteration 400, loss 1.836858868598938
iteration 500, loss 1.840342402458191
iteration 600, loss 1.834532618522644
iteration 700, loss 1.8520485162734985
iteration 800, loss 1.863424301147461
iteration 0, loss 1.8028435707092285
iteration 100, loss 1.906982421875
iteration 200, loss 1.8513667583465576
iteration 300, loss 1.841591477394104
iteration 400, loss 1.8669681549072266
iteration 500, loss 1.8344964981079102
iteration 600, loss 1.794679880142212
iteration 700, loss 1.872087836265564
iteration 800, loss 1.8823258876800537
iteration 0, loss 1.8133654594421387
iteration 100, loss 1.84164297580719
iteration 200, loss 1.787879467010498
iteration 300, loss 1.986744999885559
iteration 400, loss 1.8730900287628174
iteration 500, loss 1.8447380065917969
iteration 600, loss 1.8048219680786133
iteration 700, loss 1.8651987314224243
iteration 800, loss 1.8221981525421143
iteration 0, loss 1.9410481452941895
iteration 100, loss 1.8122864961624146
iteration 200, loss 1.9454833269119263
iteration 300, loss 1.8573933839797974
iteration 400, loss 1.8523271083831787
iteration 500, loss 1.88194739818573
iteration 600, loss 1.8746455907821655
iteration 700, loss 1.8913613557815552
iteration 800, loss 1.8196674585342407
iteration 0, loss 1.8953219652175903
iteration 100, loss 1.8676849603652954
iteration 200, loss 1.8235996961593628
iteration 300, loss 1.8782126903533936
iteration 400, loss 1.909727931022644
iteration 500, loss 1.931699514389038
iteration 600, loss 1.8928147554397583
iteration 700, loss 1.818727731704712
iteration 800, loss 1.8128581047058105
iteration 0, loss 1.9358415603637695
iteration 100, loss 1.8917104005813599
iteration 200, loss 1.8976402282714844
iteration 300, loss 1.8818680047988892
iteration 400, loss 1.8361774682998657
iteration 500, loss 1.9417260885238647
iteration 600, loss 1.8433350324630737
iteration 700, loss 1.9106626510620117
iteration 800, loss 1.9122384786605835
iteration 0, loss 1.9444382190704346
iteration 100, loss 1.9241056442260742
iteration 200, loss 1.877690315246582
iteration 300, loss 1.9746575355529785
iteration 400, loss 1.9234760999679565
iteration 500, loss 1.9075959920883179
iteration 600, loss 1.9237452745437622
iteration 700, loss 1.8973922729492188
iteration 800, loss 1.8185617923736572
iteration 0, loss 1.862447738647461
iteration 100, loss 1.929940104484558
iteration 200, loss 1.8535817861557007
iteration 300, loss 1.8271386623382568
iteration 400, loss 1.833716630935669
iteration 500, loss 1.873172402381897
iteration 600, loss 1.8323105573654175
iteration 700, loss 1.7958910465240479
iteration 800, loss 1.868648886680603
iteration 0, loss 1.8963313102722168
iteration 100, loss 1.8326998949050903
iteration 200, loss 1.8908530473709106
iteration 300, loss 1.87828528881073
iteration 400, loss 1.8605915307998657
iteration 500, loss 1.8812028169631958
iteration 600, loss 1.8214020729064941
iteration 700, loss 1.9563696384429932
iteration 800, loss 1.926254153251648
iteration 0, loss 1.9000718593597412
iteration 100, loss 1.9206372499465942
iteration 200, loss 1.916437029838562
iteration 300, loss 1.885549783706665
iteration 400, loss 1.9041460752487183
iteration 500, loss 1.884047508239746
iteration 600, loss 1.8379662036895752
iteration 700, loss 1.8755524158477783
iteration 800, loss 1.8678196668624878
iteration 0, loss 1.948195457458496
iteration 100, loss 1.8811436891555786
iteration 200, loss 1.8320972919464111
iteration 300, loss 1.8160183429718018
iteration 400, loss 1.9100154638290405
iteration 500, loss 1.8792991638183594
iteration 600, loss 1.9009957313537598
iteration 700, loss 1.8878254890441895
iteration 800, loss 1.8174725770950317
iteration 0, loss 1.8696610927581787
iteration 100, loss 1.7315959930419922
iteration 200, loss 1.9025629758834839
iteration 300, loss 1.8248155117034912
iteration 400, loss 1.8589035272598267
iteration 500, loss 1.9036586284637451
iteration 600, loss 1.7882829904556274
iteration 700, loss 1.8311066627502441
iteration 800, loss 1.857381820678711
iteration 0, loss 1.853814959526062
iteration 100, loss 1.9689967632293701
iteration 200, loss 1.803041934967041
iteration 300, loss 1.8840701580047607
iteration 400, loss 1.82088041305542
iteration 500, loss 1.8126773834228516
iteration 600, loss 1.9442050457000732
iteration 700, loss 1.9038991928100586
iteration 800, loss 1.8380858898162842
iteration 0, loss 1.8581756353378296
iteration 100, loss 1.8159146308898926
iteration 200, loss 1.9116028547286987
iteration 300, loss 1.9009686708450317
iteration 400, loss 1.858414649963379
iteration 500, loss 1.9254915714263916
iteration 600, loss 1.8491655588150024
iteration 700, loss 1.8266643285751343
iteration 800, loss 1.8926842212677002
iteration 0, loss 1.8118289709091187
iteration 100, loss 1.8437443971633911
iteration 200, loss 1.8504561185836792
iteration 300, loss 1.8687559366226196
iteration 400, loss 1.874268651008606
iteration 500, loss 1.885482907295227
iteration 600, loss 1.8727940320968628
iteration 700, loss 1.8632433414459229
iteration 800, loss 1.855445384979248
iteration 0, loss 1.9159554243087769
iteration 100, loss 1.8996834754943848
iteration 200, loss 1.9826478958129883
iteration 300, loss 1.8082919120788574
iteration 400, loss 1.861438512802124
iteration 500, loss 1.794853925704956
iteration 600, loss 1.8731722831726074
iteration 700, loss 1.8643488883972168
iteration 800, loss 1.891159176826477
iteration 0, loss 1.8713769912719727
iteration 100, loss 1.8785648345947266
iteration 200, loss 1.8861693143844604
iteration 300, loss 1.8564119338989258
iteration 400, loss 1.824065923690796
iteration 500, loss 1.8108845949172974
iteration 600, loss 1.8706374168395996
iteration 700, loss 1.9100273847579956
iteration 800, loss 1.8732335567474365
iteration 0, loss 1.7666430473327637
iteration 100, loss 1.9004913568496704
iteration 200, loss 1.8445420265197754
iteration 300, loss 1.843275547027588
iteration 400, loss 1.8644964694976807
iteration 500, loss 1.9414056539535522
iteration 600, loss 1.9329395294189453
iteration 700, loss 1.8287231922149658
iteration 800, loss 1.8340827226638794
fold 0 accuracy: 0.447
iteration 0, loss 1.8735802173614502
iteration 100, loss 1.8309884071350098
iteration 200, loss 1.9183409214019775
iteration 300, loss 1.926324725151062
iteration 400, loss 1.8778462409973145
iteration 500, loss 1.892746090888977
iteration 600, loss 1.8804988861083984
iteration 700, loss 1.885672926902771
iteration 800, loss 1.8005576133728027
iteration 0, loss 1.8728392124176025
iteration 100, loss 1.7909047603607178
iteration 200, loss 1.89402174949646
iteration 300, loss 1.8422900438308716
iteration 400, loss 1.8521735668182373
iteration 500, loss 1.923561692237854
iteration 600, loss 1.852224588394165
iteration 700, loss 1.9107294082641602
iteration 800, loss 1.8366786241531372
iteration 0, loss 1.9719480276107788
iteration 100, loss 1.8840962648391724
iteration 200, loss 1.8670380115509033
iteration 300, loss 1.8717384338378906
iteration 400, loss 1.8237004280090332
iteration 500, loss 1.8978344202041626
iteration 600, loss 1.7818348407745361
iteration 700, loss 1.886239767074585
iteration 800, loss 1.8758164644241333
iteration 0, loss 1.8525334596633911
iteration 100, loss 1.9002350568771362
iteration 200, loss 1.9350072145462036
iteration 300, loss 1.856590747833252
iteration 400, loss 1.8519368171691895
iteration 500, loss 1.9320237636566162
iteration 600, loss 1.8360549211502075
iteration 700, loss 1.8593039512634277
iteration 800, loss 1.8559879064559937
iteration 0, loss 1.8378468751907349
iteration 100, loss 1.8583638668060303
iteration 200, loss 1.8439910411834717
iteration 300, loss 1.9468430280685425
iteration 400, loss 1.8498921394348145
iteration 500, loss 1.829736351966858
iteration 600, loss 1.8974827527999878
iteration 700, loss 1.870033621788025
iteration 800, loss 1.8123496770858765
iteration 0, loss 1.867194414138794
iteration 100, loss 1.9237842559814453
iteration 200, loss 1.9070606231689453
iteration 300, loss 1.8898200988769531
iteration 400, loss 1.919041395187378
iteration 500, loss 1.9084049463272095
iteration 600, loss 1.8120667934417725
iteration 700, loss 1.875227689743042
iteration 800, loss 1.878966212272644
iteration 0, loss 1.8949072360992432
iteration 100, loss 1.842057228088379
iteration 200, loss 1.8349742889404297
iteration 300, loss 1.8387057781219482
iteration 400, loss 1.8860529661178589
iteration 500, loss 1.918678641319275
iteration 600, loss 1.9432259798049927
iteration 700, loss 1.822579026222229
iteration 800, loss 1.8707146644592285
iteration 0, loss 1.8901519775390625
iteration 100, loss 1.8586090803146362
iteration 200, loss 1.816128134727478
iteration 300, loss 1.8844901323318481
iteration 400, loss 1.9433883428573608
iteration 500, loss 1.8993605375289917
iteration 600, loss 1.7921359539031982
iteration 700, loss 1.8848116397857666
iteration 800, loss 1.9228569269180298
iteration 0, loss 1.8248361349105835
iteration 100, loss 1.8461517095565796
iteration 200, loss 1.8540894985198975
iteration 300, loss 1.9672654867172241
iteration 400, loss 1.7148444652557373
iteration 500, loss 1.9674959182739258
iteration 600, loss 1.9095418453216553
iteration 700, loss 1.8878583908081055
iteration 800, loss 1.8282270431518555
iteration 0, loss 1.885986089706421
iteration 100, loss 1.8687410354614258
iteration 200, loss 1.897367000579834
iteration 300, loss 1.8536982536315918
iteration 400, loss 1.9316601753234863
iteration 500, loss 1.7376903295516968
iteration 600, loss 1.8334909677505493
iteration 700, loss 1.7669461965560913
iteration 800, loss 1.8166583776474
iteration 0, loss 1.8200651407241821
iteration 100, loss 1.796945333480835
iteration 200, loss 1.9128878116607666
iteration 300, loss 1.7787960767745972
iteration 400, loss 1.8922373056411743
iteration 500, loss 1.983932375907898
iteration 600, loss 1.8246197700500488
iteration 700, loss 1.917296051979065
iteration 800, loss 1.8657842874526978
iteration 0, loss 1.895112156867981
iteration 100, loss 1.8403081893920898
iteration 200, loss 1.7946574687957764
iteration 300, loss 1.8508977890014648
iteration 400, loss 1.8269292116165161
iteration 500, loss 1.7982814311981201
iteration 600, loss 1.9645426273345947
iteration 700, loss 1.8909077644348145
iteration 800, loss 1.846693754196167
iteration 0, loss 1.8621206283569336
iteration 100, loss 1.8088840246200562
iteration 200, loss 1.9012839794158936
iteration 300, loss 1.8985440731048584
iteration 400, loss 1.847659707069397
iteration 500, loss 1.8525965213775635
iteration 600, loss 1.8643206357955933
iteration 700, loss 1.8770151138305664
iteration 800, loss 1.8912032842636108
iteration 0, loss 1.8254097700119019
iteration 100, loss 1.8920369148254395
iteration 200, loss 1.9188528060913086
iteration 300, loss 1.8691692352294922
iteration 400, loss 1.8590673208236694
iteration 500, loss 1.8373537063598633
iteration 600, loss 1.794057011604309
iteration 700, loss 1.9384193420410156
iteration 800, loss 1.9415814876556396
iteration 0, loss 1.9571324586868286
iteration 100, loss 1.8311712741851807
iteration 200, loss 1.8401708602905273
iteration 300, loss 1.9045077562332153
iteration 400, loss 1.8306571245193481
iteration 500, loss 1.8757809400558472
iteration 600, loss 1.8276774883270264
iteration 700, loss 1.801124930381775
iteration 800, loss 1.9649473428726196
iteration 0, loss 1.8606067895889282
iteration 100, loss 1.827648639678955
iteration 200, loss 1.9120309352874756
iteration 300, loss 1.773260235786438
iteration 400, loss 1.8620615005493164
iteration 500, loss 1.9264880418777466
iteration 600, loss 1.8787546157836914
iteration 700, loss 1.79788076877594
iteration 800, loss 1.8368738889694214
iteration 0, loss 1.9032951593399048
iteration 100, loss 1.8887337446212769
iteration 200, loss 1.9676127433776855
iteration 300, loss 1.8050799369812012
iteration 400, loss 1.8596134185791016
iteration 500, loss 2.0007357597351074
iteration 600, loss 1.8159632682800293
iteration 700, loss 1.8791028261184692
iteration 800, loss 1.836473822593689
iteration 0, loss 1.8876317739486694
iteration 100, loss 1.930122971534729
iteration 200, loss 1.8523046970367432
iteration 300, loss 1.894146203994751
iteration 400, loss 1.9181299209594727
iteration 500, loss 1.8704400062561035
iteration 600, loss 1.9592819213867188
iteration 700, loss 1.8882160186767578
iteration 800, loss 1.8645095825195312
iteration 0, loss 1.8611655235290527
iteration 100, loss 1.8065344095230103
iteration 200, loss 1.8281197547912598
iteration 300, loss 1.8634761571884155
iteration 400, loss 1.8580775260925293
iteration 500, loss 1.7933684587478638
iteration 600, loss 1.8496012687683105
iteration 700, loss 1.931479573249817
iteration 800, loss 1.8395874500274658
iteration 0, loss 1.8941051959991455
iteration 100, loss 1.8279038667678833
iteration 200, loss 1.8921207189559937
iteration 300, loss 1.869425654411316
iteration 400, loss 1.8399243354797363
iteration 500, loss 1.8523880243301392
iteration 600, loss 1.8501640558242798
iteration 700, loss 1.8833688497543335
iteration 800, loss 1.8889977931976318
iteration 0, loss 1.8877713680267334
iteration 100, loss 1.8927475214004517
iteration 200, loss 1.8875149488449097
iteration 300, loss 1.8212532997131348
iteration 400, loss 1.8513014316558838
iteration 500, loss 1.8273204565048218
iteration 600, loss 1.9175564050674438
iteration 700, loss 1.9033067226409912
iteration 800, loss 1.8414719104766846
iteration 0, loss 1.8687717914581299
iteration 100, loss 1.903818964958191
iteration 200, loss 1.9400631189346313
iteration 300, loss 1.7955528497695923
iteration 400, loss 1.922544240951538
iteration 500, loss 1.867278814315796
iteration 600, loss 1.8441942930221558
iteration 700, loss 1.803582787513733
iteration 800, loss 1.9129154682159424
iteration 0, loss 1.8344814777374268
iteration 100, loss 1.8988516330718994
iteration 200, loss 1.819024682044983
iteration 300, loss 1.8395599126815796
iteration 400, loss 1.844791054725647
iteration 500, loss 1.8165894746780396
iteration 600, loss 1.8526651859283447
iteration 700, loss 1.907893419265747
iteration 800, loss 1.8874266147613525
iteration 0, loss 1.9037443399429321
iteration 100, loss 1.8521414995193481
iteration 200, loss 1.8682963848114014
iteration 300, loss 1.8082815408706665
iteration 400, loss 1.830883264541626
iteration 500, loss 1.8669053316116333
iteration 600, loss 1.8778252601623535
iteration 700, loss 1.9332199096679688
iteration 800, loss 1.8490678071975708
iteration 0, loss 1.8460164070129395
iteration 100, loss 1.8806586265563965
iteration 200, loss 1.8503366708755493
iteration 300, loss 1.8659650087356567
iteration 400, loss 1.8808996677398682
iteration 500, loss 1.8678030967712402
iteration 600, loss 1.8398194313049316
iteration 700, loss 1.7858103513717651
iteration 800, loss 1.8804997205734253
iteration 0, loss 1.8977981805801392
iteration 100, loss 1.8262659311294556
iteration 200, loss 1.7761290073394775
iteration 300, loss 1.7627472877502441
iteration 400, loss 1.8900882005691528
iteration 500, loss 1.7956109046936035
iteration 600, loss 1.8455336093902588
iteration 700, loss 1.9219870567321777
iteration 800, loss 1.794120192527771
iteration 0, loss 1.9380031824111938
iteration 100, loss 1.9214329719543457
iteration 200, loss 1.7486075162887573
iteration 300, loss 1.8750088214874268
iteration 400, loss 1.8764033317565918
iteration 500, loss 1.8716838359832764
iteration 600, loss 1.8203043937683105
iteration 700, loss 1.9200541973114014
iteration 800, loss 1.8965719938278198
iteration 0, loss 1.9167976379394531
iteration 100, loss 1.8112850189208984
iteration 200, loss 1.8568003177642822
iteration 300, loss 1.8941280841827393
iteration 400, loss 1.8412253856658936
iteration 500, loss 1.8754233121871948
iteration 600, loss 1.878722906112671
iteration 700, loss 1.8153560161590576
iteration 800, loss 1.8400765657424927
iteration 0, loss 1.8599867820739746
iteration 100, loss 1.8371121883392334
iteration 200, loss 1.9046252965927124
iteration 300, loss 1.948571801185608
iteration 400, loss 1.871518611907959
iteration 500, loss 1.8518025875091553
iteration 600, loss 1.7817554473876953
iteration 700, loss 1.8775105476379395
iteration 800, loss 1.7795747518539429
iteration 0, loss 1.88760507106781
iteration 100, loss 1.8296394348144531
iteration 200, loss 1.8428739309310913
iteration 300, loss 1.8418344259262085
iteration 400, loss 1.8663051128387451
iteration 500, loss 1.8885784149169922
iteration 600, loss 1.7903295755386353
iteration 700, loss 1.8099067211151123
iteration 800, loss 1.8402174711227417
iteration 0, loss 1.9052871465682983
iteration 100, loss 1.8954222202301025
iteration 200, loss 2.020082712173462
iteration 300, loss 1.8153401613235474
iteration 400, loss 1.8482825756072998
iteration 500, loss 1.8353724479675293
iteration 600, loss 1.8880141973495483
iteration 700, loss 1.8848860263824463
iteration 800, loss 1.8387914896011353
iteration 0, loss 1.9899234771728516
iteration 100, loss 1.878662109375
iteration 200, loss 1.9136220216751099
iteration 300, loss 1.8948744535446167
iteration 400, loss 1.8641026020050049
iteration 500, loss 1.9352524280548096
iteration 600, loss 1.7948299646377563
iteration 700, loss 1.8584336042404175
iteration 800, loss 1.8563578128814697
iteration 0, loss 1.9043446779251099
iteration 100, loss 1.8607139587402344
iteration 200, loss 1.9486563205718994
iteration 300, loss 1.9477450847625732
iteration 400, loss 1.9073783159255981
iteration 500, loss 1.8685394525527954
iteration 600, loss 1.8100266456604004
iteration 700, loss 1.9300893545150757
iteration 800, loss 1.8998587131500244
iteration 0, loss 1.8719618320465088
iteration 100, loss 1.932273507118225
iteration 200, loss 1.856474757194519
iteration 300, loss 1.839475154876709
iteration 400, loss 1.8654710054397583
iteration 500, loss 1.869767427444458
iteration 600, loss 1.8898351192474365
iteration 700, loss 1.925337791442871
iteration 800, loss 1.8362160921096802
iteration 0, loss 1.8507100343704224
iteration 100, loss 1.840549349784851
iteration 200, loss 1.859036922454834
iteration 300, loss 1.9425104856491089
iteration 400, loss 1.753309965133667
iteration 500, loss 1.7841781377792358
iteration 600, loss 1.8063820600509644
iteration 700, loss 1.78941810131073
iteration 800, loss 1.8932039737701416
iteration 0, loss 1.9044867753982544
iteration 100, loss 1.9556176662445068
iteration 200, loss 1.9581116437911987
iteration 300, loss 1.8417028188705444
iteration 400, loss 1.8336913585662842
iteration 500, loss 1.8326044082641602
iteration 600, loss 1.8720777034759521
iteration 700, loss 1.964135766029358
iteration 800, loss 2.000638961791992
iteration 0, loss 1.8313665390014648
iteration 100, loss 1.8728201389312744
iteration 200, loss 1.901554822921753
iteration 300, loss 1.8890198469161987
iteration 400, loss 1.8038297891616821
iteration 500, loss 1.8619959354400635
iteration 600, loss 1.8306199312210083
iteration 700, loss 1.9311392307281494
iteration 800, loss 1.877615213394165
iteration 0, loss 1.7896496057510376
iteration 100, loss 1.8588883876800537
iteration 200, loss 1.8482868671417236
iteration 300, loss 1.9272751808166504
iteration 400, loss 1.8860794305801392
iteration 500, loss 1.8305723667144775
iteration 600, loss 1.8934447765350342
iteration 700, loss 1.9374721050262451
iteration 800, loss 1.929667353630066
iteration 0, loss 1.84398353099823
iteration 100, loss 1.8249238729476929
iteration 200, loss 1.874914288520813
iteration 300, loss 1.7983604669570923
iteration 400, loss 1.8676211833953857
iteration 500, loss 1.8402057886123657
iteration 600, loss 1.9164077043533325
iteration 700, loss 1.7911280393600464
iteration 800, loss 1.8395179510116577
iteration 0, loss 1.92777681350708
iteration 100, loss 1.7591747045516968
iteration 200, loss 1.8231209516525269
iteration 300, loss 1.8580057621002197
iteration 400, loss 1.817155122756958
iteration 500, loss 1.8584041595458984
iteration 600, loss 1.8726747035980225
iteration 700, loss 1.818302035331726
iteration 800, loss 1.9397218227386475
iteration 0, loss 1.83982515335083
iteration 100, loss 1.781794548034668
iteration 200, loss 1.8877582550048828
iteration 300, loss 1.832419991493225
iteration 400, loss 1.8778492212295532
iteration 500, loss 1.8867295980453491
iteration 600, loss 1.8671730756759644
iteration 700, loss 1.9457770586013794
iteration 800, loss 1.8374383449554443
iteration 0, loss 1.775592565536499
iteration 100, loss 1.9120436906814575
iteration 200, loss 1.9083698987960815
iteration 300, loss 1.842325210571289
iteration 400, loss 1.783947467803955
iteration 500, loss 1.8407078981399536
iteration 600, loss 1.9028441905975342
iteration 700, loss 1.8528627157211304
iteration 800, loss 1.8954421281814575
iteration 0, loss 1.9188567399978638
iteration 100, loss 1.8004882335662842
iteration 200, loss 1.8096736669540405
iteration 300, loss 1.8389294147491455
iteration 400, loss 1.8323516845703125
iteration 500, loss 1.8273582458496094
iteration 600, loss 1.8670135736465454
iteration 700, loss 1.865887999534607
iteration 800, loss 2.02374529838562
iteration 0, loss 1.7918771505355835
iteration 100, loss 1.8908382654190063
iteration 200, loss 1.9176336526870728
iteration 300, loss 1.8847733736038208
iteration 400, loss 1.857376217842102
iteration 500, loss 1.8799744844436646
iteration 600, loss 1.8727165460586548
iteration 700, loss 1.7642995119094849
iteration 800, loss 1.7785649299621582
iteration 0, loss 1.9572343826293945
iteration 100, loss 1.8562241792678833
iteration 200, loss 1.7973636388778687
iteration 300, loss 1.8501367568969727
iteration 400, loss 1.8386179208755493
iteration 500, loss 1.8588964939117432
iteration 600, loss 1.8438931703567505
iteration 700, loss 1.8391257524490356
iteration 800, loss 1.7660866975784302
iteration 0, loss 1.7761907577514648
iteration 100, loss 1.8453692197799683
iteration 200, loss 1.8462964296340942
iteration 300, loss 1.9188357591629028
iteration 400, loss 1.8658758401870728
iteration 500, loss 1.9444520473480225
iteration 600, loss 1.8237682580947876
iteration 700, loss 1.8553889989852905
iteration 800, loss 1.8858386278152466
iteration 0, loss 1.8518167734146118
iteration 100, loss 1.8968913555145264
iteration 200, loss 1.9128901958465576
iteration 300, loss 1.844900131225586
iteration 400, loss 1.8986737728118896
iteration 500, loss 1.8932005167007446
iteration 600, loss 1.8071637153625488
iteration 700, loss 1.813775658607483
iteration 800, loss 1.9000753164291382
iteration 0, loss 1.9359846115112305
iteration 100, loss 1.8888353109359741
iteration 200, loss 1.8774985074996948
iteration 300, loss 1.8510663509368896
iteration 400, loss 1.923420786857605
iteration 500, loss 1.8277592658996582
iteration 600, loss 1.8478848934173584
iteration 700, loss 1.8859031200408936
iteration 800, loss 1.8400307893753052
iteration 0, loss 1.7904512882232666
iteration 100, loss 1.9325693845748901
iteration 200, loss 1.8817986249923706
iteration 300, loss 1.8978430032730103
iteration 400, loss 1.9135680198669434
iteration 500, loss 1.8321044445037842
iteration 600, loss 1.8293248414993286
iteration 700, loss 1.8651537895202637
iteration 800, loss 1.8813005685806274
iteration 0, loss 1.8575084209442139
iteration 100, loss 1.879210114479065
iteration 200, loss 1.763299822807312
iteration 300, loss 1.9133598804473877
iteration 400, loss 1.8111016750335693
iteration 500, loss 1.8032045364379883
iteration 600, loss 1.8927738666534424
iteration 700, loss 1.8286062479019165
iteration 800, loss 1.8927847146987915
fold 1 accuracy: 0.4297142857142857
iteration 0, loss 1.8938276767730713
iteration 100, loss 1.8928035497665405
iteration 200, loss 1.8915891647338867
iteration 300, loss 1.79192054271698
iteration 400, loss 1.7577439546585083
iteration 500, loss 1.8230338096618652
iteration 600, loss 1.8976911306381226
iteration 700, loss 1.871062159538269
iteration 800, loss 1.8896523714065552
iteration 0, loss 1.9142298698425293
iteration 100, loss 1.781853199005127
iteration 200, loss 2.0137557983398438
iteration 300, loss 1.8731316328048706
iteration 400, loss 1.9643131494522095
iteration 500, loss 1.8207675218582153
iteration 600, loss 1.847851037979126
iteration 700, loss 1.8595337867736816
iteration 800, loss 1.8869450092315674
iteration 0, loss 1.8564404249191284
iteration 100, loss 1.881333589553833
iteration 200, loss 1.8362747430801392
iteration 300, loss 1.8739961385726929
iteration 400, loss 1.9400802850723267
iteration 500, loss 1.7920295000076294
iteration 600, loss 1.8762331008911133
iteration 700, loss 1.7779943943023682
iteration 800, loss 1.7810399532318115
iteration 0, loss 1.8868792057037354
iteration 100, loss 1.9116369485855103
iteration 200, loss 1.9119404554367065
iteration 300, loss 1.846359133720398
iteration 400, loss 1.9223687648773193
iteration 500, loss 1.9012048244476318
iteration 600, loss 1.7842450141906738
iteration 700, loss 1.8432637453079224
iteration 800, loss 1.8764530420303345
iteration 0, loss 1.8768943548202515
iteration 100, loss 1.873916745185852
iteration 200, loss 1.8839452266693115
iteration 300, loss 1.8978291749954224
iteration 400, loss 1.8655327558517456
iteration 500, loss 1.9227222204208374
iteration 600, loss 1.8637995719909668
iteration 700, loss 1.8684759140014648
iteration 800, loss 1.7993872165679932
iteration 0, loss 1.8598657846450806
iteration 100, loss 1.8552765846252441
iteration 200, loss 1.9153040647506714
iteration 300, loss 1.8872747421264648
iteration 400, loss 1.8425668478012085
iteration 500, loss 1.8603858947753906
iteration 600, loss 1.8556160926818848
iteration 700, loss 1.8441603183746338
iteration 800, loss 1.8389651775360107
iteration 0, loss 1.8801052570343018
iteration 100, loss 1.9399563074111938
iteration 200, loss 1.8448821306228638
iteration 300, loss 1.974418044090271
iteration 400, loss 1.8082901239395142
iteration 500, loss 1.8846062421798706
iteration 600, loss 1.7594645023345947
iteration 700, loss 1.8016703128814697
iteration 800, loss 1.8723498582839966
iteration 0, loss 1.9057915210723877
iteration 100, loss 1.8910460472106934
iteration 200, loss 1.8886184692382812
iteration 300, loss 1.8942190408706665
iteration 400, loss 1.8667305707931519
iteration 500, loss 1.867153286933899
iteration 600, loss 1.8119823932647705
iteration 700, loss 1.8813990354537964
iteration 800, loss 1.8978596925735474
iteration 0, loss 1.7958674430847168
iteration 100, loss 1.9209158420562744
iteration 200, loss 1.8211387395858765
iteration 300, loss 1.8045153617858887
iteration 400, loss 1.8570780754089355
iteration 500, loss 1.89017653465271
iteration 600, loss 1.9036496877670288
iteration 700, loss 1.8553448915481567
iteration 800, loss 1.8332479000091553
iteration 0, loss 1.7492873668670654
iteration 100, loss 1.852339506149292
iteration 200, loss 1.7872051000595093
iteration 300, loss 1.8361574411392212
iteration 400, loss 1.9120702743530273
iteration 500, loss 1.8948085308074951
iteration 600, loss 1.8530151844024658
iteration 700, loss 1.8875887393951416
iteration 800, loss 1.8755244016647339
iteration 0, loss 1.8777928352355957
iteration 100, loss 1.8639094829559326
iteration 200, loss 1.868715524673462
iteration 300, loss 1.9810296297073364
iteration 400, loss 1.8514753580093384
iteration 500, loss 1.7825264930725098
iteration 600, loss 1.910591959953308
iteration 700, loss 1.9274431467056274
iteration 800, loss 1.8682066202163696
iteration 0, loss 1.7854454517364502
iteration 100, loss 1.8492258787155151
iteration 200, loss 1.9051964282989502
iteration 300, loss 1.7561622858047485
iteration 400, loss 1.846665382385254
iteration 500, loss 1.83538019657135
iteration 600, loss 1.8769221305847168
iteration 700, loss 1.840442180633545
iteration 800, loss 1.9089093208312988
iteration 0, loss 1.9162557125091553
iteration 100, loss 1.8918402194976807
iteration 200, loss 1.8222800493240356
iteration 300, loss 1.9088674783706665
iteration 400, loss 1.8506230115890503
iteration 500, loss 1.894593596458435
iteration 600, loss 1.9133225679397583
iteration 700, loss 1.7735891342163086
iteration 800, loss 1.8824907541275024
iteration 0, loss 1.8461480140686035
iteration 100, loss 1.8006622791290283
iteration 200, loss 1.8337215185165405
iteration 300, loss 1.9215143918991089
iteration 400, loss 1.823835849761963
iteration 500, loss 1.8476642370224
iteration 600, loss 1.887988805770874
iteration 700, loss 1.9001760482788086
iteration 800, loss 1.8338497877120972
iteration 0, loss 1.7904229164123535
iteration 100, loss 1.874551773071289
iteration 200, loss 1.7754614353179932
iteration 300, loss 1.9229872226715088
iteration 400, loss 1.8690698146820068
iteration 500, loss 1.8543493747711182
iteration 600, loss 1.8020617961883545
iteration 700, loss 1.8668451309204102
iteration 800, loss 1.9272781610488892
iteration 0, loss 1.9346612691879272
iteration 100, loss 1.9577844142913818
iteration 200, loss 1.837165355682373
iteration 300, loss 1.9262654781341553
iteration 400, loss 1.822676658630371
iteration 500, loss 1.8047794103622437
iteration 600, loss 1.799592137336731
iteration 700, loss 1.8720130920410156
iteration 800, loss 1.8358829021453857
iteration 0, loss 1.9876220226287842
iteration 100, loss 1.8569395542144775
iteration 200, loss 1.9196852445602417
iteration 300, loss 1.887844204902649
iteration 400, loss 1.8300683498382568
iteration 500, loss 1.8244608640670776
iteration 600, loss 1.9209399223327637
iteration 700, loss 1.9475034475326538
iteration 800, loss 1.8929623365402222
iteration 0, loss 1.8953595161437988
iteration 100, loss 1.9041283130645752
iteration 200, loss 1.832844614982605
iteration 300, loss 1.9786208868026733
iteration 400, loss 1.8888300657272339
iteration 500, loss 1.8653478622436523
iteration 600, loss 1.8180322647094727
iteration 700, loss 1.9077863693237305
iteration 800, loss 1.9129993915557861
iteration 0, loss 1.8915284872055054
iteration 100, loss 1.758745551109314
iteration 200, loss 1.8506649732589722
iteration 300, loss 1.8189504146575928
iteration 400, loss 1.8427224159240723
iteration 500, loss 1.8876193761825562
iteration 600, loss 1.8804421424865723
iteration 700, loss 1.8652658462524414
iteration 800, loss 1.9337303638458252
iteration 0, loss 1.774756669998169
iteration 100, loss 1.9595425128936768
iteration 200, loss 1.857608437538147
iteration 300, loss 1.960909366607666
iteration 400, loss 1.9061821699142456
iteration 500, loss 1.872624397277832
iteration 600, loss 1.8730310201644897
iteration 700, loss 1.8828799724578857
iteration 800, loss 1.8156071901321411
iteration 0, loss 1.9399845600128174
iteration 100, loss 1.8227609395980835
iteration 200, loss 1.8142569065093994
iteration 300, loss 1.8303738832473755
iteration 400, loss 1.8279513120651245
iteration 500, loss 1.8864281177520752
iteration 600, loss 1.8988505601882935
iteration 700, loss 1.893903136253357
iteration 800, loss 1.793019413948059
iteration 0, loss 1.8982431888580322
iteration 100, loss 1.9065693616867065
iteration 200, loss 1.8362774848937988
iteration 300, loss 1.8856414556503296
iteration 400, loss 1.8347315788269043
iteration 500, loss 1.7795554399490356
iteration 600, loss 1.9115326404571533
iteration 700, loss 1.8041657209396362
iteration 800, loss 1.8416001796722412
iteration 0, loss 1.905304193496704
iteration 100, loss 1.841614842414856
iteration 200, loss 1.842539668083191
iteration 300, loss 1.8361033201217651
iteration 400, loss 1.8281402587890625
iteration 500, loss 1.9024767875671387
iteration 600, loss 1.9396171569824219
iteration 700, loss 1.891129970550537
iteration 800, loss 1.8359829187393188
iteration 0, loss 1.8180737495422363
iteration 100, loss 1.80503249168396
iteration 200, loss 1.8562458753585815
iteration 300, loss 1.8285908699035645
iteration 400, loss 1.8958356380462646
iteration 500, loss 1.8767951726913452
iteration 600, loss 1.8956875801086426
iteration 700, loss 1.8531171083450317
iteration 800, loss 1.8302839994430542
iteration 0, loss 1.8378956317901611
iteration 100, loss 1.859600305557251
iteration 200, loss 1.8396903276443481
iteration 300, loss 1.8464345932006836
iteration 400, loss 1.7985434532165527
iteration 500, loss 1.9387508630752563
iteration 600, loss 1.8085861206054688
iteration 700, loss 1.807584524154663
iteration 800, loss 1.8283002376556396
iteration 0, loss 1.9288053512573242
iteration 100, loss 1.9617961645126343
iteration 200, loss 1.8776601552963257
iteration 300, loss 1.8485945463180542
iteration 400, loss 1.8691192865371704
iteration 500, loss 1.8928873538970947
iteration 600, loss 1.8905692100524902
iteration 700, loss 1.8827552795410156
iteration 800, loss 1.778601884841919
iteration 0, loss 1.8469529151916504
iteration 100, loss 1.816739797592163
iteration 200, loss 1.86925208568573
iteration 300, loss 1.8108255863189697
iteration 400, loss 1.9551441669464111
iteration 500, loss 1.8859492540359497
iteration 600, loss 1.7923609018325806
iteration 700, loss 1.8225857019424438
iteration 800, loss 1.8933873176574707
iteration 0, loss 1.9205061197280884
iteration 100, loss 1.833769679069519
iteration 200, loss 1.805983543395996
iteration 300, loss 1.9173070192337036
iteration 400, loss 1.852538824081421
iteration 500, loss 1.817517876625061
iteration 600, loss 1.9175342321395874
iteration 700, loss 1.8754346370697021
iteration 800, loss 1.8067196607589722
iteration 0, loss 1.8213729858398438
iteration 100, loss 1.816760778427124
iteration 200, loss 1.8329620361328125
iteration 300, loss 1.8256628513336182
iteration 400, loss 1.9179892539978027
iteration 500, loss 1.9348812103271484
iteration 600, loss 1.8734097480773926
iteration 700, loss 1.9245665073394775
iteration 800, loss 1.8288187980651855
iteration 0, loss 1.809058666229248
iteration 100, loss 1.8469929695129395
iteration 200, loss 1.855609655380249
iteration 300, loss 1.8456971645355225
iteration 400, loss 1.8634469509124756
iteration 500, loss 1.8938900232315063
iteration 600, loss 1.8200771808624268
iteration 700, loss 1.858564853668213
iteration 800, loss 1.9105894565582275
iteration 0, loss 1.7933180332183838
iteration 100, loss 1.9026912450790405
iteration 200, loss 1.8522566556930542
iteration 300, loss 1.837654948234558
iteration 400, loss 1.8566240072250366
iteration 500, loss 1.8610730171203613
iteration 600, loss 1.908160924911499
iteration 700, loss 1.9334180355072021
iteration 800, loss 1.8381280899047852
iteration 0, loss 1.862808346748352
iteration 100, loss 1.8211162090301514
iteration 200, loss 1.8015211820602417
iteration 300, loss 1.8784816265106201
iteration 400, loss 1.824297308921814
iteration 500, loss 1.8680901527404785
iteration 600, loss 1.889391303062439
iteration 700, loss 1.93205726146698
iteration 800, loss 1.8283716440200806
iteration 0, loss 1.8640429973602295
iteration 100, loss 1.922640085220337
iteration 200, loss 1.928493857383728
iteration 300, loss 1.804677128791809
iteration 400, loss 1.787733554840088
iteration 500, loss 1.9362149238586426
iteration 600, loss 1.8287508487701416
iteration 700, loss 1.825257420539856
iteration 800, loss 1.8118705749511719
iteration 0, loss 1.8515585660934448
iteration 100, loss 1.8852813243865967
iteration 200, loss 1.8393604755401611
iteration 300, loss 1.8344342708587646
iteration 400, loss 1.876182198524475
iteration 500, loss 1.7766633033752441
iteration 600, loss 1.8312644958496094
iteration 700, loss 1.8476767539978027
iteration 800, loss 1.8436709642410278
iteration 0, loss 1.8488147258758545
iteration 100, loss 1.8194972276687622
iteration 200, loss 1.8144490718841553
iteration 300, loss 1.8493130207061768
iteration 400, loss 1.8314982652664185
iteration 500, loss 1.8509835004806519
iteration 600, loss 1.8458234071731567
iteration 700, loss 1.9429746866226196
iteration 800, loss 1.8599088191986084
iteration 0, loss 1.8379664421081543
iteration 100, loss 1.8305137157440186
iteration 200, loss 1.9260356426239014
iteration 300, loss 1.878075122833252
iteration 400, loss 1.8485044240951538
iteration 500, loss 1.7854199409484863
iteration 600, loss 1.838921308517456
iteration 700, loss 1.869982361793518
iteration 800, loss 1.855470895767212
iteration 0, loss 1.833467721939087
iteration 100, loss 1.8747681379318237
iteration 200, loss 1.9288041591644287
iteration 300, loss 1.8360952138900757
iteration 400, loss 1.8539185523986816
iteration 500, loss 1.8010141849517822
iteration 600, loss 1.8512574434280396
iteration 700, loss 1.8831956386566162
iteration 800, loss 1.9709781408309937
iteration 0, loss 1.8100557327270508
iteration 100, loss 1.8374087810516357
iteration 200, loss 1.9307760000228882
iteration 300, loss 1.8739385604858398
iteration 400, loss 1.855974793434143
iteration 500, loss 1.8866254091262817
iteration 600, loss 1.876610279083252
iteration 700, loss 1.9157289266586304
iteration 800, loss 1.8561360836029053
iteration 0, loss 1.932870864868164
iteration 100, loss 1.8262709379196167
iteration 200, loss 1.910181999206543
iteration 300, loss 1.8753093481063843
iteration 400, loss 1.8878757953643799
iteration 500, loss 1.843928575515747
iteration 600, loss 1.8073046207427979
iteration 700, loss 1.7920459508895874
iteration 800, loss 1.7831748723983765
iteration 0, loss 1.8560600280761719
iteration 100, loss 1.8456344604492188
iteration 200, loss 1.7807068824768066
iteration 300, loss 1.83419930934906
iteration 400, loss 1.8004906177520752
iteration 500, loss 1.8331953287124634
iteration 600, loss 1.8521168231964111
iteration 700, loss 1.8655153512954712
iteration 800, loss 1.946456789970398
iteration 0, loss 1.9050201177597046
iteration 100, loss 1.7671109437942505
iteration 200, loss 1.796595811843872
iteration 300, loss 1.864250659942627
iteration 400, loss 1.8558950424194336
iteration 500, loss 1.9262490272521973
iteration 600, loss 1.825840711593628
iteration 700, loss 1.7898943424224854
iteration 800, loss 1.934296727180481
iteration 0, loss 1.9086426496505737
iteration 100, loss 1.8237978219985962
iteration 200, loss 1.8729016780853271
iteration 300, loss 1.8942545652389526
iteration 400, loss 1.838381290435791
iteration 500, loss 1.892831802368164
iteration 600, loss 1.8561896085739136
iteration 700, loss 1.7907651662826538
iteration 800, loss 1.7974797487258911
iteration 0, loss 1.951117753982544
iteration 100, loss 1.7933228015899658
iteration 200, loss 1.7359622716903687
iteration 300, loss 1.9321985244750977
iteration 400, loss 1.840641975402832
iteration 500, loss 1.8936350345611572
iteration 600, loss 1.8984390497207642
iteration 700, loss 1.8972039222717285
iteration 800, loss 1.7740813493728638
iteration 0, loss 1.865968942642212
iteration 100, loss 1.862799048423767
iteration 200, loss 1.868688702583313
iteration 300, loss 1.8045574426651
iteration 400, loss 1.818848729133606
iteration 500, loss 1.897456169128418
iteration 600, loss 1.9691400527954102
iteration 700, loss 1.8380368947982788
iteration 800, loss 1.8624436855316162
iteration 0, loss 1.88103187084198
iteration 100, loss 1.9001383781433105
iteration 200, loss 1.8908946514129639
iteration 300, loss 2.021000623703003
iteration 400, loss 1.897511601448059
iteration 500, loss 1.8532402515411377
iteration 600, loss 1.8646005392074585
iteration 700, loss 1.8647292852401733
iteration 800, loss 1.9370582103729248
iteration 0, loss 1.8057565689086914
iteration 100, loss 1.8014470338821411
iteration 200, loss 1.768140435218811
iteration 300, loss 1.801213026046753
iteration 400, loss 1.8574799299240112
iteration 500, loss 1.8434265851974487
iteration 600, loss 1.9404438734054565
iteration 700, loss 1.8206228017807007
iteration 800, loss 1.8945971727371216
iteration 0, loss 1.8946408033370972
iteration 100, loss 1.938128113746643
iteration 200, loss 1.8612459897994995
iteration 300, loss 1.8384275436401367
iteration 400, loss 1.7537297010421753
iteration 500, loss 1.8235948085784912
iteration 600, loss 1.8278318643569946
iteration 700, loss 1.8094651699066162
iteration 800, loss 1.8784513473510742
iteration 0, loss 1.8878446817398071
iteration 100, loss 1.8233367204666138
iteration 200, loss 1.9651015996932983
iteration 300, loss 1.8816817998886108
iteration 400, loss 1.8308192491531372
iteration 500, loss 1.8022898435592651
iteration 600, loss 1.8573921918869019
iteration 700, loss 1.8160821199417114
iteration 800, loss 1.8379563093185425
iteration 0, loss 1.8419299125671387
iteration 100, loss 1.8636953830718994
iteration 200, loss 1.8438023328781128
iteration 300, loss 1.9169209003448486
iteration 400, loss 1.8661646842956543
iteration 500, loss 1.8679568767547607
iteration 600, loss 1.8538912534713745
iteration 700, loss 1.81002676486969
iteration 800, loss 1.845731496810913
iteration 0, loss 1.8375029563903809
iteration 100, loss 1.869953989982605
iteration 200, loss 1.8873504400253296
iteration 300, loss 1.795944094657898
iteration 400, loss 1.8706696033477783
iteration 500, loss 1.886030912399292
iteration 600, loss 1.8177727460861206
iteration 700, loss 1.943589210510254
iteration 800, loss 1.8579974174499512
fold 2 accuracy: 0.41528571428571426
iteration 0, loss 1.8293426036834717
iteration 100, loss 1.8988326787948608
iteration 200, loss 1.760009527206421
iteration 300, loss 1.8622198104858398
iteration 400, loss 1.9388564825057983
iteration 500, loss 1.7692537307739258
iteration 600, loss 1.8449891805648804
iteration 700, loss 1.8593597412109375
iteration 800, loss 1.894442081451416
iteration 0, loss 1.8695635795593262
iteration 100, loss 1.840238332748413
iteration 200, loss 1.8763751983642578
iteration 300, loss 1.9136221408843994
iteration 400, loss 1.8230223655700684
iteration 500, loss 1.8660027980804443
iteration 600, loss 1.8750436305999756
iteration 700, loss 1.8624989986419678
iteration 800, loss 1.7547028064727783
iteration 0, loss 1.886050820350647
iteration 100, loss 1.801344633102417
iteration 200, loss 1.8472789525985718
iteration 300, loss 1.9039347171783447
iteration 400, loss 1.8806581497192383
iteration 500, loss 1.925424337387085
iteration 600, loss 1.8054243326187134
iteration 700, loss 1.9243296384811401
iteration 800, loss 1.8861937522888184
iteration 0, loss 1.8905889987945557
iteration 100, loss 1.7889575958251953
iteration 200, loss 1.90678071975708
iteration 300, loss 1.84466552734375
iteration 400, loss 1.8368158340454102
iteration 500, loss 1.8098982572555542
iteration 600, loss 1.887866735458374
iteration 700, loss 1.846431851387024
iteration 800, loss 1.8904482126235962
iteration 0, loss 1.8903590440750122
iteration 100, loss 1.8634974956512451
iteration 200, loss 1.7960257530212402
iteration 300, loss 1.8994319438934326
iteration 400, loss 1.8907891511917114
iteration 500, loss 1.862099289894104
iteration 600, loss 1.7862839698791504
iteration 700, loss 1.887803077697754
iteration 800, loss 1.8867615461349487
iteration 0, loss 1.82746422290802
iteration 100, loss 1.716055989265442
iteration 200, loss 1.7811763286590576
iteration 300, loss 1.7975894212722778
iteration 400, loss 1.8426743745803833
iteration 500, loss 1.825009822845459
iteration 600, loss 1.8539659976959229
iteration 700, loss 1.8947397470474243
iteration 800, loss 1.8101860284805298
iteration 0, loss 1.810789942741394
iteration 100, loss 1.8179676532745361
iteration 200, loss 1.9292800426483154
iteration 300, loss 1.8623464107513428
iteration 400, loss 1.811141848564148
iteration 500, loss 1.8929955959320068
iteration 600, loss 1.9338922500610352
iteration 700, loss 1.8233230113983154
iteration 800, loss 1.8805018663406372
iteration 0, loss 1.8678529262542725
iteration 100, loss 1.8651152849197388
iteration 200, loss 1.9621731042861938
iteration 300, loss 1.8629311323165894
iteration 400, loss 1.807830572128296
iteration 500, loss 1.948599100112915
iteration 600, loss 1.83394455909729
iteration 700, loss 1.8457529544830322
iteration 800, loss 1.9887733459472656
iteration 0, loss 1.8350592851638794
iteration 100, loss 1.9056421518325806
iteration 200, loss 1.8415586948394775
iteration 300, loss 1.8944510221481323
iteration 400, loss 1.8152239322662354
iteration 500, loss 1.8904411792755127
iteration 600, loss 1.821353793144226
iteration 700, loss 1.840982437133789
iteration 800, loss 1.962981104850769
iteration 0, loss 1.8882381916046143
iteration 100, loss 1.8571316003799438
iteration 200, loss 1.8451807498931885
iteration 300, loss 1.844364047050476
iteration 400, loss 1.881932258605957
iteration 500, loss 1.8230122327804565
iteration 600, loss 1.8039058446884155
iteration 700, loss 1.8613834381103516
iteration 800, loss 1.8680630922317505
iteration 0, loss 1.9086401462554932
iteration 100, loss 1.8347570896148682
iteration 200, loss 1.881590485572815
iteration 300, loss 1.8928184509277344
iteration 400, loss 1.8910677433013916
iteration 500, loss 1.8324071168899536
iteration 600, loss 1.8426932096481323
iteration 700, loss 1.8754479885101318
iteration 800, loss 1.8546700477600098
iteration 0, loss 1.8201251029968262
iteration 100, loss 1.808679223060608
iteration 200, loss 1.8637961149215698
iteration 300, loss 1.788000464439392
iteration 400, loss 1.8165271282196045
iteration 500, loss 1.8508635759353638
iteration 600, loss 1.8555097579956055
iteration 700, loss 1.924587607383728
iteration 800, loss 1.860643982887268
iteration 0, loss 1.8514026403427124
iteration 100, loss 1.871938705444336
iteration 200, loss 1.8408312797546387
iteration 300, loss 1.7992044687271118
iteration 400, loss 1.8207242488861084
iteration 500, loss 1.81528639793396
iteration 600, loss 1.8241227865219116
iteration 700, loss 1.8434468507766724
iteration 800, loss 1.826487421989441
iteration 0, loss 1.898992657661438
iteration 100, loss 1.848443865776062
iteration 200, loss 1.9154140949249268
iteration 300, loss 1.8308144807815552
iteration 400, loss 1.8298218250274658
iteration 500, loss 1.9242372512817383
iteration 600, loss 1.8649488687515259
iteration 700, loss 1.88588547706604
iteration 800, loss 1.887017846107483
iteration 0, loss 1.942081332206726
iteration 100, loss 1.9480297565460205
iteration 200, loss 1.9332420825958252
iteration 300, loss 1.8063998222351074
iteration 400, loss 1.8928272724151611
iteration 500, loss 1.882091760635376
iteration 600, loss 1.8683977127075195
iteration 700, loss 1.8521052598953247
iteration 800, loss 1.8204542398452759
iteration 0, loss 1.840782642364502
iteration 100, loss 1.8676562309265137
iteration 200, loss 1.840003252029419
iteration 300, loss 1.8466448783874512
iteration 400, loss 1.8219859600067139
iteration 500, loss 1.818320870399475
iteration 600, loss 1.90447199344635
iteration 700, loss 1.7894136905670166
iteration 800, loss 1.8527154922485352
iteration 0, loss 1.8329635858535767
iteration 100, loss 1.816385269165039
iteration 200, loss 1.8884162902832031
iteration 300, loss 1.8357897996902466
iteration 400, loss 1.827791452407837
iteration 500, loss 1.867517352104187
iteration 600, loss 1.8365318775177002
iteration 700, loss 1.807641625404358
iteration 800, loss 1.8856654167175293
iteration 0, loss 1.7574667930603027
iteration 100, loss 1.8766669034957886
iteration 200, loss 1.8442227840423584
iteration 300, loss 1.8996286392211914
iteration 400, loss 1.877038836479187
iteration 500, loss 1.9054157733917236
iteration 600, loss 1.8439044952392578
iteration 700, loss 1.873555064201355
iteration 800, loss 1.8547499179840088
iteration 0, loss 1.8052115440368652
iteration 100, loss 1.785739541053772
iteration 200, loss 1.8626830577850342
iteration 300, loss 1.8262383937835693
iteration 400, loss 1.7935211658477783
iteration 500, loss 1.9053592681884766
iteration 600, loss 1.8609325885772705
iteration 700, loss 1.7578215599060059
iteration 800, loss 1.7863792181015015
iteration 0, loss 1.919523000717163
iteration 100, loss 1.8343602418899536
iteration 200, loss 2.0006051063537598
iteration 300, loss 1.9032831192016602
iteration 400, loss 1.8093091249465942
iteration 500, loss 1.856521487236023
iteration 600, loss 1.8007093667984009
iteration 700, loss 1.8490829467773438
iteration 800, loss 1.8802525997161865
iteration 0, loss 1.8589067459106445
iteration 100, loss 1.837588906288147
iteration 200, loss 1.8587355613708496
iteration 300, loss 1.8666163682937622
iteration 400, loss 1.8352550268173218
iteration 500, loss 1.8834733963012695
iteration 600, loss 1.8871484994888306
iteration 700, loss 1.8516005277633667
iteration 800, loss 1.916288137435913
iteration 0, loss 1.8981194496154785
iteration 100, loss 1.8394221067428589
iteration 200, loss 1.8520795106887817
iteration 300, loss 1.8870015144348145
iteration 400, loss 1.910885214805603
iteration 500, loss 1.7821069955825806
iteration 600, loss 1.8465399742126465
iteration 700, loss 1.8896853923797607
iteration 800, loss 1.8551726341247559
iteration 0, loss 1.8589928150177002
iteration 100, loss 1.8284926414489746
iteration 200, loss 1.8289237022399902
iteration 300, loss 1.8601746559143066
iteration 400, loss 1.899478554725647
iteration 500, loss 1.8722091913223267
iteration 600, loss 1.9357736110687256
iteration 700, loss 1.8354822397232056
iteration 800, loss 1.866308331489563
iteration 0, loss 1.874703049659729
iteration 100, loss 1.838006854057312
iteration 200, loss 1.8663209676742554
iteration 300, loss 1.827131986618042
iteration 400, loss 1.8149535655975342
iteration 500, loss 1.8808356523513794
iteration 600, loss 1.8819339275360107
iteration 700, loss 1.8204630613327026
iteration 800, loss 1.9083378314971924
iteration 0, loss 1.8461946249008179
iteration 100, loss 1.8935033082962036
iteration 200, loss 1.9190176725387573
iteration 300, loss 1.8531410694122314
iteration 400, loss 1.827634572982788
iteration 500, loss 1.7831350564956665
iteration 600, loss 1.737716794013977
iteration 700, loss 1.819922924041748
iteration 800, loss 1.8477317094802856
iteration 0, loss 1.8400394916534424
iteration 100, loss 1.8618955612182617
iteration 200, loss 1.8177337646484375
iteration 300, loss 1.8296598196029663
iteration 400, loss 1.8631987571716309
iteration 500, loss 1.909434199333191
iteration 600, loss 1.8236037492752075
iteration 700, loss 1.8747762441635132
iteration 800, loss 1.8004605770111084
iteration 0, loss 1.8636047840118408
iteration 100, loss 1.9425220489501953
iteration 200, loss 1.8523255586624146
iteration 300, loss 1.8676531314849854
iteration 400, loss 1.8048986196517944
iteration 500, loss 1.904500126838684
iteration 600, loss 1.8048160076141357
iteration 700, loss 1.8504726886749268
iteration 800, loss 1.8067190647125244
iteration 0, loss 1.8414905071258545
iteration 100, loss 1.8122129440307617
iteration 200, loss 1.8491841554641724
iteration 300, loss 1.9290074110031128
iteration 400, loss 1.9302518367767334
iteration 500, loss 1.9125146865844727
iteration 600, loss 1.7605555057525635
iteration 700, loss 1.8162240982055664
iteration 800, loss 1.8461956977844238
iteration 0, loss 1.8347443342208862
iteration 100, loss 1.8991551399230957
iteration 200, loss 1.7610353231430054
iteration 300, loss 1.8107023239135742
iteration 400, loss 1.8678022623062134
iteration 500, loss 1.863292932510376
iteration 600, loss 1.8158314228057861
iteration 700, loss 1.8901044130325317
iteration 800, loss 1.9147752523422241
iteration 0, loss 1.836911678314209
iteration 100, loss 1.8236255645751953
iteration 200, loss 1.825904130935669
iteration 300, loss 1.803263783454895
iteration 400, loss 1.9089411497116089
iteration 500, loss 1.7384424209594727
iteration 600, loss 1.9019192457199097
iteration 700, loss 1.8231052160263062
iteration 800, loss 1.8551559448242188
iteration 0, loss 1.9195042848587036
iteration 100, loss 1.8398401737213135
iteration 200, loss 1.8869673013687134
iteration 300, loss 1.9015966653823853
iteration 400, loss 1.763253927230835
iteration 500, loss 1.8769680261611938
iteration 600, loss 1.8249300718307495
iteration 700, loss 1.7537938356399536
iteration 800, loss 1.8301236629486084
iteration 0, loss 1.752065896987915
iteration 100, loss 1.8336613178253174
iteration 200, loss 1.8308742046356201
iteration 300, loss 1.8785967826843262
iteration 400, loss 1.8394935131072998
iteration 500, loss 1.9213937520980835
iteration 600, loss 1.8690481185913086
iteration 700, loss 1.8161118030548096
iteration 800, loss 1.917509913444519
iteration 0, loss 1.8331210613250732
iteration 100, loss 1.8294214010238647
iteration 200, loss 1.9199434518814087
iteration 300, loss 1.8902846574783325
iteration 400, loss 1.8293942213058472
iteration 500, loss 1.9212754964828491
iteration 600, loss 1.803670883178711
iteration 700, loss 1.8021628856658936
iteration 800, loss 1.781306505203247
iteration 0, loss 1.8759548664093018
iteration 100, loss 1.8478091955184937
iteration 200, loss 1.8297697305679321
iteration 300, loss 1.8360835313796997
iteration 400, loss 1.7913827896118164
iteration 500, loss 1.846462607383728
iteration 600, loss 1.946229338645935
iteration 700, loss 1.84807550907135
iteration 800, loss 1.885582447052002
iteration 0, loss 1.8390538692474365
iteration 100, loss 1.8872196674346924
iteration 200, loss 1.8846021890640259
iteration 300, loss 1.7731451988220215
iteration 400, loss 1.8487638235092163
iteration 500, loss 1.964574933052063
iteration 600, loss 1.8709248304367065
iteration 700, loss 1.918179988861084
iteration 800, loss 1.7527916431427002
iteration 0, loss 1.9446035623550415
iteration 100, loss 1.875232219696045
iteration 200, loss 1.8908352851867676
iteration 300, loss 1.8758376836776733
iteration 400, loss 1.8230189085006714
iteration 500, loss 1.8415956497192383
iteration 600, loss 1.9015036821365356
iteration 700, loss 1.804916262626648
iteration 800, loss 1.8585745096206665
iteration 0, loss 1.8009504079818726
iteration 100, loss 1.927062749862671
iteration 200, loss 1.9093855619430542
iteration 300, loss 1.8623501062393188
iteration 400, loss 1.8018652200698853
iteration 500, loss 1.8062623739242554
iteration 600, loss 1.7868398427963257
iteration 700, loss 1.840058445930481
iteration 800, loss 1.8170228004455566
iteration 0, loss 1.8127295970916748
iteration 100, loss 1.8679922819137573
iteration 200, loss 1.815671443939209
iteration 300, loss 1.8125543594360352
iteration 400, loss 1.8590691089630127
iteration 500, loss 1.928842306137085
iteration 600, loss 1.8500750064849854
iteration 700, loss 1.827557921409607
iteration 800, loss 1.819872498512268
iteration 0, loss 1.9053910970687866
iteration 100, loss 1.8479076623916626
iteration 200, loss 1.9056029319763184
iteration 300, loss 1.8744981288909912
iteration 400, loss 1.898245930671692
iteration 500, loss 1.8998005390167236
iteration 600, loss 1.8522955179214478
iteration 700, loss 1.8040870428085327
iteration 800, loss 1.8417749404907227
iteration 0, loss 1.8212640285491943
iteration 100, loss 1.820831298828125
iteration 200, loss 1.8264271020889282
iteration 300, loss 1.8139663934707642
iteration 400, loss 1.833725929260254
iteration 500, loss 1.8238688707351685
iteration 600, loss 1.8153094053268433
iteration 700, loss 1.8635061979293823
iteration 800, loss 1.828528642654419
iteration 0, loss 1.8615859746932983
iteration 100, loss 1.8588054180145264
iteration 200, loss 1.8633739948272705
iteration 300, loss 1.8640599250793457
iteration 400, loss 1.8934694528579712
iteration 500, loss 1.9018797874450684
iteration 600, loss 2.0122008323669434
iteration 700, loss 1.8238753080368042
iteration 800, loss 1.8992993831634521
iteration 0, loss 1.8262351751327515
iteration 100, loss 1.8398516178131104
iteration 200, loss 1.878625512123108
iteration 300, loss 1.806430459022522
iteration 400, loss 1.8387879133224487
iteration 500, loss 1.7709904909133911
iteration 600, loss 1.913346290588379
iteration 700, loss 1.8676387071609497
iteration 800, loss 1.7739139795303345
iteration 0, loss 1.886444091796875
iteration 100, loss 1.8819760084152222
iteration 200, loss 1.8217233419418335
iteration 300, loss 1.869936227798462
iteration 400, loss 1.845496416091919
iteration 500, loss 1.8351857662200928
iteration 600, loss 1.9376177787780762
iteration 700, loss 1.9287296533584595
iteration 800, loss 1.729575753211975
iteration 0, loss 1.8139179944992065
iteration 100, loss 1.7549514770507812
iteration 200, loss 1.8607743978500366
iteration 300, loss 1.8044673204421997
iteration 400, loss 1.7739672660827637
iteration 500, loss 1.8196319341659546
iteration 600, loss 1.8735604286193848
iteration 700, loss 1.839108943939209
iteration 800, loss 1.7715213298797607
iteration 0, loss 1.9256556034088135
iteration 100, loss 1.8879212141036987
iteration 200, loss 1.928261399269104
iteration 300, loss 1.8493680953979492
iteration 400, loss 1.815184473991394
iteration 500, loss 1.7756686210632324
iteration 600, loss 1.8305671215057373
iteration 700, loss 1.792062759399414
iteration 800, loss 1.8110271692276
iteration 0, loss 1.7948853969573975
iteration 100, loss 1.8834285736083984
iteration 200, loss 1.9144915342330933
iteration 300, loss 1.8798991441726685
iteration 400, loss 1.9315539598464966
iteration 500, loss 1.849082112312317
iteration 600, loss 1.8565363883972168
iteration 700, loss 1.8301790952682495
iteration 800, loss 2.030425548553467
iteration 0, loss 1.832228183746338
iteration 100, loss 1.7998483180999756
iteration 200, loss 1.9284414052963257
iteration 300, loss 1.842869758605957
iteration 400, loss 1.9870154857635498
iteration 500, loss 1.8535442352294922
iteration 600, loss 1.8622597455978394
iteration 700, loss 1.814502477645874
iteration 800, loss 1.8593451976776123
iteration 0, loss 1.831384539604187
iteration 100, loss 1.833457112312317
iteration 200, loss 1.8769268989562988
iteration 300, loss 1.8340705633163452
iteration 400, loss 1.8171465396881104
iteration 500, loss 1.7913250923156738
iteration 600, loss 1.897760033607483
iteration 700, loss 1.8892391920089722
iteration 800, loss 1.8550477027893066
iteration 0, loss 1.8487800359725952
iteration 100, loss 1.8994603157043457
iteration 200, loss 1.874488353729248
iteration 300, loss 1.882768154144287
iteration 400, loss 1.8792279958724976
iteration 500, loss 1.7953249216079712
iteration 600, loss 1.8690283298492432
iteration 700, loss 1.8168203830718994
iteration 800, loss 1.819169282913208
iteration 0, loss 1.8057568073272705
iteration 100, loss 1.9370903968811035
iteration 200, loss 1.9159235954284668
iteration 300, loss 1.853511095046997
iteration 400, loss 1.8894673585891724
iteration 500, loss 1.8101154565811157
iteration 600, loss 1.8828295469284058
iteration 700, loss 1.8145649433135986
iteration 800, loss 1.8016029596328735
fold 3 accuracy: 0.4337142857142857
iteration 0, loss 1.9234517812728882
iteration 100, loss 1.8871510028839111
iteration 200, loss 1.7983014583587646
iteration 300, loss 1.8602908849716187
iteration 400, loss 1.9076530933380127
iteration 500, loss 1.9213682413101196
iteration 600, loss 1.9047975540161133
iteration 700, loss 1.8578892946243286
iteration 800, loss 1.8911319971084595
iteration 0, loss 1.9198205471038818
iteration 100, loss 1.8766775131225586
iteration 200, loss 1.9868783950805664
iteration 300, loss 1.9494819641113281
iteration 400, loss 1.8854758739471436
iteration 500, loss 1.8478522300720215
iteration 600, loss 1.8593595027923584
iteration 700, loss 1.857772707939148
iteration 800, loss 1.8733689785003662
iteration 0, loss 1.8458126783370972
iteration 100, loss 1.9761394262313843
iteration 200, loss 1.8728634119033813
iteration 300, loss 1.8908281326293945
iteration 400, loss 1.8224058151245117
iteration 500, loss 1.8315789699554443
iteration 600, loss 1.922958254814148
iteration 700, loss 1.830308198928833
iteration 800, loss 1.7739768028259277
iteration 0, loss 1.803299069404602
iteration 100, loss 1.8335105180740356
iteration 200, loss 1.804694414138794
iteration 300, loss 1.9427354335784912
iteration 400, loss 1.826850175857544
iteration 500, loss 1.8445894718170166
iteration 600, loss 1.8459097146987915
iteration 700, loss 1.7988396883010864
iteration 800, loss 1.8489042520523071
iteration 0, loss 1.9022880792617798
iteration 100, loss 1.862696647644043
iteration 200, loss 1.8786327838897705
iteration 300, loss 1.865712285041809
iteration 400, loss 1.8306634426116943
iteration 500, loss 1.8068139553070068
iteration 600, loss 1.8989006280899048
iteration 700, loss 1.8281158208847046
iteration 800, loss 1.8477741479873657
iteration 0, loss 1.9878365993499756
iteration 100, loss 1.9305368661880493
iteration 200, loss 1.835211157798767
iteration 300, loss 1.797218918800354
iteration 400, loss 1.8364450931549072
iteration 500, loss 1.7728502750396729
iteration 600, loss 1.8375475406646729
iteration 700, loss 1.9543976783752441
iteration 800, loss 1.8435403108596802
iteration 0, loss 1.7789063453674316
iteration 100, loss 1.7976062297821045
iteration 200, loss 1.8898866176605225
iteration 300, loss 1.9624714851379395
iteration 400, loss 1.816620111465454
iteration 500, loss 1.910294771194458
iteration 600, loss 1.7734066247940063
iteration 700, loss 1.8266377449035645
iteration 800, loss 1.8245861530303955
iteration 0, loss 1.8255817890167236
iteration 100, loss 1.9331799745559692
iteration 200, loss 1.8764152526855469
iteration 300, loss 1.9077574014663696
iteration 400, loss 1.814949870109558
iteration 500, loss 1.831931471824646
iteration 600, loss 1.85658597946167
iteration 700, loss 1.8626655340194702
iteration 800, loss 1.9163293838500977
iteration 0, loss 1.7030326128005981
iteration 100, loss 1.8721178770065308
iteration 200, loss 1.901549220085144
iteration 300, loss 1.7665983438491821
iteration 400, loss 1.8703529834747314
iteration 500, loss 1.7817977666854858
iteration 600, loss 1.9221932888031006
iteration 700, loss 1.874692678451538
iteration 800, loss 1.8617920875549316
iteration 0, loss 1.8966829776763916
iteration 100, loss 1.8715684413909912
iteration 200, loss 1.8064942359924316
iteration 300, loss 1.932091474533081
iteration 400, loss 1.8232622146606445
iteration 500, loss 1.9103010892868042
iteration 600, loss 1.8329200744628906
iteration 700, loss 1.7886974811553955
iteration 800, loss 1.8431895971298218
iteration 0, loss 1.847476840019226
iteration 100, loss 1.8336317539215088
iteration 200, loss 1.755134105682373
iteration 300, loss 1.7944098711013794
iteration 400, loss 1.8262614011764526
iteration 500, loss 1.7978746891021729
iteration 600, loss 1.8847692012786865
iteration 700, loss 1.8696621656417847
iteration 800, loss 1.8714964389801025
iteration 0, loss 1.8198436498641968
iteration 100, loss 1.8557312488555908
iteration 200, loss 1.851931095123291
iteration 300, loss 1.9013936519622803
iteration 400, loss 1.8868776559829712
iteration 500, loss 1.8544529676437378
iteration 600, loss 1.9234142303466797
iteration 700, loss 1.8175517320632935
iteration 800, loss 1.8262985944747925
iteration 0, loss 1.8280258178710938
iteration 100, loss 1.8673896789550781
iteration 200, loss 1.8330051898956299
iteration 300, loss 1.904975175857544
iteration 400, loss 1.8589826822280884
iteration 500, loss 1.8321505784988403
iteration 600, loss 1.9061259031295776
iteration 700, loss 1.8556976318359375
iteration 800, loss 1.8414560556411743
iteration 0, loss 1.8299006223678589
iteration 100, loss 1.87277090549469
iteration 200, loss 1.8534257411956787
iteration 300, loss 1.748358964920044
iteration 400, loss 1.8007322549819946
iteration 500, loss 1.8269696235656738
iteration 600, loss 1.8355177640914917
iteration 700, loss 1.8313634395599365
iteration 800, loss 1.8950748443603516
iteration 0, loss 1.7989282608032227
iteration 100, loss 1.8747265338897705
iteration 200, loss 1.8430840969085693
iteration 300, loss 1.9055886268615723
iteration 400, loss 1.8699290752410889
iteration 500, loss 1.8178400993347168
iteration 600, loss 1.8500163555145264
iteration 700, loss 1.8834311962127686
iteration 800, loss 1.8860478401184082
iteration 0, loss 1.8722641468048096
iteration 100, loss 1.8632698059082031
iteration 200, loss 1.8743667602539062
iteration 300, loss 1.8296170234680176
iteration 400, loss 1.8960752487182617
iteration 500, loss 1.8543039560317993
iteration 600, loss 1.808060646057129
iteration 700, loss 1.86546790599823
iteration 800, loss 1.8513671159744263
iteration 0, loss 1.8450974225997925
iteration 100, loss 1.9149975776672363
iteration 200, loss 1.886279582977295
iteration 300, loss 1.845761775970459
iteration 400, loss 1.8690574169158936
iteration 500, loss 1.8953595161437988
iteration 600, loss 1.8569014072418213
iteration 700, loss 1.8459687232971191
iteration 800, loss 1.862484335899353
iteration 0, loss 1.8101545572280884
iteration 100, loss 1.792191982269287
iteration 200, loss 1.863885521888733
iteration 300, loss 1.9203646183013916
iteration 400, loss 1.8290497064590454
iteration 500, loss 1.875245213508606
iteration 600, loss 1.800965666770935
iteration 700, loss 1.9115935564041138
iteration 800, loss 1.8728773593902588
iteration 0, loss 1.806127667427063
iteration 100, loss 1.8465508222579956
iteration 200, loss 1.8955252170562744
iteration 300, loss 1.756238341331482
iteration 400, loss 1.9074642658233643
iteration 500, loss 1.8338840007781982
iteration 600, loss 1.90705144405365
iteration 700, loss 1.8883464336395264
iteration 800, loss 1.8746031522750854
iteration 0, loss 1.894750952720642
iteration 100, loss 1.8956619501113892
iteration 200, loss 1.8782968521118164
iteration 300, loss 1.8314532041549683
iteration 400, loss 1.8518636226654053
iteration 500, loss 1.8832314014434814
iteration 600, loss 1.9075394868850708
iteration 700, loss 1.798646330833435
iteration 800, loss 1.8886122703552246
iteration 0, loss 1.8288929462432861
iteration 100, loss 1.8728351593017578
iteration 200, loss 1.8047902584075928
iteration 300, loss 1.8781310319900513
iteration 400, loss 1.7945834398269653
iteration 500, loss 1.9017565250396729
iteration 600, loss 1.8787481784820557
iteration 700, loss 1.9635192155838013
iteration 800, loss 1.821028470993042
iteration 0, loss 1.8430653810501099
iteration 100, loss 1.7544589042663574
iteration 200, loss 1.783137559890747
iteration 300, loss 1.8543049097061157
iteration 400, loss 1.817459225654602
iteration 500, loss 1.8402316570281982
iteration 600, loss 1.8095424175262451
iteration 700, loss 1.8141019344329834
iteration 800, loss 1.9060097932815552
iteration 0, loss 1.8281556367874146
iteration 100, loss 1.9453500509262085
iteration 200, loss 1.808158278465271
iteration 300, loss 1.8589706420898438
iteration 400, loss 1.8254084587097168
iteration 500, loss 1.962367296218872
iteration 600, loss 1.859623670578003
iteration 700, loss 1.7712178230285645
iteration 800, loss 1.8566641807556152
iteration 0, loss 1.8550324440002441
iteration 100, loss 1.8155211210250854
iteration 200, loss 1.7866871356964111
iteration 300, loss 1.82270085811615
iteration 400, loss 1.865163803100586
iteration 500, loss 1.8286668062210083
iteration 600, loss 1.8838545083999634
iteration 700, loss 1.8304648399353027
iteration 800, loss 1.8435182571411133
iteration 0, loss 1.9255505800247192
iteration 100, loss 1.8514124155044556
iteration 200, loss 1.8115655183792114
iteration 300, loss 1.9033035039901733
iteration 400, loss 1.891971468925476
iteration 500, loss 1.7953444719314575
iteration 600, loss 1.887572169303894
iteration 700, loss 1.8956555128097534
iteration 800, loss 1.8480125665664673
iteration 0, loss 1.8795585632324219
iteration 100, loss 1.9385451078414917
iteration 200, loss 1.8615161180496216
iteration 300, loss 1.880471110343933
iteration 400, loss 1.9240849018096924
iteration 500, loss 1.8600667715072632
iteration 600, loss 1.9265280961990356
iteration 700, loss 1.819714069366455
iteration 800, loss 1.9064587354660034
iteration 0, loss 1.8572742938995361
iteration 100, loss 1.8945739269256592
iteration 200, loss 1.81771719455719
iteration 300, loss 1.8414902687072754
iteration 400, loss 1.7650164365768433
iteration 500, loss 1.8926066160202026
iteration 600, loss 1.836995005607605
iteration 700, loss 1.7654896974563599
iteration 800, loss 1.9006984233856201
iteration 0, loss 1.8845701217651367
iteration 100, loss 1.9144411087036133
iteration 200, loss 1.8305108547210693
iteration 300, loss 1.8484328985214233
iteration 400, loss 1.823486566543579
iteration 500, loss 1.9156293869018555
iteration 600, loss 1.8283259868621826
iteration 700, loss 1.805490493774414
iteration 800, loss 1.8613605499267578
iteration 0, loss 1.8805923461914062
iteration 100, loss 1.9044469594955444
iteration 200, loss 1.9193626642227173
iteration 300, loss 1.8657355308532715
iteration 400, loss 1.9654937982559204
iteration 500, loss 1.8818554878234863
iteration 600, loss 1.8841460943222046
iteration 700, loss 1.896097183227539
iteration 800, loss 1.7912037372589111
iteration 0, loss 1.8367738723754883
iteration 100, loss 1.9092737436294556
iteration 200, loss 1.976426601409912
iteration 300, loss 1.7691370248794556
iteration 400, loss 1.965250849723816
iteration 500, loss 1.8452845811843872
iteration 600, loss 1.9131922721862793
iteration 700, loss 1.884329080581665
iteration 800, loss 1.8154391050338745
iteration 0, loss 1.837658166885376
iteration 100, loss 1.9060337543487549
iteration 200, loss 1.858454704284668
iteration 300, loss 1.7943180799484253
iteration 400, loss 1.866902232170105
iteration 500, loss 1.7906196117401123
iteration 600, loss 1.875095248222351
iteration 700, loss 1.8489985466003418
iteration 800, loss 1.9347326755523682
iteration 0, loss 1.8699941635131836
iteration 100, loss 1.8902772665023804
iteration 200, loss 1.8945419788360596
iteration 300, loss 1.7554932832717896
iteration 400, loss 1.8805270195007324
iteration 500, loss 1.8144999742507935
iteration 600, loss 1.8910017013549805
iteration 700, loss 1.8182350397109985
iteration 800, loss 1.8738157749176025
iteration 0, loss 1.7792551517486572
iteration 100, loss 1.8500009775161743
iteration 200, loss 1.8534808158874512
iteration 300, loss 1.9361066818237305
iteration 400, loss 1.890549659729004
iteration 500, loss 1.7971594333648682
iteration 600, loss 1.8575737476348877
iteration 700, loss 1.7848705053329468
iteration 800, loss 1.8515746593475342
iteration 0, loss 1.7438231706619263
iteration 100, loss 1.8003697395324707
iteration 200, loss 1.7622499465942383
iteration 300, loss 1.8589377403259277
iteration 400, loss 1.8481043577194214
iteration 500, loss 1.8461990356445312
iteration 600, loss 1.8398401737213135
iteration 700, loss 1.7442400455474854
iteration 800, loss 1.8542990684509277
iteration 0, loss 1.7926167249679565
iteration 100, loss 1.809470534324646
iteration 200, loss 1.818081021308899
iteration 300, loss 1.8875333070755005
iteration 400, loss 1.9220428466796875
iteration 500, loss 1.8854838609695435
iteration 600, loss 1.9334346055984497
iteration 700, loss 1.8223339319229126
iteration 800, loss 1.7460198402404785
iteration 0, loss 1.8352279663085938
iteration 100, loss 1.8699424266815186
iteration 200, loss 1.9018566608428955
iteration 300, loss 1.7988046407699585
iteration 400, loss 1.8786300420761108
iteration 500, loss 1.8138450384140015
iteration 600, loss 1.8444651365280151
iteration 700, loss 1.881131649017334
iteration 800, loss 1.8468319177627563
iteration 0, loss 1.8225479125976562
iteration 100, loss 1.8522510528564453
iteration 200, loss 1.8973932266235352
iteration 300, loss 1.773845911026001
iteration 400, loss 1.7500169277191162
iteration 500, loss 1.8577107191085815
iteration 600, loss 1.7974578142166138
iteration 700, loss 1.8424690961837769
iteration 800, loss 1.8667584657669067
iteration 0, loss 1.8010716438293457
iteration 100, loss 1.898135781288147
iteration 200, loss 1.820963740348816
iteration 300, loss 1.876662015914917
iteration 400, loss 1.8214305639266968
iteration 500, loss 1.8040159940719604
iteration 600, loss 1.8064171075820923
iteration 700, loss 1.7950561046600342
iteration 800, loss 1.863827109336853
iteration 0, loss 1.8241925239562988
iteration 100, loss 1.8846384286880493
iteration 200, loss 1.8326882123947144
iteration 300, loss 1.8042900562286377
iteration 400, loss 1.8500734567642212
iteration 500, loss 1.82760488986969
iteration 600, loss 1.8335524797439575
iteration 700, loss 1.8698902130126953
iteration 800, loss 1.7466833591461182
iteration 0, loss 1.8459935188293457
iteration 100, loss 1.8452513217926025
iteration 200, loss 1.8154617547988892
iteration 300, loss 1.8779966831207275
iteration 400, loss 1.888132929801941
iteration 500, loss 1.8789398670196533
iteration 600, loss 1.9012924432754517
iteration 700, loss 1.8236117362976074
iteration 800, loss 1.9913870096206665
iteration 0, loss 1.8726239204406738
iteration 100, loss 1.869776964187622
iteration 200, loss 1.9342024326324463
iteration 300, loss 1.7845832109451294
iteration 400, loss 1.8814870119094849
iteration 500, loss 1.8497486114501953
iteration 600, loss 1.8854777812957764
iteration 700, loss 1.849033236503601
iteration 800, loss 1.7972335815429688
iteration 0, loss 1.8128389120101929
iteration 100, loss 1.835681438446045
iteration 200, loss 1.837496042251587
iteration 300, loss 1.8388166427612305
iteration 400, loss 1.8580588102340698
iteration 500, loss 1.8851410150527954
iteration 600, loss 1.7504844665527344
iteration 700, loss 1.9039896726608276
iteration 800, loss 1.892281174659729
iteration 0, loss 1.7734030485153198
iteration 100, loss 1.908664584159851
iteration 200, loss 1.7929595708847046
iteration 300, loss 1.9535855054855347
iteration 400, loss 1.8834302425384521
iteration 500, loss 1.8900668621063232
iteration 600, loss 1.8698617219924927
iteration 700, loss 1.8196477890014648
iteration 800, loss 1.8390722274780273
iteration 0, loss 1.8597147464752197
iteration 100, loss 1.817474603652954
iteration 200, loss 1.8206262588500977
iteration 300, loss 1.8291538953781128
iteration 400, loss 1.789811134338379
iteration 500, loss 1.825156807899475
iteration 600, loss 1.822117805480957
iteration 700, loss 1.9371248483657837
iteration 800, loss 1.8124033212661743
iteration 0, loss 1.9078023433685303
iteration 100, loss 1.8193365335464478
iteration 200, loss 1.8120954036712646
iteration 300, loss 1.8339297771453857
iteration 400, loss 1.851304054260254
iteration 500, loss 1.804411768913269
iteration 600, loss 1.952008843421936
iteration 700, loss 1.8866968154907227
iteration 800, loss 1.9626343250274658
iteration 0, loss 1.8125059604644775
iteration 100, loss 1.8221549987792969
iteration 200, loss 1.9469493627548218
iteration 300, loss 1.7977341413497925
iteration 400, loss 1.797687292098999
iteration 500, loss 1.818956732749939
iteration 600, loss 1.8806978464126587
iteration 700, loss 1.8054304122924805
iteration 800, loss 1.816153645515442
iteration 0, loss 1.8673951625823975
iteration 100, loss 1.724784255027771
iteration 200, loss 1.827271819114685
iteration 300, loss 1.7873246669769287
iteration 400, loss 1.950209140777588
iteration 500, loss 1.8820253610610962
iteration 600, loss 1.8147687911987305
iteration 700, loss 1.821299433708191
iteration 800, loss 1.8510565757751465
iteration 0, loss 1.862311840057373
iteration 100, loss 1.8672188520431519
iteration 200, loss 1.8257393836975098
iteration 300, loss 1.8573026657104492
iteration 400, loss 1.935050368309021
iteration 500, loss 1.8320657014846802
iteration 600, loss 1.8851168155670166
iteration 700, loss 1.8605180978775024
iteration 800, loss 1.9558228254318237
iteration 0, loss 1.8864030838012695
iteration 100, loss 1.820582628250122
iteration 200, loss 1.8092395067214966
iteration 300, loss 1.8167164325714111
iteration 400, loss 1.8667062520980835
iteration 500, loss 1.903070092201233
iteration 600, loss 1.812680721282959
iteration 700, loss 1.8150830268859863
iteration 800, loss 1.8094650506973267
iteration 0, loss 1.9431895017623901
iteration 100, loss 1.815985083580017
iteration 200, loss 1.8382689952850342
iteration 300, loss 1.8494532108306885
iteration 400, loss 1.801254153251648
iteration 500, loss 1.8274791240692139
iteration 600, loss 1.8906205892562866
iteration 700, loss 1.8404573202133179
iteration 800, loss 1.913132667541504
fold 4 accuracy: 0.4253571428571429
[2024-02-29 00:13:36,711] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-29 00:13:36,712] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            407.46 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.37 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '407.46 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 407.46 us = 100% latency, 1.37 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 322.1 us = 79.05% latency, 1.74 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 6.09% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-29 00:13:36,714] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
iteration 0, loss 2.2669944763183594
iteration 100, loss 2.253063201904297
iteration 200, loss 2.1603872776031494
iteration 300, loss 2.118849039077759
iteration 400, loss 2.036094903945923
iteration 500, loss 2.0518908500671387
iteration 600, loss 2.042090892791748
iteration 700, loss 2.0013327598571777
iteration 800, loss 2.055823564529419
iteration 0, loss 2.070614814758301
iteration 100, loss 1.9266984462738037
iteration 200, loss 1.9208424091339111
iteration 300, loss 1.9683526754379272
iteration 400, loss 2.02724552154541
iteration 500, loss 1.9918862581253052
iteration 600, loss 2.045469045639038
iteration 700, loss 1.962032675743103
iteration 800, loss 1.9575915336608887
iteration 0, loss 1.9697291851043701
iteration 100, loss 1.942929744720459
iteration 200, loss 1.9810309410095215
iteration 300, loss 1.8969826698303223
iteration 400, loss 1.9192776679992676
iteration 500, loss 1.9398574829101562
iteration 600, loss 1.9220032691955566
iteration 700, loss 1.9171100854873657
iteration 800, loss 1.9883867502212524
iteration 0, loss 1.9371864795684814
iteration 100, loss 1.9528672695159912
iteration 200, loss 1.9518561363220215
iteration 300, loss 1.9449913501739502
iteration 400, loss 1.9281097650527954
iteration 500, loss 1.9525936841964722
iteration 600, loss 1.929726243019104
iteration 700, loss 1.8520801067352295
iteration 800, loss 1.9525843858718872
iteration 0, loss 1.8897075653076172
iteration 100, loss 1.9456623792648315
iteration 200, loss 1.9006739854812622
iteration 300, loss 1.9130557775497437
iteration 400, loss 1.9219791889190674
iteration 500, loss 1.9278453588485718
iteration 600, loss 1.8855327367782593
iteration 700, loss 1.8942780494689941
iteration 800, loss 1.9165626764297485
iteration 0, loss 1.8927668333053589
iteration 100, loss 1.9589660167694092
iteration 200, loss 1.869973063468933
iteration 300, loss 1.9253439903259277
iteration 400, loss 1.8527871370315552
iteration 500, loss 1.9431853294372559
iteration 600, loss 1.9188885688781738
iteration 700, loss 1.852333664894104
iteration 800, loss 1.880975365638733
iteration 0, loss 1.858443021774292
iteration 100, loss 1.914033055305481
iteration 200, loss 1.8642417192459106
iteration 300, loss 1.9043052196502686
iteration 400, loss 1.8579227924346924
iteration 500, loss 1.9397947788238525
iteration 600, loss 1.8964146375656128
iteration 700, loss 1.9299144744873047
iteration 800, loss 1.8698716163635254
iteration 0, loss 1.8884440660476685
iteration 100, loss 1.9677131175994873
iteration 200, loss 1.8956923484802246
iteration 300, loss 1.8622897863388062
iteration 400, loss 1.8823294639587402
iteration 500, loss 1.8052529096603394
iteration 600, loss 1.942825436592102
iteration 700, loss 1.926133632659912
iteration 800, loss 1.8711552619934082
iteration 0, loss 1.8682528734207153
iteration 100, loss 1.8526877164840698
iteration 200, loss 1.9327589273452759
iteration 300, loss 1.8807183504104614
iteration 400, loss 1.7692644596099854
iteration 500, loss 1.9417920112609863
iteration 600, loss 1.880866289138794
iteration 700, loss 1.8363854885101318
iteration 800, loss 1.8496294021606445
iteration 0, loss 1.9236316680908203
iteration 100, loss 1.9577758312225342
iteration 200, loss 1.852426528930664
iteration 300, loss 1.9178277254104614
iteration 400, loss 1.9009370803833008
iteration 500, loss 1.8983856439590454
iteration 600, loss 1.8242264986038208
iteration 700, loss 1.8261191844940186
iteration 800, loss 1.8000012636184692
iteration 0, loss 1.7805724143981934
iteration 100, loss 1.8498362302780151
iteration 200, loss 1.8210445642471313
iteration 300, loss 1.8959084749221802
iteration 400, loss 1.856673002243042
iteration 500, loss 1.7970625162124634
iteration 600, loss 1.8864567279815674
iteration 700, loss 1.7836182117462158
iteration 800, loss 1.863184928894043
iteration 0, loss 1.8390414714813232
iteration 100, loss 1.9403281211853027
iteration 200, loss 1.8449735641479492
iteration 300, loss 1.8807406425476074
iteration 400, loss 1.8891500234603882
iteration 500, loss 1.906224250793457
iteration 600, loss 1.8865996599197388
iteration 700, loss 1.8843178749084473
iteration 800, loss 1.8883494138717651
iteration 0, loss 1.886433482170105
iteration 100, loss 1.8581818342208862
iteration 200, loss 1.8159635066986084
iteration 300, loss 1.7722375392913818
iteration 400, loss 1.8124984502792358
iteration 500, loss 1.865918517112732
iteration 600, loss 1.854095697402954
iteration 700, loss 1.8747031688690186
iteration 800, loss 1.8213939666748047
iteration 0, loss 1.914223313331604
iteration 100, loss 1.9011074304580688
iteration 200, loss 1.9511144161224365
iteration 300, loss 1.8992375135421753
iteration 400, loss 1.8554095029830933
iteration 500, loss 1.8426001071929932
iteration 600, loss 1.8440403938293457
iteration 700, loss 1.8598301410675049
iteration 800, loss 1.8017507791519165
iteration 0, loss 1.876598596572876
iteration 100, loss 1.8901031017303467
iteration 200, loss 1.9173433780670166
iteration 300, loss 1.821740746498108
iteration 400, loss 1.8539870977401733
iteration 500, loss 1.765020489692688
iteration 600, loss 1.8693888187408447
iteration 700, loss 1.9060750007629395
iteration 800, loss 1.880461573600769
iteration 0, loss 1.8143351078033447
iteration 100, loss 1.8604785203933716
iteration 200, loss 1.927284836769104
iteration 300, loss 1.9419620037078857
iteration 400, loss 1.8922654390335083
iteration 500, loss 1.7571399211883545
iteration 600, loss 1.8350228071212769
iteration 700, loss 1.8354103565216064
iteration 800, loss 1.8495032787322998
iteration 0, loss 1.843273401260376
iteration 100, loss 1.8915055990219116
iteration 200, loss 1.8151650428771973
iteration 300, loss 1.826348900794983
iteration 400, loss 1.8696544170379639
iteration 500, loss 1.7709189653396606
iteration 600, loss 1.857025146484375
iteration 700, loss 1.8027820587158203
iteration 800, loss 1.861583948135376
iteration 0, loss 1.8801661729812622
iteration 100, loss 1.8967763185501099
iteration 200, loss 1.8810696601867676
iteration 300, loss 1.8526877164840698
iteration 400, loss 1.937646508216858
iteration 500, loss 1.8804821968078613
iteration 600, loss 1.8519885540008545
iteration 700, loss 1.917211890220642
iteration 800, loss 1.9092626571655273
iteration 0, loss 1.7698431015014648
iteration 100, loss 1.8997273445129395
iteration 200, loss 1.835573673248291
iteration 300, loss 1.9641070365905762
iteration 400, loss 1.8655823469161987
iteration 500, loss 1.8718979358673096
iteration 600, loss 1.8676083087921143
iteration 700, loss 1.9182170629501343
iteration 800, loss 1.8057066202163696
iteration 0, loss 1.8460808992385864
iteration 100, loss 1.8459064960479736
iteration 200, loss 1.850624442100525
iteration 300, loss 1.8529505729675293
iteration 400, loss 1.9391756057739258
iteration 500, loss 1.796860694885254
iteration 600, loss 1.7976166009902954
iteration 700, loss 1.9298237562179565
iteration 800, loss 1.9291155338287354
iteration 0, loss 1.917888879776001
iteration 100, loss 1.8047146797180176
iteration 200, loss 1.9484039545059204
iteration 300, loss 1.831993579864502
iteration 400, loss 1.809940218925476
iteration 500, loss 1.8451486825942993
iteration 600, loss 1.8561387062072754
iteration 700, loss 1.8676342964172363
iteration 800, loss 1.8242064714431763
iteration 0, loss 1.8925853967666626
iteration 100, loss 1.7952121496200562
iteration 200, loss 1.8474289178848267
iteration 300, loss 1.8389345407485962
iteration 400, loss 1.8375146389007568
iteration 500, loss 1.8191884756088257
iteration 600, loss 1.8076529502868652
iteration 700, loss 1.858250379562378
iteration 800, loss 1.838070273399353
iteration 0, loss 1.8618277311325073
iteration 100, loss 1.7605466842651367
iteration 200, loss 1.9192479848861694
iteration 300, loss 1.9285483360290527
iteration 400, loss 1.8499486446380615
iteration 500, loss 1.8109302520751953
iteration 600, loss 1.7869900465011597
iteration 700, loss 1.8814839124679565
iteration 800, loss 1.864525318145752
iteration 0, loss 1.8509889841079712
iteration 100, loss 1.846165418624878
iteration 200, loss 1.8701272010803223
iteration 300, loss 1.853075623512268
iteration 400, loss 1.7945051193237305
iteration 500, loss 1.9212672710418701
iteration 600, loss 1.8598819971084595
iteration 700, loss 1.9489485025405884
iteration 800, loss 1.8900563716888428
iteration 0, loss 1.8034837245941162
iteration 100, loss 1.9033595323562622
iteration 200, loss 1.8262656927108765
iteration 300, loss 1.7979263067245483
iteration 400, loss 1.862724781036377
iteration 500, loss 1.8794363737106323
iteration 600, loss 1.8030682802200317
iteration 700, loss 1.8342154026031494
iteration 800, loss 1.8349478244781494
iteration 0, loss 1.913601040840149
iteration 100, loss 1.9043711423873901
iteration 200, loss 1.8912938833236694
iteration 300, loss 1.8803657293319702
iteration 400, loss 1.8209083080291748
iteration 500, loss 1.8692142963409424
iteration 600, loss 1.8171263933181763
iteration 700, loss 1.8076826333999634
iteration 800, loss 1.782456398010254
iteration 0, loss 1.8494874238967896
iteration 100, loss 1.8242855072021484
iteration 200, loss 1.8198635578155518
iteration 300, loss 1.8934195041656494
iteration 400, loss 1.8389226198196411
iteration 500, loss 1.9288461208343506
iteration 600, loss 1.8193683624267578
iteration 700, loss 1.8724309206008911
iteration 800, loss 1.8113075494766235
iteration 0, loss 1.9313980340957642
iteration 100, loss 1.9137444496154785
iteration 200, loss 1.8649680614471436
iteration 300, loss 1.7753440141677856
iteration 400, loss 1.84986412525177
iteration 500, loss 1.9567185640335083
iteration 600, loss 1.8689478635787964
iteration 700, loss 1.8255271911621094
iteration 800, loss 1.8976757526397705
iteration 0, loss 1.8325169086456299
iteration 100, loss 1.880185604095459
iteration 200, loss 1.8329015970230103
iteration 300, loss 1.9166059494018555
iteration 400, loss 1.8821589946746826
iteration 500, loss 1.8609565496444702
iteration 600, loss 1.9442436695098877
iteration 700, loss 1.8329370021820068
iteration 800, loss 1.8645081520080566
iteration 0, loss 1.790616512298584
iteration 100, loss 1.8250068426132202
iteration 200, loss 1.8428869247436523
iteration 300, loss 1.8261045217514038
iteration 400, loss 1.8424545526504517
iteration 500, loss 1.7672314643859863
iteration 600, loss 1.8690165281295776
iteration 700, loss 1.8103418350219727
iteration 800, loss 1.9031862020492554
iteration 0, loss 1.86080801486969
iteration 100, loss 1.7895399332046509
iteration 200, loss 1.8134639263153076
iteration 300, loss 1.7855595350265503
iteration 400, loss 1.8990610837936401
iteration 500, loss 1.8153159618377686
iteration 600, loss 1.8558365106582642
iteration 700, loss 1.885868787765503
iteration 800, loss 1.9159761667251587
iteration 0, loss 1.8310723304748535
iteration 100, loss 1.8922092914581299
iteration 200, loss 1.8087657690048218
iteration 300, loss 1.8832873106002808
iteration 400, loss 1.9140254259109497
iteration 500, loss 1.8550416231155396
iteration 600, loss 1.8834178447723389
iteration 700, loss 1.8869279623031616
iteration 800, loss 1.8620964288711548
iteration 0, loss 1.9384397268295288
iteration 100, loss 1.9129527807235718
iteration 200, loss 1.7965359687805176
iteration 300, loss 1.7903494834899902
iteration 400, loss 1.8106088638305664
iteration 500, loss 1.8783708810806274
iteration 600, loss 1.732358455657959
iteration 700, loss 1.8846533298492432
iteration 800, loss 1.8900113105773926
iteration 0, loss 1.8343918323516846
iteration 100, loss 1.79783034324646
iteration 200, loss 1.9454143047332764
iteration 300, loss 1.808608055114746
iteration 400, loss 1.7754489183425903
iteration 500, loss 1.8591089248657227
iteration 600, loss 1.8549331426620483
iteration 700, loss 1.8454430103302002
iteration 800, loss 1.8272640705108643
iteration 0, loss 1.7649555206298828
iteration 100, loss 1.8404836654663086
iteration 200, loss 1.7115504741668701
iteration 300, loss 1.8245166540145874
iteration 400, loss 1.8636521100997925
iteration 500, loss 1.865101933479309
iteration 600, loss 1.857151746749878
iteration 700, loss 1.907345175743103
iteration 800, loss 1.8104512691497803
iteration 0, loss 1.8664803504943848
iteration 100, loss 1.8462458848953247
iteration 200, loss 1.8541101217269897
iteration 300, loss 1.8268749713897705
iteration 400, loss 1.91639244556427
iteration 500, loss 1.904671311378479
iteration 600, loss 1.7786558866500854
iteration 700, loss 1.8512707948684692
iteration 800, loss 1.8643555641174316
iteration 0, loss 1.835999608039856
iteration 100, loss 1.8701492547988892
iteration 200, loss 1.8225555419921875
iteration 300, loss 1.7964668273925781
iteration 400, loss 1.7817336320877075
iteration 500, loss 1.8606252670288086
iteration 600, loss 1.8505369424819946
iteration 700, loss 1.8653985261917114
iteration 800, loss 1.8801865577697754
iteration 0, loss 1.8854918479919434
iteration 100, loss 1.8392605781555176
iteration 200, loss 1.8951300382614136
iteration 300, loss 1.8641475439071655
iteration 400, loss 1.8313157558441162
iteration 500, loss 1.8170071840286255
iteration 600, loss 1.816882848739624
iteration 700, loss 1.8238372802734375
iteration 800, loss 1.8733062744140625
iteration 0, loss 1.7782279253005981
iteration 100, loss 1.8389246463775635
iteration 200, loss 1.8359534740447998
iteration 300, loss 1.8998478651046753
iteration 400, loss 1.9168506860733032
iteration 500, loss 1.889782190322876
iteration 600, loss 1.9499982595443726
iteration 700, loss 1.835277795791626
iteration 800, loss 1.8161342144012451
iteration 0, loss 1.8714690208435059
iteration 100, loss 1.8440418243408203
iteration 200, loss 1.7905879020690918
iteration 300, loss 1.8251430988311768
iteration 400, loss 1.8549233675003052
iteration 500, loss 1.8010053634643555
iteration 600, loss 1.8179041147232056
iteration 700, loss 1.8393290042877197
iteration 800, loss 1.7915124893188477
iteration 0, loss 1.8191593885421753
iteration 100, loss 1.7852139472961426
iteration 200, loss 1.847834587097168
iteration 300, loss 1.810978889465332
iteration 400, loss 1.8514846563339233
iteration 500, loss 1.7874243259429932
iteration 600, loss 1.8560712337493896
iteration 700, loss 1.9026250839233398
iteration 800, loss 1.8461488485336304
iteration 0, loss 1.9068257808685303
iteration 100, loss 1.81272292137146
iteration 200, loss 1.8042991161346436
iteration 300, loss 1.7602980136871338
iteration 400, loss 1.7549245357513428
iteration 500, loss 1.823111891746521
iteration 600, loss 1.9236788749694824
iteration 700, loss 1.8702807426452637
iteration 800, loss 1.8157737255096436
iteration 0, loss 1.796886920928955
iteration 100, loss 1.8923150300979614
iteration 200, loss 1.8310770988464355
iteration 300, loss 1.8605574369430542
iteration 400, loss 1.8792146444320679
iteration 500, loss 1.7919942140579224
iteration 600, loss 1.921210765838623
iteration 700, loss 1.8244949579238892
iteration 800, loss 1.8387877941131592
iteration 0, loss 1.8563511371612549
iteration 100, loss 1.8579883575439453
iteration 200, loss 1.9332395792007446
iteration 300, loss 1.9011083841323853
iteration 400, loss 1.847183346748352
iteration 500, loss 1.8733638525009155
iteration 600, loss 1.8795430660247803
iteration 700, loss 1.775871753692627
iteration 800, loss 1.7559998035430908
iteration 0, loss 1.82559335231781
iteration 100, loss 1.849185824394226
iteration 200, loss 1.8082643747329712
iteration 300, loss 1.8083713054656982
iteration 400, loss 1.7806847095489502
iteration 500, loss 1.8535161018371582
iteration 600, loss 1.8889199495315552
iteration 700, loss 1.8756606578826904
iteration 800, loss 1.8769081830978394
iteration 0, loss 1.7756747007369995
iteration 100, loss 1.8468254804611206
iteration 200, loss 1.8231409788131714
iteration 300, loss 1.8518192768096924
iteration 400, loss 1.9402424097061157
iteration 500, loss 1.798111081123352
iteration 600, loss 1.8420969247817993
iteration 700, loss 1.8509721755981445
iteration 800, loss 1.8074733018875122
iteration 0, loss 1.8940163850784302
iteration 100, loss 1.9082340002059937
iteration 200, loss 2.034367561340332
iteration 300, loss 1.8753080368041992
iteration 400, loss 1.7221019268035889
iteration 500, loss 1.8010834455490112
iteration 600, loss 1.824795126914978
iteration 700, loss 1.8834782838821411
iteration 800, loss 1.7889269590377808
iteration 0, loss 1.8574427366256714
iteration 100, loss 1.8687751293182373
iteration 200, loss 1.7889125347137451
iteration 300, loss 1.852884292602539
iteration 400, loss 1.8511682748794556
iteration 500, loss 1.8065286874771118
iteration 600, loss 1.8754901885986328
iteration 700, loss 1.8586270809173584
iteration 800, loss 1.8007580041885376
iteration 0, loss 1.9075208902359009
iteration 100, loss 1.8089481592178345
iteration 200, loss 1.8612127304077148
iteration 300, loss 1.8131428956985474
iteration 400, loss 1.9162169694900513
iteration 500, loss 1.8577932119369507
iteration 600, loss 1.7808047533035278
iteration 700, loss 1.7855780124664307
iteration 800, loss 1.7836872339248657
iteration 0, loss 1.8026092052459717
iteration 100, loss 1.9213225841522217
iteration 200, loss 1.8494514226913452
iteration 300, loss 1.9624565839767456
iteration 400, loss 1.879577875137329
iteration 500, loss 1.8751968145370483
iteration 600, loss 1.8238246440887451
iteration 700, loss 1.9097191095352173
iteration 800, loss 1.9025561809539795
fold 0 accuracy: 0.49314285714285716
iteration 0, loss 1.9077470302581787
iteration 100, loss 1.883896827697754
iteration 200, loss 1.752053141593933
iteration 300, loss 1.824560523033142
iteration 400, loss 1.8183127641677856
iteration 500, loss 1.8457149267196655
iteration 600, loss 1.87142014503479
iteration 700, loss 1.896216869354248
iteration 800, loss 1.8586032390594482
iteration 0, loss 1.8904730081558228
iteration 100, loss 1.8588351011276245
iteration 200, loss 1.8839391469955444
iteration 300, loss 1.8839201927185059
iteration 400, loss 1.7801785469055176
iteration 500, loss 1.8221027851104736
iteration 600, loss 1.8509278297424316
iteration 700, loss 1.76597261428833
iteration 800, loss 1.8840276002883911
iteration 0, loss 1.7788622379302979
iteration 100, loss 1.834567666053772
iteration 200, loss 1.795973300933838
iteration 300, loss 1.8343322277069092
iteration 400, loss 1.8046941757202148
iteration 500, loss 1.8110809326171875
iteration 600, loss 1.9269264936447144
iteration 700, loss 1.917341709136963
iteration 800, loss 1.7791916131973267
iteration 0, loss 1.789894461631775
iteration 100, loss 1.8187552690505981
iteration 200, loss 1.8645272254943848
iteration 300, loss 1.7747513055801392
iteration 400, loss 1.8653888702392578
iteration 500, loss 1.7451491355895996
iteration 600, loss 1.832373857498169
iteration 700, loss 1.882213830947876
iteration 800, loss 1.7870676517486572
iteration 0, loss 1.8501946926116943
iteration 100, loss 1.840106725692749
iteration 200, loss 1.8248549699783325
iteration 300, loss 1.7891312837600708
iteration 400, loss 1.8637267351150513
iteration 500, loss 1.8019089698791504
iteration 600, loss 1.8591606616973877
iteration 700, loss 1.828641653060913
iteration 800, loss 1.9149988889694214
iteration 0, loss 1.892407774925232
iteration 100, loss 1.856858730316162
iteration 200, loss 1.8275350332260132
iteration 300, loss 1.7833116054534912
iteration 400, loss 1.8859446048736572
iteration 500, loss 1.7569247484207153
iteration 600, loss 1.8851081132888794
iteration 700, loss 1.9144879579544067
iteration 800, loss 1.8550710678100586
iteration 0, loss 1.8456392288208008
iteration 100, loss 1.8379853963851929
iteration 200, loss 1.8793457746505737
iteration 300, loss 1.8779832124710083
iteration 400, loss 1.8461092710494995
iteration 500, loss 1.9171364307403564
iteration 600, loss 1.847414493560791
iteration 700, loss 1.799054503440857
iteration 800, loss 1.8942320346832275
iteration 0, loss 1.8693665266036987
iteration 100, loss 1.8376879692077637
iteration 200, loss 1.8247705698013306
iteration 300, loss 1.9290680885314941
iteration 400, loss 1.8557316064834595
iteration 500, loss 1.8477842807769775
iteration 600, loss 1.862085223197937
iteration 700, loss 1.9118328094482422
iteration 800, loss 1.8507413864135742
iteration 0, loss 1.7907114028930664
iteration 100, loss 1.8616611957550049
iteration 200, loss 1.849680781364441
iteration 300, loss 1.8274304866790771
iteration 400, loss 1.8441216945648193
iteration 500, loss 1.803613305091858
iteration 600, loss 1.862126111984253
iteration 700, loss 1.8924123048782349
iteration 800, loss 1.8917232751846313
iteration 0, loss 1.7935693264007568
iteration 100, loss 1.8232579231262207
iteration 200, loss 1.8321378231048584
iteration 300, loss 1.8078155517578125
iteration 400, loss 1.7518607378005981
iteration 500, loss 1.901735782623291
iteration 600, loss 1.7852978706359863
iteration 700, loss 1.8131864070892334
iteration 800, loss 1.9068961143493652
iteration 0, loss 1.8417387008666992
iteration 100, loss 1.8854025602340698
iteration 200, loss 1.902616262435913
iteration 300, loss 1.7918906211853027
iteration 400, loss 1.868383526802063
iteration 500, loss 1.7934684753417969
iteration 600, loss 1.870724081993103
iteration 700, loss 1.823611855506897
iteration 800, loss 1.8422654867172241
iteration 0, loss 1.8448253870010376
iteration 100, loss 1.9427461624145508
iteration 200, loss 1.854797601699829
iteration 300, loss 1.9453952312469482
iteration 400, loss 1.8857401609420776
iteration 500, loss 1.8473588228225708
iteration 600, loss 1.8342387676239014
iteration 700, loss 1.8987712860107422
iteration 800, loss 1.9089196920394897
iteration 0, loss 1.7912492752075195
iteration 100, loss 1.8630892038345337
iteration 200, loss 1.9154572486877441
iteration 300, loss 1.7733337879180908
iteration 400, loss 1.8656883239746094
iteration 500, loss 1.9090512990951538
iteration 600, loss 1.8254964351654053
iteration 700, loss 1.820418119430542
iteration 800, loss 1.8112400770187378
iteration 0, loss 1.8989934921264648
iteration 100, loss 1.9645124673843384
iteration 200, loss 1.8841742277145386
iteration 300, loss 1.793755054473877
iteration 400, loss 1.8545926809310913
iteration 500, loss 1.876443862915039
iteration 600, loss 1.7848925590515137
iteration 700, loss 1.8304429054260254
iteration 800, loss 1.8584275245666504
iteration 0, loss 1.8404242992401123
iteration 100, loss 1.844043254852295
iteration 200, loss 1.820288062095642
iteration 300, loss 1.8298594951629639
iteration 400, loss 1.813454270362854
iteration 500, loss 1.8211686611175537
iteration 600, loss 1.843294620513916
iteration 700, loss 1.8537168502807617
iteration 800, loss 1.8707786798477173
iteration 0, loss 1.8918191194534302
iteration 100, loss 1.9692142009735107
iteration 200, loss 1.8290610313415527
iteration 300, loss 1.7987022399902344
iteration 400, loss 1.8975470066070557
iteration 500, loss 1.98014497756958
iteration 600, loss 1.8392438888549805
iteration 700, loss 1.7983535528182983
iteration 800, loss 1.9278126955032349
iteration 0, loss 1.8260908126831055
iteration 100, loss 1.7867082357406616
iteration 200, loss 1.7683534622192383
iteration 300, loss 1.8350441455841064
iteration 400, loss 1.815643072128296
iteration 500, loss 1.9117926359176636
iteration 600, loss 1.8109138011932373
iteration 700, loss 1.8267769813537598
iteration 800, loss 1.8563895225524902
iteration 0, loss 1.859727144241333
iteration 100, loss 1.8831809759140015
iteration 200, loss 1.8515417575836182
iteration 300, loss 1.8250705003738403
iteration 400, loss 1.8022907972335815
iteration 500, loss 1.7979614734649658
iteration 600, loss 1.8572076559066772
iteration 700, loss 1.8319408893585205
iteration 800, loss 1.8675817251205444
iteration 0, loss 1.817509651184082
iteration 100, loss 1.8362566232681274
iteration 200, loss 1.7965750694274902
iteration 300, loss 1.8758950233459473
iteration 400, loss 1.783496618270874
iteration 500, loss 1.8617764711380005
iteration 600, loss 1.8848472833633423
iteration 700, loss 1.8631155490875244
iteration 800, loss 1.8324298858642578
iteration 0, loss 1.8475079536437988
iteration 100, loss 1.8562828302383423
iteration 200, loss 1.8749034404754639
iteration 300, loss 1.8051615953445435
iteration 400, loss 1.8289546966552734
iteration 500, loss 1.7852745056152344
iteration 600, loss 1.7870546579360962
iteration 700, loss 1.872352957725525
iteration 800, loss 1.8464570045471191
iteration 0, loss 1.774630069732666
iteration 100, loss 1.8816311359405518
iteration 200, loss 1.938521146774292
iteration 300, loss 1.7966257333755493
iteration 400, loss 1.8300237655639648
iteration 500, loss 1.8894710540771484
iteration 600, loss 1.8871101140975952
iteration 700, loss 1.8264508247375488
iteration 800, loss 1.8540785312652588
iteration 0, loss 1.7528696060180664
iteration 100, loss 1.8063673973083496
iteration 200, loss 1.8544797897338867
iteration 300, loss 1.7688231468200684
iteration 400, loss 1.9349457025527954
iteration 500, loss 1.813583493232727
iteration 600, loss 1.7934539318084717
iteration 700, loss 1.875795602798462
iteration 800, loss 1.8343226909637451
iteration 0, loss 1.925915002822876
iteration 100, loss 1.8420056104660034
iteration 200, loss 1.8610925674438477
iteration 300, loss 1.824009656906128
iteration 400, loss 1.8797070980072021
iteration 500, loss 1.8885960578918457
iteration 600, loss 1.763864517211914
iteration 700, loss 1.8515113592147827
iteration 800, loss 1.8642795085906982
iteration 0, loss 1.842484951019287
iteration 100, loss 1.850082516670227
iteration 200, loss 1.9080493450164795
iteration 300, loss 1.8317272663116455
iteration 400, loss 1.8291699886322021
iteration 500, loss 1.7850866317749023
iteration 600, loss 1.743988275527954
iteration 700, loss 1.8550294637680054
iteration 800, loss 1.8467791080474854
iteration 0, loss 1.9242202043533325
iteration 100, loss 1.8327434062957764
iteration 200, loss 1.807003140449524
iteration 300, loss 1.8327105045318604
iteration 400, loss 1.9103844165802002
iteration 500, loss 1.8061325550079346
iteration 600, loss 1.7998266220092773
iteration 700, loss 1.854557991027832
iteration 800, loss 1.8806575536727905
iteration 0, loss 1.769615888595581
iteration 100, loss 1.8623052835464478
iteration 200, loss 1.8174313306808472
iteration 300, loss 1.8963427543640137
iteration 400, loss 1.753320574760437
iteration 500, loss 1.7719594240188599
iteration 600, loss 1.789781928062439
iteration 700, loss 1.7315454483032227
iteration 800, loss 1.801488995552063
iteration 0, loss 1.8874363899230957
iteration 100, loss 1.8337619304656982
iteration 200, loss 1.8567476272583008
iteration 300, loss 1.8533744812011719
iteration 400, loss 1.835626482963562
iteration 500, loss 1.857942819595337
iteration 600, loss 1.8995898962020874
iteration 700, loss 1.8913604021072388
iteration 800, loss 1.7966052293777466
iteration 0, loss 1.9040167331695557
iteration 100, loss 1.8518633842468262
iteration 200, loss 1.889214038848877
iteration 300, loss 1.8015037775039673
iteration 400, loss 1.8110255002975464
iteration 500, loss 1.771645426750183
iteration 600, loss 1.867302417755127
iteration 700, loss 1.8495010137557983
iteration 800, loss 1.7964049577713013
iteration 0, loss 1.828874945640564
iteration 100, loss 1.9169470071792603
iteration 200, loss 1.7834644317626953
iteration 300, loss 1.839347243309021
iteration 400, loss 1.7927556037902832
iteration 500, loss 1.8743250370025635
iteration 600, loss 1.9091193675994873
iteration 700, loss 1.8466923236846924
iteration 800, loss 1.8615503311157227
iteration 0, loss 1.852385401725769
iteration 100, loss 1.8655683994293213
iteration 200, loss 1.8666809797286987
iteration 300, loss 1.828294038772583
iteration 400, loss 1.7836508750915527
iteration 500, loss 1.8311923742294312
iteration 600, loss 1.8657329082489014
iteration 700, loss 1.8073375225067139
iteration 800, loss 1.7715508937835693
iteration 0, loss 1.800674557685852
iteration 100, loss 1.873154640197754
iteration 200, loss 1.8697619438171387
iteration 300, loss 1.7857468128204346
iteration 400, loss 1.8060303926467896
iteration 500, loss 1.7870961427688599
iteration 600, loss 1.8424255847930908
iteration 700, loss 1.7997055053710938
iteration 800, loss 1.8283525705337524
iteration 0, loss 1.797370433807373
iteration 100, loss 1.8866938352584839
iteration 200, loss 1.810596227645874
iteration 300, loss 1.9117026329040527
iteration 400, loss 1.8675837516784668
iteration 500, loss 1.8412102460861206
iteration 600, loss 1.8778170347213745
iteration 700, loss 1.7587406635284424
iteration 800, loss 1.8098317384719849
iteration 0, loss 1.7610187530517578
iteration 100, loss 1.8730443716049194
iteration 200, loss 1.858906865119934
iteration 300, loss 1.8590750694274902
iteration 400, loss 1.8579562902450562
iteration 500, loss 1.8265814781188965
iteration 600, loss 1.828110933303833
iteration 700, loss 1.8123276233673096
iteration 800, loss 1.8239458799362183
iteration 0, loss 1.8533351421356201
iteration 100, loss 1.8372795581817627
iteration 200, loss 1.6949082612991333
iteration 300, loss 1.8263510465621948
iteration 400, loss 1.856874942779541
iteration 500, loss 1.810434103012085
iteration 600, loss 1.8855578899383545
iteration 700, loss 1.8442883491516113
iteration 800, loss 1.8109328746795654
iteration 0, loss 1.8938610553741455
iteration 100, loss 1.8500971794128418
iteration 200, loss 1.844281554222107
iteration 300, loss 1.7709606885910034
iteration 400, loss 1.8685263395309448
iteration 500, loss 1.8580284118652344
iteration 600, loss 1.8287627696990967
iteration 700, loss 1.8630210161209106
iteration 800, loss 1.8511070013046265
iteration 0, loss 1.9299968481063843
iteration 100, loss 1.8402073383331299
iteration 200, loss 1.891076922416687
iteration 300, loss 1.916893720626831
iteration 400, loss 1.890859603881836
iteration 500, loss 1.8445883989334106
iteration 600, loss 1.7787928581237793
iteration 700, loss 1.8350306749343872
iteration 800, loss 1.8347505331039429
iteration 0, loss 1.8233795166015625
iteration 100, loss 1.8128305673599243
iteration 200, loss 1.758826494216919
iteration 300, loss 1.8084897994995117
iteration 400, loss 1.8733000755310059
iteration 500, loss 1.8238494396209717
iteration 600, loss 1.8041435480117798
iteration 700, loss 1.79202139377594
iteration 800, loss 1.9118002653121948
iteration 0, loss 1.8561294078826904
iteration 100, loss 1.7922505140304565
iteration 200, loss 1.8771693706512451
iteration 300, loss 1.9207605123519897
iteration 400, loss 1.9173051118850708
iteration 500, loss 1.893234372138977
iteration 600, loss 1.8194119930267334
iteration 700, loss 1.8874835968017578
iteration 800, loss 1.8545316457748413
iteration 0, loss 1.863633394241333
iteration 100, loss 1.8656219244003296
iteration 200, loss 1.8280260562896729
iteration 300, loss 1.8508296012878418
iteration 400, loss 1.7277309894561768
iteration 500, loss 1.8205419778823853
iteration 600, loss 1.809403896331787
iteration 700, loss 1.8696246147155762
iteration 800, loss 1.8405447006225586
iteration 0, loss 1.7713454961776733
iteration 100, loss 1.8361104726791382
iteration 200, loss 1.8945626020431519
iteration 300, loss 1.7976628541946411
iteration 400, loss 1.79646635055542
iteration 500, loss 1.7224407196044922
iteration 600, loss 1.882225751876831
iteration 700, loss 1.8063373565673828
iteration 800, loss 1.7931876182556152
iteration 0, loss 1.7576833963394165
iteration 100, loss 1.8606524467468262
iteration 200, loss 1.8110792636871338
iteration 300, loss 1.8502097129821777
iteration 400, loss 1.841033697128296
iteration 500, loss 1.8649011850357056
iteration 600, loss 1.8423603773117065
iteration 700, loss 1.791782259941101
iteration 800, loss 1.8236112594604492
iteration 0, loss 1.8571326732635498
iteration 100, loss 1.876050353050232
iteration 200, loss 1.8455067873001099
iteration 300, loss 1.8443487882614136
iteration 400, loss 1.9193111658096313
iteration 500, loss 1.729736089706421
iteration 600, loss 1.8924663066864014
iteration 700, loss 1.7175947427749634
iteration 800, loss 1.811386227607727
iteration 0, loss 1.9101088047027588
iteration 100, loss 1.858943223953247
iteration 200, loss 1.8417584896087646
iteration 300, loss 1.8661134243011475
iteration 400, loss 1.859804391860962
iteration 500, loss 1.7895784378051758
iteration 600, loss 1.8938562870025635
iteration 700, loss 1.8732597827911377
iteration 800, loss 1.827202320098877
iteration 0, loss 1.8627225160598755
iteration 100, loss 1.7995518445968628
iteration 200, loss 1.828580617904663
iteration 300, loss 1.819054365158081
iteration 400, loss 1.7935714721679688
iteration 500, loss 1.84231698513031
iteration 600, loss 1.9138545989990234
iteration 700, loss 1.8597489595413208
iteration 800, loss 1.7780460119247437
iteration 0, loss 1.8054749965667725
iteration 100, loss 1.7698969841003418
iteration 200, loss 1.7737826108932495
iteration 300, loss 1.782051920890808
iteration 400, loss 1.8221381902694702
iteration 500, loss 1.7342437505722046
iteration 600, loss 1.8713361024856567
iteration 700, loss 1.925048828125
iteration 800, loss 1.9659544229507446
iteration 0, loss 1.8130971193313599
iteration 100, loss 1.78346848487854
iteration 200, loss 1.8941731452941895
iteration 300, loss 1.8121227025985718
iteration 400, loss 1.8039255142211914
iteration 500, loss 1.9179167747497559
iteration 600, loss 1.8239591121673584
iteration 700, loss 1.8768455982208252
iteration 800, loss 1.8291947841644287
iteration 0, loss 1.8583623170852661
iteration 100, loss 1.7804455757141113
iteration 200, loss 1.8777625560760498
iteration 300, loss 1.7795765399932861
iteration 400, loss 1.7617062330245972
iteration 500, loss 1.793411374092102
iteration 600, loss 1.8187286853790283
iteration 700, loss 1.7960094213485718
iteration 800, loss 1.764614462852478
iteration 0, loss 1.837167739868164
iteration 100, loss 1.8315188884735107
iteration 200, loss 1.904783010482788
iteration 300, loss 1.793213963508606
iteration 400, loss 1.8947713375091553
iteration 500, loss 1.8176769018173218
iteration 600, loss 1.8911139965057373
iteration 700, loss 1.8137544393539429
iteration 800, loss 1.9460784196853638
iteration 0, loss 1.9318773746490479
iteration 100, loss 1.8255481719970703
iteration 200, loss 1.8584940433502197
iteration 300, loss 1.894700288772583
iteration 400, loss 1.858154296875
iteration 500, loss 1.8187958002090454
iteration 600, loss 1.8172816038131714
iteration 700, loss 1.8746620416641235
iteration 800, loss 1.7834471464157104
iteration 0, loss 1.8553707599639893
iteration 100, loss 1.7252875566482544
iteration 200, loss 1.850716471672058
iteration 300, loss 1.781183123588562
iteration 400, loss 1.8425437211990356
iteration 500, loss 1.7891801595687866
iteration 600, loss 1.8967519998550415
iteration 700, loss 1.8421351909637451
iteration 800, loss 1.850476622581482
fold 1 accuracy: 0.5115
iteration 0, loss 1.8755643367767334
iteration 100, loss 1.8514562845230103
iteration 200, loss 1.887574315071106
iteration 300, loss 1.9210069179534912
iteration 400, loss 1.7652084827423096
iteration 500, loss 1.912732720375061
iteration 600, loss 1.8205180168151855
iteration 700, loss 1.900850772857666
iteration 800, loss 1.7953548431396484
iteration 0, loss 1.7275686264038086
iteration 100, loss 1.8564915657043457
iteration 200, loss 1.7936859130859375
iteration 300, loss 1.8255813121795654
iteration 400, loss 1.8213180303573608
iteration 500, loss 1.8305665254592896
iteration 600, loss 1.8255305290222168
iteration 700, loss 1.912499189376831
iteration 800, loss 1.8021302223205566
iteration 0, loss 1.7752866744995117
iteration 100, loss 1.836714267730713
iteration 200, loss 1.8110902309417725
iteration 300, loss 1.7584540843963623
iteration 400, loss 1.8151860237121582
iteration 500, loss 1.770188570022583
iteration 600, loss 1.8202859163284302
iteration 700, loss 1.8542119264602661
iteration 800, loss 1.8837970495224
iteration 0, loss 1.8667370080947876
iteration 100, loss 1.8038239479064941
iteration 200, loss 1.8916573524475098
iteration 300, loss 1.7523133754730225
iteration 400, loss 1.80266535282135
iteration 500, loss 1.8358057737350464
iteration 600, loss 1.8063368797302246
iteration 700, loss 1.8270875215530396
iteration 800, loss 1.9275840520858765
iteration 0, loss 1.818339467048645
iteration 100, loss 1.8623571395874023
iteration 200, loss 1.8169280290603638
iteration 300, loss 1.807066559791565
iteration 400, loss 1.842862606048584
iteration 500, loss 1.8487448692321777
iteration 600, loss 1.8738062381744385
iteration 700, loss 1.7971676588058472
iteration 800, loss 1.8150010108947754
iteration 0, loss 1.8654831647872925
iteration 100, loss 1.8475936651229858
iteration 200, loss 1.8186227083206177
iteration 300, loss 1.8243308067321777
iteration 400, loss 1.7800065279006958
iteration 500, loss 1.8331378698349
iteration 600, loss 1.8273645639419556
iteration 700, loss 1.8536279201507568
iteration 800, loss 1.8210692405700684
iteration 0, loss 1.8342300653457642
iteration 100, loss 1.8364217281341553
iteration 200, loss 1.870272159576416
iteration 300, loss 1.8420217037200928
iteration 400, loss 1.873126745223999
iteration 500, loss 1.8733322620391846
iteration 600, loss 1.8143951892852783
iteration 700, loss 1.8591312170028687
iteration 800, loss 1.9166046380996704
iteration 0, loss 1.8624402284622192
iteration 100, loss 1.7600626945495605
iteration 200, loss 1.9104031324386597
iteration 300, loss 1.8435546159744263
iteration 400, loss 1.8325127363204956
iteration 500, loss 1.8858217000961304
iteration 600, loss 1.8659658432006836
iteration 700, loss 1.8724420070648193
iteration 800, loss 1.8160086870193481
iteration 0, loss 1.8620787858963013
iteration 100, loss 1.8330490589141846
iteration 200, loss 1.7850762605667114
iteration 300, loss 1.801133155822754
iteration 400, loss 1.908007025718689
iteration 500, loss 1.8357651233673096
iteration 600, loss 1.7713623046875
iteration 700, loss 1.8445472717285156
iteration 800, loss 1.8896437883377075
iteration 0, loss 1.8236041069030762
iteration 100, loss 1.821068286895752
iteration 200, loss 1.847678542137146
iteration 300, loss 1.875369668006897
iteration 400, loss 1.8623194694519043
iteration 500, loss 1.8075075149536133
iteration 600, loss 1.9206831455230713
iteration 700, loss 1.8570725917816162
iteration 800, loss 1.8459492921829224
iteration 0, loss 1.8305778503417969
iteration 100, loss 1.8154304027557373
iteration 200, loss 1.7742379903793335
iteration 300, loss 1.7239142656326294
iteration 400, loss 1.7930759191513062
iteration 500, loss 1.847823977470398
iteration 600, loss 1.9573873281478882
iteration 700, loss 1.9050918817520142
iteration 800, loss 1.8424957990646362
iteration 0, loss 1.8528549671173096
iteration 100, loss 1.941547155380249
iteration 200, loss 1.8598219156265259
iteration 300, loss 1.8324549198150635
iteration 400, loss 1.8118129968643188
iteration 500, loss 1.8395487070083618
iteration 600, loss 1.7560292482376099
iteration 700, loss 1.815774917602539
iteration 800, loss 1.854020595550537
iteration 0, loss 1.7715513706207275
iteration 100, loss 1.8091708421707153
iteration 200, loss 1.7800569534301758
iteration 300, loss 1.781339406967163
iteration 400, loss 1.8195124864578247
iteration 500, loss 1.801212191581726
iteration 600, loss 1.8701212406158447
iteration 700, loss 1.8167921304702759
iteration 800, loss 1.7656464576721191
iteration 0, loss 1.8670508861541748
iteration 100, loss 1.8698043823242188
iteration 200, loss 1.8457059860229492
iteration 300, loss 1.841874122619629
iteration 400, loss 1.8870033025741577
iteration 500, loss 1.7965224981307983
iteration 600, loss 1.8617548942565918
iteration 700, loss 1.8987561464309692
iteration 800, loss 1.8938995599746704
iteration 0, loss 1.8753446340560913
iteration 100, loss 1.7598270177841187
iteration 200, loss 1.8676990270614624
iteration 300, loss 1.7674345970153809
iteration 400, loss 1.7982300519943237
iteration 500, loss 1.857292890548706
iteration 600, loss 1.8194994926452637
iteration 700, loss 1.9029171466827393
iteration 800, loss 1.8163783550262451
iteration 0, loss 1.8911914825439453
iteration 100, loss 1.7825307846069336
iteration 200, loss 1.881866216659546
iteration 300, loss 1.8950620889663696
iteration 400, loss 1.884462594985962
iteration 500, loss 1.7700061798095703
iteration 600, loss 1.8527663946151733
iteration 700, loss 1.8110671043395996
iteration 800, loss 1.8787894248962402
iteration 0, loss 1.7701642513275146
iteration 100, loss 1.8798094987869263
iteration 200, loss 1.8946254253387451
iteration 300, loss 1.8460948467254639
iteration 400, loss 1.7876977920532227
iteration 500, loss 1.8579745292663574
iteration 600, loss 1.8171223402023315
iteration 700, loss 1.8352611064910889
iteration 800, loss 1.81138277053833
iteration 0, loss 1.83369779586792
iteration 100, loss 1.7728873491287231
iteration 200, loss 1.91883385181427
iteration 300, loss 1.9135922193527222
iteration 400, loss 1.7963629961013794
iteration 500, loss 1.8193455934524536
iteration 600, loss 1.7639667987823486
iteration 700, loss 1.7747008800506592
iteration 800, loss 1.825308918952942
iteration 0, loss 1.8034712076187134
iteration 100, loss 1.7890371084213257
iteration 200, loss 1.8095120191574097
iteration 300, loss 1.8137822151184082
iteration 400, loss 1.818278431892395
iteration 500, loss 1.8536728620529175
iteration 600, loss 1.89589262008667
iteration 700, loss 1.8412084579467773
iteration 800, loss 1.806858777999878
iteration 0, loss 1.8793009519577026
iteration 100, loss 1.8518545627593994
iteration 200, loss 1.7918211221694946
iteration 300, loss 1.907606840133667
iteration 400, loss 1.8057774305343628
iteration 500, loss 1.7752788066864014
iteration 600, loss 1.826196551322937
iteration 700, loss 1.8278603553771973
iteration 800, loss 1.8629684448242188
iteration 0, loss 1.7907804250717163
iteration 100, loss 1.8066952228546143
iteration 200, loss 1.8430144786834717
iteration 300, loss 1.7898646593093872
iteration 400, loss 1.853548288345337
iteration 500, loss 1.8245681524276733
iteration 600, loss 1.876718521118164
iteration 700, loss 1.899908185005188
iteration 800, loss 1.9013431072235107
iteration 0, loss 1.8125863075256348
iteration 100, loss 1.7854117155075073
iteration 200, loss 1.8237279653549194
iteration 300, loss 1.8372019529342651
iteration 400, loss 1.8717950582504272
iteration 500, loss 1.8234338760375977
iteration 600, loss 1.8169450759887695
iteration 700, loss 1.7863131761550903
iteration 800, loss 1.8512237071990967
iteration 0, loss 1.7902368307113647
iteration 100, loss 1.7928948402404785
iteration 200, loss 1.8602046966552734
iteration 300, loss 1.8676552772521973
iteration 400, loss 1.8528435230255127
iteration 500, loss 1.8777828216552734
iteration 600, loss 1.8517308235168457
iteration 700, loss 1.7867999076843262
iteration 800, loss 1.782205581665039
iteration 0, loss 1.8571746349334717
iteration 100, loss 1.8686001300811768
iteration 200, loss 1.788244366645813
iteration 300, loss 1.8147425651550293
iteration 400, loss 1.8945187330245972
iteration 500, loss 1.869484543800354
iteration 600, loss 1.8317320346832275
iteration 700, loss 1.9040907621383667
iteration 800, loss 1.8096814155578613
iteration 0, loss 1.8491873741149902
iteration 100, loss 1.831750750541687
iteration 200, loss 1.814978837966919
iteration 300, loss 1.7827863693237305
iteration 400, loss 1.8107410669326782
iteration 500, loss 1.7788316011428833
iteration 600, loss 1.8061541318893433
iteration 700, loss 1.9126631021499634
iteration 800, loss 1.8326823711395264
iteration 0, loss 1.7730586528778076
iteration 100, loss 1.9344645738601685
iteration 200, loss 1.8776719570159912
iteration 300, loss 1.763009786605835
iteration 400, loss 1.7741068601608276
iteration 500, loss 1.9313067197799683
iteration 600, loss 1.9087984561920166
iteration 700, loss 1.7640953063964844
iteration 800, loss 1.8320655822753906
iteration 0, loss 1.8153409957885742
iteration 100, loss 1.7959849834442139
iteration 200, loss 1.782750129699707
iteration 300, loss 1.8270223140716553
iteration 400, loss 1.8927838802337646
iteration 500, loss 1.8323032855987549
iteration 600, loss 1.7425601482391357
iteration 700, loss 1.8369624614715576
iteration 800, loss 1.7666065692901611
iteration 0, loss 1.903952717781067
iteration 100, loss 1.91194486618042
iteration 200, loss 1.798875093460083
iteration 300, loss 1.902381420135498
iteration 400, loss 1.8195972442626953
iteration 500, loss 1.7927812337875366
iteration 600, loss 1.848327875137329
iteration 700, loss 1.8397109508514404
iteration 800, loss 1.825343370437622
iteration 0, loss 1.842400312423706
iteration 100, loss 1.866843819618225
iteration 200, loss 1.850887417793274
iteration 300, loss 1.8285871744155884
iteration 400, loss 1.8242130279541016
iteration 500, loss 1.9115617275238037
iteration 600, loss 1.9152029752731323
iteration 700, loss 1.8711957931518555
iteration 800, loss 1.8897854089736938
iteration 0, loss 1.76915442943573
iteration 100, loss 1.8464922904968262
iteration 200, loss 1.8107922077178955
iteration 300, loss 1.8783392906188965
iteration 400, loss 1.8320266008377075
iteration 500, loss 1.870389699935913
iteration 600, loss 1.8179789781570435
iteration 700, loss 1.864050269126892
iteration 800, loss 1.8772878646850586
iteration 0, loss 1.8774429559707642
iteration 100, loss 1.8689950704574585
iteration 200, loss 1.8948100805282593
iteration 300, loss 1.8746167421340942
iteration 400, loss 1.7625032663345337
iteration 500, loss 1.8673152923583984
iteration 600, loss 1.806581735610962
iteration 700, loss 1.836944341659546
iteration 800, loss 1.7526031732559204
iteration 0, loss 1.8096166849136353
iteration 100, loss 1.8700191974639893
iteration 200, loss 1.794467568397522
iteration 300, loss 1.906076431274414
iteration 400, loss 1.8508896827697754
iteration 500, loss 1.9154056310653687
iteration 600, loss 1.8369486331939697
iteration 700, loss 1.9124237298965454
iteration 800, loss 1.7944204807281494
iteration 0, loss 1.8768126964569092
iteration 100, loss 1.783325433731079
iteration 200, loss 1.8078880310058594
iteration 300, loss 1.7873791456222534
iteration 400, loss 1.829506516456604
iteration 500, loss 1.8374645709991455
iteration 600, loss 1.8090134859085083
iteration 700, loss 1.8179740905761719
iteration 800, loss 1.9029134511947632
iteration 0, loss 1.848624587059021
iteration 100, loss 1.7678992748260498
iteration 200, loss 1.859168529510498
iteration 300, loss 1.8110995292663574
iteration 400, loss 1.8105065822601318
iteration 500, loss 1.7764031887054443
iteration 600, loss 1.8141943216323853
iteration 700, loss 1.8885694742202759
iteration 800, loss 1.789010763168335
iteration 0, loss 1.811532974243164
iteration 100, loss 1.9000296592712402
iteration 200, loss 1.8280435800552368
iteration 300, loss 1.8766605854034424
iteration 400, loss 1.7902486324310303
iteration 500, loss 1.8767774105072021
iteration 600, loss 1.8194513320922852
iteration 700, loss 1.8681844472885132
iteration 800, loss 1.7713772058486938
iteration 0, loss 1.8703622817993164
iteration 100, loss 1.812807321548462
iteration 200, loss 1.7924754619598389
iteration 300, loss 1.8414620161056519
iteration 400, loss 1.897482991218567
iteration 500, loss 1.7617613077163696
iteration 600, loss 1.8451817035675049
iteration 700, loss 1.8759783506393433
iteration 800, loss 1.8176754713058472
iteration 0, loss 1.8608449697494507
iteration 100, loss 1.8372021913528442
iteration 200, loss 1.7239426374435425
iteration 300, loss 1.8365702629089355
iteration 400, loss 1.8348829746246338
iteration 500, loss 1.809266448020935
iteration 600, loss 1.765282154083252
iteration 700, loss 1.874945044517517
iteration 800, loss 1.8460111618041992
iteration 0, loss 1.8414907455444336
iteration 100, loss 1.8063340187072754
iteration 200, loss 1.7799372673034668
iteration 300, loss 1.748037576675415
iteration 400, loss 1.8406544923782349
iteration 500, loss 1.8852797746658325
iteration 600, loss 1.7743237018585205
iteration 700, loss 1.7862051725387573
iteration 800, loss 1.8332136869430542
iteration 0, loss 1.7835488319396973
iteration 100, loss 1.7905104160308838
iteration 200, loss 1.891227126121521
iteration 300, loss 1.8222301006317139
iteration 400, loss 1.7844058275222778
iteration 500, loss 1.9575930833816528
iteration 600, loss 1.8663506507873535
iteration 700, loss 1.77388596534729
iteration 800, loss 1.9116978645324707
iteration 0, loss 1.8383432626724243
iteration 100, loss 1.8150123357772827
iteration 200, loss 1.8595871925354004
iteration 300, loss 1.7612496614456177
iteration 400, loss 1.8499419689178467
iteration 500, loss 1.8431425094604492
iteration 600, loss 1.7815592288970947
iteration 700, loss 1.7962630987167358
iteration 800, loss 1.8093019723892212
iteration 0, loss 1.853319525718689
iteration 100, loss 1.7677218914031982
iteration 200, loss 1.7905926704406738
iteration 300, loss 1.8460602760314941
iteration 400, loss 1.7961208820343018
iteration 500, loss 1.7899482250213623
iteration 600, loss 1.7361397743225098
iteration 700, loss 1.9055142402648926
iteration 800, loss 1.8623871803283691
iteration 0, loss 1.8474363088607788
iteration 100, loss 1.8357670307159424
iteration 200, loss 1.893892526626587
iteration 300, loss 1.884860873222351
iteration 400, loss 1.8295302391052246
iteration 500, loss 1.8116239309310913
iteration 600, loss 1.9284111261367798
iteration 700, loss 1.739614486694336
iteration 800, loss 1.8945307731628418
iteration 0, loss 1.8847519159317017
iteration 100, loss 1.8222784996032715
iteration 200, loss 1.8387326002120972
iteration 300, loss 1.8345190286636353
iteration 400, loss 1.8866205215454102
iteration 500, loss 1.7818506956100464
iteration 600, loss 1.7748900651931763
iteration 700, loss 1.7866967916488647
iteration 800, loss 1.8695018291473389
iteration 0, loss 1.8649532794952393
iteration 100, loss 1.8602126836776733
iteration 200, loss 1.8209630250930786
iteration 300, loss 1.855408787727356
iteration 400, loss 1.8131301403045654
iteration 500, loss 1.7903028726577759
iteration 600, loss 1.844017744064331
iteration 700, loss 1.8316129446029663
iteration 800, loss 1.7969073057174683
iteration 0, loss 1.8295841217041016
iteration 100, loss 1.8782906532287598
iteration 200, loss 1.7300142049789429
iteration 300, loss 1.8365567922592163
iteration 400, loss 1.8430265188217163
iteration 500, loss 1.8515214920043945
iteration 600, loss 1.8083970546722412
iteration 700, loss 1.8569200038909912
iteration 800, loss 1.783656120300293
iteration 0, loss 1.809538722038269
iteration 100, loss 1.827659249305725
iteration 200, loss 1.8037086725234985
iteration 300, loss 1.8023121356964111
iteration 400, loss 1.742944359779358
iteration 500, loss 1.847240924835205
iteration 600, loss 1.8200796842575073
iteration 700, loss 1.808557391166687
iteration 800, loss 1.8995521068572998
iteration 0, loss 1.8143655061721802
iteration 100, loss 1.8010399341583252
iteration 200, loss 1.865226149559021
iteration 300, loss 1.8486096858978271
iteration 400, loss 1.7999484539031982
iteration 500, loss 1.8329076766967773
iteration 600, loss 1.7956757545471191
iteration 700, loss 1.8421833515167236
iteration 800, loss 1.815017580986023
iteration 0, loss 1.7832595109939575
iteration 100, loss 1.808167576789856
iteration 200, loss 1.9069297313690186
iteration 300, loss 1.8605458736419678
iteration 400, loss 1.8521077632904053
iteration 500, loss 1.8385661840438843
iteration 600, loss 1.832849144935608
iteration 700, loss 1.9021332263946533
iteration 800, loss 1.8718527555465698
iteration 0, loss 1.8924717903137207
iteration 100, loss 1.795310139656067
iteration 200, loss 1.88002610206604
iteration 300, loss 1.8767197132110596
iteration 400, loss 1.8760613203048706
iteration 500, loss 1.8063249588012695
iteration 600, loss 1.7715609073638916
iteration 700, loss 1.8092625141143799
iteration 800, loss 1.764838457107544
iteration 0, loss 1.7588304281234741
iteration 100, loss 1.8085930347442627
iteration 200, loss 1.8268704414367676
iteration 300, loss 1.8160338401794434
iteration 400, loss 1.9057860374450684
iteration 500, loss 1.8686424493789673
iteration 600, loss 1.841766595840454
iteration 700, loss 1.8040668964385986
iteration 800, loss 1.7729897499084473
fold 2 accuracy: 0.5233571428571429
iteration 0, loss 1.900519847869873
iteration 100, loss 1.78163480758667
iteration 200, loss 1.9300709962844849
iteration 300, loss 1.846314787864685
iteration 400, loss 1.850119948387146
iteration 500, loss 1.8090839385986328
iteration 600, loss 1.8115087747573853
iteration 700, loss 1.8897937536239624
iteration 800, loss 1.8728348016738892
iteration 0, loss 1.7598097324371338
iteration 100, loss 1.9666144847869873
iteration 200, loss 1.910366415977478
iteration 300, loss 1.8533203601837158
iteration 400, loss 1.9037202596664429
iteration 500, loss 1.8442374467849731
iteration 600, loss 1.8351833820343018
iteration 700, loss 1.9061150550842285
iteration 800, loss 1.8299875259399414
iteration 0, loss 1.8478500843048096
iteration 100, loss 1.9013787508010864
iteration 200, loss 1.8818625211715698
iteration 300, loss 1.8058189153671265
iteration 400, loss 1.8047919273376465
iteration 500, loss 1.7348875999450684
iteration 600, loss 1.7892262935638428
iteration 700, loss 1.824476718902588
iteration 800, loss 1.8339906930923462
iteration 0, loss 1.7699763774871826
iteration 100, loss 1.8259038925170898
iteration 200, loss 1.890061616897583
iteration 300, loss 1.9121156930923462
iteration 400, loss 1.8122780323028564
iteration 500, loss 1.8015750646591187
iteration 600, loss 1.923959732055664
iteration 700, loss 1.8024990558624268
iteration 800, loss 1.8366323709487915
iteration 0, loss 1.8789021968841553
iteration 100, loss 1.8440600633621216
iteration 200, loss 1.833410382270813
iteration 300, loss 1.8514158725738525
iteration 400, loss 1.8611401319503784
iteration 500, loss 1.8211532831192017
iteration 600, loss 1.8092315196990967
iteration 700, loss 1.7893104553222656
iteration 800, loss 1.8045854568481445
iteration 0, loss 1.8593634366989136
iteration 100, loss 1.8606436252593994
iteration 200, loss 1.8564921617507935
iteration 300, loss 1.7950568199157715
iteration 400, loss 1.8079192638397217
iteration 500, loss 1.8507808446884155
iteration 600, loss 1.8413422107696533
iteration 700, loss 1.8628686666488647
iteration 800, loss 1.8330247402191162
iteration 0, loss 1.8032422065734863
iteration 100, loss 1.8577276468276978
iteration 200, loss 1.8202754259109497
iteration 300, loss 1.8807231187820435
iteration 400, loss 1.8795260190963745
iteration 500, loss 1.8892862796783447
iteration 600, loss 1.809543251991272
iteration 700, loss 1.8655719757080078
iteration 800, loss 1.88186776638031
iteration 0, loss 1.8779757022857666
iteration 100, loss 1.9464064836502075
iteration 200, loss 1.8215415477752686
iteration 300, loss 1.8234219551086426
iteration 400, loss 1.8344577550888062
iteration 500, loss 1.7921028137207031
iteration 600, loss 1.833322525024414
iteration 700, loss 1.8857907056808472
iteration 800, loss 1.7541545629501343
iteration 0, loss 1.8502187728881836
iteration 100, loss 1.8515467643737793
iteration 200, loss 1.8621546030044556
iteration 300, loss 1.83013916015625
iteration 400, loss 1.7748397588729858
iteration 500, loss 1.8415789604187012
iteration 600, loss 1.857224464416504
iteration 700, loss 1.9083106517791748
iteration 800, loss 1.7972054481506348
iteration 0, loss 1.8718109130859375
iteration 100, loss 1.7924189567565918
iteration 200, loss 1.7819316387176514
iteration 300, loss 1.8649626970291138
iteration 400, loss 1.782222032546997
iteration 500, loss 1.8250352144241333
iteration 600, loss 1.7497596740722656
iteration 700, loss 1.8846133947372437
iteration 800, loss 1.8362853527069092
iteration 0, loss 1.8043111562728882
iteration 100, loss 1.842335820198059
iteration 200, loss 1.7941113710403442
iteration 300, loss 1.7916020154953003
iteration 400, loss 1.8566685914993286
iteration 500, loss 1.797383427619934
iteration 600, loss 1.814322590827942
iteration 700, loss 1.7656227350234985
iteration 800, loss 1.9253755807876587
iteration 0, loss 1.845456600189209
iteration 100, loss 1.8353164196014404
iteration 200, loss 1.8400509357452393
iteration 300, loss 1.7210191488265991
iteration 400, loss 1.7822076082229614
iteration 500, loss 1.7893788814544678
iteration 600, loss 1.880015254020691
iteration 700, loss 1.8537571430206299
iteration 800, loss 1.839053750038147
iteration 0, loss 1.8264449834823608
iteration 100, loss 1.73561692237854
iteration 200, loss 1.838350772857666
iteration 300, loss 1.7139612436294556
iteration 400, loss 1.8218319416046143
iteration 500, loss 1.953500747680664
iteration 600, loss 1.8962488174438477
iteration 700, loss 1.8672480583190918
iteration 800, loss 1.864141583442688
iteration 0, loss 1.836905598640442
iteration 100, loss 1.858068823814392
iteration 200, loss 1.7691686153411865
iteration 300, loss 1.8344835042953491
iteration 400, loss 1.8623324632644653
iteration 500, loss 1.8186205625534058
iteration 600, loss 1.8263558149337769
iteration 700, loss 1.8239448070526123
iteration 800, loss 1.7819710969924927
iteration 0, loss 1.7957226037979126
iteration 100, loss 1.8071403503417969
iteration 200, loss 1.8400269746780396
iteration 300, loss 1.7980448007583618
iteration 400, loss 1.8522884845733643
iteration 500, loss 1.8501700162887573
iteration 600, loss 1.854513168334961
iteration 700, loss 1.8488047122955322
iteration 800, loss 1.886405110359192
iteration 0, loss 1.9010496139526367
iteration 100, loss 1.9397975206375122
iteration 200, loss 1.861011266708374
iteration 300, loss 1.7967596054077148
iteration 400, loss 1.7819595336914062
iteration 500, loss 1.848548173904419
iteration 600, loss 1.8324923515319824
iteration 700, loss 1.8104268312454224
iteration 800, loss 1.8585671186447144
iteration 0, loss 1.8366717100143433
iteration 100, loss 1.8242669105529785
iteration 200, loss 1.793352723121643
iteration 300, loss 1.7924243211746216
iteration 400, loss 1.8563541173934937
iteration 500, loss 1.8530235290527344
iteration 600, loss 1.8450162410736084
iteration 700, loss 1.869972825050354
iteration 800, loss 1.8207385540008545
iteration 0, loss 1.8689419031143188
iteration 100, loss 1.8420324325561523
iteration 200, loss 1.9394081830978394
iteration 300, loss 1.786224126815796
iteration 400, loss 1.8868199586868286
iteration 500, loss 1.9056317806243896
iteration 600, loss 1.8125582933425903
iteration 700, loss 1.8405117988586426
iteration 800, loss 1.8948206901550293
iteration 0, loss 1.813633918762207
iteration 100, loss 1.8824172019958496
iteration 200, loss 1.8211338520050049
iteration 300, loss 1.8862636089324951
iteration 400, loss 1.7603181600570679
iteration 500, loss 1.8169050216674805
iteration 600, loss 1.7587499618530273
iteration 700, loss 1.8255524635314941
iteration 800, loss 1.815084457397461
iteration 0, loss 1.780923843383789
iteration 100, loss 1.864279866218567
iteration 200, loss 1.8406492471694946
iteration 300, loss 1.8069610595703125
iteration 400, loss 1.8460623025894165
iteration 500, loss 1.8674064874649048
iteration 600, loss 1.8606306314468384
iteration 700, loss 1.7898576259613037
iteration 800, loss 1.8066635131835938
iteration 0, loss 1.7790921926498413
iteration 100, loss 1.8727725744247437
iteration 200, loss 1.815054178237915
iteration 300, loss 1.7863789796829224
iteration 400, loss 1.9415342807769775
iteration 500, loss 1.8190704584121704
iteration 600, loss 1.7985544204711914
iteration 700, loss 1.8325655460357666
iteration 800, loss 1.817577838897705
iteration 0, loss 1.811126470565796
iteration 100, loss 1.81034517288208
iteration 200, loss 1.7402305603027344
iteration 300, loss 1.7788728475570679
iteration 400, loss 1.8363529443740845
iteration 500, loss 1.827568769454956
iteration 600, loss 1.8492603302001953
iteration 700, loss 1.8622175455093384
iteration 800, loss 1.8517512083053589
iteration 0, loss 1.8121817111968994
iteration 100, loss 1.8838212490081787
iteration 200, loss 1.7924250364303589
iteration 300, loss 1.852811574935913
iteration 400, loss 1.8922135829925537
iteration 500, loss 1.9029183387756348
iteration 600, loss 1.8150665760040283
iteration 700, loss 1.786712884902954
iteration 800, loss 1.893256664276123
iteration 0, loss 1.8728957176208496
iteration 100, loss 1.8387409448623657
iteration 200, loss 1.7545527219772339
iteration 300, loss 1.8739064931869507
iteration 400, loss 2.0201575756073
iteration 500, loss 1.880599021911621
iteration 600, loss 1.7684439420700073
iteration 700, loss 1.8486963510513306
iteration 800, loss 1.8190630674362183
iteration 0, loss 1.8353612422943115
iteration 100, loss 1.7663065195083618
iteration 200, loss 1.7778681516647339
iteration 300, loss 1.7653560638427734
iteration 400, loss 1.8145711421966553
iteration 500, loss 1.8280508518218994
iteration 600, loss 1.8379697799682617
iteration 700, loss 1.8020570278167725
iteration 800, loss 1.8218188285827637
iteration 0, loss 1.7453726530075073
iteration 100, loss 1.811564326286316
iteration 200, loss 1.868050217628479
iteration 300, loss 1.8316401243209839
iteration 400, loss 1.7926020622253418
iteration 500, loss 1.8270491361618042
iteration 600, loss 1.8707462549209595
iteration 700, loss 1.8943665027618408
iteration 800, loss 1.797363042831421
iteration 0, loss 1.732984185218811
iteration 100, loss 1.7977350950241089
iteration 200, loss 1.8475253582000732
iteration 300, loss 1.9096665382385254
iteration 400, loss 1.7581723928451538
iteration 500, loss 1.7418670654296875
iteration 600, loss 1.7424802780151367
iteration 700, loss 1.7322580814361572
iteration 800, loss 1.900155782699585
iteration 0, loss 1.8536078929901123
iteration 100, loss 1.8459718227386475
iteration 200, loss 1.7595713138580322
iteration 300, loss 1.8697583675384521
iteration 400, loss 1.8244760036468506
iteration 500, loss 1.9050936698913574
iteration 600, loss 1.9071182012557983
iteration 700, loss 1.8074760437011719
iteration 800, loss 1.7909235954284668
iteration 0, loss 1.7360833883285522
iteration 100, loss 1.825758934020996
iteration 200, loss 1.8898488283157349
iteration 300, loss 1.7586474418640137
iteration 400, loss 1.8105894327163696
iteration 500, loss 1.9336602687835693
iteration 600, loss 1.7667388916015625
iteration 700, loss 1.8333035707473755
iteration 800, loss 1.8674730062484741
iteration 0, loss 1.8582433462142944
iteration 100, loss 1.873677372932434
iteration 200, loss 1.866994857788086
iteration 300, loss 1.8373345136642456
iteration 400, loss 1.9108643531799316
iteration 500, loss 1.762510895729065
iteration 600, loss 1.7434051036834717
iteration 700, loss 1.8640880584716797
iteration 800, loss 1.8899791240692139
iteration 0, loss 1.8240487575531006
iteration 100, loss 1.7804385423660278
iteration 200, loss 1.8194520473480225
iteration 300, loss 1.7858375310897827
iteration 400, loss 1.787498116493225
iteration 500, loss 1.9289032220840454
iteration 600, loss 1.7887709140777588
iteration 700, loss 1.7981112003326416
iteration 800, loss 1.9025424718856812
iteration 0, loss 1.7440659999847412
iteration 100, loss 1.886148452758789
iteration 200, loss 1.8069005012512207
iteration 300, loss 1.9000402688980103
iteration 400, loss 1.8832097053527832
iteration 500, loss 1.899274230003357
iteration 600, loss 1.880223274230957
iteration 700, loss 1.8343325853347778
iteration 800, loss 1.7382798194885254
iteration 0, loss 1.8403081893920898
iteration 100, loss 1.8307647705078125
iteration 200, loss 1.761311411857605
iteration 300, loss 1.8213739395141602
iteration 400, loss 1.8752650022506714
iteration 500, loss 1.8545804023742676
iteration 600, loss 1.867882251739502
iteration 700, loss 1.8551793098449707
iteration 800, loss 1.7853903770446777
iteration 0, loss 1.8286494016647339
iteration 100, loss 1.8150155544281006
iteration 200, loss 1.8408442735671997
iteration 300, loss 1.850218415260315
iteration 400, loss 1.8288516998291016
iteration 500, loss 1.7652579545974731
iteration 600, loss 1.910060167312622
iteration 700, loss 1.8534376621246338
iteration 800, loss 1.7399697303771973
iteration 0, loss 1.8237121105194092
iteration 100, loss 1.8208332061767578
iteration 200, loss 1.7597767114639282
iteration 300, loss 1.8907477855682373
iteration 400, loss 1.8219780921936035
iteration 500, loss 1.8318108320236206
iteration 600, loss 1.941480040550232
iteration 700, loss 1.8844892978668213
iteration 800, loss 1.829451322555542
iteration 0, loss 1.7293843030929565
iteration 100, loss 1.852748990058899
iteration 200, loss 1.7999238967895508
iteration 300, loss 1.8040565252304077
iteration 400, loss 1.8289841413497925
iteration 500, loss 1.8093665838241577
iteration 600, loss 1.819316029548645
iteration 700, loss 1.9005171060562134
iteration 800, loss 1.8708117008209229
iteration 0, loss 1.777369737625122
iteration 100, loss 1.8417919874191284
iteration 200, loss 1.8502146005630493
iteration 300, loss 1.7546043395996094
iteration 400, loss 1.8643196821212769
iteration 500, loss 1.8160470724105835
iteration 600, loss 1.8512312173843384
iteration 700, loss 1.7697463035583496
iteration 800, loss 1.7772490978240967
iteration 0, loss 1.8018966913223267
iteration 100, loss 1.7983916997909546
iteration 200, loss 1.7323133945465088
iteration 300, loss 1.8794845342636108
iteration 400, loss 1.8142542839050293
iteration 500, loss 1.811707854270935
iteration 600, loss 1.774979829788208
iteration 700, loss 1.7971947193145752
iteration 800, loss 1.9783408641815186
iteration 0, loss 1.7986499071121216
iteration 100, loss 1.7874890565872192
iteration 200, loss 1.8607066869735718
iteration 300, loss 1.958213210105896
iteration 400, loss 1.7770119905471802
iteration 500, loss 1.8236205577850342
iteration 600, loss 1.8739662170410156
iteration 700, loss 1.7960256338119507
iteration 800, loss 1.861267328262329
iteration 0, loss 1.8475338220596313
iteration 100, loss 1.7796801328659058
iteration 200, loss 1.904369592666626
iteration 300, loss 1.8118444681167603
iteration 400, loss 1.9195096492767334
iteration 500, loss 1.7794746160507202
iteration 600, loss 1.8134534358978271
iteration 700, loss 1.8735157251358032
iteration 800, loss 1.7648407220840454
iteration 0, loss 1.7767902612686157
iteration 100, loss 1.7850795984268188
iteration 200, loss 1.7974761724472046
iteration 300, loss 1.8809540271759033
iteration 400, loss 1.8408905267715454
iteration 500, loss 1.8184078931808472
iteration 600, loss 1.8980110883712769
iteration 700, loss 1.8429926633834839
iteration 800, loss 1.751808762550354
iteration 0, loss 1.8665845394134521
iteration 100, loss 1.7807819843292236
iteration 200, loss 1.9470371007919312
iteration 300, loss 1.9000263214111328
iteration 400, loss 1.7597726583480835
iteration 500, loss 1.8562489748001099
iteration 600, loss 1.8428446054458618
iteration 700, loss 1.7309788465499878
iteration 800, loss 1.7603060007095337
iteration 0, loss 1.8359653949737549
iteration 100, loss 1.7805657386779785
iteration 200, loss 1.8131515979766846
iteration 300, loss 1.8032208681106567
iteration 400, loss 1.8056679964065552
iteration 500, loss 1.8560662269592285
iteration 600, loss 1.8730286359786987
iteration 700, loss 1.8116050958633423
iteration 800, loss 1.8358001708984375
iteration 0, loss 1.8431345224380493
iteration 100, loss 1.8337382078170776
iteration 200, loss 1.8097089529037476
iteration 300, loss 1.8187272548675537
iteration 400, loss 1.8749898672103882
iteration 500, loss 1.8477197885513306
iteration 600, loss 1.7536976337432861
iteration 700, loss 1.844580054283142
iteration 800, loss 1.8590034246444702
iteration 0, loss 1.80330228805542
iteration 100, loss 1.7417635917663574
iteration 200, loss 1.8209614753723145
iteration 300, loss 1.7887065410614014
iteration 400, loss 1.8596299886703491
iteration 500, loss 1.8354697227478027
iteration 600, loss 1.8670547008514404
iteration 700, loss 1.8518104553222656
iteration 800, loss 1.8121864795684814
iteration 0, loss 1.8754041194915771
iteration 100, loss 1.8494659662246704
iteration 200, loss 1.765647053718567
iteration 300, loss 1.8621073961257935
iteration 400, loss 1.799759030342102
iteration 500, loss 1.8644673824310303
iteration 600, loss 1.8092319965362549
iteration 700, loss 1.7478870153427124
iteration 800, loss 1.8305065631866455
iteration 0, loss 1.7458412647247314
iteration 100, loss 1.8255022764205933
iteration 200, loss 1.9124062061309814
iteration 300, loss 1.7792953252792358
iteration 400, loss 1.8317068815231323
iteration 500, loss 1.8096070289611816
iteration 600, loss 1.876930594444275
iteration 700, loss 1.8680458068847656
iteration 800, loss 1.855666995048523
iteration 0, loss 1.8327423334121704
iteration 100, loss 1.7733155488967896
iteration 200, loss 1.8999348878860474
iteration 300, loss 1.7961697578430176
iteration 400, loss 1.8750642538070679
iteration 500, loss 1.7980619668960571
iteration 600, loss 1.8058167695999146
iteration 700, loss 1.7746262550354004
iteration 800, loss 1.815970778465271
iteration 0, loss 1.7769391536712646
iteration 100, loss 1.7402654886245728
iteration 200, loss 1.9010270833969116
iteration 300, loss 1.8407747745513916
iteration 400, loss 1.7779854536056519
iteration 500, loss 1.7608921527862549
iteration 600, loss 1.8502719402313232
iteration 700, loss 1.8929916620254517
iteration 800, loss 1.7807151079177856
iteration 0, loss 1.788958191871643
iteration 100, loss 1.810793161392212
iteration 200, loss 1.8613028526306152
iteration 300, loss 1.8084371089935303
iteration 400, loss 1.8200485706329346
iteration 500, loss 1.8202918767929077
iteration 600, loss 1.8698794841766357
iteration 700, loss 1.8032324314117432
iteration 800, loss 1.788411021232605
fold 3 accuracy: 0.5143571428571428
iteration 0, loss 1.8844788074493408
iteration 100, loss 1.8563892841339111
iteration 200, loss 1.7831319570541382
iteration 300, loss 1.8191436529159546
iteration 400, loss 1.9077436923980713
iteration 500, loss 1.9292452335357666
iteration 600, loss 1.825344204902649
iteration 700, loss 1.8907829523086548
iteration 800, loss 1.81572425365448
iteration 0, loss 1.7383480072021484
iteration 100, loss 1.9191083908081055
iteration 200, loss 1.8567835092544556
iteration 300, loss 1.8793143033981323
iteration 400, loss 1.873300552368164
iteration 500, loss 1.7540247440338135
iteration 600, loss 1.853971242904663
iteration 700, loss 1.7934174537658691
iteration 800, loss 1.810292363166809
iteration 0, loss 1.895255208015442
iteration 100, loss 1.7798233032226562
iteration 200, loss 1.8227205276489258
iteration 300, loss 1.7799204587936401
iteration 400, loss 1.9211742877960205
iteration 500, loss 1.8746776580810547
iteration 600, loss 1.8308584690093994
iteration 700, loss 1.8175597190856934
iteration 800, loss 1.834096908569336
iteration 0, loss 1.807334303855896
iteration 100, loss 1.8908473253250122
iteration 200, loss 1.9070732593536377
iteration 300, loss 1.8884556293487549
iteration 400, loss 1.8672810792922974
iteration 500, loss 1.7975579500198364
iteration 600, loss 1.9134012460708618
iteration 700, loss 1.8479580879211426
iteration 800, loss 1.8642371892929077
iteration 0, loss 1.7778757810592651
iteration 100, loss 1.8310726881027222
iteration 200, loss 1.9157979488372803
iteration 300, loss 1.8475341796875
iteration 400, loss 1.8534184694290161
iteration 500, loss 1.7543716430664062
iteration 600, loss 1.7853068113327026
iteration 700, loss 1.9090909957885742
iteration 800, loss 1.920148253440857
iteration 0, loss 1.7974735498428345
iteration 100, loss 1.8421913385391235
iteration 200, loss 1.811922311782837
iteration 300, loss 1.7634739875793457
iteration 400, loss 1.8269851207733154
iteration 500, loss 1.8604782819747925
iteration 600, loss 1.8363837003707886
iteration 700, loss 1.8112599849700928
iteration 800, loss 1.8359731435775757
iteration 0, loss 1.8424012660980225
iteration 100, loss 1.791482925415039
iteration 200, loss 1.81024169921875
iteration 300, loss 1.8014845848083496
iteration 400, loss 1.9118218421936035
iteration 500, loss 1.8039897680282593
iteration 600, loss 1.7718791961669922
iteration 700, loss 1.8311132192611694
iteration 800, loss 1.8462986946105957
iteration 0, loss 1.7986074686050415
iteration 100, loss 1.8709837198257446
iteration 200, loss 1.8641045093536377
iteration 300, loss 1.7867262363433838
iteration 400, loss 1.850090503692627
iteration 500, loss 1.7634199857711792
iteration 600, loss 1.790431022644043
iteration 700, loss 1.8102842569351196
iteration 800, loss 1.8654463291168213
iteration 0, loss 1.8071584701538086
iteration 100, loss 1.793269395828247
iteration 200, loss 1.8501635789871216
iteration 300, loss 1.9380824565887451
iteration 400, loss 1.8189094066619873
iteration 500, loss 1.7917425632476807
iteration 600, loss 1.8060994148254395
iteration 700, loss 1.8409664630889893
iteration 800, loss 1.8524366617202759
iteration 0, loss 1.9075210094451904
iteration 100, loss 1.9252008199691772
iteration 200, loss 1.7817858457565308
iteration 300, loss 1.8588701486587524
iteration 400, loss 1.8603811264038086
iteration 500, loss 1.875657558441162
iteration 600, loss 1.7468483448028564
iteration 700, loss 1.7671148777008057
iteration 800, loss 1.851726770401001
iteration 0, loss 1.8007453680038452
iteration 100, loss 1.8516737222671509
iteration 200, loss 1.8586257696151733
iteration 300, loss 1.7957617044448853
iteration 400, loss 1.8757466077804565
iteration 500, loss 1.865487813949585
iteration 600, loss 1.8724700212478638
iteration 700, loss 1.932659387588501
iteration 800, loss 1.8495734930038452
iteration 0, loss 1.7742031812667847
iteration 100, loss 1.9099688529968262
iteration 200, loss 1.8403716087341309
iteration 300, loss 1.8745893239974976
iteration 400, loss 1.7927839756011963
iteration 500, loss 1.7708327770233154
iteration 600, loss 1.8637126684188843
iteration 700, loss 1.7784568071365356
iteration 800, loss 1.931219458580017
iteration 0, loss 1.8762625455856323
iteration 100, loss 1.8433046340942383
iteration 200, loss 1.9102861881256104
iteration 300, loss 1.8549003601074219
iteration 400, loss 1.9023451805114746
iteration 500, loss 1.8410588502883911
iteration 600, loss 1.858931541442871
iteration 700, loss 1.8955140113830566
iteration 800, loss 1.8209636211395264
iteration 0, loss 1.8308366537094116
iteration 100, loss 1.8212502002716064
iteration 200, loss 1.8601839542388916
iteration 300, loss 1.816422939300537
iteration 400, loss 1.8369026184082031
iteration 500, loss 1.8881444931030273
iteration 600, loss 1.7908358573913574
iteration 700, loss 1.889029622077942
iteration 800, loss 1.7428011894226074
iteration 0, loss 1.9466851949691772
iteration 100, loss 1.8281941413879395
iteration 200, loss 1.7716689109802246
iteration 300, loss 1.8487677574157715
iteration 400, loss 1.791778802871704
iteration 500, loss 1.8343544006347656
iteration 600, loss 1.8627572059631348
iteration 700, loss 1.7993406057357788
iteration 800, loss 1.8868308067321777
iteration 0, loss 1.802719235420227
iteration 100, loss 1.794593095779419
iteration 200, loss 1.8504612445831299
iteration 300, loss 1.8259862661361694
iteration 400, loss 1.7881426811218262
iteration 500, loss 1.8158702850341797
iteration 600, loss 1.7966865301132202
iteration 700, loss 1.8829751014709473
iteration 800, loss 1.8078416585922241
iteration 0, loss 1.873384952545166
iteration 100, loss 1.8443472385406494
iteration 200, loss 1.7849010229110718
iteration 300, loss 1.8505332469940186
iteration 400, loss 1.8482614755630493
iteration 500, loss 1.8326698541641235
iteration 600, loss 1.8399003744125366
iteration 700, loss 1.857295036315918
iteration 800, loss 1.8729968070983887
iteration 0, loss 1.800517201423645
iteration 100, loss 1.8920351266860962
iteration 200, loss 1.8661458492279053
iteration 300, loss 1.8005757331848145
iteration 400, loss 1.7473695278167725
iteration 500, loss 1.895932912826538
iteration 600, loss 1.8865609169006348
iteration 700, loss 1.8617552518844604
iteration 800, loss 1.7737629413604736
iteration 0, loss 1.7398664951324463
iteration 100, loss 1.7997642755508423
iteration 200, loss 1.8239173889160156
iteration 300, loss 1.8318917751312256
iteration 400, loss 1.8234599828720093
iteration 500, loss 1.8762681484222412
iteration 600, loss 1.795849323272705
iteration 700, loss 1.8138426542282104
iteration 800, loss 1.8278822898864746
iteration 0, loss 1.9189833402633667
iteration 100, loss 1.8458393812179565
iteration 200, loss 1.8926920890808105
iteration 300, loss 1.8359172344207764
iteration 400, loss 1.800403356552124
iteration 500, loss 1.7810001373291016
iteration 600, loss 1.815459966659546
iteration 700, loss 1.7867623567581177
iteration 800, loss 1.8223211765289307
iteration 0, loss 1.8394755125045776
iteration 100, loss 1.7976984977722168
iteration 200, loss 1.8582676649093628
iteration 300, loss 1.7923471927642822
iteration 400, loss 1.8835091590881348
iteration 500, loss 1.8288013935089111
iteration 600, loss 1.77424156665802
iteration 700, loss 1.8282456398010254
iteration 800, loss 1.7422380447387695
iteration 0, loss 1.8611820936203003
iteration 100, loss 1.8357322216033936
iteration 200, loss 1.8694581985473633
iteration 300, loss 1.8902108669281006
iteration 400, loss 1.8889329433441162
iteration 500, loss 1.7985889911651611
iteration 600, loss 1.9318301677703857
iteration 700, loss 1.8200204372406006
iteration 800, loss 1.857153296470642
iteration 0, loss 1.817038893699646
iteration 100, loss 1.8110692501068115
iteration 200, loss 1.8095800876617432
iteration 300, loss 1.8382340669631958
iteration 400, loss 1.8360285758972168
iteration 500, loss 1.8433538675308228
iteration 600, loss 1.8471755981445312
iteration 700, loss 1.8592932224273682
iteration 800, loss 1.9222688674926758
iteration 0, loss 1.8048895597457886
iteration 100, loss 1.8489060401916504
iteration 200, loss 1.8026461601257324
iteration 300, loss 1.8030205965042114
iteration 400, loss 1.7501124143600464
iteration 500, loss 1.760781168937683
iteration 600, loss 1.759486436843872
iteration 700, loss 1.8212372064590454
iteration 800, loss 1.8431309461593628
iteration 0, loss 1.8091150522232056
iteration 100, loss 1.8808400630950928
iteration 200, loss 1.9269566535949707
iteration 300, loss 1.8125882148742676
iteration 400, loss 1.798264980316162
iteration 500, loss 1.8149738311767578
iteration 600, loss 1.8635159730911255
iteration 700, loss 1.822668194770813
iteration 800, loss 1.8717200756072998
iteration 0, loss 1.785069227218628
iteration 100, loss 1.8418176174163818
iteration 200, loss 1.9415892362594604
iteration 300, loss 1.8232566118240356
iteration 400, loss 1.7554785013198853
iteration 500, loss 1.80765700340271
iteration 600, loss 1.8077318668365479
iteration 700, loss 1.856680989265442
iteration 800, loss 1.781636118888855
iteration 0, loss 1.8875285387039185
iteration 100, loss 1.909989595413208
iteration 200, loss 1.8878191709518433
iteration 300, loss 1.7981005907058716
iteration 400, loss 1.796445608139038
iteration 500, loss 1.7377030849456787
iteration 600, loss 1.7646230459213257
iteration 700, loss 1.8602558374404907
iteration 800, loss 1.843824863433838
iteration 0, loss 1.804824709892273
iteration 100, loss 1.8199901580810547
iteration 200, loss 1.8009166717529297
iteration 300, loss 1.909132480621338
iteration 400, loss 1.7494195699691772
iteration 500, loss 1.7705378532409668
iteration 600, loss 1.8767622709274292
iteration 700, loss 1.7559839487075806
iteration 800, loss 1.7637022733688354
iteration 0, loss 1.7329232692718506
iteration 100, loss 1.8221876621246338
iteration 200, loss 1.809497594833374
iteration 300, loss 1.8376872539520264
iteration 400, loss 1.7464240789413452
iteration 500, loss 1.832728624343872
iteration 600, loss 1.8791074752807617
iteration 700, loss 1.8726282119750977
iteration 800, loss 1.895928144454956
iteration 0, loss 1.8712458610534668
iteration 100, loss 1.8517918586730957
iteration 200, loss 1.7810436487197876
iteration 300, loss 1.8178306818008423
iteration 400, loss 1.8014507293701172
iteration 500, loss 1.8663398027420044
iteration 600, loss 1.7520884275436401
iteration 700, loss 1.792720079421997
iteration 800, loss 1.8922747373580933
iteration 0, loss 1.8566603660583496
iteration 100, loss 1.8533512353897095
iteration 200, loss 1.829865574836731
iteration 300, loss 1.7968087196350098
iteration 400, loss 1.8643583059310913
iteration 500, loss 1.9749237298965454
iteration 600, loss 1.889776587486267
iteration 700, loss 1.798190951347351
iteration 800, loss 1.72087824344635
iteration 0, loss 1.8362977504730225
iteration 100, loss 1.869364619255066
iteration 200, loss 1.862218976020813
iteration 300, loss 1.8561663627624512
iteration 400, loss 1.830112099647522
iteration 500, loss 1.8177598714828491
iteration 600, loss 1.8883334398269653
iteration 700, loss 1.7942347526550293
iteration 800, loss 1.8779958486557007
iteration 0, loss 1.735998272895813
iteration 100, loss 1.791062355041504
iteration 200, loss 1.8798264265060425
iteration 300, loss 1.9279149770736694
iteration 400, loss 1.825175404548645
iteration 500, loss 1.76470947265625
iteration 600, loss 1.7256485223770142
iteration 700, loss 1.7824376821517944
iteration 800, loss 1.8802838325500488
iteration 0, loss 1.826425552368164
iteration 100, loss 1.827437400817871
iteration 200, loss 1.8294140100479126
iteration 300, loss 1.7742877006530762
iteration 400, loss 1.7744100093841553
iteration 500, loss 1.8259599208831787
iteration 600, loss 1.7725523710250854
iteration 700, loss 1.841019630432129
iteration 800, loss 1.835541009902954
iteration 0, loss 1.8504855632781982
iteration 100, loss 1.8549071550369263
iteration 200, loss 1.8087289333343506
iteration 300, loss 1.8815593719482422
iteration 400, loss 1.8278188705444336
iteration 500, loss 1.8162394762039185
iteration 600, loss 1.7828251123428345
iteration 700, loss 1.8343993425369263
iteration 800, loss 1.8300137519836426
iteration 0, loss 1.7441940307617188
iteration 100, loss 1.862191081047058
iteration 200, loss 1.8350043296813965
iteration 300, loss 1.8111815452575684
iteration 400, loss 1.841538667678833
iteration 500, loss 1.804697036743164
iteration 600, loss 1.7704344987869263
iteration 700, loss 1.8140299320220947
iteration 800, loss 1.770863652229309
iteration 0, loss 1.8517985343933105
iteration 100, loss 1.8523401021957397
iteration 200, loss 1.8880420923233032
iteration 300, loss 1.7988392114639282
iteration 400, loss 1.8909974098205566
iteration 500, loss 1.8128222227096558
iteration 600, loss 1.824177861213684
iteration 700, loss 1.8632299900054932
iteration 800, loss 1.7487915754318237
iteration 0, loss 1.8666493892669678
iteration 100, loss 1.8758728504180908
iteration 200, loss 1.78553307056427
iteration 300, loss 1.834914207458496
iteration 400, loss 1.870573878288269
iteration 500, loss 1.8543765544891357
iteration 600, loss 1.7600849866867065
iteration 700, loss 1.8470144271850586
iteration 800, loss 1.8786641359329224
iteration 0, loss 1.864850640296936
iteration 100, loss 1.7577016353607178
iteration 200, loss 1.8658349514007568
iteration 300, loss 1.788355827331543
iteration 400, loss 1.861553430557251
iteration 500, loss 1.7474569082260132
iteration 600, loss 1.8160232305526733
iteration 700, loss 1.9145749807357788
iteration 800, loss 1.7601972818374634
iteration 0, loss 1.8116061687469482
iteration 100, loss 1.894261360168457
iteration 200, loss 1.8737996816635132
iteration 300, loss 1.8589763641357422
iteration 400, loss 1.8060886859893799
iteration 500, loss 1.8293181657791138
iteration 600, loss 1.8902342319488525
iteration 700, loss 1.8420106172561646
iteration 800, loss 1.836125373840332
iteration 0, loss 1.8331018686294556
iteration 100, loss 1.816542625427246
iteration 200, loss 1.8635586500167847
iteration 300, loss 1.8004674911499023
iteration 400, loss 1.7883350849151611
iteration 500, loss 1.7651302814483643
iteration 600, loss 1.9221324920654297
iteration 700, loss 1.8961843252182007
iteration 800, loss 1.7806874513626099
iteration 0, loss 1.8697324991226196
iteration 100, loss 1.7779821157455444
iteration 200, loss 1.777690052986145
iteration 300, loss 1.8759208917617798
iteration 400, loss 1.8419697284698486
iteration 500, loss 1.8927836418151855
iteration 600, loss 1.8951761722564697
iteration 700, loss 1.9369841814041138
iteration 800, loss 1.8310149908065796
iteration 0, loss 1.8871797323226929
iteration 100, loss 1.8512343168258667
iteration 200, loss 1.8670059442520142
iteration 300, loss 1.8057303428649902
iteration 400, loss 1.8345627784729004
iteration 500, loss 1.8089282512664795
iteration 600, loss 1.8881514072418213
iteration 700, loss 1.7543270587921143
iteration 800, loss 1.8466969728469849
iteration 0, loss 1.7814515829086304
iteration 100, loss 1.8617277145385742
iteration 200, loss 1.774320125579834
iteration 300, loss 1.8459193706512451
iteration 400, loss 1.7911275625228882
iteration 500, loss 1.8596837520599365
iteration 600, loss 1.91551673412323
iteration 700, loss 1.856881856918335
iteration 800, loss 1.8551934957504272
iteration 0, loss 1.731789469718933
iteration 100, loss 1.91495943069458
iteration 200, loss 1.7794922590255737
iteration 300, loss 1.807950496673584
iteration 400, loss 1.863464117050171
iteration 500, loss 1.7865837812423706
iteration 600, loss 1.831194281578064
iteration 700, loss 1.8942254781723022
iteration 800, loss 1.7354326248168945
iteration 0, loss 1.8571155071258545
iteration 100, loss 1.711158037185669
iteration 200, loss 1.929067611694336
iteration 300, loss 1.804334282875061
iteration 400, loss 1.7809942960739136
iteration 500, loss 1.8570107221603394
iteration 600, loss 1.7975049018859863
iteration 700, loss 1.813659906387329
iteration 800, loss 1.8522404432296753
iteration 0, loss 1.811070203781128
iteration 100, loss 1.8422117233276367
iteration 200, loss 1.840658187866211
iteration 300, loss 1.9210084676742554
iteration 400, loss 1.8422867059707642
iteration 500, loss 1.7965278625488281
iteration 600, loss 1.7722280025482178
iteration 700, loss 1.9701749086380005
iteration 800, loss 1.7829382419586182
iteration 0, loss 1.822279930114746
iteration 100, loss 1.6989502906799316
iteration 200, loss 1.852496862411499
iteration 300, loss 1.753915786743164
iteration 400, loss 1.8188164234161377
iteration 500, loss 1.848336935043335
iteration 600, loss 1.7563931941986084
iteration 700, loss 1.8379367589950562
iteration 800, loss 1.8508697748184204
iteration 0, loss 1.88633131980896
iteration 100, loss 1.8384181261062622
iteration 200, loss 1.8137849569320679
iteration 300, loss 1.8913823366165161
iteration 400, loss 1.840207815170288
iteration 500, loss 1.868760347366333
iteration 600, loss 1.8597133159637451
iteration 700, loss 1.8189606666564941
iteration 800, loss 1.8767290115356445
iteration 0, loss 1.7666558027267456
iteration 100, loss 1.7656174898147583
iteration 200, loss 1.849401831626892
iteration 300, loss 1.7554616928100586
iteration 400, loss 1.8287568092346191
iteration 500, loss 1.8448742628097534
iteration 600, loss 1.849732756614685
iteration 700, loss 1.8062323331832886
iteration 800, loss 1.8224252462387085
fold 4 accuracy: 0.5242857142857142
[2024-02-29 00:34:22,826] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-29 00:34:22,827] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            373.13 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.5 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '373.13 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 373.13 us = 100% latency, 1.5 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 281.33 us = 75.4% latency, 1.99 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 26.94 us = 7.22% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-29 00:34:22,829] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
iteration 0, loss 2.290963649749756
iteration 100, loss 2.3297879695892334
iteration 200, loss 2.206400156021118
iteration 300, loss 2.086019515991211
iteration 400, loss 2.080361843109131
iteration 500, loss 2.0557265281677246
iteration 600, loss 1.9532437324523926
iteration 700, loss 1.9955382347106934
iteration 800, loss 1.9616317749023438
iteration 0, loss 2.0096945762634277
iteration 100, loss 1.9867247343063354
iteration 200, loss 1.9367872476577759
iteration 300, loss 1.9690290689468384
iteration 400, loss 1.9909685850143433
iteration 500, loss 1.988865852355957
iteration 600, loss 1.9420561790466309
iteration 700, loss 1.9708812236785889
iteration 800, loss 1.97030508518219
iteration 0, loss 1.9758557081222534
iteration 100, loss 1.9185304641723633
iteration 200, loss 1.9578509330749512
iteration 300, loss 1.9790815114974976
iteration 400, loss 1.8943166732788086
iteration 500, loss 1.872941255569458
iteration 600, loss 1.9733631610870361
iteration 700, loss 1.976586937904358
iteration 800, loss 1.9840749502182007
iteration 0, loss 1.9652886390686035
iteration 100, loss 1.9094432592391968
iteration 200, loss 1.9646294116973877
iteration 300, loss 1.9642897844314575
iteration 400, loss 1.9751522541046143
iteration 500, loss 1.9339916706085205
iteration 600, loss 2.011333465576172
iteration 700, loss 1.941802978515625
iteration 800, loss 2.004514694213867
iteration 0, loss 1.9715676307678223
iteration 100, loss 1.90045166015625
iteration 200, loss 1.922377109527588
iteration 300, loss 1.953111171722412
iteration 400, loss 1.954744577407837
iteration 500, loss 1.9594433307647705
iteration 600, loss 1.941522479057312
iteration 700, loss 1.9990167617797852
iteration 800, loss 1.899375081062317
iteration 0, loss 1.9426461458206177
iteration 100, loss 1.9270614385604858
iteration 200, loss 1.9435549974441528
iteration 300, loss 1.9472365379333496
iteration 400, loss 1.8844749927520752
iteration 500, loss 1.9494954347610474
iteration 600, loss 1.9166983366012573
iteration 700, loss 1.928749918937683
iteration 800, loss 1.9612035751342773
iteration 0, loss 1.9208749532699585
iteration 100, loss 1.896856427192688
iteration 200, loss 1.973821997642517
iteration 300, loss 1.8372840881347656
iteration 400, loss 1.9914519786834717
iteration 500, loss 1.8892711400985718
iteration 600, loss 1.9361190795898438
iteration 700, loss 1.8842048645019531
iteration 800, loss 1.9655479192733765
iteration 0, loss 1.961743950843811
iteration 100, loss 1.9078634977340698
iteration 200, loss 1.9625327587127686
iteration 300, loss 1.911498785018921
iteration 400, loss 2.0036792755126953
iteration 500, loss 1.949517011642456
iteration 600, loss 1.9311374425888062
iteration 700, loss 1.8630224466323853
iteration 800, loss 1.9275022745132446
iteration 0, loss 1.9168874025344849
iteration 100, loss 1.9918396472930908
iteration 200, loss 1.8785768747329712
iteration 300, loss 1.9122540950775146
iteration 400, loss 1.8768446445465088
iteration 500, loss 1.9770362377166748
iteration 600, loss 1.8745619058609009
iteration 700, loss 1.9685333967208862
iteration 800, loss 1.9590710401535034
iteration 0, loss 1.9566528797149658
iteration 100, loss 1.8854007720947266
iteration 200, loss 1.924160361289978
iteration 300, loss 1.9543540477752686
iteration 400, loss 1.9729084968566895
iteration 500, loss 1.856232762336731
iteration 600, loss 1.9699275493621826
iteration 700, loss 1.9352391958236694
iteration 800, loss 1.863633632659912
iteration 0, loss 1.8766921758651733
iteration 100, loss 1.9331014156341553
iteration 200, loss 1.879158616065979
iteration 300, loss 1.9156328439712524
iteration 400, loss 2.009796380996704
iteration 500, loss 1.8731201887130737
iteration 600, loss 1.9289871454238892
iteration 700, loss 1.9394683837890625
iteration 800, loss 1.9371616840362549
iteration 0, loss 1.9066317081451416
iteration 100, loss 1.8689829111099243
iteration 200, loss 1.8869130611419678
iteration 300, loss 1.9310803413391113
iteration 400, loss 1.9165831804275513
iteration 500, loss 1.9314863681793213
iteration 600, loss 1.930086374282837
iteration 700, loss 1.9097962379455566
iteration 800, loss 1.8597028255462646
iteration 0, loss 1.915024757385254
iteration 100, loss 1.9164363145828247
iteration 200, loss 1.9721546173095703
iteration 300, loss 1.912505865097046
iteration 400, loss 1.9702812433242798
iteration 500, loss 1.933955192565918
iteration 600, loss 1.9567301273345947
iteration 700, loss 1.9064847230911255
iteration 800, loss 1.9294140338897705
iteration 0, loss 1.969587802886963
iteration 100, loss 2.006603956222534
iteration 200, loss 1.9405791759490967
iteration 300, loss 1.9619022607803345
iteration 400, loss 1.9514693021774292
iteration 500, loss 1.8537299633026123
iteration 600, loss 1.948309063911438
iteration 700, loss 1.9261939525604248
iteration 800, loss 1.9650577306747437
iteration 0, loss 1.9861316680908203
iteration 100, loss 1.9539759159088135
iteration 200, loss 1.9060097932815552
iteration 300, loss 1.9388542175292969
iteration 400, loss 1.9377473592758179
iteration 500, loss 1.9550907611846924
iteration 600, loss 1.9069373607635498
iteration 700, loss 1.9357050657272339
iteration 800, loss 1.9298491477966309
iteration 0, loss 1.925813913345337
iteration 100, loss 1.9197944402694702
iteration 200, loss 1.931122064590454
iteration 300, loss 1.9503822326660156
iteration 400, loss 1.906083583831787
iteration 500, loss 1.9734604358673096
iteration 600, loss 1.8961836099624634
iteration 700, loss 1.9516527652740479
iteration 800, loss 1.903479814529419
iteration 0, loss 1.9603838920593262
iteration 100, loss 1.8361188173294067
iteration 200, loss 2.029486656188965
iteration 300, loss 1.887177586555481
iteration 400, loss 1.8996641635894775
iteration 500, loss 1.9162524938583374
iteration 600, loss 1.8685883283615112
iteration 700, loss 1.8564084768295288
iteration 800, loss 1.8845752477645874
iteration 0, loss 1.9215233325958252
iteration 100, loss 1.9536116123199463
iteration 200, loss 1.8799173831939697
iteration 300, loss 1.8657894134521484
iteration 400, loss 1.906384825706482
iteration 500, loss 1.9629048109054565
iteration 600, loss 1.966022253036499
iteration 700, loss 1.9152774810791016
iteration 800, loss 1.9395641088485718
iteration 0, loss 1.9226237535476685
iteration 100, loss 1.8878731727600098
iteration 200, loss 1.9004651308059692
iteration 300, loss 1.883551001548767
iteration 400, loss 1.8571689128875732
iteration 500, loss 1.9631012678146362
iteration 600, loss 1.8737322092056274
iteration 700, loss 1.8766306638717651
iteration 800, loss 1.8689137697219849
iteration 0, loss 1.9264686107635498
iteration 100, loss 1.905951976776123
iteration 200, loss 1.9387153387069702
iteration 300, loss 1.95751953125
iteration 400, loss 1.9089195728302002
iteration 500, loss 1.8994667530059814
iteration 600, loss 1.9285212755203247
iteration 700, loss 1.8780698776245117
iteration 800, loss 1.880224585533142
iteration 0, loss 1.921494722366333
iteration 100, loss 1.879226803779602
iteration 200, loss 1.8991237878799438
iteration 300, loss 1.8513778448104858
iteration 400, loss 1.912859320640564
iteration 500, loss 1.916161060333252
iteration 600, loss 1.9771592617034912
iteration 700, loss 1.947698712348938
iteration 800, loss 1.9102851152420044
iteration 0, loss 1.851209282875061
iteration 100, loss 1.9483050107955933
iteration 200, loss 1.9974826574325562
iteration 300, loss 1.9081565141677856
iteration 400, loss 1.882405400276184
iteration 500, loss 1.9698913097381592
iteration 600, loss 1.951554775238037
iteration 700, loss 1.8550243377685547
iteration 800, loss 1.975005865097046
iteration 0, loss 1.877162218093872
iteration 100, loss 1.920117735862732
iteration 200, loss 1.909136176109314
iteration 300, loss 1.9026576280593872
iteration 400, loss 1.8917455673217773
iteration 500, loss 1.9975268840789795
iteration 600, loss 1.9113373756408691
iteration 700, loss 1.8196382522583008
iteration 800, loss 1.9147549867630005
iteration 0, loss 1.8934780359268188
iteration 100, loss 1.8514541387557983
iteration 200, loss 1.8946797847747803
iteration 300, loss 1.8491051197052002
iteration 400, loss 1.8894094228744507
iteration 500, loss 1.9369288682937622
iteration 600, loss 1.8996318578720093
iteration 700, loss 1.9150965213775635
iteration 800, loss 1.834873080253601
iteration 0, loss 1.915626883506775
iteration 100, loss 1.8746525049209595
iteration 200, loss 1.852105975151062
iteration 300, loss 1.9604085683822632
iteration 400, loss 1.9456619024276733
iteration 500, loss 1.8206695318222046
iteration 600, loss 2.022991180419922
iteration 700, loss 1.9338959455490112
iteration 800, loss 1.8503578901290894
iteration 0, loss 1.9572312831878662
iteration 100, loss 1.8985188007354736
iteration 200, loss 1.9295917749404907
iteration 300, loss 1.8995660543441772
iteration 400, loss 1.88933265209198
iteration 500, loss 1.9873549938201904
iteration 600, loss 1.870546579360962
iteration 700, loss 1.9418960809707642
iteration 800, loss 1.801010251045227
iteration 0, loss 1.858808994293213
iteration 100, loss 1.889979362487793
iteration 200, loss 1.9414952993392944
iteration 300, loss 1.9396486282348633
iteration 400, loss 1.979536533355713
iteration 500, loss 1.98843514919281
iteration 600, loss 1.9606893062591553
iteration 700, loss 1.9039400815963745
iteration 800, loss 1.867767095565796
iteration 0, loss 1.8217036724090576
iteration 100, loss 1.8970317840576172
iteration 200, loss 1.9180660247802734
iteration 300, loss 1.898539662361145
iteration 400, loss 1.905955195426941
iteration 500, loss 1.9017605781555176
iteration 600, loss 1.8701961040496826
iteration 700, loss 1.920478105545044
iteration 800, loss 1.954363226890564
iteration 0, loss 1.8833367824554443
iteration 100, loss 1.922353982925415
iteration 200, loss 1.9264922142028809
iteration 300, loss 1.8272788524627686
iteration 400, loss 1.9020971059799194
iteration 500, loss 1.933985710144043
iteration 600, loss 1.9165419340133667
iteration 700, loss 1.9134433269500732
iteration 800, loss 1.9604840278625488
iteration 0, loss 1.8742985725402832
iteration 100, loss 1.9794166088104248
iteration 200, loss 1.9041246175765991
iteration 300, loss 1.847976565361023
iteration 400, loss 1.89955472946167
iteration 500, loss 1.8462573289871216
iteration 600, loss 1.8832019567489624
iteration 700, loss 1.940854549407959
iteration 800, loss 1.9292082786560059
iteration 0, loss 1.9356273412704468
iteration 100, loss 1.967374324798584
iteration 200, loss 1.866393804550171
iteration 300, loss 1.921636700630188
iteration 400, loss 1.806277871131897
iteration 500, loss 1.9524545669555664
iteration 600, loss 1.922131896018982
iteration 700, loss 1.866965651512146
iteration 800, loss 1.941932201385498
iteration 0, loss 1.8951196670532227
iteration 100, loss 1.8665028810501099
iteration 200, loss 1.9330886602401733
iteration 300, loss 1.8768656253814697
iteration 400, loss 1.9364441633224487
iteration 500, loss 1.852478265762329
iteration 600, loss 1.901821255683899
iteration 700, loss 1.9139763116836548
iteration 800, loss 1.8221025466918945
iteration 0, loss 1.850903868675232
iteration 100, loss 1.8686001300811768
iteration 200, loss 1.9087491035461426
iteration 300, loss 1.8832440376281738
iteration 400, loss 1.9137755632400513
iteration 500, loss 1.8890846967697144
iteration 600, loss 1.9147964715957642
iteration 700, loss 1.9027236700057983
iteration 800, loss 1.929950475692749
iteration 0, loss 1.8633688688278198
iteration 100, loss 1.959066390991211
iteration 200, loss 1.8973453044891357
iteration 300, loss 1.8990718126296997
iteration 400, loss 1.901851773262024
iteration 500, loss 1.9241859912872314
iteration 600, loss 1.8286620378494263
iteration 700, loss 1.8487335443496704
iteration 800, loss 1.8810290098190308
iteration 0, loss 1.9069303274154663
iteration 100, loss 1.8814777135849
iteration 200, loss 1.8471601009368896
iteration 300, loss 1.8980401754379272
iteration 400, loss 1.875818133354187
iteration 500, loss 1.892246961593628
iteration 600, loss 1.853005290031433
iteration 700, loss 1.9074599742889404
iteration 800, loss 1.987874150276184
iteration 0, loss 1.927826166152954
iteration 100, loss 1.8948712348937988
iteration 200, loss 1.9001797437667847
iteration 300, loss 1.831229567527771
iteration 400, loss 1.8740053176879883
iteration 500, loss 1.8955984115600586
iteration 600, loss 1.9081878662109375
iteration 700, loss 1.851025938987732
iteration 800, loss 1.9386570453643799
iteration 0, loss 1.8909308910369873
iteration 100, loss 1.8807744979858398
iteration 200, loss 1.8527157306671143
iteration 300, loss 1.9208569526672363
iteration 400, loss 1.864261507987976
iteration 500, loss 1.90395188331604
iteration 600, loss 1.859096884727478
iteration 700, loss 1.8759645223617554
iteration 800, loss 1.8599013090133667
iteration 0, loss 1.8487199544906616
iteration 100, loss 1.8789492845535278
iteration 200, loss 1.8757903575897217
iteration 300, loss 1.9561060667037964
iteration 400, loss 1.9405875205993652
iteration 500, loss 1.9114959239959717
iteration 600, loss 1.9140442609786987
iteration 700, loss 1.887747883796692
iteration 800, loss 1.8928685188293457
iteration 0, loss 1.8646836280822754
iteration 100, loss 1.8931808471679688
iteration 200, loss 1.9405345916748047
iteration 300, loss 1.9165347814559937
iteration 400, loss 1.9264036417007446
iteration 500, loss 1.9133013486862183
iteration 600, loss 1.8510761260986328
iteration 700, loss 1.9171228408813477
iteration 800, loss 1.9099830389022827
iteration 0, loss 1.8656283617019653
iteration 100, loss 1.9115324020385742
iteration 200, loss 1.8638995885849
iteration 300, loss 1.8518692255020142
iteration 400, loss 1.915931224822998
iteration 500, loss 1.9128141403198242
iteration 600, loss 1.8640367984771729
iteration 700, loss 1.8499135971069336
iteration 800, loss 1.9461073875427246
iteration 0, loss 1.9244428873062134
iteration 100, loss 1.9044065475463867
iteration 200, loss 1.941798448562622
iteration 300, loss 1.9308042526245117
iteration 400, loss 1.9802850484848022
iteration 500, loss 1.8511145114898682
iteration 600, loss 1.820997714996338
iteration 700, loss 1.923837423324585
iteration 800, loss 1.8118807077407837
iteration 0, loss 1.8792921304702759
iteration 100, loss 1.9541081190109253
iteration 200, loss 1.9736894369125366
iteration 300, loss 1.863671064376831
iteration 400, loss 1.8616126775741577
iteration 500, loss 1.8699718713760376
iteration 600, loss 1.9840049743652344
iteration 700, loss 1.8086743354797363
iteration 800, loss 1.9714596271514893
iteration 0, loss 1.8647764921188354
iteration 100, loss 1.90714693069458
iteration 200, loss 1.8495068550109863
iteration 300, loss 1.8747912645339966
iteration 400, loss 1.8533339500427246
iteration 500, loss 1.8659594058990479
iteration 600, loss 1.7964798212051392
iteration 700, loss 1.9042614698410034
iteration 800, loss 1.897952675819397
iteration 0, loss 1.8561382293701172
iteration 100, loss 1.9103941917419434
iteration 200, loss 1.8804142475128174
iteration 300, loss 1.842482089996338
iteration 400, loss 1.87372887134552
iteration 500, loss 1.9373598098754883
iteration 600, loss 1.934584379196167
iteration 700, loss 1.9165101051330566
iteration 800, loss 1.887907862663269
iteration 0, loss 1.9111462831497192
iteration 100, loss 1.883453607559204
iteration 200, loss 1.9132840633392334
iteration 300, loss 1.9535517692565918
iteration 400, loss 1.8958837985992432
iteration 500, loss 1.8701733350753784
iteration 600, loss 1.8509294986724854
iteration 700, loss 1.939559817314148
iteration 800, loss 1.9033942222595215
iteration 0, loss 1.8863768577575684
iteration 100, loss 1.8642187118530273
iteration 200, loss 1.9200340509414673
iteration 300, loss 1.8816643953323364
iteration 400, loss 1.9190657138824463
iteration 500, loss 1.8793801069259644
iteration 600, loss 1.8642550706863403
iteration 700, loss 1.9673583507537842
iteration 800, loss 1.8699668645858765
iteration 0, loss 1.8926657438278198
iteration 100, loss 1.881468415260315
iteration 200, loss 1.8633716106414795
iteration 300, loss 1.820386290550232
iteration 400, loss 1.8387024402618408
iteration 500, loss 1.9835114479064941
iteration 600, loss 1.8142145872116089
iteration 700, loss 1.8812739849090576
iteration 800, loss 1.8839490413665771
iteration 0, loss 1.8708544969558716
iteration 100, loss 1.9244333505630493
iteration 200, loss 1.8836686611175537
iteration 300, loss 1.7981345653533936
iteration 400, loss 1.8735008239746094
iteration 500, loss 1.912696123123169
iteration 600, loss 1.8788905143737793
iteration 700, loss 1.8962640762329102
iteration 800, loss 1.8536263704299927
iteration 0, loss 1.8941727876663208
iteration 100, loss 1.841649055480957
iteration 200, loss 1.8623110055923462
iteration 300, loss 1.8798322677612305
iteration 400, loss 1.8982118368148804
iteration 500, loss 1.867720365524292
iteration 600, loss 1.9051905870437622
iteration 700, loss 1.9349987506866455
iteration 800, loss 1.8698872327804565
iteration 0, loss 1.8703585863113403
iteration 100, loss 1.893737554550171
iteration 200, loss 1.8936541080474854
iteration 300, loss 1.8732945919036865
iteration 400, loss 1.83777916431427
iteration 500, loss 1.9262210130691528
iteration 600, loss 1.837952971458435
iteration 700, loss 1.8942432403564453
iteration 800, loss 1.8544327020645142
fold 0 accuracy: 0.44342857142857145
iteration 0, loss 1.8981348276138306
iteration 100, loss 1.9066554307937622
iteration 200, loss 1.919309139251709
iteration 300, loss 1.8585526943206787
iteration 400, loss 1.8723421096801758
iteration 500, loss 1.9098002910614014
iteration 600, loss 1.8414421081542969
iteration 700, loss 1.9402645826339722
iteration 800, loss 1.9477519989013672
iteration 0, loss 1.8823044300079346
iteration 100, loss 1.895285725593567
iteration 200, loss 1.926342487335205
iteration 300, loss 1.8798459768295288
iteration 400, loss 1.8767849206924438
iteration 500, loss 1.8747212886810303
iteration 600, loss 1.82500422000885
iteration 700, loss 1.8155550956726074
iteration 800, loss 1.872066855430603
iteration 0, loss 1.863321304321289
iteration 100, loss 1.8546704053878784
iteration 200, loss 1.8801323175430298
iteration 300, loss 1.9538120031356812
iteration 400, loss 1.8046795129776
iteration 500, loss 1.9536532163619995
iteration 600, loss 1.8811672925949097
iteration 700, loss 1.9529435634613037
iteration 800, loss 1.9461265802383423
iteration 0, loss 1.849014401435852
iteration 100, loss 1.9185962677001953
iteration 200, loss 1.9130288362503052
iteration 300, loss 1.9398492574691772
iteration 400, loss 1.8972300291061401
iteration 500, loss 1.837837815284729
iteration 600, loss 1.9035190343856812
iteration 700, loss 1.8521891832351685
iteration 800, loss 1.8560494184494019
iteration 0, loss 1.8856241703033447
iteration 100, loss 1.8851548433303833
iteration 200, loss 1.9459877014160156
iteration 300, loss 1.9233595132827759
iteration 400, loss 1.9033620357513428
iteration 500, loss 1.8431754112243652
iteration 600, loss 1.8389697074890137
iteration 700, loss 1.8316571712493896
iteration 800, loss 1.9589923620224
iteration 0, loss 1.8888721466064453
iteration 100, loss 1.8416413068771362
iteration 200, loss 1.855811595916748
iteration 300, loss 1.8851078748703003
iteration 400, loss 1.8535972833633423
iteration 500, loss 1.88302481174469
iteration 600, loss 1.8503327369689941
iteration 700, loss 1.8587028980255127
iteration 800, loss 1.8913483619689941
iteration 0, loss 1.8449479341506958
iteration 100, loss 1.8638832569122314
iteration 200, loss 1.8845633268356323
iteration 300, loss 1.8381996154785156
iteration 400, loss 1.8751261234283447
iteration 500, loss 1.9519879817962646
iteration 600, loss 1.9011867046356201
iteration 700, loss 1.818404197692871
iteration 800, loss 1.8729634284973145
iteration 0, loss 1.845123052597046
iteration 100, loss 1.8330072164535522
iteration 200, loss 1.888270616531372
iteration 300, loss 1.8451939821243286
iteration 400, loss 1.9105180501937866
iteration 500, loss 1.8560110330581665
iteration 600, loss 1.8702468872070312
iteration 700, loss 1.8304474353790283
iteration 800, loss 1.899879813194275
iteration 0, loss 1.82334566116333
iteration 100, loss 1.8328876495361328
iteration 200, loss 1.8853565454483032
iteration 300, loss 1.8108549118041992
iteration 400, loss 1.843355417251587
iteration 500, loss 1.876783847808838
iteration 600, loss 1.82194983959198
iteration 700, loss 1.8910263776779175
iteration 800, loss 1.8976598978042603
iteration 0, loss 1.8734112977981567
iteration 100, loss 1.886023759841919
iteration 200, loss 1.9300618171691895
iteration 300, loss 1.843672513961792
iteration 400, loss 1.9102314710617065
iteration 500, loss 1.8554489612579346
iteration 600, loss 1.8323431015014648
iteration 700, loss 1.8415030241012573
iteration 800, loss 1.814328670501709
iteration 0, loss 1.9380357265472412
iteration 100, loss 1.9463733434677124
iteration 200, loss 1.8527164459228516
iteration 300, loss 1.8186572790145874
iteration 400, loss 1.925683617591858
iteration 500, loss 1.8322495222091675
iteration 600, loss 1.9428315162658691
iteration 700, loss 1.8045347929000854
iteration 800, loss 1.8833560943603516
iteration 0, loss 1.8236244916915894
iteration 100, loss 1.8994618654251099
iteration 200, loss 1.9031882286071777
iteration 300, loss 1.8956867456436157
iteration 400, loss 1.7887574434280396
iteration 500, loss 1.8138972520828247
iteration 600, loss 1.859991431236267
iteration 700, loss 1.8483688831329346
iteration 800, loss 1.8651978969573975
iteration 0, loss 1.9047040939331055
iteration 100, loss 1.87064790725708
iteration 200, loss 1.8724480867385864
iteration 300, loss 1.8413794040679932
iteration 400, loss 1.8546762466430664
iteration 500, loss 1.8243348598480225
iteration 600, loss 1.8611347675323486
iteration 700, loss 1.9365448951721191
iteration 800, loss 1.8983060121536255
iteration 0, loss 1.8584259748458862
iteration 100, loss 1.892665147781372
iteration 200, loss 1.857779622077942
iteration 300, loss 1.8574963808059692
iteration 400, loss 1.8164970874786377
iteration 500, loss 1.908331036567688
iteration 600, loss 1.8953129053115845
iteration 700, loss 1.8271429538726807
iteration 800, loss 1.891210675239563
iteration 0, loss 1.9309579133987427
iteration 100, loss 1.910559892654419
iteration 200, loss 1.8409968614578247
iteration 300, loss 1.8902257680892944
iteration 400, loss 1.8127822875976562
iteration 500, loss 1.8326531648635864
iteration 600, loss 1.9353249073028564
iteration 700, loss 1.9020607471466064
iteration 800, loss 1.8884660005569458
iteration 0, loss 1.8507987260818481
iteration 100, loss 1.8965944051742554
iteration 200, loss 1.8534998893737793
iteration 300, loss 1.8517109155654907
iteration 400, loss 1.824565052986145
iteration 500, loss 1.8748782873153687
iteration 600, loss 1.8374449014663696
iteration 700, loss 1.8694353103637695
iteration 800, loss 1.884502649307251
iteration 0, loss 1.8356642723083496
iteration 100, loss 1.867682933807373
iteration 200, loss 1.8565952777862549
iteration 300, loss 1.8636647462844849
iteration 400, loss 1.8210406303405762
iteration 500, loss 1.8378641605377197
iteration 600, loss 1.8024091720581055
iteration 700, loss 1.9447851181030273
iteration 800, loss 1.9009019136428833
iteration 0, loss 1.902414083480835
iteration 100, loss 1.8536503314971924
iteration 200, loss 1.8024531602859497
iteration 300, loss 1.8011775016784668
iteration 400, loss 1.9173812866210938
iteration 500, loss 1.886111855506897
iteration 600, loss 1.9216117858886719
iteration 700, loss 1.8319051265716553
iteration 800, loss 1.8915951251983643
iteration 0, loss 1.8626747131347656
iteration 100, loss 1.9809422492980957
iteration 200, loss 1.884110689163208
iteration 300, loss 1.8180465698242188
iteration 400, loss 1.885711431503296
iteration 500, loss 1.8802862167358398
iteration 600, loss 1.8217021226882935
iteration 700, loss 1.9050419330596924
iteration 800, loss 1.8880807161331177
iteration 0, loss 1.9518413543701172
iteration 100, loss 1.9346402883529663
iteration 200, loss 1.8561854362487793
iteration 300, loss 1.8714900016784668
iteration 400, loss 1.836498498916626
iteration 500, loss 1.8732681274414062
iteration 600, loss 1.8538192510604858
iteration 700, loss 1.875935435295105
iteration 800, loss 1.8425816297531128
iteration 0, loss 1.8847683668136597
iteration 100, loss 1.8927319049835205
iteration 200, loss 1.864500880241394
iteration 300, loss 1.9317892789840698
iteration 400, loss 1.9036792516708374
iteration 500, loss 1.8303264379501343
iteration 600, loss 1.773725152015686
iteration 700, loss 1.8727285861968994
iteration 800, loss 1.8187975883483887
iteration 0, loss 1.865486979484558
iteration 100, loss 1.8775756359100342
iteration 200, loss 1.8211365938186646
iteration 300, loss 1.8768318891525269
iteration 400, loss 1.8385465145111084
iteration 500, loss 1.81730055809021
iteration 600, loss 1.8184213638305664
iteration 700, loss 1.8131675720214844
iteration 800, loss 1.8474191427230835
iteration 0, loss 1.8262149095535278
iteration 100, loss 1.8056789636611938
iteration 200, loss 1.8126206398010254
iteration 300, loss 1.8515970706939697
iteration 400, loss 1.7642297744750977
iteration 500, loss 1.9044743776321411
iteration 600, loss 1.857852578163147
iteration 700, loss 1.8855396509170532
iteration 800, loss 1.8166850805282593
iteration 0, loss 1.9144608974456787
iteration 100, loss 1.832968831062317
iteration 200, loss 1.9374388456344604
iteration 300, loss 1.8189728260040283
iteration 400, loss 1.8626904487609863
iteration 500, loss 1.8679094314575195
iteration 600, loss 1.8254024982452393
iteration 700, loss 1.8326724767684937
iteration 800, loss 1.8835415840148926
iteration 0, loss 1.8752139806747437
iteration 100, loss 1.8455321788787842
iteration 200, loss 1.867458462715149
iteration 300, loss 1.879416823387146
iteration 400, loss 1.8350917100906372
iteration 500, loss 1.870705485343933
iteration 600, loss 1.8778223991394043
iteration 700, loss 1.888203740119934
iteration 800, loss 1.9363152980804443
iteration 0, loss 1.897289514541626
iteration 100, loss 1.8666191101074219
iteration 200, loss 1.8407330513000488
iteration 300, loss 1.8523775339126587
iteration 400, loss 1.855215072631836
iteration 500, loss 1.8068310022354126
iteration 600, loss 1.8379651308059692
iteration 700, loss 1.8492133617401123
iteration 800, loss 1.8667778968811035
iteration 0, loss 1.8363761901855469
iteration 100, loss 1.8962572813034058
iteration 200, loss 1.8911159038543701
iteration 300, loss 1.8663350343704224
iteration 400, loss 1.9129602909088135
iteration 500, loss 1.7879265546798706
iteration 600, loss 1.8119956254959106
iteration 700, loss 1.8814979791641235
iteration 800, loss 1.859183669090271
iteration 0, loss 1.9186997413635254
iteration 100, loss 1.8335124254226685
iteration 200, loss 1.8127244710922241
iteration 300, loss 1.88219153881073
iteration 400, loss 1.8597335815429688
iteration 500, loss 1.8828436136245728
iteration 600, loss 1.85982084274292
iteration 700, loss 1.8488298654556274
iteration 800, loss 1.8511841297149658
iteration 0, loss 1.8999013900756836
iteration 100, loss 1.8200526237487793
iteration 200, loss 1.8421860933303833
iteration 300, loss 1.8173326253890991
iteration 400, loss 1.8302433490753174
iteration 500, loss 1.8877272605895996
iteration 600, loss 1.8606878519058228
iteration 700, loss 1.8598966598510742
iteration 800, loss 1.8201773166656494
iteration 0, loss 1.855896234512329
iteration 100, loss 1.8010057210922241
iteration 200, loss 1.8777567148208618
iteration 300, loss 1.84580397605896
iteration 400, loss 1.950430154800415
iteration 500, loss 1.8037331104278564
iteration 600, loss 1.8144031763076782
iteration 700, loss 1.8451966047286987
iteration 800, loss 1.8225494623184204
iteration 0, loss 1.922789454460144
iteration 100, loss 1.8036664724349976
iteration 200, loss 1.839199423789978
iteration 300, loss 1.8170444965362549
iteration 400, loss 1.8302785158157349
iteration 500, loss 1.8039027452468872
iteration 600, loss 1.8403239250183105
iteration 700, loss 1.8477940559387207
iteration 800, loss 1.7920836210250854
iteration 0, loss 1.8654797077178955
iteration 100, loss 1.8944860696792603
iteration 200, loss 1.8531242609024048
iteration 300, loss 1.9268434047698975
iteration 400, loss 1.8106756210327148
iteration 500, loss 1.7956480979919434
iteration 600, loss 1.8119157552719116
iteration 700, loss 1.842051386833191
iteration 800, loss 1.8869905471801758
iteration 0, loss 1.7893260717391968
iteration 100, loss 1.8761112689971924
iteration 200, loss 1.8385990858078003
iteration 300, loss 1.819385290145874
iteration 400, loss 1.8321521282196045
iteration 500, loss 1.895616888999939
iteration 600, loss 1.8307965993881226
iteration 700, loss 1.7872463464736938
iteration 800, loss 1.8294947147369385
iteration 0, loss 1.7975428104400635
iteration 100, loss 1.8541873693466187
iteration 200, loss 1.800746202468872
iteration 300, loss 1.8493916988372803
iteration 400, loss 1.8452695608139038
iteration 500, loss 1.8000869750976562
iteration 600, loss 1.9383697509765625
iteration 700, loss 1.9195353984832764
iteration 800, loss 1.8046269416809082
iteration 0, loss 1.894261121749878
iteration 100, loss 1.7973090410232544
iteration 200, loss 1.8558933734893799
iteration 300, loss 1.8141071796417236
iteration 400, loss 1.8815109729766846
iteration 500, loss 1.8640371561050415
iteration 600, loss 1.8505107164382935
iteration 700, loss 1.8800235986709595
iteration 800, loss 1.8591235876083374
iteration 0, loss 1.8588343858718872
iteration 100, loss 1.879857063293457
iteration 200, loss 1.7916713953018188
iteration 300, loss 1.783511757850647
iteration 400, loss 1.862404704093933
iteration 500, loss 1.9215245246887207
iteration 600, loss 1.8596376180648804
iteration 700, loss 1.8325368165969849
iteration 800, loss 1.8242096900939941
iteration 0, loss 1.7671833038330078
iteration 100, loss 1.8888061046600342
iteration 200, loss 1.8607451915740967
iteration 300, loss 1.8543126583099365
iteration 400, loss 1.8925691843032837
iteration 500, loss 1.8359979391098022
iteration 600, loss 1.8260853290557861
iteration 700, loss 1.8758106231689453
iteration 800, loss 1.8465677499771118
iteration 0, loss 1.8659635782241821
iteration 100, loss 1.8662467002868652
iteration 200, loss 1.8983248472213745
iteration 300, loss 1.8425076007843018
iteration 400, loss 1.8565798997879028
iteration 500, loss 1.8188714981079102
iteration 600, loss 1.8217586278915405
iteration 700, loss 1.8030954599380493
iteration 800, loss 1.7744224071502686
iteration 0, loss 1.8034950494766235
iteration 100, loss 1.9252080917358398
iteration 200, loss 1.8593010902404785
iteration 300, loss 1.9077922105789185
iteration 400, loss 1.8508117198944092
iteration 500, loss 1.8573579788208008
iteration 600, loss 1.878163456916809
iteration 700, loss 1.8580634593963623
iteration 800, loss 1.8501121997833252
iteration 0, loss 1.8385798931121826
iteration 100, loss 1.8541566133499146
iteration 200, loss 1.8480584621429443
iteration 300, loss 1.8883694410324097
iteration 400, loss 1.843191385269165
iteration 500, loss 1.858411431312561
iteration 600, loss 1.8890162706375122
iteration 700, loss 1.8254426717758179
iteration 800, loss 1.8879891633987427
iteration 0, loss 1.8754552602767944
iteration 100, loss 1.8635765314102173
iteration 200, loss 1.8006200790405273
iteration 300, loss 1.8731739521026611
iteration 400, loss 1.8593441247940063
iteration 500, loss 1.8270130157470703
iteration 600, loss 1.8240137100219727
iteration 700, loss 1.7688181400299072
iteration 800, loss 1.8146977424621582
iteration 0, loss 1.8218265771865845
iteration 100, loss 1.829755425453186
iteration 200, loss 1.7913177013397217
iteration 300, loss 1.8522080183029175
iteration 400, loss 1.8871275186538696
iteration 500, loss 1.8976097106933594
iteration 600, loss 1.8992747068405151
iteration 700, loss 1.8357250690460205
iteration 800, loss 1.8433263301849365
iteration 0, loss 1.8176310062408447
iteration 100, loss 1.8453847169876099
iteration 200, loss 1.848400592803955
iteration 300, loss 1.850851058959961
iteration 400, loss 1.7818411588668823
iteration 500, loss 1.7922829389572144
iteration 600, loss 1.8515733480453491
iteration 700, loss 1.8888256549835205
iteration 800, loss 1.8496147394180298
iteration 0, loss 1.8807735443115234
iteration 100, loss 1.8271740674972534
iteration 200, loss 1.8097397089004517
iteration 300, loss 1.7954175472259521
iteration 400, loss 1.9202899932861328
iteration 500, loss 1.8582617044448853
iteration 600, loss 1.8447048664093018
iteration 700, loss 1.9102898836135864
iteration 800, loss 1.8964934349060059
iteration 0, loss 1.8450915813446045
iteration 100, loss 1.9558932781219482
iteration 200, loss 1.8711044788360596
iteration 300, loss 1.8019405603408813
iteration 400, loss 1.8305940628051758
iteration 500, loss 1.867377758026123
iteration 600, loss 1.8993169069290161
iteration 700, loss 1.8437823057174683
iteration 800, loss 1.8787765502929688
iteration 0, loss 1.8903238773345947
iteration 100, loss 1.8850268125534058
iteration 200, loss 1.8479279279708862
iteration 300, loss 1.807361364364624
iteration 400, loss 1.793497920036316
iteration 500, loss 1.8065531253814697
iteration 600, loss 1.8645325899124146
iteration 700, loss 1.8742828369140625
iteration 800, loss 1.7931488752365112
iteration 0, loss 1.8384605646133423
iteration 100, loss 1.8591622114181519
iteration 200, loss 1.8372352123260498
iteration 300, loss 1.875607967376709
iteration 400, loss 1.8040902614593506
iteration 500, loss 1.8947702646255493
iteration 600, loss 1.812587857246399
iteration 700, loss 1.8887853622436523
iteration 800, loss 1.790771245956421
iteration 0, loss 1.8887747526168823
iteration 100, loss 1.853184461593628
iteration 200, loss 1.8892731666564941
iteration 300, loss 1.8073956966400146
iteration 400, loss 1.80709969997406
iteration 500, loss 1.8970959186553955
iteration 600, loss 1.832346796989441
iteration 700, loss 1.816670536994934
iteration 800, loss 1.8070920705795288
iteration 0, loss 1.794240117073059
iteration 100, loss 1.8248610496520996
iteration 200, loss 1.8628287315368652
iteration 300, loss 1.8020788431167603
iteration 400, loss 1.8079402446746826
iteration 500, loss 1.8488881587982178
iteration 600, loss 1.876212477684021
iteration 700, loss 1.8600780963897705
iteration 800, loss 1.8328156471252441
iteration 0, loss 1.830335021018982
iteration 100, loss 1.891998052597046
iteration 200, loss 1.7802164554595947
iteration 300, loss 1.8846081495285034
iteration 400, loss 1.819050669670105
iteration 500, loss 1.842029333114624
iteration 600, loss 1.845138669013977
iteration 700, loss 1.8780187368392944
iteration 800, loss 1.8044190406799316
fold 1 accuracy: 0.462
iteration 0, loss 1.8639352321624756
iteration 100, loss 1.8346270322799683
iteration 200, loss 1.8827006816864014
iteration 300, loss 1.8018450736999512
iteration 400, loss 1.8142837285995483
iteration 500, loss 1.8286291360855103
iteration 600, loss 1.8104068040847778
iteration 700, loss 1.8665573596954346
iteration 800, loss 1.9017524719238281
iteration 0, loss 1.9860831499099731
iteration 100, loss 1.8158204555511475
iteration 200, loss 1.7854571342468262
iteration 300, loss 1.863732933998108
iteration 400, loss 1.9509902000427246
iteration 500, loss 1.784922480583191
iteration 600, loss 1.8097960948944092
iteration 700, loss 1.7908660173416138
iteration 800, loss 1.863001823425293
iteration 0, loss 1.8139668703079224
iteration 100, loss 1.8476076126098633
iteration 200, loss 1.8734171390533447
iteration 300, loss 1.8488649129867554
iteration 400, loss 1.8312382698059082
iteration 500, loss 1.8561304807662964
iteration 600, loss 1.8631657361984253
iteration 700, loss 1.7751065492630005
iteration 800, loss 1.8441542387008667
iteration 0, loss 1.7743440866470337
iteration 100, loss 1.832214117050171
iteration 200, loss 1.8170024156570435
iteration 300, loss 1.8568800687789917
iteration 400, loss 1.8207402229309082
iteration 500, loss 1.8713457584381104
iteration 600, loss 1.8334729671478271
iteration 700, loss 1.8834012746810913
iteration 800, loss 1.8879034519195557
iteration 0, loss 1.8269506692886353
iteration 100, loss 1.8664559125900269
iteration 200, loss 1.8699924945831299
iteration 300, loss 1.8551626205444336
iteration 400, loss 1.8155263662338257
iteration 500, loss 1.8531709909439087
iteration 600, loss 1.8371574878692627
iteration 700, loss 1.8020027875900269
iteration 800, loss 1.8414090871810913
iteration 0, loss 1.8129135370254517
iteration 100, loss 1.8060799837112427
iteration 200, loss 1.876030445098877
iteration 300, loss 1.814052700996399
iteration 400, loss 1.8196706771850586
iteration 500, loss 1.8765192031860352
iteration 600, loss 1.875079870223999
iteration 700, loss 1.8638683557510376
iteration 800, loss 1.855002999305725
iteration 0, loss 1.7993884086608887
iteration 100, loss 1.8631643056869507
iteration 200, loss 1.8581969738006592
iteration 300, loss 1.9162485599517822
iteration 400, loss 1.8925361633300781
iteration 500, loss 1.9172712564468384
iteration 600, loss 1.841429591178894
iteration 700, loss 1.853716254234314
iteration 800, loss 1.8950004577636719
iteration 0, loss 1.7897443771362305
iteration 100, loss 1.841769814491272
iteration 200, loss 1.812271237373352
iteration 300, loss 1.8352806568145752
iteration 400, loss 1.8281618356704712
iteration 500, loss 1.819138765335083
iteration 600, loss 1.798143744468689
iteration 700, loss 1.77001953125
iteration 800, loss 1.8717334270477295
iteration 0, loss 1.7698371410369873
iteration 100, loss 1.8594123125076294
iteration 200, loss 1.8751602172851562
iteration 300, loss 1.8806178569793701
iteration 400, loss 1.8707484006881714
iteration 500, loss 1.8367111682891846
iteration 600, loss 1.7546710968017578
iteration 700, loss 1.863202452659607
iteration 800, loss 1.822862148284912
iteration 0, loss 1.7835073471069336
iteration 100, loss 1.7710902690887451
iteration 200, loss 1.837263822555542
iteration 300, loss 1.8223882913589478
iteration 400, loss 1.86892831325531
iteration 500, loss 1.8897545337677002
iteration 600, loss 1.854992389678955
iteration 700, loss 1.8574405908584595
iteration 800, loss 1.891870379447937
iteration 0, loss 1.8184337615966797
iteration 100, loss 1.87064790725708
iteration 200, loss 1.9073326587677002
iteration 300, loss 1.885542392730713
iteration 400, loss 1.8794670104980469
iteration 500, loss 1.8427590131759644
iteration 600, loss 1.8245463371276855
iteration 700, loss 1.7834429740905762
iteration 800, loss 1.8459701538085938
iteration 0, loss 1.849555253982544
iteration 100, loss 1.8476026058197021
iteration 200, loss 1.8344112634658813
iteration 300, loss 1.8886855840682983
iteration 400, loss 1.8639521598815918
iteration 500, loss 1.7940725088119507
iteration 600, loss 1.8095730543136597
iteration 700, loss 1.841041088104248
iteration 800, loss 1.8133704662322998
iteration 0, loss 1.8375284671783447
iteration 100, loss 1.7774714231491089
iteration 200, loss 1.8143625259399414
iteration 300, loss 1.8178972005844116
iteration 400, loss 1.9115641117095947
iteration 500, loss 1.799180030822754
iteration 600, loss 1.854968786239624
iteration 700, loss 1.8692818880081177
iteration 800, loss 1.8428943157196045
iteration 0, loss 1.8244205713272095
iteration 100, loss 1.8011749982833862
iteration 200, loss 1.8315201997756958
iteration 300, loss 1.8362144231796265
iteration 400, loss 1.8299360275268555
iteration 500, loss 1.8895288705825806
iteration 600, loss 1.8120170831680298
iteration 700, loss 1.8484952449798584
iteration 800, loss 1.8041425943374634
iteration 0, loss 1.8200719356536865
iteration 100, loss 1.810616374015808
iteration 200, loss 1.80989670753479
iteration 300, loss 1.8296599388122559
iteration 400, loss 1.8411853313446045
iteration 500, loss 1.748866319656372
iteration 600, loss 1.8047895431518555
iteration 700, loss 1.8470922708511353
iteration 800, loss 1.851070523262024
iteration 0, loss 1.8574720621109009
iteration 100, loss 1.8634625673294067
iteration 200, loss 1.881170630455017
iteration 300, loss 1.8252161741256714
iteration 400, loss 1.851208209991455
iteration 500, loss 1.9411606788635254
iteration 600, loss 1.808471441268921
iteration 700, loss 1.8833045959472656
iteration 800, loss 1.898686408996582
iteration 0, loss 1.785690426826477
iteration 100, loss 1.9077057838439941
iteration 200, loss 1.802891731262207
iteration 300, loss 1.8375177383422852
iteration 400, loss 1.9411816596984863
iteration 500, loss 1.90279221534729
iteration 600, loss 1.8333523273468018
iteration 700, loss 1.839319109916687
iteration 800, loss 1.8532403707504272
iteration 0, loss 1.76483154296875
iteration 100, loss 1.8232285976409912
iteration 200, loss 1.845638394355774
iteration 300, loss 1.7977502346038818
iteration 400, loss 1.9608806371688843
iteration 500, loss 1.8237167596817017
iteration 600, loss 1.8474206924438477
iteration 700, loss 1.8438682556152344
iteration 800, loss 1.7915675640106201
iteration 0, loss 1.841821312904358
iteration 100, loss 1.8621196746826172
iteration 200, loss 1.9435302019119263
iteration 300, loss 1.8964616060256958
iteration 400, loss 1.8090425729751587
iteration 500, loss 1.7982984781265259
iteration 600, loss 1.7891088724136353
iteration 700, loss 1.780439019203186
iteration 800, loss 1.8605316877365112
iteration 0, loss 1.8063617944717407
iteration 100, loss 1.891025185585022
iteration 200, loss 1.972981333732605
iteration 300, loss 1.8397284746170044
iteration 400, loss 1.8701201677322388
iteration 500, loss 1.885685682296753
iteration 600, loss 1.8052349090576172
iteration 700, loss 1.8122152090072632
iteration 800, loss 1.8278652429580688
iteration 0, loss 1.8521373271942139
iteration 100, loss 1.893125057220459
iteration 200, loss 1.8043956756591797
iteration 300, loss 1.792394995689392
iteration 400, loss 1.8515111207962036
iteration 500, loss 1.7929954528808594
iteration 600, loss 1.856776475906372
iteration 700, loss 1.8538391590118408
iteration 800, loss 1.843253254890442
iteration 0, loss 1.8709884881973267
iteration 100, loss 1.8258944749832153
iteration 200, loss 1.8226925134658813
iteration 300, loss 1.8628119230270386
iteration 400, loss 1.8461450338363647
iteration 500, loss 1.829496145248413
iteration 600, loss 1.8253564834594727
iteration 700, loss 1.8057823181152344
iteration 800, loss 1.8079007863998413
iteration 0, loss 1.8936785459518433
iteration 100, loss 1.8519775867462158
iteration 200, loss 1.8968725204467773
iteration 300, loss 1.7983165979385376
iteration 400, loss 1.7958600521087646
iteration 500, loss 1.8333210945129395
iteration 600, loss 1.8264877796173096
iteration 700, loss 1.7885146141052246
iteration 800, loss 1.8119467496871948
iteration 0, loss 1.8225221633911133
iteration 100, loss 1.8231614828109741
iteration 200, loss 1.941145896911621
iteration 300, loss 1.8041783571243286
iteration 400, loss 1.8340455293655396
iteration 500, loss 1.7679446935653687
iteration 600, loss 1.8781794309616089
iteration 700, loss 1.8173991441726685
iteration 800, loss 1.8881553411483765
iteration 0, loss 1.8376808166503906
iteration 100, loss 1.9238303899765015
iteration 200, loss 1.8249366283416748
iteration 300, loss 1.8845033645629883
iteration 400, loss 1.8128291368484497
iteration 500, loss 1.8315153121948242
iteration 600, loss 1.8124257326126099
iteration 700, loss 1.8175525665283203
iteration 800, loss 1.8311350345611572
iteration 0, loss 1.7984728813171387
iteration 100, loss 1.9066320657730103
iteration 200, loss 1.8214045763015747
iteration 300, loss 1.8385155200958252
iteration 400, loss 1.8386095762252808
iteration 500, loss 1.9201123714447021
iteration 600, loss 1.8394947052001953
iteration 700, loss 1.8005918264389038
iteration 800, loss 1.8392963409423828
iteration 0, loss 1.9364193677902222
iteration 100, loss 1.77724289894104
iteration 200, loss 1.8003005981445312
iteration 300, loss 1.8105580806732178
iteration 400, loss 1.8436596393585205
iteration 500, loss 1.8625811338424683
iteration 600, loss 1.8538819551467896
iteration 700, loss 1.824204683303833
iteration 800, loss 1.7809154987335205
iteration 0, loss 1.812403917312622
iteration 100, loss 1.7894259691238403
iteration 200, loss 1.8643202781677246
iteration 300, loss 1.8286644220352173
iteration 400, loss 1.7947661876678467
iteration 500, loss 1.9250552654266357
iteration 600, loss 1.8551287651062012
iteration 700, loss 1.8215817213058472
iteration 800, loss 1.8652769327163696
iteration 0, loss 1.9054690599441528
iteration 100, loss 1.7884399890899658
iteration 200, loss 1.8104597330093384
iteration 300, loss 1.8651313781738281
iteration 400, loss 1.8272278308868408
iteration 500, loss 1.7892369031906128
iteration 600, loss 1.8341137170791626
iteration 700, loss 1.8708699941635132
iteration 800, loss 1.8267104625701904
iteration 0, loss 1.8081992864608765
iteration 100, loss 1.8910282850265503
iteration 200, loss 1.8180581331253052
iteration 300, loss 1.80522882938385
iteration 400, loss 1.7969499826431274
iteration 500, loss 1.8707441091537476
iteration 600, loss 1.798215627670288
iteration 700, loss 1.8594871759414673
iteration 800, loss 1.7665984630584717
iteration 0, loss 1.8649052381515503
iteration 100, loss 1.7641364336013794
iteration 200, loss 1.8110532760620117
iteration 300, loss 1.8662351369857788
iteration 400, loss 1.8028374910354614
iteration 500, loss 1.8411741256713867
iteration 600, loss 1.7909027338027954
iteration 700, loss 1.820005178451538
iteration 800, loss 1.8172308206558228
iteration 0, loss 1.8388690948486328
iteration 100, loss 1.9162414073944092
iteration 200, loss 1.8029786348342896
iteration 300, loss 1.794669270515442
iteration 400, loss 1.8411500453948975
iteration 500, loss 1.8582056760787964
iteration 600, loss 1.7726973295211792
iteration 700, loss 1.951934576034546
iteration 800, loss 1.8055959939956665
iteration 0, loss 1.8471671342849731
iteration 100, loss 1.7858264446258545
iteration 200, loss 1.8373658657073975
iteration 300, loss 1.8763198852539062
iteration 400, loss 1.774497389793396
iteration 500, loss 1.9534964561462402
iteration 600, loss 1.787914514541626
iteration 700, loss 1.8894283771514893
iteration 800, loss 1.8928887844085693
iteration 0, loss 1.804181456565857
iteration 100, loss 1.9106258153915405
iteration 200, loss 1.812690019607544
iteration 300, loss 1.8471590280532837
iteration 400, loss 1.832868218421936
iteration 500, loss 1.9098477363586426
iteration 600, loss 1.8491257429122925
iteration 700, loss 1.8235445022583008
iteration 800, loss 1.848947286605835
iteration 0, loss 1.823702335357666
iteration 100, loss 1.8369594812393188
iteration 200, loss 1.874635934829712
iteration 300, loss 1.7707899808883667
iteration 400, loss 1.81840980052948
iteration 500, loss 1.779554843902588
iteration 600, loss 1.833958625793457
iteration 700, loss 1.8761532306671143
iteration 800, loss 1.8116512298583984
iteration 0, loss 1.8426645994186401
iteration 100, loss 1.8574867248535156
iteration 200, loss 1.8928195238113403
iteration 300, loss 1.7901570796966553
iteration 400, loss 1.8159109354019165
iteration 500, loss 1.8787248134613037
iteration 600, loss 1.8914719820022583
iteration 700, loss 1.8650294542312622
iteration 800, loss 1.8640540838241577
iteration 0, loss 1.8041718006134033
iteration 100, loss 1.8260198831558228
iteration 200, loss 1.8224252462387085
iteration 300, loss 1.8364380598068237
iteration 400, loss 1.8043570518493652
iteration 500, loss 1.8216540813446045
iteration 600, loss 1.8696722984313965
iteration 700, loss 1.9682567119598389
iteration 800, loss 1.9279255867004395
iteration 0, loss 1.783340573310852
iteration 100, loss 1.7605211734771729
iteration 200, loss 1.878945231437683
iteration 300, loss 1.822840929031372
iteration 400, loss 1.8049367666244507
iteration 500, loss 1.8675445318222046
iteration 600, loss 1.8341197967529297
iteration 700, loss 1.8958783149719238
iteration 800, loss 1.803571343421936
iteration 0, loss 1.797471523284912
iteration 100, loss 1.8303086757659912
iteration 200, loss 1.771670937538147
iteration 300, loss 1.7804818153381348
iteration 400, loss 1.744964838027954
iteration 500, loss 1.7503012418746948
iteration 600, loss 1.8642164468765259
iteration 700, loss 1.8967053890228271
iteration 800, loss 1.839228630065918
iteration 0, loss 1.8089582920074463
iteration 100, loss 1.8131012916564941
iteration 200, loss 1.8027324676513672
iteration 300, loss 1.8172389268875122
iteration 400, loss 1.8585777282714844
iteration 500, loss 1.824545979499817
iteration 600, loss 1.8329453468322754
iteration 700, loss 1.8850858211517334
iteration 800, loss 1.7794560194015503
iteration 0, loss 1.8708680868148804
iteration 100, loss 1.8237643241882324
iteration 200, loss 1.8271714448928833
iteration 300, loss 1.8380504846572876
iteration 400, loss 1.7765568494796753
iteration 500, loss 1.8398020267486572
iteration 600, loss 1.890945315361023
iteration 700, loss 1.8657783269882202
iteration 800, loss 1.8044321537017822
iteration 0, loss 1.8466213941574097
iteration 100, loss 1.7582471370697021
iteration 200, loss 1.797143578529358
iteration 300, loss 1.8031914234161377
iteration 400, loss 1.8335872888565063
iteration 500, loss 1.8482619524002075
iteration 600, loss 1.7814732789993286
iteration 700, loss 1.7707452774047852
iteration 800, loss 1.828606367111206
iteration 0, loss 1.7952051162719727
iteration 100, loss 1.8255698680877686
iteration 200, loss 1.825825810432434
iteration 300, loss 1.7987055778503418
iteration 400, loss 1.8342641592025757
iteration 500, loss 1.8240277767181396
iteration 600, loss 1.7885881662368774
iteration 700, loss 1.8148393630981445
iteration 800, loss 1.7829619646072388
iteration 0, loss 1.935422420501709
iteration 100, loss 1.8209189176559448
iteration 200, loss 1.8384692668914795
iteration 300, loss 1.8026167154312134
iteration 400, loss 1.8920822143554688
iteration 500, loss 1.7742925882339478
iteration 600, loss 1.7855116128921509
iteration 700, loss 1.859025001525879
iteration 800, loss 1.8008503913879395
iteration 0, loss 1.7772762775421143
iteration 100, loss 1.7990350723266602
iteration 200, loss 1.7396749258041382
iteration 300, loss 1.7841110229492188
iteration 400, loss 1.894788146018982
iteration 500, loss 1.8404357433319092
iteration 600, loss 1.8050589561462402
iteration 700, loss 1.8596770763397217
iteration 800, loss 1.8110074996948242
iteration 0, loss 1.8451327085494995
iteration 100, loss 1.7792491912841797
iteration 200, loss 1.7290074825286865
iteration 300, loss 1.9328306913375854
iteration 400, loss 1.8392611742019653
iteration 500, loss 1.8492358922958374
iteration 600, loss 1.8120166063308716
iteration 700, loss 1.8119263648986816
iteration 800, loss 1.8247288465499878
iteration 0, loss 1.8079233169555664
iteration 100, loss 1.8481167554855347
iteration 200, loss 1.7914360761642456
iteration 300, loss 1.7872319221496582
iteration 400, loss 1.8228352069854736
iteration 500, loss 1.8326865434646606
iteration 600, loss 1.8183263540267944
iteration 700, loss 1.7978315353393555
iteration 800, loss 1.7760813236236572
iteration 0, loss 1.8057246208190918
iteration 100, loss 1.8112711906433105
iteration 200, loss 1.849856972694397
iteration 300, loss 1.8305684328079224
iteration 400, loss 1.8223323822021484
iteration 500, loss 1.821764349937439
iteration 600, loss 1.7606662511825562
iteration 700, loss 1.835601568222046
iteration 800, loss 1.8368574380874634
iteration 0, loss 1.8429430723190308
iteration 100, loss 1.7760268449783325
iteration 200, loss 1.7861440181732178
iteration 300, loss 1.847998857498169
iteration 400, loss 1.8236522674560547
iteration 500, loss 1.810506820678711
iteration 600, loss 1.8131736516952515
iteration 700, loss 1.8505409955978394
iteration 800, loss 1.8048304319381714
iteration 0, loss 1.7703310251235962
iteration 100, loss 1.8971954584121704
iteration 200, loss 1.8479069471359253
iteration 300, loss 1.7943284511566162
iteration 400, loss 1.8416839838027954
iteration 500, loss 1.8111969232559204
iteration 600, loss 1.7625279426574707
iteration 700, loss 1.9503295421600342
iteration 800, loss 1.8349244594573975
fold 2 accuracy: 0.4637142857142857
iteration 0, loss 1.8623709678649902
iteration 100, loss 1.8607938289642334
iteration 200, loss 1.8609813451766968
iteration 300, loss 1.8429195880889893
iteration 400, loss 1.7972328662872314
iteration 500, loss 1.836906909942627
iteration 600, loss 1.7695178985595703
iteration 700, loss 1.8825173377990723
iteration 800, loss 1.8062100410461426
iteration 0, loss 1.8597111701965332
iteration 100, loss 1.7282295227050781
iteration 200, loss 1.8236929178237915
iteration 300, loss 1.7982901334762573
iteration 400, loss 1.8515456914901733
iteration 500, loss 1.8523913621902466
iteration 600, loss 1.8195055723190308
iteration 700, loss 1.7894320487976074
iteration 800, loss 1.786724328994751
iteration 0, loss 1.875239372253418
iteration 100, loss 1.7995539903640747
iteration 200, loss 1.853088140487671
iteration 300, loss 1.804842233657837
iteration 400, loss 1.8238368034362793
iteration 500, loss 1.8092445135116577
iteration 600, loss 1.8400260210037231
iteration 700, loss 1.8158495426177979
iteration 800, loss 1.8278605937957764
iteration 0, loss 1.79939866065979
iteration 100, loss 1.8448610305786133
iteration 200, loss 1.8405358791351318
iteration 300, loss 1.856121301651001
iteration 400, loss 1.8178443908691406
iteration 500, loss 1.8715038299560547
iteration 600, loss 1.855360984802246
iteration 700, loss 1.8024543523788452
iteration 800, loss 1.7832894325256348
iteration 0, loss 1.794071078300476
iteration 100, loss 1.7791489362716675
iteration 200, loss 1.789874792098999
iteration 300, loss 1.8057345151901245
iteration 400, loss 1.8272614479064941
iteration 500, loss 1.8717014789581299
iteration 600, loss 1.8257153034210205
iteration 700, loss 1.8010227680206299
iteration 800, loss 1.9221512079238892
iteration 0, loss 1.8207676410675049
iteration 100, loss 1.7537143230438232
iteration 200, loss 1.8143059015274048
iteration 300, loss 1.8599610328674316
iteration 400, loss 1.8424338102340698
iteration 500, loss 1.7941937446594238
iteration 600, loss 1.7851758003234863
iteration 700, loss 1.8253521919250488
iteration 800, loss 1.8429701328277588
iteration 0, loss 1.7903558015823364
iteration 100, loss 1.8101019859313965
iteration 200, loss 1.8356536626815796
iteration 300, loss 1.8402045965194702
iteration 400, loss 1.8765672445297241
iteration 500, loss 1.7530739307403564
iteration 600, loss 1.7854535579681396
iteration 700, loss 1.8895825147628784
iteration 800, loss 1.7381776571273804
iteration 0, loss 1.8458712100982666
iteration 100, loss 1.7935210466384888
iteration 200, loss 1.8314294815063477
iteration 300, loss 1.7956445217132568
iteration 400, loss 1.8280318975448608
iteration 500, loss 1.7952102422714233
iteration 600, loss 1.9158315658569336
iteration 700, loss 1.878100037574768
iteration 800, loss 1.8887274265289307
iteration 0, loss 1.7899197340011597
iteration 100, loss 1.7789757251739502
iteration 200, loss 1.88121497631073
iteration 300, loss 1.7575764656066895
iteration 400, loss 1.8757866621017456
iteration 500, loss 1.7935422658920288
iteration 600, loss 1.790798544883728
iteration 700, loss 1.903591275215149
iteration 800, loss 1.9063749313354492
iteration 0, loss 1.8188024759292603
iteration 100, loss 1.8491829633712769
iteration 200, loss 1.824789047241211
iteration 300, loss 1.814763069152832
iteration 400, loss 1.8542430400848389
iteration 500, loss 1.7775517702102661
iteration 600, loss 1.8222476243972778
iteration 700, loss 1.82667076587677
iteration 800, loss 1.819798231124878
iteration 0, loss 1.763736605644226
iteration 100, loss 1.8275917768478394
iteration 200, loss 1.7303972244262695
iteration 300, loss 1.7679439783096313
iteration 400, loss 1.8003270626068115
iteration 500, loss 1.8293360471725464
iteration 600, loss 1.8118062019348145
iteration 700, loss 1.8665165901184082
iteration 800, loss 1.7906842231750488
iteration 0, loss 1.735241174697876
iteration 100, loss 1.8090896606445312
iteration 200, loss 1.8495572805404663
iteration 300, loss 1.7814730405807495
iteration 400, loss 1.852379322052002
iteration 500, loss 1.8371447324752808
iteration 600, loss 1.8001172542572021
iteration 700, loss 1.773917555809021
iteration 800, loss 1.7922829389572144
iteration 0, loss 1.8217544555664062
iteration 100, loss 1.7997612953186035
iteration 200, loss 1.8085509538650513
iteration 300, loss 1.8846821784973145
iteration 400, loss 1.8361157178878784
iteration 500, loss 1.8278969526290894
iteration 600, loss 1.7636828422546387
iteration 700, loss 1.8453552722930908
iteration 800, loss 1.8195745944976807
iteration 0, loss 1.81119966506958
iteration 100, loss 1.8369473218917847
iteration 200, loss 1.8268814086914062
iteration 300, loss 1.7880420684814453
iteration 400, loss 1.786826491355896
iteration 500, loss 1.8565775156021118
iteration 600, loss 1.7488970756530762
iteration 700, loss 1.836930513381958
iteration 800, loss 1.866000771522522
iteration 0, loss 1.8655955791473389
iteration 100, loss 1.827306866645813
iteration 200, loss 1.8942395448684692
iteration 300, loss 1.7741020917892456
iteration 400, loss 1.8918577432632446
iteration 500, loss 1.8495237827301025
iteration 600, loss 1.7834867238998413
iteration 700, loss 1.837270975112915
iteration 800, loss 1.8344838619232178
iteration 0, loss 1.8061110973358154
iteration 100, loss 1.8470749855041504
iteration 200, loss 1.8306293487548828
iteration 300, loss 1.7874257564544678
iteration 400, loss 1.7201663255691528
iteration 500, loss 1.800386667251587
iteration 600, loss 1.8231825828552246
iteration 700, loss 1.8603971004486084
iteration 800, loss 1.7765237092971802
iteration 0, loss 1.7610464096069336
iteration 100, loss 1.8852412700653076
iteration 200, loss 1.8046293258666992
iteration 300, loss 1.8688703775405884
iteration 400, loss 1.8174546957015991
iteration 500, loss 1.8861883878707886
iteration 600, loss 1.8501174449920654
iteration 700, loss 1.7748388051986694
iteration 800, loss 1.7577210664749146
iteration 0, loss 1.8360942602157593
iteration 100, loss 1.7643706798553467
iteration 200, loss 1.7773334980010986
iteration 300, loss 1.7798988819122314
iteration 400, loss 1.8188750743865967
iteration 500, loss 1.797082781791687
iteration 600, loss 1.7701572179794312
iteration 700, loss 1.7532618045806885
iteration 800, loss 1.753281831741333
iteration 0, loss 1.811974287033081
iteration 100, loss 1.8190003633499146
iteration 200, loss 1.8747917413711548
iteration 300, loss 1.8375498056411743
iteration 400, loss 1.8453640937805176
iteration 500, loss 1.7531685829162598
iteration 600, loss 1.9274742603302002
iteration 700, loss 1.7372335195541382
iteration 800, loss 1.7621053457260132
iteration 0, loss 1.8675230741500854
iteration 100, loss 1.819222331047058
iteration 200, loss 1.8540630340576172
iteration 300, loss 1.8416755199432373
iteration 400, loss 1.78216552734375
iteration 500, loss 1.7899967432022095
iteration 600, loss 1.7961008548736572
iteration 700, loss 1.8145089149475098
iteration 800, loss 1.7562175989151
iteration 0, loss 1.792479157447815
iteration 100, loss 1.8496901988983154
iteration 200, loss 1.828308343887329
iteration 300, loss 1.8355164527893066
iteration 400, loss 1.8310778141021729
iteration 500, loss 1.8039555549621582
iteration 600, loss 1.8391430377960205
iteration 700, loss 1.8249701261520386
iteration 800, loss 1.8014379739761353
iteration 0, loss 1.79128098487854
iteration 100, loss 1.812665343284607
iteration 200, loss 1.7883015871047974
iteration 300, loss 1.7706791162490845
iteration 400, loss 1.8235483169555664
iteration 500, loss 1.8227018117904663
iteration 600, loss 1.807517409324646
iteration 700, loss 1.8007298707962036
iteration 800, loss 1.7845807075500488
iteration 0, loss 1.7449685335159302
iteration 100, loss 1.7542154788970947
iteration 200, loss 1.7886720895767212
iteration 300, loss 1.792710542678833
iteration 400, loss 1.8375343084335327
iteration 500, loss 1.9252989292144775
iteration 600, loss 1.8323326110839844
iteration 700, loss 1.7682242393493652
iteration 800, loss 1.8542401790618896
iteration 0, loss 1.8024564981460571
iteration 100, loss 1.8773705959320068
iteration 200, loss 1.7867670059204102
iteration 300, loss 1.7984555959701538
iteration 400, loss 1.8112913370132446
iteration 500, loss 1.7811442613601685
iteration 600, loss 1.8474184274673462
iteration 700, loss 1.7890408039093018
iteration 800, loss 1.886231541633606
iteration 0, loss 1.8131884336471558
iteration 100, loss 1.8245398998260498
iteration 200, loss 1.8528327941894531
iteration 300, loss 1.8172742128372192
iteration 400, loss 1.7933247089385986
iteration 500, loss 1.8437767028808594
iteration 600, loss 1.8446553945541382
iteration 700, loss 1.7769182920455933
iteration 800, loss 1.8900094032287598
iteration 0, loss 1.8306885957717896
iteration 100, loss 1.7530561685562134
iteration 200, loss 1.8092031478881836
iteration 300, loss 1.8831130266189575
iteration 400, loss 1.7180689573287964
iteration 500, loss 1.8243849277496338
iteration 600, loss 1.8747494220733643
iteration 700, loss 1.7597756385803223
iteration 800, loss 1.8141807317733765
iteration 0, loss 1.8655531406402588
iteration 100, loss 1.8819745779037476
iteration 200, loss 1.7719554901123047
iteration 300, loss 1.7571077346801758
iteration 400, loss 1.7608839273452759
iteration 500, loss 1.8537708520889282
iteration 600, loss 1.7611000537872314
iteration 700, loss 1.811704158782959
iteration 800, loss 1.8213974237442017
iteration 0, loss 1.7993950843811035
iteration 100, loss 1.8065763711929321
iteration 200, loss 1.8674559593200684
iteration 300, loss 1.7852566242218018
iteration 400, loss 1.8007100820541382
iteration 500, loss 1.8297239542007446
iteration 600, loss 1.8224717378616333
iteration 700, loss 1.8240349292755127
iteration 800, loss 1.840618371963501
iteration 0, loss 1.8653993606567383
iteration 100, loss 1.8273862600326538
iteration 200, loss 1.8967608213424683
iteration 300, loss 1.822453498840332
iteration 400, loss 1.8772774934768677
iteration 500, loss 1.8971892595291138
iteration 600, loss 1.797696590423584
iteration 700, loss 1.8225376605987549
iteration 800, loss 1.905969500541687
iteration 0, loss 1.8089648485183716
iteration 100, loss 1.8098317384719849
iteration 200, loss 1.8014124631881714
iteration 300, loss 1.8009634017944336
iteration 400, loss 1.9065077304840088
iteration 500, loss 1.8013062477111816
iteration 600, loss 1.8667963743209839
iteration 700, loss 1.8228412866592407
iteration 800, loss 1.8407878875732422
iteration 0, loss 1.8165054321289062
iteration 100, loss 1.7749038934707642
iteration 200, loss 1.747361183166504
iteration 300, loss 1.7873196601867676
iteration 400, loss 1.7958954572677612
iteration 500, loss 1.8429903984069824
iteration 600, loss 1.821178913116455
iteration 700, loss 1.918965220451355
iteration 800, loss 1.787105679512024
iteration 0, loss 1.8562908172607422
iteration 100, loss 1.8353564739227295
iteration 200, loss 1.8006746768951416
iteration 300, loss 1.8616712093353271
iteration 400, loss 1.8066297769546509
iteration 500, loss 1.793733835220337
iteration 600, loss 1.835087776184082
iteration 700, loss 1.834028959274292
iteration 800, loss 1.8406922817230225
iteration 0, loss 1.846524715423584
iteration 100, loss 1.8452255725860596
iteration 200, loss 1.8202868700027466
iteration 300, loss 1.8515729904174805
iteration 400, loss 1.8726458549499512
iteration 500, loss 1.8118410110473633
iteration 600, loss 1.7921777963638306
iteration 700, loss 1.8389666080474854
iteration 800, loss 1.8928712606430054
iteration 0, loss 1.841938853263855
iteration 100, loss 1.8000181913375854
iteration 200, loss 1.8416873216629028
iteration 300, loss 1.860135555267334
iteration 400, loss 1.8199458122253418
iteration 500, loss 1.8241389989852905
iteration 600, loss 1.8305784463882446
iteration 700, loss 1.8013533353805542
iteration 800, loss 1.8545762300491333
iteration 0, loss 1.7995513677597046
iteration 100, loss 1.868162751197815
iteration 200, loss 1.7892682552337646
iteration 300, loss 1.7486891746520996
iteration 400, loss 1.7776390314102173
iteration 500, loss 1.8209096193313599
iteration 600, loss 1.7759008407592773
iteration 700, loss 1.8076252937316895
iteration 800, loss 1.784372329711914
iteration 0, loss 1.8522826433181763
iteration 100, loss 1.788621187210083
iteration 200, loss 1.8074052333831787
iteration 300, loss 1.8028188943862915
iteration 400, loss 1.8330609798431396
iteration 500, loss 1.763076663017273
iteration 600, loss 1.7945818901062012
iteration 700, loss 1.7994104623794556
iteration 800, loss 1.7526137828826904
iteration 0, loss 1.7558153867721558
iteration 100, loss 1.8382441997528076
iteration 200, loss 1.769579291343689
iteration 300, loss 1.7872111797332764
iteration 400, loss 1.8414413928985596
iteration 500, loss 1.7927837371826172
iteration 600, loss 1.8533470630645752
iteration 700, loss 1.8151296377182007
iteration 800, loss 1.8453013896942139
iteration 0, loss 1.8035428524017334
iteration 100, loss 1.8092347383499146
iteration 200, loss 1.8351131677627563
iteration 300, loss 1.756630778312683
iteration 400, loss 1.7768266201019287
iteration 500, loss 1.7971938848495483
iteration 600, loss 1.8226372003555298
iteration 700, loss 1.7811105251312256
iteration 800, loss 1.8298290967941284
iteration 0, loss 1.7895647287368774
iteration 100, loss 1.7698105573654175
iteration 200, loss 1.8369327783584595
iteration 300, loss 1.8205597400665283
iteration 400, loss 1.8242897987365723
iteration 500, loss 1.8274507522583008
iteration 600, loss 1.798397421836853
iteration 700, loss 1.8402775526046753
iteration 800, loss 1.8765504360198975
iteration 0, loss 1.7933293581008911
iteration 100, loss 1.862287998199463
iteration 200, loss 1.7626863718032837
iteration 300, loss 1.8868409395217896
iteration 400, loss 1.835001826286316
iteration 500, loss 1.876379132270813
iteration 600, loss 1.8322359323501587
iteration 700, loss 1.8229670524597168
iteration 800, loss 1.799865484237671
iteration 0, loss 1.7643885612487793
iteration 100, loss 1.8291412591934204
iteration 200, loss 1.7916909456253052
iteration 300, loss 1.8212541341781616
iteration 400, loss 1.8368136882781982
iteration 500, loss 1.841338038444519
iteration 600, loss 1.8988319635391235
iteration 700, loss 1.8466875553131104
iteration 800, loss 1.821325421333313
iteration 0, loss 1.8134841918945312
iteration 100, loss 1.8078707456588745
iteration 200, loss 1.828566074371338
iteration 300, loss 1.7643324136734009
iteration 400, loss 1.8080813884735107
iteration 500, loss 1.804703712463379
iteration 600, loss 1.848117470741272
iteration 700, loss 1.855225920677185
iteration 800, loss 1.7595226764678955
iteration 0, loss 1.9451358318328857
iteration 100, loss 1.7990626096725464
iteration 200, loss 1.8356223106384277
iteration 300, loss 1.7359973192214966
iteration 400, loss 1.7471559047698975
iteration 500, loss 1.8111001253128052
iteration 600, loss 1.7839298248291016
iteration 700, loss 1.7596242427825928
iteration 800, loss 1.816015601158142
iteration 0, loss 1.8257901668548584
iteration 100, loss 1.8330055475234985
iteration 200, loss 1.8483959436416626
iteration 300, loss 1.8333337306976318
iteration 400, loss 1.8665050268173218
iteration 500, loss 1.819817304611206
iteration 600, loss 1.8079404830932617
iteration 700, loss 1.801265001296997
iteration 800, loss 1.8554242849349976
iteration 0, loss 1.8220831155776978
iteration 100, loss 1.8510286808013916
iteration 200, loss 1.8234810829162598
iteration 300, loss 1.7949376106262207
iteration 400, loss 1.8397523164749146
iteration 500, loss 1.8379206657409668
iteration 600, loss 1.8231611251831055
iteration 700, loss 1.890857219696045
iteration 800, loss 1.8160759210586548
iteration 0, loss 1.7338820695877075
iteration 100, loss 1.7959364652633667
iteration 200, loss 1.7716996669769287
iteration 300, loss 1.8118726015090942
iteration 400, loss 1.7511591911315918
iteration 500, loss 1.7800477743148804
iteration 600, loss 1.7242791652679443
iteration 700, loss 1.7677700519561768
iteration 800, loss 1.8362714052200317
iteration 0, loss 1.7862770557403564
iteration 100, loss 1.842329740524292
iteration 200, loss 1.82832670211792
iteration 300, loss 1.8000586032867432
iteration 400, loss 1.8914306163787842
iteration 500, loss 1.837377905845642
iteration 600, loss 1.877592921257019
iteration 700, loss 1.8032810688018799
iteration 800, loss 1.8076521158218384
iteration 0, loss 1.8074711561203003
iteration 100, loss 1.808167815208435
iteration 200, loss 1.8802235126495361
iteration 300, loss 1.8261735439300537
iteration 400, loss 1.8676217794418335
iteration 500, loss 1.8389924764633179
iteration 600, loss 1.7609570026397705
iteration 700, loss 1.9105783700942993
iteration 800, loss 1.7649545669555664
iteration 0, loss 1.7695518732070923
iteration 100, loss 1.8131706714630127
iteration 200, loss 1.8656108379364014
iteration 300, loss 1.8956879377365112
iteration 400, loss 1.9254158735275269
iteration 500, loss 1.7556893825531006
iteration 600, loss 1.8433194160461426
iteration 700, loss 1.8291513919830322
iteration 800, loss 1.8095529079437256
iteration 0, loss 1.8514002561569214
iteration 100, loss 1.7716319561004639
iteration 200, loss 1.8681961297988892
iteration 300, loss 1.7749996185302734
iteration 400, loss 1.8794279098510742
iteration 500, loss 1.8681373596191406
iteration 600, loss 1.794374704360962
iteration 700, loss 1.8503830432891846
iteration 800, loss 1.7823312282562256
fold 3 accuracy: 0.4765
iteration 0, loss 1.8019416332244873
iteration 100, loss 1.7849944829940796
iteration 200, loss 1.7565330266952515
iteration 300, loss 1.7430813312530518
iteration 400, loss 1.7799420356750488
iteration 500, loss 1.7906005382537842
iteration 600, loss 1.86778724193573
iteration 700, loss 1.7968716621398926
iteration 800, loss 1.784396767616272
iteration 0, loss 1.7626317739486694
iteration 100, loss 1.8456441164016724
iteration 200, loss 1.7873353958129883
iteration 300, loss 1.8313541412353516
iteration 400, loss 1.854729175567627
iteration 500, loss 1.7687333822250366
iteration 600, loss 1.8067653179168701
iteration 700, loss 1.779536485671997
iteration 800, loss 1.8785226345062256
iteration 0, loss 1.810349464416504
iteration 100, loss 1.8329540491104126
iteration 200, loss 1.8115200996398926
iteration 300, loss 1.7926298379898071
iteration 400, loss 1.8121671676635742
iteration 500, loss 1.7901368141174316
iteration 600, loss 1.7467659711837769
iteration 700, loss 1.7963274717330933
iteration 800, loss 1.8107244968414307
iteration 0, loss 1.698862910270691
iteration 100, loss 1.8532531261444092
iteration 200, loss 1.8577072620391846
iteration 300, loss 1.7720065116882324
iteration 400, loss 1.7830688953399658
iteration 500, loss 1.8298285007476807
iteration 600, loss 1.8796195983886719
iteration 700, loss 1.7710589170455933
iteration 800, loss 1.8090187311172485
iteration 0, loss 1.8449273109436035
iteration 100, loss 1.8280915021896362
iteration 200, loss 1.7766627073287964
iteration 300, loss 1.784818410873413
iteration 400, loss 1.7770328521728516
iteration 500, loss 1.8196238279342651
iteration 600, loss 1.8726226091384888
iteration 700, loss 1.792258620262146
iteration 800, loss 1.8047940731048584
iteration 0, loss 1.9200102090835571
iteration 100, loss 1.7383280992507935
iteration 200, loss 1.860241413116455
iteration 300, loss 1.833982229232788
iteration 400, loss 1.8210903406143188
iteration 500, loss 1.8457220792770386
iteration 600, loss 1.8065241575241089
iteration 700, loss 1.8841744661331177
iteration 800, loss 1.7681355476379395
iteration 0, loss 1.7902075052261353
iteration 100, loss 1.8282839059829712
iteration 200, loss 1.7929942607879639
iteration 300, loss 1.77935791015625
iteration 400, loss 1.8197823762893677
iteration 500, loss 1.7667100429534912
iteration 600, loss 1.7686618566513062
iteration 700, loss 1.881964921951294
iteration 800, loss 1.8009699583053589
iteration 0, loss 1.8560322523117065
iteration 100, loss 1.8596502542495728
iteration 200, loss 1.7421244382858276
iteration 300, loss 1.7552859783172607
iteration 400, loss 1.7658771276474
iteration 500, loss 1.845826268196106
iteration 600, loss 1.743135690689087
iteration 700, loss 1.7304989099502563
iteration 800, loss 1.8282493352890015
iteration 0, loss 1.790542721748352
iteration 100, loss 1.7718740701675415
iteration 200, loss 1.7861974239349365
iteration 300, loss 1.895025610923767
iteration 400, loss 1.8239731788635254
iteration 500, loss 1.8173964023590088
iteration 600, loss 1.8416502475738525
iteration 700, loss 1.766164779663086
iteration 800, loss 1.7954457998275757
iteration 0, loss 1.8378026485443115
iteration 100, loss 1.8390454053878784
iteration 200, loss 1.797062635421753
iteration 300, loss 1.8735769987106323
iteration 400, loss 1.8443039655685425
iteration 500, loss 1.773577332496643
iteration 600, loss 1.8241900205612183
iteration 700, loss 1.839191198348999
iteration 800, loss 1.7648035287857056
iteration 0, loss 1.778279423713684
iteration 100, loss 1.8638664484024048
iteration 200, loss 1.756330966949463
iteration 300, loss 1.8372138738632202
iteration 400, loss 1.8436580896377563
iteration 500, loss 1.7930035591125488
iteration 600, loss 1.7900454998016357
iteration 700, loss 1.767804503440857
iteration 800, loss 1.7874778509140015
iteration 0, loss 1.8172489404678345
iteration 100, loss 1.7794734239578247
iteration 200, loss 1.8573715686798096
iteration 300, loss 1.8037636280059814
iteration 400, loss 1.7898932695388794
iteration 500, loss 1.854257583618164
iteration 600, loss 1.8512458801269531
iteration 700, loss 1.8738592863082886
iteration 800, loss 1.7428416013717651
iteration 0, loss 1.753316879272461
iteration 100, loss 1.7904833555221558
iteration 200, loss 1.8039729595184326
iteration 300, loss 1.731003761291504
iteration 400, loss 1.7767524719238281
iteration 500, loss 1.8878250122070312
iteration 600, loss 1.7827682495117188
iteration 700, loss 1.814572811126709
iteration 800, loss 1.7930456399917603
iteration 0, loss 1.7648725509643555
iteration 100, loss 1.812694787979126
iteration 200, loss 1.8243162631988525
iteration 300, loss 1.826026439666748
iteration 400, loss 1.8098260164260864
iteration 500, loss 1.8346335887908936
iteration 600, loss 1.7939472198486328
iteration 700, loss 1.8126320838928223
iteration 800, loss 1.8065084218978882
iteration 0, loss 1.79537832736969
iteration 100, loss 1.7921719551086426
iteration 200, loss 1.8330615758895874
iteration 300, loss 1.8350639343261719
iteration 400, loss 1.7714736461639404
iteration 500, loss 1.838459849357605
iteration 600, loss 1.7940559387207031
iteration 700, loss 1.8114086389541626
iteration 800, loss 1.8196526765823364
iteration 0, loss 1.8065810203552246
iteration 100, loss 1.763433575630188
iteration 200, loss 1.847758173942566
iteration 300, loss 1.7912687063217163
iteration 400, loss 1.8755539655685425
iteration 500, loss 1.9007055759429932
iteration 600, loss 1.7748973369598389
iteration 700, loss 1.9188458919525146
iteration 800, loss 1.8095645904541016
iteration 0, loss 1.8181970119476318
iteration 100, loss 1.8359473943710327
iteration 200, loss 1.8189239501953125
iteration 300, loss 1.7971543073654175
iteration 400, loss 1.8170161247253418
iteration 500, loss 1.8628982305526733
iteration 600, loss 1.8382536172866821
iteration 700, loss 1.8335143327713013
iteration 800, loss 1.8217928409576416
iteration 0, loss 1.7594472169876099
iteration 100, loss 1.769917607307434
iteration 200, loss 1.7814064025878906
iteration 300, loss 1.854349136352539
iteration 400, loss 1.829822063446045
iteration 500, loss 1.8240513801574707
iteration 600, loss 1.795743703842163
iteration 700, loss 1.8162333965301514
iteration 800, loss 1.7903581857681274
iteration 0, loss 1.795634388923645
iteration 100, loss 1.8567922115325928
iteration 200, loss 1.7128950357437134
iteration 300, loss 1.7369654178619385
iteration 400, loss 1.8156113624572754
iteration 500, loss 1.8387562036514282
iteration 600, loss 1.8176189661026
iteration 700, loss 1.8564826250076294
iteration 800, loss 1.782594084739685
iteration 0, loss 1.9307029247283936
iteration 100, loss 1.827488660812378
iteration 200, loss 1.727813482284546
iteration 300, loss 1.851207971572876
iteration 400, loss 1.8396358489990234
iteration 500, loss 1.7484396696090698
iteration 600, loss 1.7797185182571411
iteration 700, loss 1.7591925859451294
iteration 800, loss 1.8016242980957031
iteration 0, loss 1.8450895547866821
iteration 100, loss 1.8221992254257202
iteration 200, loss 1.7923805713653564
iteration 300, loss 1.8474444150924683
iteration 400, loss 1.8651942014694214
iteration 500, loss 1.8380290269851685
iteration 600, loss 1.803052306175232
iteration 700, loss 1.8018877506256104
iteration 800, loss 1.7727891206741333
iteration 0, loss 1.7977970838546753
iteration 100, loss 1.8103244304656982
iteration 200, loss 1.8087762594223022
iteration 300, loss 1.7748726606369019
iteration 400, loss 1.8551688194274902
iteration 500, loss 1.8015717267990112
iteration 600, loss 1.7960760593414307
iteration 700, loss 1.7881664037704468
iteration 800, loss 1.8135179281234741
iteration 0, loss 1.7907664775848389
iteration 100, loss 1.813820242881775
iteration 200, loss 1.7990816831588745
iteration 300, loss 1.7483230829238892
iteration 400, loss 1.7382038831710815
iteration 500, loss 1.7928253412246704
iteration 600, loss 1.7971504926681519
iteration 700, loss 1.7622349262237549
iteration 800, loss 1.7624274492263794
iteration 0, loss 1.8172600269317627
iteration 100, loss 1.8575080633163452
iteration 200, loss 1.795217514038086
iteration 300, loss 1.809613585472107
iteration 400, loss 1.8446863889694214
iteration 500, loss 1.8547821044921875
iteration 600, loss 1.806028962135315
iteration 700, loss 1.7533293962478638
iteration 800, loss 1.8533213138580322
iteration 0, loss 1.8010529279708862
iteration 100, loss 1.8714089393615723
iteration 200, loss 1.8407179117202759
iteration 300, loss 1.8015352487564087
iteration 400, loss 1.7985286712646484
iteration 500, loss 1.9404209852218628
iteration 600, loss 1.807039499282837
iteration 700, loss 1.8475557565689087
iteration 800, loss 1.8234683275222778
iteration 0, loss 1.7909961938858032
iteration 100, loss 1.776483416557312
iteration 200, loss 1.7718886137008667
iteration 300, loss 1.7768336534500122
iteration 400, loss 1.852689266204834
iteration 500, loss 1.8143328428268433
iteration 600, loss 1.8069995641708374
iteration 700, loss 1.8081283569335938
iteration 800, loss 1.8245351314544678
iteration 0, loss 1.7525596618652344
iteration 100, loss 1.8310372829437256
iteration 200, loss 1.8492240905761719
iteration 300, loss 1.8054759502410889
iteration 400, loss 1.7447558641433716
iteration 500, loss 1.807930588722229
iteration 600, loss 1.863438367843628
iteration 700, loss 1.8763822317123413
iteration 800, loss 1.8117327690124512
iteration 0, loss 1.7414534091949463
iteration 100, loss 1.824283242225647
iteration 200, loss 1.8500704765319824
iteration 300, loss 1.7728558778762817
iteration 400, loss 1.8072998523712158
iteration 500, loss 1.8203001022338867
iteration 600, loss 1.8216265439987183
iteration 700, loss 1.8214385509490967
iteration 800, loss 1.7832518815994263
iteration 0, loss 1.8338443040847778
iteration 100, loss 1.7482794523239136
iteration 200, loss 1.79917311668396
iteration 300, loss 1.8766953945159912
iteration 400, loss 1.8914681673049927
iteration 500, loss 1.7762601375579834
iteration 600, loss 1.7279200553894043
iteration 700, loss 1.7690798044204712
iteration 800, loss 1.8232500553131104
iteration 0, loss 1.8615343570709229
iteration 100, loss 1.7647199630737305
iteration 200, loss 1.7907017469406128
iteration 300, loss 1.8069692850112915
iteration 400, loss 1.8094550371170044
iteration 500, loss 1.7824386358261108
iteration 600, loss 1.8160984516143799
iteration 700, loss 1.7886399030685425
iteration 800, loss 1.7529778480529785
iteration 0, loss 1.8345345258712769
iteration 100, loss 1.808993935585022
iteration 200, loss 1.811784267425537
iteration 300, loss 1.7878758907318115
iteration 400, loss 1.8223152160644531
iteration 500, loss 1.830378532409668
iteration 600, loss 1.810092568397522
iteration 700, loss 1.8339680433273315
iteration 800, loss 1.799045443534851
iteration 0, loss 1.809617042541504
iteration 100, loss 1.7954685688018799
iteration 200, loss 1.8615443706512451
iteration 300, loss 1.8765150308609009
iteration 400, loss 1.7348872423171997
iteration 500, loss 1.837669849395752
iteration 600, loss 1.846797227859497
iteration 700, loss 1.7972230911254883
iteration 800, loss 1.7405314445495605
iteration 0, loss 1.8046386241912842
iteration 100, loss 1.8373082876205444
iteration 200, loss 1.7867615222930908
iteration 300, loss 1.8348097801208496
iteration 400, loss 1.8357206583023071
iteration 500, loss 1.7685317993164062
iteration 600, loss 1.8512613773345947
iteration 700, loss 1.7943692207336426
iteration 800, loss 1.793053388595581
iteration 0, loss 1.7792234420776367
iteration 100, loss 1.8324511051177979
iteration 200, loss 1.8647851943969727
iteration 300, loss 1.8454689979553223
iteration 400, loss 1.8626185655593872
iteration 500, loss 1.8341331481933594
iteration 600, loss 1.7954155206680298
iteration 700, loss 1.7504702806472778
iteration 800, loss 1.8670926094055176
iteration 0, loss 1.811704397201538
iteration 100, loss 1.8010146617889404
iteration 200, loss 1.780847191810608
iteration 300, loss 1.7521909475326538
iteration 400, loss 1.9253607988357544
iteration 500, loss 1.93282151222229
iteration 600, loss 1.8058655261993408
iteration 700, loss 1.8569574356079102
iteration 800, loss 1.828429937362671
iteration 0, loss 1.8458725214004517
iteration 100, loss 1.861031413078308
iteration 200, loss 1.7624374628067017
iteration 300, loss 1.7944636344909668
iteration 400, loss 1.8428760766983032
iteration 500, loss 1.8773858547210693
iteration 600, loss 1.830282211303711
iteration 700, loss 1.812319040298462
iteration 800, loss 1.8242920637130737
iteration 0, loss 1.7949045896530151
iteration 100, loss 1.815760612487793
iteration 200, loss 1.7981195449829102
iteration 300, loss 1.8331491947174072
iteration 400, loss 1.7742218971252441
iteration 500, loss 1.8623197078704834
iteration 600, loss 1.8636049032211304
iteration 700, loss 1.7886106967926025
iteration 800, loss 1.774688720703125
iteration 0, loss 1.8567402362823486
iteration 100, loss 1.7794768810272217
iteration 200, loss 1.8907862901687622
iteration 300, loss 1.7644553184509277
iteration 400, loss 1.7417405843734741
iteration 500, loss 1.7833956480026245
iteration 600, loss 1.8484987020492554
iteration 700, loss 1.7408181428909302
iteration 800, loss 1.8224842548370361
iteration 0, loss 1.7657948732376099
iteration 100, loss 1.8559614419937134
iteration 200, loss 1.8258154392242432
iteration 300, loss 1.8403970003128052
iteration 400, loss 1.7057921886444092
iteration 500, loss 1.7033629417419434
iteration 600, loss 1.8231092691421509
iteration 700, loss 1.7948322296142578
iteration 800, loss 1.8215117454528809
iteration 0, loss 1.7592829465866089
iteration 100, loss 1.7547835111618042
iteration 200, loss 1.8660749197006226
iteration 300, loss 1.831924557685852
iteration 400, loss 1.8092690706253052
iteration 500, loss 1.7912707328796387
iteration 600, loss 1.846903681755066
iteration 700, loss 1.8435372114181519
iteration 800, loss 1.8146371841430664
iteration 0, loss 1.8177106380462646
iteration 100, loss 1.8008100986480713
iteration 200, loss 1.7956568002700806
iteration 300, loss 1.7895443439483643
iteration 400, loss 1.7856814861297607
iteration 500, loss 1.7734878063201904
iteration 600, loss 1.7941832542419434
iteration 700, loss 1.8126602172851562
iteration 800, loss 1.8078782558441162
iteration 0, loss 1.8676584959030151
iteration 100, loss 1.7449729442596436
iteration 200, loss 1.7772257328033447
iteration 300, loss 1.8282943964004517
iteration 400, loss 1.8890146017074585
iteration 500, loss 1.8821070194244385
iteration 600, loss 1.7650123834609985
iteration 700, loss 1.7547351121902466
iteration 800, loss 1.8105041980743408
iteration 0, loss 1.8293652534484863
iteration 100, loss 1.8013262748718262
iteration 200, loss 1.8491086959838867
iteration 300, loss 1.8613804578781128
iteration 400, loss 1.7617977857589722
iteration 500, loss 1.8626797199249268
iteration 600, loss 1.857871174812317
iteration 700, loss 1.7917917966842651
iteration 800, loss 1.8157979249954224
iteration 0, loss 1.7806118726730347
iteration 100, loss 1.7974332571029663
iteration 200, loss 1.7710840702056885
iteration 300, loss 1.8091599941253662
iteration 400, loss 1.8499969244003296
iteration 500, loss 1.8070106506347656
iteration 600, loss 1.7682991027832031
iteration 700, loss 1.7595423460006714
iteration 800, loss 1.8029077053070068
iteration 0, loss 1.8355916738510132
iteration 100, loss 1.8193676471710205
iteration 200, loss 1.851238489151001
iteration 300, loss 1.789587378501892
iteration 400, loss 1.8090879917144775
iteration 500, loss 1.807489275932312
iteration 600, loss 1.8083009719848633
iteration 700, loss 1.8035008907318115
iteration 800, loss 1.8045299053192139
iteration 0, loss 1.9171658754348755
iteration 100, loss 1.8031461238861084
iteration 200, loss 1.7759130001068115
iteration 300, loss 1.8530519008636475
iteration 400, loss 1.7593472003936768
iteration 500, loss 1.8281229734420776
iteration 600, loss 1.7908309698104858
iteration 700, loss 1.7372112274169922
iteration 800, loss 1.7682448625564575
iteration 0, loss 1.731245517730713
iteration 100, loss 1.8549383878707886
iteration 200, loss 1.845229148864746
iteration 300, loss 1.7746341228485107
iteration 400, loss 1.8322396278381348
iteration 500, loss 1.7425307035446167
iteration 600, loss 1.7773510217666626
iteration 700, loss 1.8107119798660278
iteration 800, loss 1.8332207202911377
iteration 0, loss 1.7311972379684448
iteration 100, loss 1.8232150077819824
iteration 200, loss 1.7773350477218628
iteration 300, loss 1.7891725301742554
iteration 400, loss 1.802479863166809
iteration 500, loss 1.792096495628357
iteration 600, loss 1.7547686100006104
iteration 700, loss 1.8389724493026733
iteration 800, loss 1.8302956819534302
iteration 0, loss 1.7733122110366821
iteration 100, loss 1.8677387237548828
iteration 200, loss 1.7117435932159424
iteration 300, loss 1.7775983810424805
iteration 400, loss 1.8472346067428589
iteration 500, loss 1.8055042028427124
iteration 600, loss 1.7696702480316162
iteration 700, loss 1.8551117181777954
iteration 800, loss 1.8097048997879028
iteration 0, loss 1.753265142440796
iteration 100, loss 1.782254934310913
iteration 200, loss 1.8384438753128052
iteration 300, loss 1.8442387580871582
iteration 400, loss 1.8300023078918457
iteration 500, loss 1.8326436281204224
iteration 600, loss 1.7872374057769775
iteration 700, loss 1.8333022594451904
iteration 800, loss 1.8282592296600342
fold 4 accuracy: 0.4752142857142857
[2024-02-29 00:55:25,355] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-29 00:55:25,357] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         262     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       280 MACs
fwd flops per GPU:                                                      560     
fwd flops of model = fwd flops per GPU * mp_size:                       560     
fwd latency:                                                            324.73 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    1.72 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '262'}
    MACs        - {'KronLayer': '280 MACs'}
    fwd latency - {'KronLayer': '324.73 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  262 = 100% Params, 280 MACs = 100% MACs, 324.73 us = 100% latency, 1.72 MFLOPS
  (kronlinear): KronLinear(262 = 100% Params, 280 MACs = 100% MACs, 230.79 us = 71.07% latency, 2.43 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 27.66 us = 8.52% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-29 00:55:25,358] [INFO] [profiler.py:226:end_profile] Flops profiler finished
560  262
iteration 0, loss 2.3074357509613037
iteration 100, loss 2.350698709487915
iteration 200, loss 2.305081367492676
iteration 300, loss 2.1123993396759033
iteration 400, loss 2.118021011352539
iteration 500, loss 2.0410337448120117
iteration 600, loss 2.0865299701690674
iteration 700, loss 1.9811335802078247
iteration 800, loss 1.99404776096344
iteration 0, loss 1.9679738283157349
iteration 100, loss 1.9879308938980103
iteration 200, loss 1.9874193668365479
iteration 300, loss 1.9365979433059692
iteration 400, loss 1.9131803512573242
iteration 500, loss 1.9951914548873901
iteration 600, loss 1.9335582256317139
iteration 700, loss 1.9329583644866943
iteration 800, loss 1.9454420804977417
iteration 0, loss 1.9338645935058594
iteration 100, loss 1.9256364107131958
iteration 200, loss 1.9865403175354004
iteration 300, loss 1.957589030265808
iteration 400, loss 1.8757619857788086
iteration 500, loss 1.9300199747085571
iteration 600, loss 1.9893447160720825
iteration 700, loss 1.984795331954956
iteration 800, loss 1.9049636125564575
iteration 0, loss 1.9107109308242798
iteration 100, loss 1.960202932357788
iteration 200, loss 1.975614070892334
iteration 300, loss 1.9171819686889648
iteration 400, loss 1.9391300678253174
iteration 500, loss 1.861306071281433
iteration 600, loss 1.865105390548706
iteration 700, loss 1.9486234188079834
iteration 800, loss 1.8841710090637207
iteration 0, loss 1.9065488576889038
iteration 100, loss 1.961035966873169
iteration 200, loss 1.96964693069458
iteration 300, loss 1.9426777362823486
iteration 400, loss 1.9321016073226929
iteration 500, loss 1.9610719680786133
iteration 600, loss 1.9046484231948853
iteration 700, loss 1.888535976409912
iteration 800, loss 1.9314255714416504
iteration 0, loss 1.9164135456085205
iteration 100, loss 1.900444507598877
iteration 200, loss 1.944745421409607
iteration 300, loss 1.9208136796951294
iteration 400, loss 1.904491901397705
iteration 500, loss 1.8851174116134644
iteration 600, loss 1.8792688846588135
iteration 700, loss 1.8704115152359009
iteration 800, loss 1.9077459573745728
iteration 0, loss 1.9022315740585327
iteration 100, loss 1.9132353067398071
iteration 200, loss 1.9174883365631104
iteration 300, loss 1.9582133293151855
iteration 400, loss 1.970629096031189
iteration 500, loss 1.9283212423324585
iteration 600, loss 1.9110851287841797
iteration 700, loss 1.9795880317687988
iteration 800, loss 1.8519421815872192
iteration 0, loss 1.8702807426452637
iteration 100, loss 1.8513513803482056
iteration 200, loss 1.902923583984375
iteration 300, loss 1.8810651302337646
iteration 400, loss 1.8950059413909912
iteration 500, loss 1.886898398399353
iteration 600, loss 1.888662338256836
iteration 700, loss 1.9561784267425537
iteration 800, loss 1.8713973760604858
iteration 0, loss 1.917855978012085
iteration 100, loss 1.906291127204895
iteration 200, loss 1.9457281827926636
iteration 300, loss 1.904386281967163
iteration 400, loss 1.9188008308410645
iteration 500, loss 1.9389489889144897
iteration 600, loss 1.884721040725708
iteration 700, loss 1.8962444067001343
iteration 800, loss 1.9481779336929321
iteration 0, loss 1.9163261651992798
iteration 100, loss 1.8692052364349365
iteration 200, loss 1.9011043310165405
iteration 300, loss 1.8943166732788086
iteration 400, loss 1.8988163471221924
iteration 500, loss 1.9427311420440674
iteration 600, loss 1.868118405342102
iteration 700, loss 1.9580224752426147
iteration 800, loss 1.864847183227539
iteration 0, loss 1.9205561876296997
iteration 100, loss 1.9059016704559326
iteration 200, loss 1.926263451576233
iteration 300, loss 1.8496906757354736
iteration 400, loss 1.9402720928192139
iteration 500, loss 1.9316986799240112
iteration 600, loss 1.8881938457489014
iteration 700, loss 1.8996778726577759
iteration 800, loss 1.883377194404602
iteration 0, loss 1.8243154287338257
iteration 100, loss 2.004732847213745
iteration 200, loss 1.9461119174957275
iteration 300, loss 1.9119486808776855
iteration 400, loss 1.879582166671753
iteration 500, loss 1.9651927947998047
iteration 600, loss 1.9148414134979248
iteration 700, loss 1.8780536651611328
iteration 800, loss 1.8970022201538086
iteration 0, loss 1.9298630952835083
iteration 100, loss 1.8703136444091797
iteration 200, loss 1.9267792701721191
iteration 300, loss 1.8628567457199097
iteration 400, loss 1.9559433460235596
iteration 500, loss 1.9469391107559204
iteration 600, loss 1.842674970626831
iteration 700, loss 1.8877865076065063
iteration 800, loss 1.8721959590911865
iteration 0, loss 1.890369176864624
iteration 100, loss 1.8695757389068604
iteration 200, loss 1.9379631280899048
iteration 300, loss 1.9501636028289795
iteration 400, loss 1.8745321035385132
iteration 500, loss 1.8446969985961914
iteration 600, loss 1.8845763206481934
iteration 700, loss 1.962602138519287
iteration 800, loss 1.9093382358551025
iteration 0, loss 1.8626130819320679
iteration 100, loss 1.926466941833496
iteration 200, loss 1.8734074831008911
iteration 300, loss 1.9350425004959106
iteration 400, loss 1.861285924911499
iteration 500, loss 1.850935935974121
iteration 600, loss 1.8679569959640503
iteration 700, loss 1.8562548160552979
iteration 800, loss 1.8509124517440796
iteration 0, loss 1.985581398010254
iteration 100, loss 1.921908974647522
iteration 200, loss 1.9399049282073975
iteration 300, loss 1.8508166074752808
iteration 400, loss 1.8908054828643799
iteration 500, loss 1.8678879737854004
iteration 600, loss 1.8224161863327026
iteration 700, loss 1.9310808181762695
iteration 800, loss 1.854803204536438
iteration 0, loss 1.8870850801467896
iteration 100, loss 1.9160511493682861
iteration 200, loss 1.8839339017868042
iteration 300, loss 1.9118469953536987
iteration 400, loss 1.8601346015930176
iteration 500, loss 1.9387577772140503
iteration 600, loss 1.9396278858184814
iteration 700, loss 1.8718243837356567
iteration 800, loss 1.9105887413024902
iteration 0, loss 1.8036460876464844
iteration 100, loss 1.924161672592163
iteration 200, loss 1.8550150394439697
iteration 300, loss 1.8598209619522095
iteration 400, loss 1.8320082426071167
iteration 500, loss 1.9051977396011353
iteration 600, loss 1.8539695739746094
iteration 700, loss 1.9227806329727173
iteration 800, loss 1.869612455368042
iteration 0, loss 1.910177230834961
iteration 100, loss 1.889019250869751
iteration 200, loss 1.8788479566574097
iteration 300, loss 1.8565318584442139
iteration 400, loss 1.8989523649215698
iteration 500, loss 1.79970121383667
iteration 600, loss 1.8677465915679932
iteration 700, loss 1.9085378646850586
iteration 800, loss 1.8318842649459839
iteration 0, loss 1.9310628175735474
iteration 100, loss 1.8649944067001343
iteration 200, loss 1.8572134971618652
iteration 300, loss 1.837656855583191
iteration 400, loss 1.9045592546463013
iteration 500, loss 1.9686781167984009
iteration 600, loss 1.8273485898971558
iteration 700, loss 1.7778184413909912
iteration 800, loss 1.9533523321151733
iteration 0, loss 1.9560483694076538
iteration 100, loss 1.820229172706604
iteration 200, loss 1.864492416381836
iteration 300, loss 1.8034651279449463
iteration 400, loss 1.876640796661377
iteration 500, loss 1.9518704414367676
iteration 600, loss 1.82974112033844
iteration 700, loss 1.8621783256530762
iteration 800, loss 1.9566081762313843
iteration 0, loss 1.9709322452545166
iteration 100, loss 1.9233518838882446
iteration 200, loss 1.9225341081619263
iteration 300, loss 1.931962251663208
iteration 400, loss 1.8748829364776611
iteration 500, loss 1.9579358100891113
iteration 600, loss 1.911690354347229
iteration 700, loss 1.919282078742981
iteration 800, loss 1.9144604206085205
iteration 0, loss 1.838302493095398
iteration 100, loss 1.9518800973892212
iteration 200, loss 1.9445226192474365
iteration 300, loss 1.8715533018112183
iteration 400, loss 1.8691447973251343
iteration 500, loss 1.8448935747146606
iteration 600, loss 1.8469157218933105
iteration 700, loss 1.8804433345794678
iteration 800, loss 1.8206169605255127
iteration 0, loss 1.8481438159942627
iteration 100, loss 1.885690450668335
iteration 200, loss 1.8978419303894043
iteration 300, loss 1.889548420906067
iteration 400, loss 1.9337897300720215
iteration 500, loss 1.851171851158142
iteration 600, loss 1.9155219793319702
iteration 700, loss 1.828005075454712
iteration 800, loss 1.9348845481872559
iteration 0, loss 1.9119439125061035
iteration 100, loss 1.8595768213272095
iteration 200, loss 1.8148819208145142
iteration 300, loss 1.9170035123825073
iteration 400, loss 1.8167576789855957
iteration 500, loss 1.8522566556930542
iteration 600, loss 1.8764721155166626
iteration 700, loss 1.8380935192108154
iteration 800, loss 1.8984802961349487
iteration 0, loss 1.8591408729553223
iteration 100, loss 1.959800124168396
iteration 200, loss 1.8367865085601807
iteration 300, loss 1.8540668487548828
iteration 400, loss 1.8506872653961182
iteration 500, loss 1.8350836038589478
iteration 600, loss 1.8764554262161255
iteration 700, loss 1.8988138437271118
iteration 800, loss 1.926776647567749
iteration 0, loss 1.8469982147216797
iteration 100, loss 1.9260263442993164
iteration 200, loss 1.925317406654358
iteration 300, loss 1.8909807205200195
iteration 400, loss 1.9524966478347778
iteration 500, loss 1.8795613050460815
iteration 600, loss 1.8537089824676514
iteration 700, loss 1.930948257446289
iteration 800, loss 1.876773715019226
iteration 0, loss 1.9230494499206543
iteration 100, loss 1.8003485202789307
iteration 200, loss 1.8818979263305664
iteration 300, loss 1.832628607749939
iteration 400, loss 1.9084885120391846
iteration 500, loss 1.930204153060913
iteration 600, loss 1.8434189558029175
iteration 700, loss 1.87527596950531
iteration 800, loss 1.8750576972961426
iteration 0, loss 1.8516196012496948
iteration 100, loss 1.8645743131637573
iteration 200, loss 1.865427851676941
iteration 300, loss 1.8544416427612305
iteration 400, loss 1.8775403499603271
iteration 500, loss 1.8339570760726929
iteration 600, loss 1.8791958093643188
iteration 700, loss 1.93157958984375
iteration 800, loss 1.9079322814941406
iteration 0, loss 1.8762835264205933
iteration 100, loss 1.8591408729553223
iteration 200, loss 1.8850977420806885
iteration 300, loss 1.948112964630127
iteration 400, loss 1.9614869356155396
iteration 500, loss 1.8692169189453125
iteration 600, loss 1.8279197216033936
iteration 700, loss 1.946238398551941
iteration 800, loss 1.947036623954773
iteration 0, loss 1.8399542570114136
iteration 100, loss 1.885195016860962
iteration 200, loss 1.777806282043457
iteration 300, loss 1.8723151683807373
iteration 400, loss 1.8511133193969727
iteration 500, loss 1.8029054403305054
iteration 600, loss 1.8486597537994385
iteration 700, loss 1.8439233303070068
iteration 800, loss 1.8998035192489624
iteration 0, loss 1.8223915100097656
iteration 100, loss 1.8747522830963135
iteration 200, loss 1.8498337268829346
iteration 300, loss 1.8033322095870972
iteration 400, loss 1.8620142936706543
iteration 500, loss 1.889272928237915
iteration 600, loss 1.8603320121765137
iteration 700, loss 1.8402965068817139
iteration 800, loss 1.882957100868225
iteration 0, loss 1.9356286525726318
iteration 100, loss 1.8334927558898926
iteration 200, loss 1.8549871444702148
iteration 300, loss 1.9271783828735352
iteration 400, loss 1.86984384059906
iteration 500, loss 1.8353641033172607
iteration 600, loss 1.866149663925171
iteration 700, loss 1.9037953615188599
iteration 800, loss 1.942512035369873
iteration 0, loss 1.8894312381744385
iteration 100, loss 1.86002779006958
iteration 200, loss 1.8973664045333862
iteration 300, loss 1.8776642084121704
iteration 400, loss 1.8191940784454346
iteration 500, loss 1.7912318706512451
iteration 600, loss 1.8671342134475708
iteration 700, loss 1.8488678932189941
iteration 800, loss 1.9050201177597046
iteration 0, loss 1.8522850275039673
iteration 100, loss 1.9221539497375488
iteration 200, loss 1.8590621948242188
iteration 300, loss 1.910709261894226
iteration 400, loss 1.9037798643112183
iteration 500, loss 1.843530297279358
iteration 600, loss 1.8985693454742432
iteration 700, loss 1.9268181324005127
iteration 800, loss 1.8308117389678955
iteration 0, loss 1.9005368947982788
iteration 100, loss 1.9072712659835815
iteration 200, loss 1.8959681987762451
iteration 300, loss 1.8218148946762085
iteration 400, loss 1.949907660484314
iteration 500, loss 1.8735601902008057
iteration 600, loss 1.8789377212524414
iteration 700, loss 1.8503026962280273
iteration 800, loss 1.8323537111282349
iteration 0, loss 1.8402881622314453
iteration 100, loss 1.9623987674713135
iteration 200, loss 1.9117225408554077
iteration 300, loss 1.8719227313995361
iteration 400, loss 1.8894250392913818
iteration 500, loss 1.863261342048645
iteration 600, loss 1.8437458276748657
iteration 700, loss 1.8761695623397827
iteration 800, loss 1.889242172241211
iteration 0, loss 1.8971374034881592
iteration 100, loss 1.8615773916244507
iteration 200, loss 1.8299577236175537
iteration 300, loss 1.8152923583984375
iteration 400, loss 1.8473777770996094
iteration 500, loss 1.9275553226470947
iteration 600, loss 1.8804274797439575
iteration 700, loss 1.8323495388031006
iteration 800, loss 1.9097418785095215
iteration 0, loss 1.958595633506775
iteration 100, loss 1.8774936199188232
iteration 200, loss 1.8397514820098877
iteration 300, loss 1.8007862567901611
iteration 400, loss 1.9231712818145752
iteration 500, loss 1.8968058824539185
iteration 600, loss 1.8817116022109985
iteration 700, loss 1.8692314624786377
iteration 800, loss 1.9368258714675903
iteration 0, loss 1.8912917375564575
iteration 100, loss 1.810614824295044
iteration 200, loss 1.8605079650878906
iteration 300, loss 1.8601553440093994
iteration 400, loss 1.8612350225448608
iteration 500, loss 1.7995710372924805
iteration 600, loss 1.8477884531021118
iteration 700, loss 1.8991093635559082
iteration 800, loss 1.9538053274154663
iteration 0, loss 1.89104425907135
iteration 100, loss 1.8412275314331055
iteration 200, loss 1.7966632843017578
iteration 300, loss 1.8402987718582153
iteration 400, loss 1.891576886177063
iteration 500, loss 1.850659728050232
iteration 600, loss 1.8731720447540283
iteration 700, loss 1.892096996307373
iteration 800, loss 1.8486210107803345
iteration 0, loss 1.7875996828079224
iteration 100, loss 1.8456299304962158
iteration 200, loss 1.8955495357513428
iteration 300, loss 1.8509403467178345
iteration 400, loss 1.8470358848571777
iteration 500, loss 1.867832064628601
iteration 600, loss 1.8785810470581055
iteration 700, loss 1.8263726234436035
iteration 800, loss 1.845471739768982
iteration 0, loss 1.921078085899353
iteration 100, loss 1.83216392993927
iteration 200, loss 1.8481873273849487
iteration 300, loss 1.8197107315063477
iteration 400, loss 1.8549790382385254
iteration 500, loss 1.8652594089508057
iteration 600, loss 1.884714961051941
iteration 700, loss 1.8543827533721924
iteration 800, loss 1.8326854705810547
iteration 0, loss 1.8548617362976074
iteration 100, loss 1.7758820056915283
iteration 200, loss 1.9051135778427124
iteration 300, loss 1.8966748714447021
iteration 400, loss 1.8545199632644653
iteration 500, loss 1.7947694063186646
iteration 600, loss 1.8268578052520752
iteration 700, loss 1.8121455907821655
iteration 800, loss 1.8653862476348877
iteration 0, loss 1.8790274858474731
iteration 100, loss 1.8517265319824219
iteration 200, loss 1.9052464962005615
iteration 300, loss 1.888615369796753
iteration 400, loss 1.8360240459442139
iteration 500, loss 1.886672019958496
iteration 600, loss 1.9192843437194824
iteration 700, loss 1.8280854225158691
iteration 800, loss 1.8875994682312012
iteration 0, loss 1.8538037538528442
iteration 100, loss 1.8737128973007202
iteration 200, loss 1.819057583808899
iteration 300, loss 1.8781561851501465
iteration 400, loss 1.8704214096069336
iteration 500, loss 1.8685940504074097
iteration 600, loss 1.7948594093322754
iteration 700, loss 1.9688372611999512
iteration 800, loss 1.8791701793670654
iteration 0, loss 1.8660718202590942
iteration 100, loss 1.8852996826171875
iteration 200, loss 1.825564980506897
iteration 300, loss 1.8524467945098877
iteration 400, loss 1.8860583305358887
iteration 500, loss 1.7993578910827637
iteration 600, loss 1.8424735069274902
iteration 700, loss 1.8873465061187744
iteration 800, loss 1.9325059652328491
iteration 0, loss 1.9161254167556763
iteration 100, loss 1.7590662240982056
iteration 200, loss 1.8552782535552979
iteration 300, loss 1.891360878944397
iteration 400, loss 1.8935152292251587
iteration 500, loss 1.8989176750183105
iteration 600, loss 1.8532109260559082
iteration 700, loss 1.8469629287719727
iteration 800, loss 1.8699549436569214
iteration 0, loss 1.834677815437317
iteration 100, loss 1.8289655447006226
iteration 200, loss 1.8200474977493286
iteration 300, loss 1.8676509857177734
iteration 400, loss 1.8221811056137085
iteration 500, loss 1.8846771717071533
iteration 600, loss 1.9443432092666626
iteration 700, loss 1.9060561656951904
iteration 800, loss 1.9864907264709473
iteration 0, loss 1.786876916885376
iteration 100, loss 1.9388716220855713
iteration 200, loss 1.8631892204284668
iteration 300, loss 1.8389216661453247
iteration 400, loss 1.888249397277832
iteration 500, loss 1.8924013376235962
iteration 600, loss 1.8953710794448853
iteration 700, loss 1.908416986465454
iteration 800, loss 1.7795891761779785
fold 0 accuracy: 0.4744285714285714
iteration 0, loss 1.9392492771148682
iteration 100, loss 1.7574695348739624
iteration 200, loss 1.8654769659042358
iteration 300, loss 1.9131882190704346
iteration 400, loss 1.8483235836029053
iteration 500, loss 1.8443970680236816
iteration 600, loss 1.8138391971588135
iteration 700, loss 1.792902946472168
iteration 800, loss 1.829911708831787
iteration 0, loss 1.8463380336761475
iteration 100, loss 1.8523482084274292
iteration 200, loss 1.894803524017334
iteration 300, loss 1.8256301879882812
iteration 400, loss 1.8555716276168823
iteration 500, loss 1.8632965087890625
iteration 600, loss 1.8570677042007446
iteration 700, loss 1.8391685485839844
iteration 800, loss 1.8296443223953247
iteration 0, loss 1.8331332206726074
iteration 100, loss 1.8466393947601318
iteration 200, loss 1.8292531967163086
iteration 300, loss 1.9090155363082886
iteration 400, loss 1.8416303396224976
iteration 500, loss 1.8327648639678955
iteration 600, loss 1.9436790943145752
iteration 700, loss 1.8080154657363892
iteration 800, loss 1.8640457391738892
iteration 0, loss 1.8572630882263184
iteration 100, loss 1.8187353610992432
iteration 200, loss 1.9113088846206665
iteration 300, loss 1.8242671489715576
iteration 400, loss 1.9252350330352783
iteration 500, loss 1.8430145978927612
iteration 600, loss 1.8526551723480225
iteration 700, loss 1.7837982177734375
iteration 800, loss 1.8467055559158325
iteration 0, loss 1.8086804151535034
iteration 100, loss 1.921006202697754
iteration 200, loss 1.7958499193191528
iteration 300, loss 1.9233615398406982
iteration 400, loss 1.9170174598693848
iteration 500, loss 1.9014854431152344
iteration 600, loss 1.8351130485534668
iteration 700, loss 1.8533145189285278
iteration 800, loss 1.8920832872390747
iteration 0, loss 1.9799336194992065
iteration 100, loss 1.8696447610855103
iteration 200, loss 1.879903793334961
iteration 300, loss 1.856963038444519
iteration 400, loss 1.8444414138793945
iteration 500, loss 1.8734557628631592
iteration 600, loss 1.8797123432159424
iteration 700, loss 1.8001816272735596
iteration 800, loss 1.9760440587997437
iteration 0, loss 1.8987716436386108
iteration 100, loss 1.829086184501648
iteration 200, loss 1.8406867980957031
iteration 300, loss 1.8575515747070312
iteration 400, loss 1.8358349800109863
iteration 500, loss 1.8394168615341187
iteration 600, loss 1.9599337577819824
iteration 700, loss 1.8111454248428345
iteration 800, loss 1.7921957969665527
iteration 0, loss 1.7558053731918335
iteration 100, loss 1.9408936500549316
iteration 200, loss 1.8501075506210327
iteration 300, loss 1.863034963607788
iteration 400, loss 1.8299086093902588
iteration 500, loss 1.7896958589553833
iteration 600, loss 1.8747954368591309
iteration 700, loss 1.8895190954208374
iteration 800, loss 1.8937561511993408
iteration 0, loss 1.7801254987716675
iteration 100, loss 1.8493043184280396
iteration 200, loss 1.8649201393127441
iteration 300, loss 1.8818213939666748
iteration 400, loss 1.8631631135940552
iteration 500, loss 1.8416029214859009
iteration 600, loss 1.7887043952941895
iteration 700, loss 1.8582909107208252
iteration 800, loss 1.9306237697601318
iteration 0, loss 1.8407583236694336
iteration 100, loss 1.8294527530670166
iteration 200, loss 1.8434799909591675
iteration 300, loss 1.8400317430496216
iteration 400, loss 1.8373221158981323
iteration 500, loss 1.9762369394302368
iteration 600, loss 1.889930248260498
iteration 700, loss 1.893244743347168
iteration 800, loss 1.9637869596481323
iteration 0, loss 1.8073277473449707
iteration 100, loss 1.907872200012207
iteration 200, loss 1.7868177890777588
iteration 300, loss 1.9207899570465088
iteration 400, loss 1.8814016580581665
iteration 500, loss 1.8901671171188354
iteration 600, loss 1.8353246450424194
iteration 700, loss 1.8356775045394897
iteration 800, loss 1.869192123413086
iteration 0, loss 1.827878713607788
iteration 100, loss 1.8721870183944702
iteration 200, loss 1.8580232858657837
iteration 300, loss 1.8649760484695435
iteration 400, loss 1.7994108200073242
iteration 500, loss 1.8464832305908203
iteration 600, loss 1.8829783201217651
iteration 700, loss 1.9043680429458618
iteration 800, loss 1.9183539152145386
iteration 0, loss 1.9217132329940796
iteration 100, loss 1.857129454612732
iteration 200, loss 1.811004877090454
iteration 300, loss 1.7920928001403809
iteration 400, loss 1.9272119998931885
iteration 500, loss 1.8706825971603394
iteration 600, loss 1.8303196430206299
iteration 700, loss 1.820634126663208
iteration 800, loss 1.8413960933685303
iteration 0, loss 1.9376206398010254
iteration 100, loss 1.8740957975387573
iteration 200, loss 1.8757727146148682
iteration 300, loss 1.858521580696106
iteration 400, loss 1.938449501991272
iteration 500, loss 1.905651569366455
iteration 600, loss 1.8833802938461304
iteration 700, loss 1.8027867078781128
iteration 800, loss 1.8483909368515015
iteration 0, loss 1.8789703845977783
iteration 100, loss 1.8251926898956299
iteration 200, loss 1.8741320371627808
iteration 300, loss 1.9454678297042847
iteration 400, loss 1.860800862312317
iteration 500, loss 1.9059921503067017
iteration 600, loss 1.8470485210418701
iteration 700, loss 1.842643141746521
iteration 800, loss 1.9311928749084473
iteration 0, loss 1.7828093767166138
iteration 100, loss 1.8165717124938965
iteration 200, loss 1.8228886127471924
iteration 300, loss 1.854841709136963
iteration 400, loss 1.8404865264892578
iteration 500, loss 1.8700261116027832
iteration 600, loss 1.828955888748169
iteration 700, loss 1.8645482063293457
iteration 800, loss 1.8669471740722656
iteration 0, loss 1.8399651050567627
iteration 100, loss 1.8634002208709717
iteration 200, loss 1.9251149892807007
iteration 300, loss 1.8408982753753662
iteration 400, loss 1.9429080486297607
iteration 500, loss 1.8451323509216309
iteration 600, loss 1.8705006837844849
iteration 700, loss 1.8145713806152344
iteration 800, loss 1.8884146213531494
iteration 0, loss 1.8819122314453125
iteration 100, loss 1.8466651439666748
iteration 200, loss 1.8743672370910645
iteration 300, loss 1.9076207876205444
iteration 400, loss 1.8559043407440186
iteration 500, loss 1.8193352222442627
iteration 600, loss 1.839111328125
iteration 700, loss 1.822442650794983
iteration 800, loss 1.8000109195709229
iteration 0, loss 1.8356767892837524
iteration 100, loss 1.9518948793411255
iteration 200, loss 1.9954286813735962
iteration 300, loss 1.8403757810592651
iteration 400, loss 1.9020411968231201
iteration 500, loss 1.8357839584350586
iteration 600, loss 1.8556790351867676
iteration 700, loss 1.7607407569885254
iteration 800, loss 1.858952522277832
iteration 0, loss 1.9270652532577515
iteration 100, loss 1.896031141281128
iteration 200, loss 1.8525147438049316
iteration 300, loss 1.9419220685958862
iteration 400, loss 1.907835841178894
iteration 500, loss 1.920088291168213
iteration 600, loss 1.8399194478988647
iteration 700, loss 1.8960670232772827
iteration 800, loss 1.8498271703720093
iteration 0, loss 1.8642935752868652
iteration 100, loss 1.8867486715316772
iteration 200, loss 1.90684974193573
iteration 300, loss 1.7790955305099487
iteration 400, loss 1.7931883335113525
iteration 500, loss 1.839522123336792
iteration 600, loss 1.8359304666519165
iteration 700, loss 1.8287467956542969
iteration 800, loss 1.8964393138885498
iteration 0, loss 1.9029606580734253
iteration 100, loss 1.770331621170044
iteration 200, loss 1.8912343978881836
iteration 300, loss 1.8089326620101929
iteration 400, loss 1.867967128753662
iteration 500, loss 1.863620400428772
iteration 600, loss 1.8692278861999512
iteration 700, loss 1.894171118736267
iteration 800, loss 1.9826202392578125
iteration 0, loss 1.8988851308822632
iteration 100, loss 1.8919364213943481
iteration 200, loss 1.871017336845398
iteration 300, loss 1.9578505754470825
iteration 400, loss 1.833066701889038
iteration 500, loss 1.823874831199646
iteration 600, loss 1.898176670074463
iteration 700, loss 1.9398657083511353
iteration 800, loss 1.804029941558838
iteration 0, loss 1.8449018001556396
iteration 100, loss 1.8649922609329224
iteration 200, loss 1.8472262620925903
iteration 300, loss 1.8294669389724731
iteration 400, loss 1.898490309715271
iteration 500, loss 1.8581923246383667
iteration 600, loss 1.863391637802124
iteration 700, loss 1.7948737144470215
iteration 800, loss 1.8802963495254517
iteration 0, loss 1.8373732566833496
iteration 100, loss 1.8495886325836182
iteration 200, loss 1.8421642780303955
iteration 300, loss 1.8859047889709473
iteration 400, loss 2.0195388793945312
iteration 500, loss 1.8845603466033936
iteration 600, loss 1.8764578104019165
iteration 700, loss 1.8301773071289062
iteration 800, loss 1.8308078050613403
iteration 0, loss 1.8728129863739014
iteration 100, loss 1.8282761573791504
iteration 200, loss 1.8987064361572266
iteration 300, loss 1.8615176677703857
iteration 400, loss 1.8775659799575806
iteration 500, loss 1.806109070777893
iteration 600, loss 1.916244626045227
iteration 700, loss 1.9142341613769531
iteration 800, loss 1.8465194702148438
iteration 0, loss 1.913962960243225
iteration 100, loss 1.8659555912017822
iteration 200, loss 1.8370176553726196
iteration 300, loss 1.874247670173645
iteration 400, loss 1.8555394411087036
iteration 500, loss 1.8142284154891968
iteration 600, loss 1.8593668937683105
iteration 700, loss 1.757904052734375
iteration 800, loss 1.8783411979675293
iteration 0, loss 1.865232229232788
iteration 100, loss 1.9000200033187866
iteration 200, loss 1.8363112211227417
iteration 300, loss 1.847059726715088
iteration 400, loss 1.887847661972046
iteration 500, loss 1.9134840965270996
iteration 600, loss 1.9126014709472656
iteration 700, loss 1.9230871200561523
iteration 800, loss 1.916965126991272
iteration 0, loss 1.8481478691101074
iteration 100, loss 1.8934844732284546
iteration 200, loss 1.8971600532531738
iteration 300, loss 1.8328487873077393
iteration 400, loss 1.8834338188171387
iteration 500, loss 1.8633015155792236
iteration 600, loss 1.86874258518219
iteration 700, loss 1.8578189611434937
iteration 800, loss 1.8913018703460693
iteration 0, loss 1.9651190042495728
iteration 100, loss 1.8458092212677002
iteration 200, loss 1.962790608406067
iteration 300, loss 1.8167415857315063
iteration 400, loss 1.8759205341339111
iteration 500, loss 1.8149923086166382
iteration 600, loss 1.861291527748108
iteration 700, loss 1.8305048942565918
iteration 800, loss 1.8748009204864502
iteration 0, loss 1.9398643970489502
iteration 100, loss 1.8491597175598145
iteration 200, loss 1.9085779190063477
iteration 300, loss 1.884036898612976
iteration 400, loss 1.7910716533660889
iteration 500, loss 1.8489503860473633
iteration 600, loss 1.90500009059906
iteration 700, loss 1.9578171968460083
iteration 800, loss 1.8204423189163208
iteration 0, loss 1.8161581754684448
iteration 100, loss 1.919028878211975
iteration 200, loss 1.8328166007995605
iteration 300, loss 1.897255301475525
iteration 400, loss 1.845711588859558
iteration 500, loss 1.858557939529419
iteration 600, loss 1.9030241966247559
iteration 700, loss 1.9205213785171509
iteration 800, loss 1.7983161211013794
iteration 0, loss 1.8959846496582031
iteration 100, loss 1.8381545543670654
iteration 200, loss 1.8754045963287354
iteration 300, loss 1.897651195526123
iteration 400, loss 1.8471907377243042
iteration 500, loss 1.8282994031906128
iteration 600, loss 1.8903212547302246
iteration 700, loss 1.8236560821533203
iteration 800, loss 1.8954001665115356
iteration 0, loss 1.8447136878967285
iteration 100, loss 1.9326883554458618
iteration 200, loss 1.8420079946517944
iteration 300, loss 1.9181610345840454
iteration 400, loss 1.8814436197280884
iteration 500, loss 1.832800030708313
iteration 600, loss 1.8976199626922607
iteration 700, loss 1.9101685285568237
iteration 800, loss 1.8761366605758667
iteration 0, loss 1.8417718410491943
iteration 100, loss 1.879073977470398
iteration 200, loss 1.920334815979004
iteration 300, loss 1.7761857509613037
iteration 400, loss 1.8234556913375854
iteration 500, loss 1.9158878326416016
iteration 600, loss 1.8929728269577026
iteration 700, loss 1.9199326038360596
iteration 800, loss 1.9023070335388184
iteration 0, loss 1.8293529748916626
iteration 100, loss 1.83664870262146
iteration 200, loss 1.8712587356567383
iteration 300, loss 1.7984083890914917
iteration 400, loss 1.8639706373214722
iteration 500, loss 1.895928978919983
iteration 600, loss 1.9443122148513794
iteration 700, loss 1.8504313230514526
iteration 800, loss 1.860656499862671
iteration 0, loss 1.9241448640823364
iteration 100, loss 1.8220927715301514
iteration 200, loss 1.8960785865783691
iteration 300, loss 1.8355317115783691
iteration 400, loss 1.9072973728179932
iteration 500, loss 1.8752694129943848
iteration 600, loss 1.8481217622756958
iteration 700, loss 1.7873201370239258
iteration 800, loss 1.8977162837982178
iteration 0, loss 1.874917984008789
iteration 100, loss 1.8812392950057983
iteration 200, loss 1.8535951375961304
iteration 300, loss 1.7399883270263672
iteration 400, loss 1.8531079292297363
iteration 500, loss 1.843050479888916
iteration 600, loss 1.9038656949996948
iteration 700, loss 1.926468014717102
iteration 800, loss 1.8388339281082153
iteration 0, loss 1.8831346035003662
iteration 100, loss 1.8839999437332153
iteration 200, loss 1.8511770963668823
iteration 300, loss 1.8870781660079956
iteration 400, loss 1.843317985534668
iteration 500, loss 1.9512032270431519
iteration 600, loss 1.821510672569275
iteration 700, loss 1.849513292312622
iteration 800, loss 1.8610856533050537
iteration 0, loss 1.859224796295166
iteration 100, loss 1.8716378211975098
iteration 200, loss 1.8488470315933228
iteration 300, loss 1.878139853477478
iteration 400, loss 1.8130029439926147
iteration 500, loss 1.883360505104065
iteration 600, loss 1.8411613702774048
iteration 700, loss 1.8372242450714111
iteration 800, loss 1.9063457250595093
iteration 0, loss 1.8667293787002563
iteration 100, loss 1.8681992292404175
iteration 200, loss 1.85335111618042
iteration 300, loss 1.8658225536346436
iteration 400, loss 1.8587472438812256
iteration 500, loss 1.9215573072433472
iteration 600, loss 1.9254238605499268
iteration 700, loss 1.9067752361297607
iteration 800, loss 1.8291300535202026
iteration 0, loss 1.7980972528457642
iteration 100, loss 1.8456838130950928
iteration 200, loss 1.8744910955429077
iteration 300, loss 1.857154369354248
iteration 400, loss 1.8566087484359741
iteration 500, loss 1.8310871124267578
iteration 600, loss 1.8855910301208496
iteration 700, loss 1.853316307067871
iteration 800, loss 1.861466407775879
iteration 0, loss 1.8404548168182373
iteration 100, loss 1.8875210285186768
iteration 200, loss 1.9085432291030884
iteration 300, loss 1.8577508926391602
iteration 400, loss 1.865250825881958
iteration 500, loss 1.856799840927124
iteration 600, loss 1.7917407751083374
iteration 700, loss 1.8514480590820312
iteration 800, loss 1.7967395782470703
iteration 0, loss 1.852888822555542
iteration 100, loss 1.8740428686141968
iteration 200, loss 1.8294390439987183
iteration 300, loss 1.8595974445343018
iteration 400, loss 1.9373936653137207
iteration 500, loss 1.7782777547836304
iteration 600, loss 1.8904625177383423
iteration 700, loss 1.876899003982544
iteration 800, loss 1.8414705991744995
iteration 0, loss 1.933363914489746
iteration 100, loss 1.828576683998108
iteration 200, loss 1.963245153427124
iteration 300, loss 1.892490267753601
iteration 400, loss 1.882641315460205
iteration 500, loss 1.8745185136795044
iteration 600, loss 1.898545265197754
iteration 700, loss 1.8415708541870117
iteration 800, loss 1.859574317932129
iteration 0, loss 1.8704782724380493
iteration 100, loss 1.8848683834075928
iteration 200, loss 1.818516492843628
iteration 300, loss 1.8049942255020142
iteration 400, loss 1.7822813987731934
iteration 500, loss 1.880160927772522
iteration 600, loss 1.9006540775299072
iteration 700, loss 1.8865326642990112
iteration 800, loss 1.7882996797561646
iteration 0, loss 1.8178452253341675
iteration 100, loss 1.9039827585220337
iteration 200, loss 1.9230085611343384
iteration 300, loss 1.8136060237884521
iteration 400, loss 1.917632818222046
iteration 500, loss 1.8374948501586914
iteration 600, loss 1.8818827867507935
iteration 700, loss 1.7805265188217163
iteration 800, loss 1.8035221099853516
iteration 0, loss 1.8795427083969116
iteration 100, loss 1.8247328996658325
iteration 200, loss 1.8231159448623657
iteration 300, loss 1.8818161487579346
iteration 400, loss 1.9166018962860107
iteration 500, loss 1.8607121706008911
iteration 600, loss 1.862328290939331
iteration 700, loss 1.8256949186325073
iteration 800, loss 1.860365390777588
iteration 0, loss 1.8912131786346436
iteration 100, loss 1.9510738849639893
iteration 200, loss 1.8567310571670532
iteration 300, loss 1.8076964616775513
iteration 400, loss 1.8368473052978516
iteration 500, loss 1.8563265800476074
iteration 600, loss 1.8890260457992554
iteration 700, loss 1.858627438545227
iteration 800, loss 1.775046706199646
iteration 0, loss 1.8274595737457275
iteration 100, loss 1.915748119354248
iteration 200, loss 1.8801860809326172
iteration 300, loss 1.7885314226150513
iteration 400, loss 1.852271556854248
iteration 500, loss 1.84067702293396
iteration 600, loss 1.8144829273223877
iteration 700, loss 1.817081093788147
iteration 800, loss 1.8495121002197266
fold 1 accuracy: 0.48364285714285715
iteration 0, loss 1.8178303241729736
iteration 100, loss 1.8931750059127808
iteration 200, loss 1.8803333044052124
iteration 300, loss 1.9118857383728027
iteration 400, loss 1.823090672492981
iteration 500, loss 1.8600049018859863
iteration 600, loss 1.8629939556121826
iteration 700, loss 1.7683560848236084
iteration 800, loss 1.8989763259887695
iteration 0, loss 1.865049958229065
iteration 100, loss 1.8363723754882812
iteration 200, loss 1.9453084468841553
iteration 300, loss 1.892250657081604
iteration 400, loss 1.8687019348144531
iteration 500, loss 1.8245623111724854
iteration 600, loss 1.8289644718170166
iteration 700, loss 1.8187183141708374
iteration 800, loss 1.8813377618789673
iteration 0, loss 1.8066182136535645
iteration 100, loss 1.9248977899551392
iteration 200, loss 1.8178579807281494
iteration 300, loss 1.8694162368774414
iteration 400, loss 1.848540186882019
iteration 500, loss 1.9332926273345947
iteration 600, loss 1.8820033073425293
iteration 700, loss 1.837726354598999
iteration 800, loss 1.7723239660263062
iteration 0, loss 1.9536292552947998
iteration 100, loss 1.862635612487793
iteration 200, loss 1.9185725450515747
iteration 300, loss 1.9054327011108398
iteration 400, loss 1.8790779113769531
iteration 500, loss 1.8699723482131958
iteration 600, loss 1.8057091236114502
iteration 700, loss 1.876418948173523
iteration 800, loss 1.8578733205795288
iteration 0, loss 1.8807086944580078
iteration 100, loss 1.8311625719070435
iteration 200, loss 1.8394280672073364
iteration 300, loss 1.8727929592132568
iteration 400, loss 1.8236380815505981
iteration 500, loss 1.8421369791030884
iteration 600, loss 1.9274895191192627
iteration 700, loss 1.9371638298034668
iteration 800, loss 1.7856045961380005
iteration 0, loss 1.8485325574874878
iteration 100, loss 1.8648254871368408
iteration 200, loss 1.8175759315490723
iteration 300, loss 1.8526241779327393
iteration 400, loss 1.8119453191757202
iteration 500, loss 1.8014264106750488
iteration 600, loss 1.9270552396774292
iteration 700, loss 1.8590203523635864
iteration 800, loss 1.8430898189544678
iteration 0, loss 1.857344627380371
iteration 100, loss 1.9428142309188843
iteration 200, loss 1.837632417678833
iteration 300, loss 1.8183090686798096
iteration 400, loss 1.820728063583374
iteration 500, loss 1.9112331867218018
iteration 600, loss 1.876258373260498
iteration 700, loss 1.8043540716171265
iteration 800, loss 1.9198353290557861
iteration 0, loss 1.7551525831222534
iteration 100, loss 1.8512928485870361
iteration 200, loss 1.8646066188812256
iteration 300, loss 1.843247413635254
iteration 400, loss 1.8883118629455566
iteration 500, loss 1.7937912940979004
iteration 600, loss 1.7902170419692993
iteration 700, loss 1.8565176725387573
iteration 800, loss 1.8564257621765137
iteration 0, loss 1.9926656484603882
iteration 100, loss 1.8073515892028809
iteration 200, loss 1.8417983055114746
iteration 300, loss 1.815162181854248
iteration 400, loss 1.8578941822052002
iteration 500, loss 1.9183608293533325
iteration 600, loss 1.8576768636703491
iteration 700, loss 1.8345304727554321
iteration 800, loss 1.8652215003967285
iteration 0, loss 1.9535200595855713
iteration 100, loss 1.9098947048187256
iteration 200, loss 1.9141401052474976
iteration 300, loss 1.8926599025726318
iteration 400, loss 1.9105697870254517
iteration 500, loss 1.8221162557601929
iteration 600, loss 1.8629951477050781
iteration 700, loss 1.8641393184661865
iteration 800, loss 1.8089286088943481
iteration 0, loss 1.7964038848876953
iteration 100, loss 1.8728904724121094
iteration 200, loss 1.864501714706421
iteration 300, loss 1.8775261640548706
iteration 400, loss 1.8971354961395264
iteration 500, loss 1.9171812534332275
iteration 600, loss 1.8391097784042358
iteration 700, loss 1.9064491987228394
iteration 800, loss 1.8518506288528442
iteration 0, loss 1.8237922191619873
iteration 100, loss 1.8864521980285645
iteration 200, loss 1.8914940357208252
iteration 300, loss 1.8085060119628906
iteration 400, loss 1.9474214315414429
iteration 500, loss 1.8647565841674805
iteration 600, loss 1.971503496170044
iteration 700, loss 1.8549087047576904
iteration 800, loss 1.8818466663360596
iteration 0, loss 1.9363579750061035
iteration 100, loss 1.8232415914535522
iteration 200, loss 1.8700392246246338
iteration 300, loss 1.8476216793060303
iteration 400, loss 1.8243955373764038
iteration 500, loss 1.880557894706726
iteration 600, loss 1.875096082687378
iteration 700, loss 1.8450239896774292
iteration 800, loss 1.8840429782867432
iteration 0, loss 1.8630927801132202
iteration 100, loss 1.8128639459609985
iteration 200, loss 1.923262596130371
iteration 300, loss 1.8771053552627563
iteration 400, loss 1.8946894407272339
iteration 500, loss 1.8279309272766113
iteration 600, loss 1.8507139682769775
iteration 700, loss 1.8919788599014282
iteration 800, loss 1.8521689176559448
iteration 0, loss 1.7960888147354126
iteration 100, loss 1.8327593803405762
iteration 200, loss 1.9527918100357056
iteration 300, loss 1.8821637630462646
iteration 400, loss 1.884177327156067
iteration 500, loss 1.8778952360153198
iteration 600, loss 1.8424237966537476
iteration 700, loss 1.814252495765686
iteration 800, loss 1.9254730939865112
iteration 0, loss 1.8149492740631104
iteration 100, loss 1.9146308898925781
iteration 200, loss 1.8749499320983887
iteration 300, loss 1.7947192192077637
iteration 400, loss 1.8498748540878296
iteration 500, loss 1.8949720859527588
iteration 600, loss 1.8791872262954712
iteration 700, loss 1.83591628074646
iteration 800, loss 1.8586726188659668
iteration 0, loss 1.8669604063034058
iteration 100, loss 1.9175435304641724
iteration 200, loss 1.8458515405654907
iteration 300, loss 1.8514498472213745
iteration 400, loss 1.8578479290008545
iteration 500, loss 1.9100056886672974
iteration 600, loss 1.9255094528198242
iteration 700, loss 1.8269833326339722
iteration 800, loss 1.9212981462478638
iteration 0, loss 1.8863041400909424
iteration 100, loss 1.8393011093139648
iteration 200, loss 1.860198736190796
iteration 300, loss 1.8356168270111084
iteration 400, loss 1.9046525955200195
iteration 500, loss 1.8182541131973267
iteration 600, loss 1.9209363460540771
iteration 700, loss 1.8271830081939697
iteration 800, loss 1.8108577728271484
iteration 0, loss 1.8353629112243652
iteration 100, loss 1.8876432180404663
iteration 200, loss 1.8907328844070435
iteration 300, loss 1.8723828792572021
iteration 400, loss 1.7679333686828613
iteration 500, loss 1.8964813947677612
iteration 600, loss 1.9069453477859497
iteration 700, loss 1.852144718170166
iteration 800, loss 1.8796759843826294
iteration 0, loss 1.8332736492156982
iteration 100, loss 1.8528400659561157
iteration 200, loss 1.8698688745498657
iteration 300, loss 1.8342258930206299
iteration 400, loss 1.8730233907699585
iteration 500, loss 1.9308817386627197
iteration 600, loss 1.8616843223571777
iteration 700, loss 1.8589836359024048
iteration 800, loss 1.889519214630127
iteration 0, loss 1.9028804302215576
iteration 100, loss 1.8133437633514404
iteration 200, loss 1.8275737762451172
iteration 300, loss 1.95111882686615
iteration 400, loss 1.8665388822555542
iteration 500, loss 1.9260220527648926
iteration 600, loss 1.913948893547058
iteration 700, loss 1.8131380081176758
iteration 800, loss 1.8612005710601807
iteration 0, loss 1.9367984533309937
iteration 100, loss 1.8733218908309937
iteration 200, loss 1.8672912120819092
iteration 300, loss 1.8571964502334595
iteration 400, loss 1.8349801301956177
iteration 500, loss 1.8338232040405273
iteration 600, loss 1.836979866027832
iteration 700, loss 1.8832539319992065
iteration 800, loss 1.8781144618988037
iteration 0, loss 1.786576271057129
iteration 100, loss 1.825600266456604
iteration 200, loss 1.8143632411956787
iteration 300, loss 1.8429723978042603
iteration 400, loss 1.8173085451126099
iteration 500, loss 1.890629529953003
iteration 600, loss 1.8162510395050049
iteration 700, loss 1.851087212562561
iteration 800, loss 1.8447513580322266
iteration 0, loss 1.8427910804748535
iteration 100, loss 1.8362524509429932
iteration 200, loss 1.8755677938461304
iteration 300, loss 1.7522836923599243
iteration 400, loss 1.916900396347046
iteration 500, loss 1.8229258060455322
iteration 600, loss 1.8353588581085205
iteration 700, loss 1.8806235790252686
iteration 800, loss 1.8550224304199219
iteration 0, loss 1.8976844549179077
iteration 100, loss 1.8384547233581543
iteration 200, loss 1.8399165868759155
iteration 300, loss 1.9147260189056396
iteration 400, loss 1.8355388641357422
iteration 500, loss 1.851384162902832
iteration 600, loss 1.8136000633239746
iteration 700, loss 1.8839943408966064
iteration 800, loss 1.8061209917068481
iteration 0, loss 1.8194735050201416
iteration 100, loss 1.8591195344924927
iteration 200, loss 1.8903342485427856
iteration 300, loss 1.8187235593795776
iteration 400, loss 1.8276616334915161
iteration 500, loss 1.7820543050765991
iteration 600, loss 1.8596808910369873
iteration 700, loss 1.7545430660247803
iteration 800, loss 1.883532166481018
iteration 0, loss 1.893290638923645
iteration 100, loss 1.8125183582305908
iteration 200, loss 1.821342945098877
iteration 300, loss 1.927065372467041
iteration 400, loss 1.8624826669692993
iteration 500, loss 1.816309928894043
iteration 600, loss 1.7910535335540771
iteration 700, loss 1.9119166135787964
iteration 800, loss 1.8380216360092163
iteration 0, loss 1.8849074840545654
iteration 100, loss 1.8072128295898438
iteration 200, loss 1.9517147541046143
iteration 300, loss 1.864980697631836
iteration 400, loss 1.8964195251464844
iteration 500, loss 1.8264234066009521
iteration 600, loss 1.8982717990875244
iteration 700, loss 1.7991218566894531
iteration 800, loss 1.8922091722488403
iteration 0, loss 1.8263978958129883
iteration 100, loss 1.7921143770217896
iteration 200, loss 1.948162317276001
iteration 300, loss 1.8979376554489136
iteration 400, loss 1.874192476272583
iteration 500, loss 1.8773143291473389
iteration 600, loss 1.8305144309997559
iteration 700, loss 1.8224977254867554
iteration 800, loss 1.8831439018249512
iteration 0, loss 1.7907638549804688
iteration 100, loss 1.8299074172973633
iteration 200, loss 1.8548390865325928
iteration 300, loss 1.798471212387085
iteration 400, loss 1.8804703950881958
iteration 500, loss 1.8734568357467651
iteration 600, loss 1.917310118675232
iteration 700, loss 1.9009199142456055
iteration 800, loss 1.8961279392242432
iteration 0, loss 1.8998026847839355
iteration 100, loss 1.849636435508728
iteration 200, loss 1.8041930198669434
iteration 300, loss 1.8320233821868896
iteration 400, loss 1.8538322448730469
iteration 500, loss 1.8298401832580566
iteration 600, loss 1.9251490831375122
iteration 700, loss 1.8881103992462158
iteration 800, loss 1.8604060411453247
iteration 0, loss 1.8660330772399902
iteration 100, loss 1.9780794382095337
iteration 200, loss 1.8552712202072144
iteration 300, loss 1.89547860622406
iteration 400, loss 1.8479037284851074
iteration 500, loss 1.8687776327133179
iteration 600, loss 1.8181487321853638
iteration 700, loss 1.8396121263504028
iteration 800, loss 1.8904222249984741
iteration 0, loss 1.8892072439193726
iteration 100, loss 1.7356460094451904
iteration 200, loss 1.8944652080535889
iteration 300, loss 1.8492512702941895
iteration 400, loss 1.8560911417007446
iteration 500, loss 1.9683549404144287
iteration 600, loss 1.925612449645996
iteration 700, loss 1.7815943956375122
iteration 800, loss 1.851453423500061
iteration 0, loss 1.7965240478515625
iteration 100, loss 1.897310733795166
iteration 200, loss 1.8392399549484253
iteration 300, loss 1.9171223640441895
iteration 400, loss 1.8358458280563354
iteration 500, loss 1.8174126148223877
iteration 600, loss 1.94724440574646
iteration 700, loss 1.8579097986221313
iteration 800, loss 1.8112438917160034
iteration 0, loss 1.8457504510879517
iteration 100, loss 1.8076386451721191
iteration 200, loss 1.8635063171386719
iteration 300, loss 1.9037344455718994
iteration 400, loss 1.865738868713379
iteration 500, loss 1.8387762308120728
iteration 600, loss 1.9498594999313354
iteration 700, loss 1.8086739778518677
iteration 800, loss 1.8500217199325562
iteration 0, loss 1.8266470432281494
iteration 100, loss 1.8453086614608765
iteration 200, loss 1.85783851146698
iteration 300, loss 1.9029120206832886
iteration 400, loss 1.876192569732666
iteration 500, loss 1.939765214920044
iteration 600, loss 1.9224904775619507
iteration 700, loss 1.791656494140625
iteration 800, loss 1.796036720275879
iteration 0, loss 1.859236717224121
iteration 100, loss 1.8827948570251465
iteration 200, loss 1.8602267503738403
iteration 300, loss 1.8650026321411133
iteration 400, loss 1.847095251083374
iteration 500, loss 1.8590096235275269
iteration 600, loss 1.8593298196792603
iteration 700, loss 1.8540675640106201
iteration 800, loss 1.7902308702468872
iteration 0, loss 1.8860913515090942
iteration 100, loss 1.840524435043335
iteration 200, loss 1.877292275428772
iteration 300, loss 1.8317338228225708
iteration 400, loss 1.89409339427948
iteration 500, loss 1.8692667484283447
iteration 600, loss 1.8877732753753662
iteration 700, loss 1.873606562614441
iteration 800, loss 1.925443410873413
iteration 0, loss 1.9136204719543457
iteration 100, loss 1.8011314868927002
iteration 200, loss 1.9317830801010132
iteration 300, loss 1.9315850734710693
iteration 400, loss 1.8767729997634888
iteration 500, loss 1.886906385421753
iteration 600, loss 1.9005298614501953
iteration 700, loss 1.8393051624298096
iteration 800, loss 1.8144065141677856
iteration 0, loss 1.7818118333816528
iteration 100, loss 1.893352746963501
iteration 200, loss 1.8954968452453613
iteration 300, loss 1.873278260231018
iteration 400, loss 1.8449058532714844
iteration 500, loss 1.7988274097442627
iteration 600, loss 1.8852778673171997
iteration 700, loss 1.9298083782196045
iteration 800, loss 1.872598648071289
iteration 0, loss 1.887631893157959
iteration 100, loss 1.8086453676223755
iteration 200, loss 1.8159066438674927
iteration 300, loss 1.8893049955368042
iteration 400, loss 1.8924680948257446
iteration 500, loss 1.8667351007461548
iteration 600, loss 1.7689423561096191
iteration 700, loss 1.921391487121582
iteration 800, loss 1.8583076000213623
iteration 0, loss 1.8539397716522217
iteration 100, loss 1.8382420539855957
iteration 200, loss 1.8605687618255615
iteration 300, loss 1.8361549377441406
iteration 400, loss 1.9345173835754395
iteration 500, loss 1.9326610565185547
iteration 600, loss 1.823219656944275
iteration 700, loss 1.8732266426086426
iteration 800, loss 1.9205113649368286
iteration 0, loss 1.9178285598754883
iteration 100, loss 1.9391816854476929
iteration 200, loss 1.8056623935699463
iteration 300, loss 1.8252395391464233
iteration 400, loss 1.9035670757293701
iteration 500, loss 1.8935067653656006
iteration 600, loss 1.9293787479400635
iteration 700, loss 1.795732855796814
iteration 800, loss 1.794114589691162
iteration 0, loss 1.8062705993652344
iteration 100, loss 1.8881826400756836
iteration 200, loss 1.7695095539093018
iteration 300, loss 1.893600583076477
iteration 400, loss 1.8921056985855103
iteration 500, loss 1.861788272857666
iteration 600, loss 1.8195966482162476
iteration 700, loss 1.8610124588012695
iteration 800, loss 1.871283769607544
iteration 0, loss 1.8311288356781006
iteration 100, loss 1.864134669303894
iteration 200, loss 1.8947081565856934
iteration 300, loss 1.8980214595794678
iteration 400, loss 1.8659051656723022
iteration 500, loss 1.8849695920944214
iteration 600, loss 1.8448489904403687
iteration 700, loss 1.8593436479568481
iteration 800, loss 1.838771104812622
iteration 0, loss 1.8701621294021606
iteration 100, loss 1.8829145431518555
iteration 200, loss 1.9377624988555908
iteration 300, loss 1.8382484912872314
iteration 400, loss 1.8665964603424072
iteration 500, loss 1.9437940120697021
iteration 600, loss 1.8111255168914795
iteration 700, loss 1.84445321559906
iteration 800, loss 1.8088085651397705
iteration 0, loss 1.86855947971344
iteration 100, loss 1.9070570468902588
iteration 200, loss 1.8146971464157104
iteration 300, loss 1.9194093942642212
iteration 400, loss 1.842602014541626
iteration 500, loss 1.8915799856185913
iteration 600, loss 1.9383127689361572
iteration 700, loss 1.903357744216919
iteration 800, loss 1.7619351148605347
iteration 0, loss 1.912787675857544
iteration 100, loss 1.782454490661621
iteration 200, loss 1.8232924938201904
iteration 300, loss 1.9188166856765747
iteration 400, loss 1.8372926712036133
iteration 500, loss 1.852343201637268
iteration 600, loss 1.8789489269256592
iteration 700, loss 1.8436070680618286
iteration 800, loss 1.7966433763504028
iteration 0, loss 1.862870216369629
iteration 100, loss 1.8518822193145752
iteration 200, loss 1.8278741836547852
iteration 300, loss 1.8132861852645874
iteration 400, loss 1.853905439376831
iteration 500, loss 1.8687481880187988
iteration 600, loss 1.827309012413025
iteration 700, loss 1.831791639328003
iteration 800, loss 1.9007816314697266
iteration 0, loss 1.823923110961914
iteration 100, loss 1.8072912693023682
iteration 200, loss 1.7672390937805176
iteration 300, loss 1.8340405225753784
iteration 400, loss 1.8454174995422363
iteration 500, loss 1.8605760335922241
iteration 600, loss 1.8298709392547607
iteration 700, loss 1.901799201965332
iteration 800, loss 1.8747892379760742
fold 2 accuracy: 0.48957142857142855
iteration 0, loss 1.8784865140914917
iteration 100, loss 1.8509870767593384
iteration 200, loss 1.8349156379699707
iteration 300, loss 1.8176013231277466
iteration 400, loss 1.9239777326583862
iteration 500, loss 1.842268943786621
iteration 600, loss 1.7913479804992676
iteration 700, loss 1.8599650859832764
iteration 800, loss 1.8365206718444824
iteration 0, loss 1.8734939098358154
iteration 100, loss 1.8628202676773071
iteration 200, loss 1.8916822671890259
iteration 300, loss 1.8276063203811646
iteration 400, loss 1.7983894348144531
iteration 500, loss 1.8682066202163696
iteration 600, loss 1.8626717329025269
iteration 700, loss 1.8585706949234009
iteration 800, loss 1.9190868139266968
iteration 0, loss 1.8387775421142578
iteration 100, loss 1.9050986766815186
iteration 200, loss 1.8116581439971924
iteration 300, loss 1.9608007669448853
iteration 400, loss 1.8654696941375732
iteration 500, loss 1.8762438297271729
iteration 600, loss 1.8977354764938354
iteration 700, loss 1.822394847869873
iteration 800, loss 1.8180547952651978
iteration 0, loss 1.824623942375183
iteration 100, loss 1.9189976453781128
iteration 200, loss 1.8158174753189087
iteration 300, loss 1.8140190839767456
iteration 400, loss 1.9255316257476807
iteration 500, loss 1.8006216287612915
iteration 600, loss 1.8483943939208984
iteration 700, loss 1.8497350215911865
iteration 800, loss 1.9084501266479492
iteration 0, loss 1.833027958869934
iteration 100, loss 1.8392726182937622
iteration 200, loss 1.8624969720840454
iteration 300, loss 1.8515193462371826
iteration 400, loss 1.8646806478500366
iteration 500, loss 1.8641070127487183
iteration 600, loss 1.8021217584609985
iteration 700, loss 1.8164676427841187
iteration 800, loss 1.8321508169174194
iteration 0, loss 1.7953190803527832
iteration 100, loss 1.8127048015594482
iteration 200, loss 1.8654423952102661
iteration 300, loss 1.8561233282089233
iteration 400, loss 1.8526277542114258
iteration 500, loss 1.9711650609970093
iteration 600, loss 1.8387999534606934
iteration 700, loss 1.825118899345398
iteration 800, loss 1.84770929813385
iteration 0, loss 1.855542778968811
iteration 100, loss 1.8278753757476807
iteration 200, loss 1.890138030052185
iteration 300, loss 1.8830348253250122
iteration 400, loss 1.8148956298828125
iteration 500, loss 1.9151983261108398
iteration 600, loss 1.871275782585144
iteration 700, loss 1.922623872756958
iteration 800, loss 1.816740870475769
iteration 0, loss 1.7890305519104004
iteration 100, loss 1.9668097496032715
iteration 200, loss 1.8613394498825073
iteration 300, loss 1.9030088186264038
iteration 400, loss 1.888671636581421
iteration 500, loss 1.756420612335205
iteration 600, loss 1.8053160905838013
iteration 700, loss 1.838474988937378
iteration 800, loss 1.8594619035720825
iteration 0, loss 1.8504862785339355
iteration 100, loss 1.8262850046157837
iteration 200, loss 1.7627301216125488
iteration 300, loss 1.8959954977035522
iteration 400, loss 1.8182064294815063
iteration 500, loss 1.8825687170028687
iteration 600, loss 1.8594647645950317
iteration 700, loss 1.8488380908966064
iteration 800, loss 1.880473256111145
iteration 0, loss 1.8362489938735962
iteration 100, loss 1.8692612648010254
iteration 200, loss 1.8629581928253174
iteration 300, loss 1.9263675212860107
iteration 400, loss 1.9172312021255493
iteration 500, loss 1.8037172555923462
iteration 600, loss 1.9336553812026978
iteration 700, loss 1.917103886604309
iteration 800, loss 1.8799113035202026
iteration 0, loss 1.7896997928619385
iteration 100, loss 1.801573395729065
iteration 200, loss 1.9176557064056396
iteration 300, loss 1.8646633625030518
iteration 400, loss 1.7878444194793701
iteration 500, loss 1.776158094406128
iteration 600, loss 1.8095346689224243
iteration 700, loss 1.8673650026321411
iteration 800, loss 1.9576722383499146
iteration 0, loss 1.9219059944152832
iteration 100, loss 1.860607624053955
iteration 200, loss 1.7950912714004517
iteration 300, loss 1.9481890201568604
iteration 400, loss 1.821381688117981
iteration 500, loss 1.7857658863067627
iteration 600, loss 1.8544082641601562
iteration 700, loss 1.8815491199493408
iteration 800, loss 1.7725783586502075
iteration 0, loss 1.8626452684402466
iteration 100, loss 1.8250494003295898
iteration 200, loss 1.8949042558670044
iteration 300, loss 1.8391551971435547
iteration 400, loss 1.8848936557769775
iteration 500, loss 1.8474669456481934
iteration 600, loss 1.8197147846221924
iteration 700, loss 1.9043928384780884
iteration 800, loss 1.815380334854126
iteration 0, loss 1.7793028354644775
iteration 100, loss 1.8184894323349
iteration 200, loss 1.7663459777832031
iteration 300, loss 1.9076968431472778
iteration 400, loss 1.8294496536254883
iteration 500, loss 1.9013051986694336
iteration 600, loss 1.867197871208191
iteration 700, loss 1.8928934335708618
iteration 800, loss 1.896382451057434
iteration 0, loss 1.812523365020752
iteration 100, loss 1.8647699356079102
iteration 200, loss 1.8234851360321045
iteration 300, loss 1.8984582424163818
iteration 400, loss 1.9149125814437866
iteration 500, loss 1.881050705909729
iteration 600, loss 1.8051817417144775
iteration 700, loss 1.8334742784500122
iteration 800, loss 1.8392770290374756
iteration 0, loss 1.9319736957550049
iteration 100, loss 2.003399133682251
iteration 200, loss 1.7947574853897095
iteration 300, loss 1.8446125984191895
iteration 400, loss 1.8378857374191284
iteration 500, loss 1.8407224416732788
iteration 600, loss 1.8909506797790527
iteration 700, loss 1.8623101711273193
iteration 800, loss 1.8845595121383667
iteration 0, loss 1.8934264183044434
iteration 100, loss 1.8483198881149292
iteration 200, loss 1.8040395975112915
iteration 300, loss 1.7924960851669312
iteration 400, loss 1.8120481967926025
iteration 500, loss 1.9101815223693848
iteration 600, loss 1.8937641382217407
iteration 700, loss 1.8827195167541504
iteration 800, loss 1.8121416568756104
iteration 0, loss 1.8807272911071777
iteration 100, loss 1.844454050064087
iteration 200, loss 1.8441129922866821
iteration 300, loss 1.8442583084106445
iteration 400, loss 1.8573555946350098
iteration 500, loss 1.837202787399292
iteration 600, loss 1.9107389450073242
iteration 700, loss 1.9000111818313599
iteration 800, loss 1.8720171451568604
iteration 0, loss 1.8254196643829346
iteration 100, loss 1.8405934572219849
iteration 200, loss 1.9691277742385864
iteration 300, loss 1.8606144189834595
iteration 400, loss 1.8614579439163208
iteration 500, loss 1.846818447113037
iteration 600, loss 1.8098958730697632
iteration 700, loss 1.7640513181686401
iteration 800, loss 1.8962953090667725
iteration 0, loss 1.9133363962173462
iteration 100, loss 1.9049112796783447
iteration 200, loss 1.8099066019058228
iteration 300, loss 1.850069284439087
iteration 400, loss 1.9386885166168213
iteration 500, loss 1.8728675842285156
iteration 600, loss 1.8473678827285767
iteration 700, loss 1.834970474243164
iteration 800, loss 1.9034663438796997
iteration 0, loss 1.8003838062286377
iteration 100, loss 1.8221592903137207
iteration 200, loss 1.7783094644546509
iteration 300, loss 1.9291681051254272
iteration 400, loss 1.9290943145751953
iteration 500, loss 1.8180161714553833
iteration 600, loss 1.9139834642410278
iteration 700, loss 1.8788336515426636
iteration 800, loss 1.9138638973236084
iteration 0, loss 1.8869061470031738
iteration 100, loss 1.817924976348877
iteration 200, loss 1.8807315826416016
iteration 300, loss 1.8754762411117554
iteration 400, loss 1.8897939920425415
iteration 500, loss 1.893415093421936
iteration 600, loss 1.8621549606323242
iteration 700, loss 1.8269075155258179
iteration 800, loss 1.9519550800323486
iteration 0, loss 1.8272074460983276
iteration 100, loss 1.8944209814071655
iteration 200, loss 1.8868284225463867
iteration 300, loss 1.8094288110733032
iteration 400, loss 1.8406940698623657
iteration 500, loss 1.8722954988479614
iteration 600, loss 1.8998264074325562
iteration 700, loss 1.8614460229873657
iteration 800, loss 1.8239108324050903
iteration 0, loss 1.8413923978805542
iteration 100, loss 1.7615045309066772
iteration 200, loss 1.8661754131317139
iteration 300, loss 1.8494863510131836
iteration 400, loss 1.866256594657898
iteration 500, loss 1.873336911201477
iteration 600, loss 1.925553560256958
iteration 700, loss 1.8509596586227417
iteration 800, loss 1.8691973686218262
iteration 0, loss 1.8322985172271729
iteration 100, loss 1.7991796731948853
iteration 200, loss 1.8908158540725708
iteration 300, loss 1.8677555322647095
iteration 400, loss 1.8448317050933838
iteration 500, loss 1.798032522201538
iteration 600, loss 1.8724725246429443
iteration 700, loss 1.891278624534607
iteration 800, loss 1.8831372261047363
iteration 0, loss 1.9013123512268066
iteration 100, loss 1.9282381534576416
iteration 200, loss 1.8407453298568726
iteration 300, loss 1.8635421991348267
iteration 400, loss 1.831216812133789
iteration 500, loss 1.7827441692352295
iteration 600, loss 1.8926856517791748
iteration 700, loss 1.8033496141433716
iteration 800, loss 1.8519185781478882
iteration 0, loss 1.833017110824585
iteration 100, loss 1.8936035633087158
iteration 200, loss 1.8190165758132935
iteration 300, loss 1.80027174949646
iteration 400, loss 1.8499919176101685
iteration 500, loss 1.896367073059082
iteration 600, loss 1.88360595703125
iteration 700, loss 1.8278741836547852
iteration 800, loss 1.8588937520980835
iteration 0, loss 1.8849948644638062
iteration 100, loss 1.8414087295532227
iteration 200, loss 1.8468343019485474
iteration 300, loss 1.8823121786117554
iteration 400, loss 1.8564354181289673
iteration 500, loss 1.8875921964645386
iteration 600, loss 1.8279073238372803
iteration 700, loss 1.8361231088638306
iteration 800, loss 1.8601326942443848
iteration 0, loss 1.8048228025436401
iteration 100, loss 1.8188663721084595
iteration 200, loss 1.9338682889938354
iteration 300, loss 1.7649682760238647
iteration 400, loss 1.855393409729004
iteration 500, loss 1.8338754177093506
iteration 600, loss 1.849832534790039
iteration 700, loss 1.8448548316955566
iteration 800, loss 1.8211958408355713
iteration 0, loss 1.8108258247375488
iteration 100, loss 1.9493361711502075
iteration 200, loss 1.81406831741333
iteration 300, loss 1.9026638269424438
iteration 400, loss 1.8035434484481812
iteration 500, loss 1.8684056997299194
iteration 600, loss 1.9041680097579956
iteration 700, loss 1.900506615638733
iteration 800, loss 1.8613786697387695
iteration 0, loss 1.8833091259002686
iteration 100, loss 1.8603347539901733
iteration 200, loss 1.8203461170196533
iteration 300, loss 1.8538258075714111
iteration 400, loss 1.9191572666168213
iteration 500, loss 1.9799052476882935
iteration 600, loss 1.933584451675415
iteration 700, loss 1.8743970394134521
iteration 800, loss 1.8293051719665527
iteration 0, loss 1.8107887506484985
iteration 100, loss 1.7894070148468018
iteration 200, loss 1.8188183307647705
iteration 300, loss 1.8795583248138428
iteration 400, loss 1.9297462701797485
iteration 500, loss 1.8364402055740356
iteration 600, loss 1.8801950216293335
iteration 700, loss 1.8421087265014648
iteration 800, loss 1.8282042741775513
iteration 0, loss 1.8457491397857666
iteration 100, loss 1.9285743236541748
iteration 200, loss 1.861528754234314
iteration 300, loss 1.8517351150512695
iteration 400, loss 1.8786320686340332
iteration 500, loss 1.9241466522216797
iteration 600, loss 1.9395802021026611
iteration 700, loss 1.7978615760803223
iteration 800, loss 1.8298076391220093
iteration 0, loss 1.8276233673095703
iteration 100, loss 1.8098866939544678
iteration 200, loss 1.8382878303527832
iteration 300, loss 1.8800348043441772
iteration 400, loss 1.8498295545578003
iteration 500, loss 1.8779715299606323
iteration 600, loss 1.8435908555984497
iteration 700, loss 1.8686797618865967
iteration 800, loss 1.860093116760254
iteration 0, loss 1.812571406364441
iteration 100, loss 1.7846707105636597
iteration 200, loss 1.881256103515625
iteration 300, loss 1.8644185066223145
iteration 400, loss 1.8151694536209106
iteration 500, loss 1.87338125705719
iteration 600, loss 1.870972752571106
iteration 700, loss 1.870105504989624
iteration 800, loss 1.8272624015808105
iteration 0, loss 1.8388042449951172
iteration 100, loss 1.7717041969299316
iteration 200, loss 1.927423357963562
iteration 300, loss 1.8030524253845215
iteration 400, loss 1.8213882446289062
iteration 500, loss 1.8906859159469604
iteration 600, loss 1.806166410446167
iteration 700, loss 1.8322428464889526
iteration 800, loss 1.91024649143219
iteration 0, loss 1.8715887069702148
iteration 100, loss 1.8491101264953613
iteration 200, loss 1.8825106620788574
iteration 300, loss 1.7671244144439697
iteration 400, loss 1.8676584959030151
iteration 500, loss 1.8358380794525146
iteration 600, loss 1.830909013748169
iteration 700, loss 1.8513813018798828
iteration 800, loss 1.889635682106018
iteration 0, loss 1.845807433128357
iteration 100, loss 1.869768500328064
iteration 200, loss 1.8745476007461548
iteration 300, loss 1.7939039468765259
iteration 400, loss 1.8298327922821045
iteration 500, loss 1.8772177696228027
iteration 600, loss 1.9318716526031494
iteration 700, loss 1.8398752212524414
iteration 800, loss 1.8714202642440796
iteration 0, loss 1.8866077661514282
iteration 100, loss 1.8631707429885864
iteration 200, loss 1.837180256843567
iteration 300, loss 1.851426362991333
iteration 400, loss 1.906229853630066
iteration 500, loss 1.8695517778396606
iteration 600, loss 1.765699028968811
iteration 700, loss 1.8219358921051025
iteration 800, loss 1.9258053302764893
iteration 0, loss 1.8819835186004639
iteration 100, loss 1.8944309949874878
iteration 200, loss 1.850717306137085
iteration 300, loss 1.8288660049438477
iteration 400, loss 1.9322869777679443
iteration 500, loss 1.799824833869934
iteration 600, loss 1.8519160747528076
iteration 700, loss 1.8389102220535278
iteration 800, loss 1.8382070064544678
iteration 0, loss 1.8476226329803467
iteration 100, loss 1.847764253616333
iteration 200, loss 1.911972999572754
iteration 300, loss 1.830930233001709
iteration 400, loss 1.9280463457107544
iteration 500, loss 1.8661024570465088
iteration 600, loss 1.8146336078643799
iteration 700, loss 1.9355500936508179
iteration 800, loss 1.9054508209228516
iteration 0, loss 1.9049978256225586
iteration 100, loss 1.933794379234314
iteration 200, loss 1.8486535549163818
iteration 300, loss 1.7889721393585205
iteration 400, loss 1.8694461584091187
iteration 500, loss 1.8676387071609497
iteration 600, loss 1.7805981636047363
iteration 700, loss 1.8909295797348022
iteration 800, loss 1.7900934219360352
iteration 0, loss 1.82284414768219
iteration 100, loss 1.8724825382232666
iteration 200, loss 1.8340963125228882
iteration 300, loss 1.8681391477584839
iteration 400, loss 1.8425709009170532
iteration 500, loss 1.9704225063323975
iteration 600, loss 1.885420560836792
iteration 700, loss 1.9197665452957153
iteration 800, loss 1.8016605377197266
iteration 0, loss 1.924802541732788
iteration 100, loss 1.831039547920227
iteration 200, loss 1.8965725898742676
iteration 300, loss 1.89115571975708
iteration 400, loss 1.8237210512161255
iteration 500, loss 1.8461990356445312
iteration 600, loss 1.8531142473220825
iteration 700, loss 1.8937299251556396
iteration 800, loss 1.9030818939208984
iteration 0, loss 1.7901860475540161
iteration 100, loss 1.813208818435669
iteration 200, loss 1.80390465259552
iteration 300, loss 1.772286295890808
iteration 400, loss 1.8270905017852783
iteration 500, loss 1.8817100524902344
iteration 600, loss 1.9527490139007568
iteration 700, loss 1.8658429384231567
iteration 800, loss 1.835350513458252
iteration 0, loss 1.8217929601669312
iteration 100, loss 1.840459942817688
iteration 200, loss 1.7762682437896729
iteration 300, loss 1.8502498865127563
iteration 400, loss 1.8370709419250488
iteration 500, loss 1.8266875743865967
iteration 600, loss 1.8125871419906616
iteration 700, loss 1.9000163078308105
iteration 800, loss 1.888513445854187
iteration 0, loss 1.8082181215286255
iteration 100, loss 1.8109947443008423
iteration 200, loss 1.8692469596862793
iteration 300, loss 1.8530902862548828
iteration 400, loss 1.84341561794281
iteration 500, loss 1.8750600814819336
iteration 600, loss 1.8838982582092285
iteration 700, loss 1.8203370571136475
iteration 800, loss 1.7999248504638672
iteration 0, loss 1.8800041675567627
iteration 100, loss 1.8932439088821411
iteration 200, loss 1.9219386577606201
iteration 300, loss 1.8743901252746582
iteration 400, loss 1.7708708047866821
iteration 500, loss 1.9015443325042725
iteration 600, loss 1.7636314630508423
iteration 700, loss 1.8864459991455078
iteration 800, loss 1.8124041557312012
iteration 0, loss 1.9261183738708496
iteration 100, loss 1.8119593858718872
iteration 200, loss 1.911704421043396
iteration 300, loss 1.8263245820999146
iteration 400, loss 1.83915114402771
iteration 500, loss 1.9121313095092773
iteration 600, loss 1.7964500188827515
iteration 700, loss 1.8624190092086792
iteration 800, loss 1.8182125091552734
iteration 0, loss 1.8706238269805908
iteration 100, loss 1.8446342945098877
iteration 200, loss 1.9409782886505127
iteration 300, loss 1.9112253189086914
iteration 400, loss 1.9287779331207275
iteration 500, loss 1.958864688873291
iteration 600, loss 1.7980977296829224
iteration 700, loss 1.9079182147979736
iteration 800, loss 1.8882482051849365
fold 3 accuracy: 0.49907142857142855
iteration 0, loss 1.8301293849945068
iteration 100, loss 1.7698643207550049
iteration 200, loss 1.8314520120620728
iteration 300, loss 1.8679656982421875
iteration 400, loss 1.8180118799209595
iteration 500, loss 1.7809827327728271
iteration 600, loss 1.9563407897949219
iteration 700, loss 1.8204752206802368
iteration 800, loss 1.8029862642288208
iteration 0, loss 1.8599478006362915
iteration 100, loss 1.8680061101913452
iteration 200, loss 1.8697971105575562
iteration 300, loss 1.849665880203247
iteration 400, loss 1.8045989274978638
iteration 500, loss 1.8725032806396484
iteration 600, loss 1.9295361042022705
iteration 700, loss 1.841848611831665
iteration 800, loss 1.8518716096878052
iteration 0, loss 1.8249421119689941
iteration 100, loss 1.8990528583526611
iteration 200, loss 1.8962647914886475
iteration 300, loss 1.849950909614563
iteration 400, loss 1.9007512331008911
iteration 500, loss 1.907360315322876
iteration 600, loss 1.8828758001327515
iteration 700, loss 1.9127886295318604
iteration 800, loss 1.8086692094802856
iteration 0, loss 1.8687719106674194
iteration 100, loss 1.861852765083313
iteration 200, loss 1.8885184526443481
iteration 300, loss 1.8278779983520508
iteration 400, loss 1.8319247961044312
iteration 500, loss 1.8232330083847046
iteration 600, loss 1.9390838146209717
iteration 700, loss 1.808700680732727
iteration 800, loss 1.7900351285934448
iteration 0, loss 1.9040480852127075
iteration 100, loss 1.829606294631958
iteration 200, loss 1.8275673389434814
iteration 300, loss 1.8660186529159546
iteration 400, loss 1.8286691904067993
iteration 500, loss 1.8291113376617432
iteration 600, loss 1.8273980617523193
iteration 700, loss 1.822003960609436
iteration 800, loss 1.9142236709594727
iteration 0, loss 1.849463701248169
iteration 100, loss 1.799422025680542
iteration 200, loss 1.875849723815918
iteration 300, loss 1.8243029117584229
iteration 400, loss 1.7943084239959717
iteration 500, loss 1.8869494199752808
iteration 600, loss 1.9310088157653809
iteration 700, loss 1.8693863153457642
iteration 800, loss 1.8708889484405518
iteration 0, loss 1.8177218437194824
iteration 100, loss 1.8149603605270386
iteration 200, loss 1.888010859489441
iteration 300, loss 1.8688660860061646
iteration 400, loss 1.8546093702316284
iteration 500, loss 1.9896764755249023
iteration 600, loss 1.783620834350586
iteration 700, loss 1.8832844495773315
iteration 800, loss 1.772444725036621
iteration 0, loss 1.7807362079620361
iteration 100, loss 1.938633680343628
iteration 200, loss 1.7880373001098633
iteration 300, loss 1.7908623218536377
iteration 400, loss 1.9025952816009521
iteration 500, loss 1.912121295928955
iteration 600, loss 1.8571603298187256
iteration 700, loss 1.9535341262817383
iteration 800, loss 1.8849598169326782
iteration 0, loss 1.931726098060608
iteration 100, loss 1.9678351879119873
iteration 200, loss 1.8356447219848633
iteration 300, loss 1.8219847679138184
iteration 400, loss 1.8397365808486938
iteration 500, loss 1.8640731573104858
iteration 600, loss 1.863000750541687
iteration 700, loss 1.8631205558776855
iteration 800, loss 1.9294153451919556
iteration 0, loss 1.814775824546814
iteration 100, loss 1.8226839303970337
iteration 200, loss 1.81283700466156
iteration 300, loss 1.8157998323440552
iteration 400, loss 1.858252763748169
iteration 500, loss 1.8331975936889648
iteration 600, loss 1.883322834968567
iteration 700, loss 1.8762357234954834
iteration 800, loss 1.8745225667953491
iteration 0, loss 1.8664906024932861
iteration 100, loss 1.8346134424209595
iteration 200, loss 1.863318920135498
iteration 300, loss 1.7638558149337769
iteration 400, loss 1.8782057762145996
iteration 500, loss 1.8016197681427002
iteration 600, loss 1.8906208276748657
iteration 700, loss 1.855479121208191
iteration 800, loss 1.8406556844711304
iteration 0, loss 1.8723560571670532
iteration 100, loss 1.7991995811462402
iteration 200, loss 1.9034632444381714
iteration 300, loss 1.8419219255447388
iteration 400, loss 1.8895984888076782
iteration 500, loss 1.9420901536941528
iteration 600, loss 1.876704454421997
iteration 700, loss 1.8967519998550415
iteration 800, loss 1.8939450979232788
iteration 0, loss 1.8482654094696045
iteration 100, loss 1.8748759031295776
iteration 200, loss 1.8562339544296265
iteration 300, loss 1.8872544765472412
iteration 400, loss 1.8364542722702026
iteration 500, loss 1.887025237083435
iteration 600, loss 1.752294898033142
iteration 700, loss 1.8853375911712646
iteration 800, loss 1.8701040744781494
iteration 0, loss 1.8921735286712646
iteration 100, loss 1.8196278810501099
iteration 200, loss 1.766608715057373
iteration 300, loss 1.858505129814148
iteration 400, loss 1.7864835262298584
iteration 500, loss 1.8531389236450195
iteration 600, loss 1.8327614068984985
iteration 700, loss 1.889233112335205
iteration 800, loss 1.9300339221954346
iteration 0, loss 1.8495092391967773
iteration 100, loss 1.8976486921310425
iteration 200, loss 1.8857871294021606
iteration 300, loss 1.8515719175338745
iteration 400, loss 1.931555151939392
iteration 500, loss 1.9347045421600342
iteration 600, loss 1.7968693971633911
iteration 700, loss 1.8677209615707397
iteration 800, loss 1.932254433631897
iteration 0, loss 1.8270679712295532
iteration 100, loss 1.7891004085540771
iteration 200, loss 1.8013347387313843
iteration 300, loss 1.9038441181182861
iteration 400, loss 1.8663179874420166
iteration 500, loss 1.837302327156067
iteration 600, loss 1.8919600248336792
iteration 700, loss 1.8449825048446655
iteration 800, loss 1.834568977355957
iteration 0, loss 1.8436346054077148
iteration 100, loss 1.9107869863510132
iteration 200, loss 1.7659013271331787
iteration 300, loss 1.8609449863433838
iteration 400, loss 1.8974430561065674
iteration 500, loss 1.8196396827697754
iteration 600, loss 1.8700772523880005
iteration 700, loss 1.7878434658050537
iteration 800, loss 1.825576663017273
iteration 0, loss 1.894827127456665
iteration 100, loss 1.8173161745071411
iteration 200, loss 1.9006707668304443
iteration 300, loss 1.8756457567214966
iteration 400, loss 1.8277369737625122
iteration 500, loss 1.7623507976531982
iteration 600, loss 1.8479970693588257
iteration 700, loss 1.8754823207855225
iteration 800, loss 1.7925691604614258
iteration 0, loss 1.8492251634597778
iteration 100, loss 1.8245277404785156
iteration 200, loss 1.8491644859313965
iteration 300, loss 1.7390530109405518
iteration 400, loss 1.8418523073196411
iteration 500, loss 1.7837611436843872
iteration 600, loss 1.8764657974243164
iteration 700, loss 1.8281898498535156
iteration 800, loss 1.9532233476638794
iteration 0, loss 1.8565391302108765
iteration 100, loss 1.754752278327942
iteration 200, loss 1.8314929008483887
iteration 300, loss 1.8775888681411743
iteration 400, loss 1.9841697216033936
iteration 500, loss 1.8855308294296265
iteration 600, loss 1.935217022895813
iteration 700, loss 1.7923600673675537
iteration 800, loss 1.8181997537612915
iteration 0, loss 1.7961217164993286
iteration 100, loss 1.9358128309249878
iteration 200, loss 1.8549857139587402
iteration 300, loss 1.9184397459030151
iteration 400, loss 1.9027855396270752
iteration 500, loss 1.903334140777588
iteration 600, loss 1.847139835357666
iteration 700, loss 1.8821076154708862
iteration 800, loss 1.855747938156128
iteration 0, loss 1.7427012920379639
iteration 100, loss 1.9255263805389404
iteration 200, loss 1.8285324573516846
iteration 300, loss 1.8791157007217407
iteration 400, loss 1.8930587768554688
iteration 500, loss 1.8557525873184204
iteration 600, loss 1.923730731010437
iteration 700, loss 1.8674447536468506
iteration 800, loss 1.8517677783966064
iteration 0, loss 1.8496650457382202
iteration 100, loss 1.8356002569198608
iteration 200, loss 1.9241652488708496
iteration 300, loss 1.8287181854248047
iteration 400, loss 1.8201850652694702
iteration 500, loss 1.9160438776016235
iteration 600, loss 1.8274661302566528
iteration 700, loss 1.8601999282836914
iteration 800, loss 1.8399852514266968
iteration 0, loss 1.8614968061447144
iteration 100, loss 1.893989086151123
iteration 200, loss 1.8074959516525269
iteration 300, loss 1.9059027433395386
iteration 400, loss 1.8834713697433472
iteration 500, loss 1.8671578168869019
iteration 600, loss 1.8734893798828125
iteration 700, loss 1.946725845336914
iteration 800, loss 1.805191993713379
iteration 0, loss 1.8762913942337036
iteration 100, loss 1.9197611808776855
iteration 200, loss 1.830329418182373
iteration 300, loss 1.874859094619751
iteration 400, loss 1.8630744218826294
iteration 500, loss 1.948103904724121
iteration 600, loss 1.8715476989746094
iteration 700, loss 1.8346143960952759
iteration 800, loss 1.8270269632339478
iteration 0, loss 1.9072041511535645
iteration 100, loss 1.7660675048828125
iteration 200, loss 1.8814682960510254
iteration 300, loss 1.7577909231185913
iteration 400, loss 1.8753759860992432
iteration 500, loss 1.8288614749908447
iteration 600, loss 1.7974988222122192
iteration 700, loss 1.86578369140625
iteration 800, loss 1.8207753896713257
iteration 0, loss 1.8538326025009155
iteration 100, loss 1.8750759363174438
iteration 200, loss 1.8397303819656372
iteration 300, loss 1.932356834411621
iteration 400, loss 1.816378116607666
iteration 500, loss 1.9571137428283691
iteration 600, loss 1.7966891527175903
iteration 700, loss 1.8538172245025635
iteration 800, loss 1.8287811279296875
iteration 0, loss 1.8484406471252441
iteration 100, loss 2.000361919403076
iteration 200, loss 1.888209581375122
iteration 300, loss 1.8510923385620117
iteration 400, loss 1.8550318479537964
iteration 500, loss 1.8440423011779785
iteration 600, loss 1.858078122138977
iteration 700, loss 1.8788692951202393
iteration 800, loss 1.8040246963500977
iteration 0, loss 1.8600473403930664
iteration 100, loss 1.8398292064666748
iteration 200, loss 1.8408385515213013
iteration 300, loss 1.855088710784912
iteration 400, loss 1.860412836074829
iteration 500, loss 1.798494577407837
iteration 600, loss 1.9367632865905762
iteration 700, loss 1.9416261911392212
iteration 800, loss 1.882112979888916
iteration 0, loss 1.8353712558746338
iteration 100, loss 1.8903690576553345
iteration 200, loss 1.8162429332733154
iteration 300, loss 1.9351348876953125
iteration 400, loss 1.8585431575775146
iteration 500, loss 1.8377280235290527
iteration 600, loss 1.8685190677642822
iteration 700, loss 1.7986990213394165
iteration 800, loss 1.8647900819778442
iteration 0, loss 1.8560699224472046
iteration 100, loss 1.86203932762146
iteration 200, loss 1.7347278594970703
iteration 300, loss 1.8461757898330688
iteration 400, loss 1.8282017707824707
iteration 500, loss 1.8523814678192139
iteration 600, loss 1.8644007444381714
iteration 700, loss 1.9134739637374878
iteration 800, loss 1.770671010017395
iteration 0, loss 1.8664586544036865
iteration 100, loss 1.8700027465820312
iteration 200, loss 1.9622035026550293
iteration 300, loss 1.7956316471099854
iteration 400, loss 1.870681643486023
iteration 500, loss 1.8417840003967285
iteration 600, loss 1.909085988998413
iteration 700, loss 1.8439339399337769
iteration 800, loss 1.8344740867614746
iteration 0, loss 1.8378759622573853
iteration 100, loss 1.8104907274246216
iteration 200, loss 1.8363831043243408
iteration 300, loss 1.9058889150619507
iteration 400, loss 1.8758586645126343
iteration 500, loss 1.910473108291626
iteration 600, loss 1.8559232950210571
iteration 700, loss 1.9166513681411743
iteration 800, loss 1.8222404718399048
iteration 0, loss 1.890668272972107
iteration 100, loss 1.8624640703201294
iteration 200, loss 1.7391929626464844
iteration 300, loss 1.8767646551132202
iteration 400, loss 1.829184651374817
iteration 500, loss 1.8237425088882446
iteration 600, loss 1.7851653099060059
iteration 700, loss 1.8748054504394531
iteration 800, loss 1.8511996269226074
iteration 0, loss 1.8119579553604126
iteration 100, loss 1.8392282724380493
iteration 200, loss 1.8055968284606934
iteration 300, loss 1.8358144760131836
iteration 400, loss 1.8277721405029297
iteration 500, loss 1.836262822151184
iteration 600, loss 1.8563401699066162
iteration 700, loss 1.85530424118042
iteration 800, loss 1.8402752876281738
iteration 0, loss 1.8474289178848267
iteration 100, loss 1.936733365058899
iteration 200, loss 1.8326464891433716
iteration 300, loss 1.86011803150177
iteration 400, loss 1.8663458824157715
iteration 500, loss 1.8928595781326294
iteration 600, loss 1.7991163730621338
iteration 700, loss 1.8816664218902588
iteration 800, loss 1.8783800601959229
iteration 0, loss 1.883485198020935
iteration 100, loss 1.73960542678833
iteration 200, loss 1.8385695219039917
iteration 300, loss 1.9008322954177856
iteration 400, loss 1.8656980991363525
iteration 500, loss 1.8494699001312256
iteration 600, loss 1.8772737979888916
iteration 700, loss 1.8965870141983032
iteration 800, loss 1.8688982725143433
iteration 0, loss 1.848366141319275
iteration 100, loss 1.8239376544952393
iteration 200, loss 1.8145232200622559
iteration 300, loss 1.8043537139892578
iteration 400, loss 1.8609851598739624
iteration 500, loss 1.8568124771118164
iteration 600, loss 1.8662868738174438
iteration 700, loss 1.8469699621200562
iteration 800, loss 1.8147156238555908
iteration 0, loss 1.8442211151123047
iteration 100, loss 1.8404593467712402
iteration 200, loss 1.8988423347473145
iteration 300, loss 1.8318572044372559
iteration 400, loss 1.9215055704116821
iteration 500, loss 1.9236916303634644
iteration 600, loss 1.8285421133041382
iteration 700, loss 1.8477905988693237
iteration 800, loss 1.8272175788879395
iteration 0, loss 1.8010060787200928
iteration 100, loss 1.8645657300949097
iteration 200, loss 1.8454192876815796
iteration 300, loss 1.9203689098358154
iteration 400, loss 1.925758719444275
iteration 500, loss 1.859052062034607
iteration 600, loss 1.851003646850586
iteration 700, loss 1.8766690492630005
iteration 800, loss 1.8905224800109863
iteration 0, loss 1.8427200317382812
iteration 100, loss 1.7816671133041382
iteration 200, loss 1.9364427328109741
iteration 300, loss 1.9567980766296387
iteration 400, loss 1.8787716627120972
iteration 500, loss 1.8513224124908447
iteration 600, loss 1.7890452146530151
iteration 700, loss 1.7506414651870728
iteration 800, loss 1.8763295412063599
iteration 0, loss 1.8564269542694092
iteration 100, loss 1.8456569910049438
iteration 200, loss 1.8407104015350342
iteration 300, loss 1.8208695650100708
iteration 400, loss 1.925672173500061
iteration 500, loss 1.7892792224884033
iteration 600, loss 1.8621395826339722
iteration 700, loss 1.8382669687271118
iteration 800, loss 1.8618693351745605
iteration 0, loss 1.808262586593628
iteration 100, loss 1.859291672706604
iteration 200, loss 1.8794598579406738
iteration 300, loss 1.890626072883606
iteration 400, loss 1.8503068685531616
iteration 500, loss 1.882568597793579
iteration 600, loss 1.7969133853912354
iteration 700, loss 1.8010673522949219
iteration 800, loss 1.8903000354766846
iteration 0, loss 1.830073356628418
iteration 100, loss 1.8855453729629517
iteration 200, loss 1.8673521280288696
iteration 300, loss 1.8794822692871094
iteration 400, loss 1.7898410558700562
iteration 500, loss 1.8031030893325806
iteration 600, loss 1.8523597717285156
iteration 700, loss 1.7446709871292114
iteration 800, loss 1.7765471935272217
iteration 0, loss 1.8984462022781372
iteration 100, loss 1.8542053699493408
iteration 200, loss 1.9032981395721436
iteration 300, loss 1.8989520072937012
iteration 400, loss 1.8574659824371338
iteration 500, loss 1.7965246438980103
iteration 600, loss 1.8932063579559326
iteration 700, loss 1.8063206672668457
iteration 800, loss 1.873323917388916
iteration 0, loss 1.934788703918457
iteration 100, loss 1.9039400815963745
iteration 200, loss 1.9006710052490234
iteration 300, loss 1.7909635305404663
iteration 400, loss 1.7833163738250732
iteration 500, loss 1.8243328332901
iteration 600, loss 1.857519507408142
iteration 700, loss 1.9044042825698853
iteration 800, loss 1.8931248188018799
iteration 0, loss 1.848220705986023
iteration 100, loss 1.7685294151306152
iteration 200, loss 1.7812517881393433
iteration 300, loss 1.8359034061431885
iteration 400, loss 1.8579295873641968
iteration 500, loss 1.8555333614349365
iteration 600, loss 1.8857015371322632
iteration 700, loss 1.8595551252365112
iteration 800, loss 1.8735774755477905
iteration 0, loss 1.8028104305267334
iteration 100, loss 1.7648870944976807
iteration 200, loss 1.845650315284729
iteration 300, loss 1.8515039682388306
iteration 400, loss 1.7659063339233398
iteration 500, loss 1.7434051036834717
iteration 600, loss 1.8622304201126099
iteration 700, loss 1.7951685190200806
iteration 800, loss 1.8541312217712402
iteration 0, loss 1.8566060066223145
iteration 100, loss 1.8142216205596924
iteration 200, loss 1.8603427410125732
iteration 300, loss 1.8746614456176758
iteration 400, loss 1.8988661766052246
iteration 500, loss 1.8522921800613403
iteration 600, loss 1.7798811197280884
iteration 700, loss 1.8409967422485352
iteration 800, loss 1.8087453842163086
iteration 0, loss 1.8855397701263428
iteration 100, loss 1.9182064533233643
iteration 200, loss 1.8578375577926636
iteration 300, loss 1.920528769493103
iteration 400, loss 1.860790491104126
iteration 500, loss 1.7721377611160278
iteration 600, loss 1.9589085578918457
iteration 700, loss 1.7982720136642456
iteration 800, loss 1.9207205772399902
fold 4 accuracy: 0.48964285714285716
[2024-02-29 01:16:03,179] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-29 01:16:03,180] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         458     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       560 MACs
fwd flops per GPU:                                                      1.12 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.12 K  
fwd latency:                                                            343.08 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.26 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '458'}
    MACs        - {'KronLayer': '560 MACs'}
    fwd latency - {'KronLayer': '343.08 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  458 = 100% Params, 560 MACs = 100% MACs, 343.08 us = 100% latency, 3.26 MFLOPS
  (kronlinear): KronLinear(458 = 100% Params, 560 MACs = 100% MACs, 254.39 us = 74.15% latency, 4.4 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 26.94 us = 7.85% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-29 01:16:03,182] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.12 K 458
iteration 0, loss 2.300248861312866
iteration 100, loss 2.265800952911377
iteration 200, loss 2.120384931564331
iteration 300, loss 1.9975857734680176
iteration 400, loss 1.979989767074585
iteration 500, loss 1.9023423194885254
iteration 600, loss 1.979338526725769
iteration 700, loss 1.9271655082702637
iteration 800, loss 1.9099076986312866
iteration 0, loss 1.8863271474838257
iteration 100, loss 1.86784827709198
iteration 200, loss 1.902691125869751
iteration 300, loss 1.8450440168380737
iteration 400, loss 1.8654013872146606
iteration 500, loss 1.8626190423965454
iteration 600, loss 1.9088724851608276
iteration 700, loss 1.8605866432189941
iteration 800, loss 1.8191273212432861
iteration 0, loss 1.8936092853546143
iteration 100, loss 1.8473129272460938
iteration 200, loss 1.8125665187835693
iteration 300, loss 1.8355584144592285
iteration 400, loss 1.8932887315750122
iteration 500, loss 1.8107590675354004
iteration 600, loss 1.8234835863113403
iteration 700, loss 1.8559706211090088
iteration 800, loss 1.8241310119628906
iteration 0, loss 1.783470869064331
iteration 100, loss 1.8511754274368286
iteration 200, loss 1.8669813871383667
iteration 300, loss 1.8190275430679321
iteration 400, loss 1.8176069259643555
iteration 500, loss 1.8376270532608032
iteration 600, loss 1.821370005607605
iteration 700, loss 1.7559481859207153
iteration 800, loss 1.7931430339813232
iteration 0, loss 1.730526089668274
iteration 100, loss 1.8262345790863037
iteration 200, loss 1.7620044946670532
iteration 300, loss 1.773796796798706
iteration 400, loss 1.8047782182693481
iteration 500, loss 1.7568211555480957
iteration 600, loss 1.868975043296814
iteration 700, loss 1.8533872365951538
iteration 800, loss 1.7291032075881958
iteration 0, loss 1.823769211769104
iteration 100, loss 1.758614182472229
iteration 200, loss 1.8352515697479248
iteration 300, loss 1.7310731410980225
iteration 400, loss 1.7983448505401611
iteration 500, loss 1.7939444780349731
iteration 600, loss 1.8584126234054565
iteration 700, loss 1.8035415410995483
iteration 800, loss 1.7543264627456665
iteration 0, loss 1.8005231618881226
iteration 100, loss 1.8634356260299683
iteration 200, loss 1.8733999729156494
iteration 300, loss 1.8394638299942017
iteration 400, loss 1.8037078380584717
iteration 500, loss 1.771827220916748
iteration 600, loss 1.8154163360595703
iteration 700, loss 1.7635715007781982
iteration 800, loss 1.7844669818878174
iteration 0, loss 1.757039189338684
iteration 100, loss 1.7771272659301758
iteration 200, loss 1.8517775535583496
iteration 300, loss 1.756888508796692
iteration 400, loss 1.8149694204330444
iteration 500, loss 1.7912039756774902
iteration 600, loss 1.786717176437378
iteration 700, loss 1.7316980361938477
iteration 800, loss 1.7584095001220703
iteration 0, loss 1.8059526681900024
iteration 100, loss 1.794311285018921
iteration 200, loss 1.7432621717453003
iteration 300, loss 1.7505686283111572
iteration 400, loss 1.7133030891418457
iteration 500, loss 1.7437107563018799
iteration 600, loss 1.7254879474639893
iteration 700, loss 1.7898619174957275
iteration 800, loss 1.7802884578704834
iteration 0, loss 1.7515430450439453
iteration 100, loss 1.8033802509307861
iteration 200, loss 1.7859246730804443
iteration 300, loss 1.8080934286117554
iteration 400, loss 1.7513455152511597
iteration 500, loss 1.771406650543213
iteration 600, loss 1.8062149286270142
iteration 700, loss 1.7408450841903687
iteration 800, loss 1.7568080425262451
iteration 0, loss 1.79256272315979
iteration 100, loss 1.738142967224121
iteration 200, loss 1.8285690546035767
iteration 300, loss 1.7101401090621948
iteration 400, loss 1.7270095348358154
iteration 500, loss 1.7500911951065063
iteration 600, loss 1.7797106504440308
iteration 700, loss 1.792525053024292
iteration 800, loss 1.780694603919983
iteration 0, loss 1.778864860534668
iteration 100, loss 1.7713359594345093
iteration 200, loss 1.6890130043029785
iteration 300, loss 1.7851694822311401
iteration 400, loss 1.7770235538482666
iteration 500, loss 1.750151515007019
iteration 600, loss 1.80402410030365
iteration 700, loss 1.7896554470062256
iteration 800, loss 1.7625632286071777
iteration 0, loss 1.6967213153839111
iteration 100, loss 1.7261806726455688
iteration 200, loss 1.7106311321258545
iteration 300, loss 1.746434211730957
iteration 400, loss 1.7716562747955322
iteration 500, loss 1.765398383140564
iteration 600, loss 1.7992570400238037
iteration 700, loss 1.782419204711914
iteration 800, loss 1.8184643983840942
iteration 0, loss 1.6973903179168701
iteration 100, loss 1.7813421487808228
iteration 200, loss 1.8189642429351807
iteration 300, loss 1.6966321468353271
iteration 400, loss 1.8048429489135742
iteration 500, loss 1.719669222831726
iteration 600, loss 1.745203971862793
iteration 700, loss 1.742244005203247
iteration 800, loss 1.8217267990112305
iteration 0, loss 1.7542589902877808
iteration 100, loss 1.6953562498092651
iteration 200, loss 1.6959412097930908
iteration 300, loss 1.754709243774414
iteration 400, loss 1.6960086822509766
iteration 500, loss 1.7318629026412964
iteration 600, loss 1.7291309833526611
iteration 700, loss 1.7256669998168945
iteration 800, loss 1.7423264980316162
iteration 0, loss 1.7339128255844116
iteration 100, loss 1.8422147035598755
iteration 200, loss 1.6982871294021606
iteration 300, loss 1.7475947141647339
iteration 400, loss 1.7317310571670532
iteration 500, loss 1.7187933921813965
iteration 600, loss 1.7302610874176025
iteration 700, loss 1.765635371208191
iteration 800, loss 1.6868717670440674
iteration 0, loss 1.7513983249664307
iteration 100, loss 1.6903527975082397
iteration 200, loss 1.766387939453125
iteration 300, loss 1.7692641019821167
iteration 400, loss 1.7270034551620483
iteration 500, loss 1.7495218515396118
iteration 600, loss 1.7794984579086304
iteration 700, loss 1.6904710531234741
iteration 800, loss 1.7559564113616943
iteration 0, loss 1.744213581085205
iteration 100, loss 1.8164284229278564
iteration 200, loss 1.658150315284729
iteration 300, loss 1.7740719318389893
iteration 400, loss 1.7427006959915161
iteration 500, loss 1.7626806497573853
iteration 600, loss 1.7295805215835571
iteration 700, loss 1.7403309345245361
iteration 800, loss 1.7344789505004883
iteration 0, loss 1.7420974969863892
iteration 100, loss 1.8049637079238892
iteration 200, loss 1.7371364831924438
iteration 300, loss 1.6888433694839478
iteration 400, loss 1.801246166229248
iteration 500, loss 1.792696237564087
iteration 600, loss 1.771025538444519
iteration 700, loss 1.7790836095809937
iteration 800, loss 1.835451602935791
iteration 0, loss 1.7279386520385742
iteration 100, loss 1.7670754194259644
iteration 200, loss 1.7212296724319458
iteration 300, loss 1.7895269393920898
iteration 400, loss 1.694390892982483
iteration 500, loss 1.728966474533081
iteration 600, loss 1.7380551099777222
iteration 700, loss 1.7020870447158813
iteration 800, loss 1.8017871379852295
iteration 0, loss 1.774397373199463
iteration 100, loss 1.7668107748031616
iteration 200, loss 1.755318284034729
iteration 300, loss 1.7283871173858643
iteration 400, loss 1.7783641815185547
iteration 500, loss 1.7204153537750244
iteration 600, loss 1.6826637983322144
iteration 700, loss 1.7683995962142944
iteration 800, loss 1.7293992042541504
iteration 0, loss 1.7220056056976318
iteration 100, loss 1.749328374862671
iteration 200, loss 1.6799675226211548
iteration 300, loss 1.7283852100372314
iteration 400, loss 1.7170010805130005
iteration 500, loss 1.6690764427185059
iteration 600, loss 1.7061173915863037
iteration 700, loss 1.684543251991272
iteration 800, loss 1.7468371391296387
iteration 0, loss 1.7243680953979492
iteration 100, loss 1.7198950052261353
iteration 200, loss 1.7502027750015259
iteration 300, loss 1.669861912727356
iteration 400, loss 1.8620718717575073
iteration 500, loss 1.7281477451324463
iteration 600, loss 1.7197990417480469
iteration 700, loss 1.720790147781372
iteration 800, loss 1.678030252456665
iteration 0, loss 1.7553857564926147
iteration 100, loss 1.747657060623169
iteration 200, loss 1.715903639793396
iteration 300, loss 1.7096116542816162
iteration 400, loss 1.70464026927948
iteration 500, loss 1.6912506818771362
iteration 600, loss 1.7340168952941895
iteration 700, loss 1.724192500114441
iteration 800, loss 1.7451457977294922
iteration 0, loss 1.6737297773361206
iteration 100, loss 1.7564442157745361
iteration 200, loss 1.790854811668396
iteration 300, loss 1.7715699672698975
iteration 400, loss 1.7655324935913086
iteration 500, loss 1.7359305620193481
iteration 600, loss 1.7051962614059448
iteration 700, loss 1.7458264827728271
iteration 800, loss 1.6594034433364868
iteration 0, loss 1.6916042566299438
iteration 100, loss 1.6988065242767334
iteration 200, loss 1.7356194257736206
iteration 300, loss 1.7482621669769287
iteration 400, loss 1.7120916843414307
iteration 500, loss 1.7475653886795044
iteration 600, loss 1.7326221466064453
iteration 700, loss 1.73257577419281
iteration 800, loss 1.7010693550109863
iteration 0, loss 1.6844251155853271
iteration 100, loss 1.7355051040649414
iteration 200, loss 1.7389919757843018
iteration 300, loss 1.7173449993133545
iteration 400, loss 1.690786600112915
iteration 500, loss 1.7035062313079834
iteration 600, loss 1.695030927658081
iteration 700, loss 1.8233975172042847
iteration 800, loss 1.7094577550888062
iteration 0, loss 1.7099089622497559
iteration 100, loss 1.7274075746536255
iteration 200, loss 1.6630083322525024
iteration 300, loss 1.7260063886642456
iteration 400, loss 1.696494460105896
iteration 500, loss 1.7264786958694458
iteration 600, loss 1.7217994928359985
iteration 700, loss 1.7910971641540527
iteration 800, loss 1.8084044456481934
iteration 0, loss 1.712609052658081
iteration 100, loss 1.7699521780014038
iteration 200, loss 1.689850926399231
iteration 300, loss 1.753640055656433
iteration 400, loss 1.7167086601257324
iteration 500, loss 1.7042901515960693
iteration 600, loss 1.7019776105880737
iteration 700, loss 1.6712783575057983
iteration 800, loss 1.759857177734375
iteration 0, loss 1.6697534322738647
iteration 100, loss 1.7818011045455933
iteration 200, loss 1.7865625619888306
iteration 300, loss 1.758771300315857
iteration 400, loss 1.6801429986953735
iteration 500, loss 1.7105129957199097
iteration 600, loss 1.7376632690429688
iteration 700, loss 1.693790316581726
iteration 800, loss 1.6693485975265503
iteration 0, loss 1.7085597515106201
iteration 100, loss 1.8093653917312622
iteration 200, loss 1.6863585710525513
iteration 300, loss 1.7664302587509155
iteration 400, loss 1.6888831853866577
iteration 500, loss 1.755705714225769
iteration 600, loss 1.7136136293411255
iteration 700, loss 1.7538219690322876
iteration 800, loss 1.7369120121002197
iteration 0, loss 1.7244138717651367
iteration 100, loss 1.7265417575836182
iteration 200, loss 1.7180328369140625
iteration 300, loss 1.683323621749878
iteration 400, loss 1.7163323163986206
iteration 500, loss 1.7443829774856567
iteration 600, loss 1.7403732538223267
iteration 700, loss 1.7949105501174927
iteration 800, loss 1.7656148672103882
iteration 0, loss 1.7001322507858276
iteration 100, loss 1.7346802949905396
iteration 200, loss 1.734148383140564
iteration 300, loss 1.687170147895813
iteration 400, loss 1.7398853302001953
iteration 500, loss 1.6926662921905518
iteration 600, loss 1.692221760749817
iteration 700, loss 1.6960880756378174
iteration 800, loss 1.744249939918518
iteration 0, loss 1.6846944093704224
iteration 100, loss 1.7044095993041992
iteration 200, loss 1.7243876457214355
iteration 300, loss 1.6932631731033325
iteration 400, loss 1.6710764169692993
iteration 500, loss 1.8033092021942139
iteration 600, loss 1.7752230167388916
iteration 700, loss 1.709555745124817
iteration 800, loss 1.7192823886871338
iteration 0, loss 1.7121789455413818
iteration 100, loss 1.6771931648254395
iteration 200, loss 1.6757088899612427
iteration 300, loss 1.7150843143463135
iteration 400, loss 1.6698859930038452
iteration 500, loss 1.69968843460083
iteration 600, loss 1.752683401107788
iteration 700, loss 1.680456280708313
iteration 800, loss 1.71037757396698
iteration 0, loss 1.735285758972168
iteration 100, loss 1.7333635091781616
iteration 200, loss 1.7814655303955078
iteration 300, loss 1.7042946815490723
iteration 400, loss 1.7191681861877441
iteration 500, loss 1.6911364793777466
iteration 600, loss 1.7466284036636353
iteration 700, loss 1.734573245048523
iteration 800, loss 1.7196707725524902
iteration 0, loss 1.6878936290740967
iteration 100, loss 1.7286622524261475
iteration 200, loss 1.718053936958313
iteration 300, loss 1.7205140590667725
iteration 400, loss 1.7117420434951782
iteration 500, loss 1.729877233505249
iteration 600, loss 1.7677141427993774
iteration 700, loss 1.661195993423462
iteration 800, loss 1.6891076564788818
iteration 0, loss 1.7157822847366333
iteration 100, loss 1.662031650543213
iteration 200, loss 1.7041757106781006
iteration 300, loss 1.7029061317443848
iteration 400, loss 1.745621919631958
iteration 500, loss 1.6769068241119385
iteration 600, loss 1.7529759407043457
iteration 700, loss 1.7397350072860718
iteration 800, loss 1.7285714149475098
iteration 0, loss 1.665470838546753
iteration 100, loss 1.7957439422607422
iteration 200, loss 1.7681635618209839
iteration 300, loss 1.7097421884536743
iteration 400, loss 1.6960065364837646
iteration 500, loss 1.707905650138855
iteration 600, loss 1.6888645887374878
iteration 700, loss 1.740191102027893
iteration 800, loss 1.7133575677871704
iteration 0, loss 1.727210521697998
iteration 100, loss 1.7185745239257812
iteration 200, loss 1.6897870302200317
iteration 300, loss 1.667718768119812
iteration 400, loss 1.7165749073028564
iteration 500, loss 1.641982078552246
iteration 600, loss 1.6805280447006226
iteration 700, loss 1.7486097812652588
iteration 800, loss 1.6809104681015015
iteration 0, loss 1.7452608346939087
iteration 100, loss 1.7348062992095947
iteration 200, loss 1.7334421873092651
iteration 300, loss 1.7096316814422607
iteration 400, loss 1.7063493728637695
iteration 500, loss 1.7244884967803955
iteration 600, loss 1.6694579124450684
iteration 700, loss 1.7138501405715942
iteration 800, loss 1.7417511940002441
iteration 0, loss 1.7229368686676025
iteration 100, loss 1.6832443475723267
iteration 200, loss 1.7492446899414062
iteration 300, loss 1.7617805004119873
iteration 400, loss 1.7644686698913574
iteration 500, loss 1.7184678316116333
iteration 600, loss 1.7251418828964233
iteration 700, loss 1.691386103630066
iteration 800, loss 1.7066309452056885
iteration 0, loss 1.6948490142822266
iteration 100, loss 1.7289105653762817
iteration 200, loss 1.7593659162521362
iteration 300, loss 1.7043747901916504
iteration 400, loss 1.7234103679656982
iteration 500, loss 1.6836814880371094
iteration 600, loss 1.706695795059204
iteration 700, loss 1.7450860738754272
iteration 800, loss 1.7316542863845825
iteration 0, loss 1.6928813457489014
iteration 100, loss 1.6516778469085693
iteration 200, loss 1.7113926410675049
iteration 300, loss 1.690462350845337
iteration 400, loss 1.7327306270599365
iteration 500, loss 1.6863231658935547
iteration 600, loss 1.7344828844070435
iteration 700, loss 1.7287615537643433
iteration 800, loss 1.7107605934143066
iteration 0, loss 1.6721320152282715
iteration 100, loss 1.7160978317260742
iteration 200, loss 1.7591098546981812
iteration 300, loss 1.7047834396362305
iteration 400, loss 1.7424647808074951
iteration 500, loss 1.7069262266159058
iteration 600, loss 1.6913384199142456
iteration 700, loss 1.7310702800750732
iteration 800, loss 1.7545983791351318
iteration 0, loss 1.6538910865783691
iteration 100, loss 1.740290641784668
iteration 200, loss 1.674203634262085
iteration 300, loss 1.7098197937011719
iteration 400, loss 1.7925844192504883
iteration 500, loss 1.7456635236740112
iteration 600, loss 1.6882224082946777
iteration 700, loss 1.7983304262161255
iteration 800, loss 1.7091423273086548
iteration 0, loss 1.7479242086410522
iteration 100, loss 1.73275625705719
iteration 200, loss 1.6592305898666382
iteration 300, loss 1.7634344100952148
iteration 400, loss 1.6922270059585571
iteration 500, loss 1.7041672468185425
iteration 600, loss 1.743078351020813
iteration 700, loss 1.6762748956680298
iteration 800, loss 1.6706640720367432
iteration 0, loss 1.6618552207946777
iteration 100, loss 1.7953600883483887
iteration 200, loss 1.7666932344436646
iteration 300, loss 1.7716212272644043
iteration 400, loss 1.650802731513977
iteration 500, loss 1.758601188659668
iteration 600, loss 1.7008750438690186
iteration 700, loss 1.7083098888397217
iteration 800, loss 1.709362506866455
iteration 0, loss 1.723018765449524
iteration 100, loss 1.7541768550872803
iteration 200, loss 1.7050800323486328
iteration 300, loss 1.7226834297180176
iteration 400, loss 1.6645597219467163
iteration 500, loss 1.6715023517608643
iteration 600, loss 1.670676827430725
iteration 700, loss 1.7231721878051758
iteration 800, loss 1.7296029329299927
iteration 0, loss 1.697636365890503
iteration 100, loss 1.7369539737701416
iteration 200, loss 1.71212899684906
iteration 300, loss 1.6809228658676147
iteration 400, loss 1.7299011945724487
iteration 500, loss 1.6851966381072998
iteration 600, loss 1.7239903211593628
iteration 700, loss 1.768664836883545
iteration 800, loss 1.7541263103485107
fold 0 accuracy: 0.6622142857142858
iteration 0, loss 1.7642748355865479
iteration 100, loss 1.71467125415802
iteration 200, loss 1.6977496147155762
iteration 300, loss 1.7237591743469238
iteration 400, loss 1.7850711345672607
iteration 500, loss 1.7421772480010986
iteration 600, loss 1.7344528436660767
iteration 700, loss 1.7180473804473877
iteration 800, loss 1.707383394241333
iteration 0, loss 1.691520094871521
iteration 100, loss 1.7587308883666992
iteration 200, loss 1.6406360864639282
iteration 300, loss 1.7146599292755127
iteration 400, loss 1.7351584434509277
iteration 500, loss 1.7048503160476685
iteration 600, loss 1.765960693359375
iteration 700, loss 1.6906005144119263
iteration 800, loss 1.768627405166626
iteration 0, loss 1.7183371782302856
iteration 100, loss 1.7035467624664307
iteration 200, loss 1.687174916267395
iteration 300, loss 1.7116272449493408
iteration 400, loss 1.658056616783142
iteration 500, loss 1.729536771774292
iteration 600, loss 1.6327146291732788
iteration 700, loss 1.6490534543991089
iteration 800, loss 1.669379472732544
iteration 0, loss 1.623319387435913
iteration 100, loss 1.7261192798614502
iteration 200, loss 1.8043662309646606
iteration 300, loss 1.7289113998413086
iteration 400, loss 1.7028381824493408
iteration 500, loss 1.7850592136383057
iteration 600, loss 1.7084264755249023
iteration 700, loss 1.7133759260177612
iteration 800, loss 1.7009083032608032
iteration 0, loss 1.738213300704956
iteration 100, loss 1.7257328033447266
iteration 200, loss 1.732434630393982
iteration 300, loss 1.7007189989089966
iteration 400, loss 1.7577024698257446
iteration 500, loss 1.7484461069107056
iteration 600, loss 1.7516659498214722
iteration 700, loss 1.6389563083648682
iteration 800, loss 1.6546332836151123
iteration 0, loss 1.7508478164672852
iteration 100, loss 1.7426925897598267
iteration 200, loss 1.6805123090744019
iteration 300, loss 1.7327126264572144
iteration 400, loss 1.7351322174072266
iteration 500, loss 1.6834121942520142
iteration 600, loss 1.7721999883651733
iteration 700, loss 1.7292157411575317
iteration 800, loss 1.6831496953964233
iteration 0, loss 1.687215805053711
iteration 100, loss 1.6885707378387451
iteration 200, loss 1.689801812171936
iteration 300, loss 1.7362983226776123
iteration 400, loss 1.6690479516983032
iteration 500, loss 1.7409889698028564
iteration 600, loss 1.6482508182525635
iteration 700, loss 1.7350226640701294
iteration 800, loss 1.7479243278503418
iteration 0, loss 1.7636852264404297
iteration 100, loss 1.7022730112075806
iteration 200, loss 1.7174988985061646
iteration 300, loss 1.689967393875122
iteration 400, loss 1.7481358051300049
iteration 500, loss 1.6714297533035278
iteration 600, loss 1.70501708984375
iteration 700, loss 1.703425645828247
iteration 800, loss 1.6408320665359497
iteration 0, loss 1.6804436445236206
iteration 100, loss 1.6590888500213623
iteration 200, loss 1.719638466835022
iteration 300, loss 1.7277930974960327
iteration 400, loss 1.6966789960861206
iteration 500, loss 1.6816102266311646
iteration 600, loss 1.719321370124817
iteration 700, loss 1.687454104423523
iteration 800, loss 1.64246666431427
iteration 0, loss 1.6801096200942993
iteration 100, loss 1.7280654907226562
iteration 200, loss 1.7024592161178589
iteration 300, loss 1.7348147630691528
iteration 400, loss 1.7427600622177124
iteration 500, loss 1.766427755355835
iteration 600, loss 1.7313696146011353
iteration 700, loss 1.7179467678070068
iteration 800, loss 1.704966425895691
iteration 0, loss 1.704423189163208
iteration 100, loss 1.7171282768249512
iteration 200, loss 1.695648193359375
iteration 300, loss 1.6670057773590088
iteration 400, loss 1.6674915552139282
iteration 500, loss 1.6952815055847168
iteration 600, loss 1.7018433809280396
iteration 700, loss 1.7402609586715698
iteration 800, loss 1.7107107639312744
iteration 0, loss 1.730027675628662
iteration 100, loss 1.7522732019424438
iteration 200, loss 1.8227670192718506
iteration 300, loss 1.7689461708068848
iteration 400, loss 1.6844526529312134
iteration 500, loss 1.735751986503601
iteration 600, loss 1.747912883758545
iteration 700, loss 1.7681854963302612
iteration 800, loss 1.6636712551116943
iteration 0, loss 1.6504127979278564
iteration 100, loss 1.6970717906951904
iteration 200, loss 1.6993870735168457
iteration 300, loss 1.7231217622756958
iteration 400, loss 1.7403603792190552
iteration 500, loss 1.6893556118011475
iteration 600, loss 1.7329907417297363
iteration 700, loss 1.733216643333435
iteration 800, loss 1.7105413675308228
iteration 0, loss 1.7593772411346436
iteration 100, loss 1.6819484233856201
iteration 200, loss 1.7442506551742554
iteration 300, loss 1.7297022342681885
iteration 400, loss 1.741328239440918
iteration 500, loss 1.7449734210968018
iteration 600, loss 1.7096900939941406
iteration 700, loss 1.7025986909866333
iteration 800, loss 1.7535548210144043
iteration 0, loss 1.6593499183654785
iteration 100, loss 1.6745268106460571
iteration 200, loss 1.756182074546814
iteration 300, loss 1.702279806137085
iteration 400, loss 1.711073398590088
iteration 500, loss 1.639303207397461
iteration 600, loss 1.6871981620788574
iteration 700, loss 1.7111949920654297
iteration 800, loss 1.818548560142517
iteration 0, loss 1.6677683591842651
iteration 100, loss 1.7847750186920166
iteration 200, loss 1.7008322477340698
iteration 300, loss 1.7022348642349243
iteration 400, loss 1.7052221298217773
iteration 500, loss 1.695428490638733
iteration 600, loss 1.7320327758789062
iteration 700, loss 1.7165048122406006
iteration 800, loss 1.661563754081726
iteration 0, loss 1.7460789680480957
iteration 100, loss 1.6723650693893433
iteration 200, loss 1.7157409191131592
iteration 300, loss 1.6811212301254272
iteration 400, loss 1.7658331394195557
iteration 500, loss 1.663192868232727
iteration 600, loss 1.7217906713485718
iteration 700, loss 1.7024664878845215
iteration 800, loss 1.6780825853347778
iteration 0, loss 1.7275131940841675
iteration 100, loss 1.7151130437850952
iteration 200, loss 1.8092280626296997
iteration 300, loss 1.7414973974227905
iteration 400, loss 1.7717934846878052
iteration 500, loss 1.698409080505371
iteration 600, loss 1.6527111530303955
iteration 700, loss 1.6820929050445557
iteration 800, loss 1.7064841985702515
iteration 0, loss 1.7100542783737183
iteration 100, loss 1.692177653312683
iteration 200, loss 1.7015321254730225
iteration 300, loss 1.7421256303787231
iteration 400, loss 1.7828398942947388
iteration 500, loss 1.6661841869354248
iteration 600, loss 1.7666095495224
iteration 700, loss 1.735672950744629
iteration 800, loss 1.7008219957351685
iteration 0, loss 1.6571418046951294
iteration 100, loss 1.624556541442871
iteration 200, loss 1.7272645235061646
iteration 300, loss 1.6940789222717285
iteration 400, loss 1.6579716205596924
iteration 500, loss 1.681718349456787
iteration 600, loss 1.7344976663589478
iteration 700, loss 1.734818458557129
iteration 800, loss 1.675889015197754
iteration 0, loss 1.7019069194793701
iteration 100, loss 1.7005857229232788
iteration 200, loss 1.7045280933380127
iteration 300, loss 1.6962662935256958
iteration 400, loss 1.682989239692688
iteration 500, loss 1.7124186754226685
iteration 600, loss 1.7274690866470337
iteration 700, loss 1.7234933376312256
iteration 800, loss 1.6436841487884521
iteration 0, loss 1.6878536939620972
iteration 100, loss 1.6838178634643555
iteration 200, loss 1.7448773384094238
iteration 300, loss 1.6646801233291626
iteration 400, loss 1.6165239810943604
iteration 500, loss 1.7481745481491089
iteration 600, loss 1.7198896408081055
iteration 700, loss 1.685068130493164
iteration 800, loss 1.6406400203704834
iteration 0, loss 1.656872272491455
iteration 100, loss 1.762451171875
iteration 200, loss 1.7046692371368408
iteration 300, loss 1.6806373596191406
iteration 400, loss 1.7200464010238647
iteration 500, loss 1.7094002962112427
iteration 600, loss 1.667724847793579
iteration 700, loss 1.6954774856567383
iteration 800, loss 1.6817035675048828
iteration 0, loss 1.7119868993759155
iteration 100, loss 1.6766388416290283
iteration 200, loss 1.7128363847732544
iteration 300, loss 1.7421839237213135
iteration 400, loss 1.7110399007797241
iteration 500, loss 1.677051305770874
iteration 600, loss 1.6894314289093018
iteration 700, loss 1.7352397441864014
iteration 800, loss 1.6504645347595215
iteration 0, loss 1.6857942342758179
iteration 100, loss 1.7534884214401245
iteration 200, loss 1.6908730268478394
iteration 300, loss 1.7204563617706299
iteration 400, loss 1.7237290143966675
iteration 500, loss 1.671844720840454
iteration 600, loss 1.706752061843872
iteration 700, loss 1.704850435256958
iteration 800, loss 1.706233024597168
iteration 0, loss 1.6882109642028809
iteration 100, loss 1.6618882417678833
iteration 200, loss 1.7162240743637085
iteration 300, loss 1.664039969444275
iteration 400, loss 1.657063603401184
iteration 500, loss 1.7074800729751587
iteration 600, loss 1.7152583599090576
iteration 700, loss 1.781293511390686
iteration 800, loss 1.7039122581481934
iteration 0, loss 1.6924139261245728
iteration 100, loss 1.6769802570343018
iteration 200, loss 1.7351588010787964
iteration 300, loss 1.7310411930084229
iteration 400, loss 1.7009763717651367
iteration 500, loss 1.7892667055130005
iteration 600, loss 1.665467619895935
iteration 700, loss 1.7357728481292725
iteration 800, loss 1.7127459049224854
iteration 0, loss 1.6883190870285034
iteration 100, loss 1.6709606647491455
iteration 200, loss 1.7302188873291016
iteration 300, loss 1.6208930015563965
iteration 400, loss 1.729283332824707
iteration 500, loss 1.7187886238098145
iteration 600, loss 1.6939531564712524
iteration 700, loss 1.7269577980041504
iteration 800, loss 1.635203242301941
iteration 0, loss 1.745631456375122
iteration 100, loss 1.668372392654419
iteration 200, loss 1.6764180660247803
iteration 300, loss 1.6885786056518555
iteration 400, loss 1.7268331050872803
iteration 500, loss 1.7407594919204712
iteration 600, loss 1.7264342308044434
iteration 700, loss 1.7177144289016724
iteration 800, loss 1.7458347082138062
iteration 0, loss 1.6654397249221802
iteration 100, loss 1.7162672281265259
iteration 200, loss 1.6691997051239014
iteration 300, loss 1.7304377555847168
iteration 400, loss 1.6933048963546753
iteration 500, loss 1.7060937881469727
iteration 600, loss 1.6778019666671753
iteration 700, loss 1.6420570611953735
iteration 800, loss 1.6220816373825073
iteration 0, loss 1.6913491487503052
iteration 100, loss 1.7696692943572998
iteration 200, loss 1.6514849662780762
iteration 300, loss 1.8324278593063354
iteration 400, loss 1.6782125234603882
iteration 500, loss 1.6983728408813477
iteration 600, loss 1.6770094633102417
iteration 700, loss 1.7448747158050537
iteration 800, loss 1.6547883749008179
iteration 0, loss 1.7579236030578613
iteration 100, loss 1.7173082828521729
iteration 200, loss 1.7242000102996826
iteration 300, loss 1.7150743007659912
iteration 400, loss 1.6911207437515259
iteration 500, loss 1.7019119262695312
iteration 600, loss 1.7420547008514404
iteration 700, loss 1.7080868482589722
iteration 800, loss 1.723502278327942
iteration 0, loss 1.7155216932296753
iteration 100, loss 1.6897528171539307
iteration 200, loss 1.6649070978164673
iteration 300, loss 1.720158576965332
iteration 400, loss 1.752774715423584
iteration 500, loss 1.6606391668319702
iteration 600, loss 1.7548496723175049
iteration 700, loss 1.785796880722046
iteration 800, loss 1.6566035747528076
iteration 0, loss 1.6996289491653442
iteration 100, loss 1.687811255455017
iteration 200, loss 1.7431238889694214
iteration 300, loss 1.7386353015899658
iteration 400, loss 1.7255698442459106
iteration 500, loss 1.662217378616333
iteration 600, loss 1.7152382135391235
iteration 700, loss 1.755624532699585
iteration 800, loss 1.7232224941253662
iteration 0, loss 1.757964849472046
iteration 100, loss 1.7128796577453613
iteration 200, loss 1.7364481687545776
iteration 300, loss 1.7541139125823975
iteration 400, loss 1.6623817682266235
iteration 500, loss 1.662038803100586
iteration 600, loss 1.6584124565124512
iteration 700, loss 1.6439688205718994
iteration 800, loss 1.6850486993789673
iteration 0, loss 1.734283447265625
iteration 100, loss 1.7012525796890259
iteration 200, loss 1.6790839433670044
iteration 300, loss 1.7271639108657837
iteration 400, loss 1.660367488861084
iteration 500, loss 1.6520929336547852
iteration 600, loss 1.6829848289489746
iteration 700, loss 1.799043893814087
iteration 800, loss 1.7250896692276
iteration 0, loss 1.6826807260513306
iteration 100, loss 1.658600926399231
iteration 200, loss 1.7659926414489746
iteration 300, loss 1.6962339878082275
iteration 400, loss 1.738179087638855
iteration 500, loss 1.6651086807250977
iteration 600, loss 1.7491624355316162
iteration 700, loss 1.7669352293014526
iteration 800, loss 1.6851457357406616
iteration 0, loss 1.7290208339691162
iteration 100, loss 1.7192482948303223
iteration 200, loss 1.692685604095459
iteration 300, loss 1.7187174558639526
iteration 400, loss 1.6623026132583618
iteration 500, loss 1.731187105178833
iteration 600, loss 1.7116514444351196
iteration 700, loss 1.7544950246810913
iteration 800, loss 1.6931337118148804
iteration 0, loss 1.6865512132644653
iteration 100, loss 1.7192676067352295
iteration 200, loss 1.7471060752868652
iteration 300, loss 1.7774182558059692
iteration 400, loss 1.715075135231018
iteration 500, loss 1.6625678539276123
iteration 600, loss 1.7620924711227417
iteration 700, loss 1.69669508934021
iteration 800, loss 1.6559240818023682
iteration 0, loss 1.667180061340332
iteration 100, loss 1.7717311382293701
iteration 200, loss 1.714842677116394
iteration 300, loss 1.7935830354690552
iteration 400, loss 1.7620964050292969
iteration 500, loss 1.6550427675247192
iteration 600, loss 1.724265456199646
iteration 700, loss 1.7190289497375488
iteration 800, loss 1.6685563325881958
iteration 0, loss 1.7190773487091064
iteration 100, loss 1.7122747898101807
iteration 200, loss 1.6898002624511719
iteration 300, loss 1.7276006937026978
iteration 400, loss 1.801497220993042
iteration 500, loss 1.610203742980957
iteration 600, loss 1.7326780557632446
iteration 700, loss 1.6736854314804077
iteration 800, loss 1.7823963165283203
iteration 0, loss 1.70552659034729
iteration 100, loss 1.7347807884216309
iteration 200, loss 1.6505399942398071
iteration 300, loss 1.7107244729995728
iteration 400, loss 1.7953848838806152
iteration 500, loss 1.6621533632278442
iteration 600, loss 1.6780925989151
iteration 700, loss 1.7544822692871094
iteration 800, loss 1.7364968061447144
iteration 0, loss 1.614786148071289
iteration 100, loss 1.6834484338760376
iteration 200, loss 1.722857117652893
iteration 300, loss 1.6812489032745361
iteration 400, loss 1.6181010007858276
iteration 500, loss 1.687591314315796
iteration 600, loss 1.7317594289779663
iteration 700, loss 1.6742738485336304
iteration 800, loss 1.7175439596176147
iteration 0, loss 1.6810683012008667
iteration 100, loss 1.6932233572006226
iteration 200, loss 1.649266004562378
iteration 300, loss 1.7513808012008667
iteration 400, loss 1.7214184999465942
iteration 500, loss 1.7726668119430542
iteration 600, loss 1.728597640991211
iteration 700, loss 1.7258806228637695
iteration 800, loss 1.6579113006591797
iteration 0, loss 1.7323137521743774
iteration 100, loss 1.6610188484191895
iteration 200, loss 1.6762675046920776
iteration 300, loss 1.7168184518814087
iteration 400, loss 1.667094349861145
iteration 500, loss 1.757016897201538
iteration 600, loss 1.729117751121521
iteration 700, loss 1.705077886581421
iteration 800, loss 1.67933189868927
iteration 0, loss 1.6958725452423096
iteration 100, loss 1.7178549766540527
iteration 200, loss 1.699668049812317
iteration 300, loss 1.632451057434082
iteration 400, loss 1.7078346014022827
iteration 500, loss 1.6368054151535034
iteration 600, loss 1.681944727897644
iteration 700, loss 1.6642909049987793
iteration 800, loss 1.6893067359924316
iteration 0, loss 1.6967185735702515
iteration 100, loss 1.735280990600586
iteration 200, loss 1.672989010810852
iteration 300, loss 1.7136566638946533
iteration 400, loss 1.6540985107421875
iteration 500, loss 1.6854661703109741
iteration 600, loss 1.7105364799499512
iteration 700, loss 1.6796852350234985
iteration 800, loss 1.7601193189620972
iteration 0, loss 1.6533350944519043
iteration 100, loss 1.6478166580200195
iteration 200, loss 1.745571255683899
iteration 300, loss 1.7275331020355225
iteration 400, loss 1.7063231468200684
iteration 500, loss 1.720548391342163
iteration 600, loss 1.714268445968628
iteration 700, loss 1.7052531242370605
iteration 800, loss 1.7994481325149536
iteration 0, loss 1.7164617776870728
iteration 100, loss 1.6891613006591797
iteration 200, loss 1.6973705291748047
iteration 300, loss 1.6737374067306519
iteration 400, loss 1.760449767112732
iteration 500, loss 1.6407756805419922
iteration 600, loss 1.6920721530914307
iteration 700, loss 1.688231110572815
iteration 800, loss 1.7022112607955933
iteration 0, loss 1.6771734952926636
iteration 100, loss 1.7592540979385376
iteration 200, loss 1.716408133506775
iteration 300, loss 1.6821118593215942
iteration 400, loss 1.692602276802063
iteration 500, loss 1.7287214994430542
iteration 600, loss 1.721987247467041
iteration 700, loss 1.6709160804748535
iteration 800, loss 1.7864044904708862
fold 1 accuracy: 0.7124285714285714
iteration 0, loss 1.7768840789794922
iteration 100, loss 1.7889387607574463
iteration 200, loss 1.6998465061187744
iteration 300, loss 1.7113550901412964
iteration 400, loss 1.694642424583435
iteration 500, loss 1.70931077003479
iteration 600, loss 1.6940325498580933
iteration 700, loss 1.661215901374817
iteration 800, loss 1.7246066331863403
iteration 0, loss 1.682890772819519
iteration 100, loss 1.6353626251220703
iteration 200, loss 1.7515232563018799
iteration 300, loss 1.7191963195800781
iteration 400, loss 1.6378748416900635
iteration 500, loss 1.6765122413635254
iteration 600, loss 1.733397126197815
iteration 700, loss 1.744806170463562
iteration 800, loss 1.6226283311843872
iteration 0, loss 1.6770986318588257
iteration 100, loss 1.7406361103057861
iteration 200, loss 1.6842097043991089
iteration 300, loss 1.7631800174713135
iteration 400, loss 1.653480052947998
iteration 500, loss 1.7473609447479248
iteration 600, loss 1.6610337495803833
iteration 700, loss 1.658629298210144
iteration 800, loss 1.7235349416732788
iteration 0, loss 1.6546486616134644
iteration 100, loss 1.7215712070465088
iteration 200, loss 1.6877212524414062
iteration 300, loss 1.7116005420684814
iteration 400, loss 1.6993381977081299
iteration 500, loss 1.7383257150650024
iteration 600, loss 1.702077865600586
iteration 700, loss 1.6424304246902466
iteration 800, loss 1.6717239618301392
iteration 0, loss 1.7027292251586914
iteration 100, loss 1.7172330617904663
iteration 200, loss 1.7046622037887573
iteration 300, loss 1.7081947326660156
iteration 400, loss 1.7031316757202148
iteration 500, loss 1.6835633516311646
iteration 600, loss 1.7148141860961914
iteration 700, loss 1.695316195487976
iteration 800, loss 1.6541411876678467
iteration 0, loss 1.7043386697769165
iteration 100, loss 1.7419509887695312
iteration 200, loss 1.6857390403747559
iteration 300, loss 1.7131942510604858
iteration 400, loss 1.6661715507507324
iteration 500, loss 1.7173051834106445
iteration 600, loss 1.7221399545669556
iteration 700, loss 1.6718930006027222
iteration 800, loss 1.676279902458191
iteration 0, loss 1.7058557271957397
iteration 100, loss 1.629676342010498
iteration 200, loss 1.7179256677627563
iteration 300, loss 1.6650798320770264
iteration 400, loss 1.6990079879760742
iteration 500, loss 1.70834481716156
iteration 600, loss 1.7811145782470703
iteration 700, loss 1.7200905084609985
iteration 800, loss 1.7022271156311035
iteration 0, loss 1.758599877357483
iteration 100, loss 1.7440184354782104
iteration 200, loss 1.7095733880996704
iteration 300, loss 1.7325245141983032
iteration 400, loss 1.6941368579864502
iteration 500, loss 1.744162917137146
iteration 600, loss 1.7475578784942627
iteration 700, loss 1.706945776939392
iteration 800, loss 1.6736615896224976
iteration 0, loss 1.7422983646392822
iteration 100, loss 1.681205153465271
iteration 200, loss 1.7136937379837036
iteration 300, loss 1.7515944242477417
iteration 400, loss 1.670128583908081
iteration 500, loss 1.7512657642364502
iteration 600, loss 1.694759488105774
iteration 700, loss 1.6868752241134644
iteration 800, loss 1.6633840799331665
iteration 0, loss 1.7197742462158203
iteration 100, loss 1.65366792678833
iteration 200, loss 1.6863473653793335
iteration 300, loss 1.6658294200897217
iteration 400, loss 1.7677433490753174
iteration 500, loss 1.7503741979599
iteration 600, loss 1.6766775846481323
iteration 700, loss 1.679111123085022
iteration 800, loss 1.687785029411316
iteration 0, loss 1.739863395690918
iteration 100, loss 1.623483419418335
iteration 200, loss 1.678870439529419
iteration 300, loss 1.6894667148590088
iteration 400, loss 1.6543781757354736
iteration 500, loss 1.722293734550476
iteration 600, loss 1.6667321920394897
iteration 700, loss 1.676145076751709
iteration 800, loss 1.652391791343689
iteration 0, loss 1.777302622795105
iteration 100, loss 1.7317988872528076
iteration 200, loss 1.709065556526184
iteration 300, loss 1.718677043914795
iteration 400, loss 1.7505335807800293
iteration 500, loss 1.6247589588165283
iteration 600, loss 1.6407256126403809
iteration 700, loss 1.7316250801086426
iteration 800, loss 1.6856945753097534
iteration 0, loss 1.693402647972107
iteration 100, loss 1.7381398677825928
iteration 200, loss 1.717879295349121
iteration 300, loss 1.652875542640686
iteration 400, loss 1.7185777425765991
iteration 500, loss 1.6697418689727783
iteration 600, loss 1.6542302370071411
iteration 700, loss 1.6625356674194336
iteration 800, loss 1.7092065811157227
iteration 0, loss 1.6644200086593628
iteration 100, loss 1.686943531036377
iteration 200, loss 1.7254244089126587
iteration 300, loss 1.6303383111953735
iteration 400, loss 1.6379039287567139
iteration 500, loss 1.6953892707824707
iteration 600, loss 1.7080243825912476
iteration 700, loss 1.7608613967895508
iteration 800, loss 1.6571406126022339
iteration 0, loss 1.7623554468154907
iteration 100, loss 1.6378449201583862
iteration 200, loss 1.690750241279602
iteration 300, loss 1.667496919631958
iteration 400, loss 1.7088297605514526
iteration 500, loss 1.7411936521530151
iteration 600, loss 1.7194453477859497
iteration 700, loss 1.7178113460540771
iteration 800, loss 1.7317534685134888
iteration 0, loss 1.6823666095733643
iteration 100, loss 1.687515139579773
iteration 200, loss 1.710181474685669
iteration 300, loss 1.6272900104522705
iteration 400, loss 1.6670844554901123
iteration 500, loss 1.7353984117507935
iteration 600, loss 1.706624984741211
iteration 700, loss 1.7561990022659302
iteration 800, loss 1.6696736812591553
iteration 0, loss 1.6868575811386108
iteration 100, loss 1.6601803302764893
iteration 200, loss 1.7279133796691895
iteration 300, loss 1.652955174446106
iteration 400, loss 1.6960508823394775
iteration 500, loss 1.760254979133606
iteration 600, loss 1.7339632511138916
iteration 700, loss 1.7188202142715454
iteration 800, loss 1.7283289432525635
iteration 0, loss 1.7568693161010742
iteration 100, loss 1.6770203113555908
iteration 200, loss 1.7585458755493164
iteration 300, loss 1.7166849374771118
iteration 400, loss 1.6997281312942505
iteration 500, loss 1.7585245370864868
iteration 600, loss 1.749265432357788
iteration 700, loss 1.7367935180664062
iteration 800, loss 1.7659327983856201
iteration 0, loss 1.7557765245437622
iteration 100, loss 1.6619919538497925
iteration 200, loss 1.6857593059539795
iteration 300, loss 1.7127937078475952
iteration 400, loss 1.7230194807052612
iteration 500, loss 1.6395769119262695
iteration 600, loss 1.6481002569198608
iteration 700, loss 1.6747840642929077
iteration 800, loss 1.6687240600585938
iteration 0, loss 1.708017110824585
iteration 100, loss 1.6719510555267334
iteration 200, loss 1.675783395767212
iteration 300, loss 1.6598137617111206
iteration 400, loss 1.6654952764511108
iteration 500, loss 1.6350890398025513
iteration 600, loss 1.7247822284698486
iteration 700, loss 1.6721359491348267
iteration 800, loss 1.7536267042160034
iteration 0, loss 1.6971771717071533
iteration 100, loss 1.6797398328781128
iteration 200, loss 1.7225918769836426
iteration 300, loss 1.7024728059768677
iteration 400, loss 1.7362115383148193
iteration 500, loss 1.6644675731658936
iteration 600, loss 1.6452884674072266
iteration 700, loss 1.6572201251983643
iteration 800, loss 1.695013165473938
iteration 0, loss 1.7036690711975098
iteration 100, loss 1.6927978992462158
iteration 200, loss 1.701102614402771
iteration 300, loss 1.6763644218444824
iteration 400, loss 1.7208545207977295
iteration 500, loss 1.6433229446411133
iteration 600, loss 1.6523749828338623
iteration 700, loss 1.767446756362915
iteration 800, loss 1.7980711460113525
iteration 0, loss 1.6716779470443726
iteration 100, loss 1.6833268404006958
iteration 200, loss 1.6674785614013672
iteration 300, loss 1.699168086051941
iteration 400, loss 1.7470017671585083
iteration 500, loss 1.7385889291763306
iteration 600, loss 1.7249696254730225
iteration 700, loss 1.7283505201339722
iteration 800, loss 1.7105873823165894
iteration 0, loss 1.730332851409912
iteration 100, loss 1.7042988538742065
iteration 200, loss 1.641638159751892
iteration 300, loss 1.7604120969772339
iteration 400, loss 1.666467308998108
iteration 500, loss 1.7090821266174316
iteration 600, loss 1.7316635847091675
iteration 700, loss 1.6903316974639893
iteration 800, loss 1.693109154701233
iteration 0, loss 1.6689140796661377
iteration 100, loss 1.6772502660751343
iteration 200, loss 1.7337009906768799
iteration 300, loss 1.7248256206512451
iteration 400, loss 1.7173709869384766
iteration 500, loss 1.6805572509765625
iteration 600, loss 1.7264797687530518
iteration 700, loss 1.6742897033691406
iteration 800, loss 1.723844289779663
iteration 0, loss 1.6455001831054688
iteration 100, loss 1.701474666595459
iteration 200, loss 1.7652065753936768
iteration 300, loss 1.6345572471618652
iteration 400, loss 1.69528067111969
iteration 500, loss 1.7283987998962402
iteration 600, loss 1.6751922369003296
iteration 700, loss 1.6621029376983643
iteration 800, loss 1.7055691480636597
iteration 0, loss 1.669942021369934
iteration 100, loss 1.7333035469055176
iteration 200, loss 1.7667357921600342
iteration 300, loss 1.704419732093811
iteration 400, loss 1.7196025848388672
iteration 500, loss 1.752192497253418
iteration 600, loss 1.7261457443237305
iteration 700, loss 1.7463105916976929
iteration 800, loss 1.632364273071289
iteration 0, loss 1.7556447982788086
iteration 100, loss 1.7347984313964844
iteration 200, loss 1.7594448328018188
iteration 300, loss 1.73129141330719
iteration 400, loss 1.6722395420074463
iteration 500, loss 1.7010029554367065
iteration 600, loss 1.7135566473007202
iteration 700, loss 1.6618924140930176
iteration 800, loss 1.6829581260681152
iteration 0, loss 1.6870663166046143
iteration 100, loss 1.748033046722412
iteration 200, loss 1.7399853467941284
iteration 300, loss 1.7055535316467285
iteration 400, loss 1.6786856651306152
iteration 500, loss 1.732908844947815
iteration 600, loss 1.7015563249588013
iteration 700, loss 1.7614355087280273
iteration 800, loss 1.7256367206573486
iteration 0, loss 1.7019203901290894
iteration 100, loss 1.7358982563018799
iteration 200, loss 1.69428551197052
iteration 300, loss 1.6781178712844849
iteration 400, loss 1.7479113340377808
iteration 500, loss 1.6945980787277222
iteration 600, loss 1.7084213495254517
iteration 700, loss 1.7118741273880005
iteration 800, loss 1.6878725290298462
iteration 0, loss 1.7034367322921753
iteration 100, loss 1.7237969636917114
iteration 200, loss 1.628172516822815
iteration 300, loss 1.6996299028396606
iteration 400, loss 1.6866295337677002
iteration 500, loss 1.6435558795928955
iteration 600, loss 1.6729216575622559
iteration 700, loss 1.707956075668335
iteration 800, loss 1.6731339693069458
iteration 0, loss 1.6907628774642944
iteration 100, loss 1.651543378829956
iteration 200, loss 1.7159029245376587
iteration 300, loss 1.714224100112915
iteration 400, loss 1.6373412609100342
iteration 500, loss 1.684339165687561
iteration 600, loss 1.7472513914108276
iteration 700, loss 1.6945241689682007
iteration 800, loss 1.6855372190475464
iteration 0, loss 1.627327561378479
iteration 100, loss 1.6765812635421753
iteration 200, loss 1.7494478225708008
iteration 300, loss 1.6605931520462036
iteration 400, loss 1.6547129154205322
iteration 500, loss 1.7689512968063354
iteration 600, loss 1.7358263731002808
iteration 700, loss 1.6870146989822388
iteration 800, loss 1.757378101348877
iteration 0, loss 1.7352842092514038
iteration 100, loss 1.7160004377365112
iteration 200, loss 1.7129578590393066
iteration 300, loss 1.7220462560653687
iteration 400, loss 1.6491072177886963
iteration 500, loss 1.6781667470932007
iteration 600, loss 1.7674652338027954
iteration 700, loss 1.748565435409546
iteration 800, loss 1.6921590566635132
iteration 0, loss 1.6866644620895386
iteration 100, loss 1.689623475074768
iteration 200, loss 1.6667765378952026
iteration 300, loss 1.691731333732605
iteration 400, loss 1.6634474992752075
iteration 500, loss 1.6359385251998901
iteration 600, loss 1.649855375289917
iteration 700, loss 1.7907748222351074
iteration 800, loss 1.7144255638122559
iteration 0, loss 1.7989277839660645
iteration 100, loss 1.598905086517334
iteration 200, loss 1.7284125089645386
iteration 300, loss 1.6609982252120972
iteration 400, loss 1.6792515516281128
iteration 500, loss 1.742496371269226
iteration 600, loss 1.7194985151290894
iteration 700, loss 1.6789352893829346
iteration 800, loss 1.7338449954986572
iteration 0, loss 1.715004801750183
iteration 100, loss 1.6779216527938843
iteration 200, loss 1.6378446817398071
iteration 300, loss 1.7272191047668457
iteration 400, loss 1.6844488382339478
iteration 500, loss 1.6581329107284546
iteration 600, loss 1.6767935752868652
iteration 700, loss 1.720381736755371
iteration 800, loss 1.6829349994659424
iteration 0, loss 1.7125176191329956
iteration 100, loss 1.6862457990646362
iteration 200, loss 1.6923946142196655
iteration 300, loss 1.6922369003295898
iteration 400, loss 1.7208547592163086
iteration 500, loss 1.6769737005233765
iteration 600, loss 1.8184775114059448
iteration 700, loss 1.7233490943908691
iteration 800, loss 1.719651699066162
iteration 0, loss 1.6949174404144287
iteration 100, loss 1.719020962715149
iteration 200, loss 1.6703780889511108
iteration 300, loss 1.7336615324020386
iteration 400, loss 1.6887811422348022
iteration 500, loss 1.6681113243103027
iteration 600, loss 1.7033891677856445
iteration 700, loss 1.697371482849121
iteration 800, loss 1.6568406820297241
iteration 0, loss 1.696554183959961
iteration 100, loss 1.6196494102478027
iteration 200, loss 1.649519443511963
iteration 300, loss 1.758188009262085
iteration 400, loss 1.651943325996399
iteration 500, loss 1.7521477937698364
iteration 600, loss 1.7321271896362305
iteration 700, loss 1.6991180181503296
iteration 800, loss 1.6773884296417236
iteration 0, loss 1.6570020914077759
iteration 100, loss 1.7193940877914429
iteration 200, loss 1.6766277551651
iteration 300, loss 1.7067787647247314
iteration 400, loss 1.7256652116775513
iteration 500, loss 1.6419142484664917
iteration 600, loss 1.6830168962478638
iteration 700, loss 1.6079572439193726
iteration 800, loss 1.7211209535598755
iteration 0, loss 1.765846610069275
iteration 100, loss 1.662083387374878
iteration 200, loss 1.672142744064331
iteration 300, loss 1.6803439855575562
iteration 400, loss 1.636548399925232
iteration 500, loss 1.7593696117401123
iteration 600, loss 1.742696762084961
iteration 700, loss 1.720082402229309
iteration 800, loss 1.7024132013320923
iteration 0, loss 1.7267735004425049
iteration 100, loss 1.6591265201568604
iteration 200, loss 1.7411755323410034
iteration 300, loss 1.672664761543274
iteration 400, loss 1.6730091571807861
iteration 500, loss 1.6996665000915527
iteration 600, loss 1.6906129121780396
iteration 700, loss 1.6556063890457153
iteration 800, loss 1.6766853332519531
iteration 0, loss 1.6657190322875977
iteration 100, loss 1.703980803489685
iteration 200, loss 1.6901249885559082
iteration 300, loss 1.6857823133468628
iteration 400, loss 1.6956180334091187
iteration 500, loss 1.6910669803619385
iteration 600, loss 1.6715519428253174
iteration 700, loss 1.6875073909759521
iteration 800, loss 1.6681901216506958
iteration 0, loss 1.6502792835235596
iteration 100, loss 1.7072991132736206
iteration 200, loss 1.6998072862625122
iteration 300, loss 1.7207719087600708
iteration 400, loss 1.7205836772918701
iteration 500, loss 1.6670715808868408
iteration 600, loss 1.7032774686813354
iteration 700, loss 1.715744972229004
iteration 800, loss 1.642858862876892
iteration 0, loss 1.6868363618850708
iteration 100, loss 1.6865354776382446
iteration 200, loss 1.7232083082199097
iteration 300, loss 1.6800616979599
iteration 400, loss 1.7440639734268188
iteration 500, loss 1.7207417488098145
iteration 600, loss 1.695405125617981
iteration 700, loss 1.7222647666931152
iteration 800, loss 1.6953973770141602
iteration 0, loss 1.669704794883728
iteration 100, loss 1.721057415008545
iteration 200, loss 1.7356348037719727
iteration 300, loss 1.6455341577529907
iteration 400, loss 1.707889437675476
iteration 500, loss 1.7080055475234985
iteration 600, loss 1.7386788129806519
iteration 700, loss 1.730526328086853
iteration 800, loss 1.7129048109054565
iteration 0, loss 1.6216012239456177
iteration 100, loss 1.7165645360946655
iteration 200, loss 1.6518268585205078
iteration 300, loss 1.683974027633667
iteration 400, loss 1.6996686458587646
iteration 500, loss 1.712418556213379
iteration 600, loss 1.777151346206665
iteration 700, loss 1.6644643545150757
iteration 800, loss 1.622540831565857
iteration 0, loss 1.6690648794174194
iteration 100, loss 1.6333457231521606
iteration 200, loss 1.6257684230804443
iteration 300, loss 1.7281235456466675
iteration 400, loss 1.7128368616104126
iteration 500, loss 1.686755895614624
iteration 600, loss 1.7011629343032837
iteration 700, loss 1.6331981420516968
iteration 800, loss 1.6662230491638184
iteration 0, loss 1.723522424697876
iteration 100, loss 1.7438806295394897
iteration 200, loss 1.7130070924758911
iteration 300, loss 1.7554731369018555
iteration 400, loss 1.6728615760803223
iteration 500, loss 1.7192409038543701
iteration 600, loss 1.6702498197555542
iteration 700, loss 1.7382296323776245
iteration 800, loss 1.7388423681259155
fold 2 accuracy: 0.7195
iteration 0, loss 1.6515905857086182
iteration 100, loss 1.7048238515853882
iteration 200, loss 1.7088429927825928
iteration 300, loss 1.740175485610962
iteration 400, loss 1.6599323749542236
iteration 500, loss 1.6897507905960083
iteration 600, loss 1.6541725397109985
iteration 700, loss 1.6990028619766235
iteration 800, loss 1.7060483694076538
iteration 0, loss 1.683654546737671
iteration 100, loss 1.7082326412200928
iteration 200, loss 1.7003334760665894
iteration 300, loss 1.725900411605835
iteration 400, loss 1.8078256845474243
iteration 500, loss 1.703060507774353
iteration 600, loss 1.7295602560043335
iteration 700, loss 1.697900652885437
iteration 800, loss 1.609744906425476
iteration 0, loss 1.662231683731079
iteration 100, loss 1.7351435422897339
iteration 200, loss 1.6642695665359497
iteration 300, loss 1.647193431854248
iteration 400, loss 1.7407423257827759
iteration 500, loss 1.6653821468353271
iteration 600, loss 1.73581063747406
iteration 700, loss 1.7410457134246826
iteration 800, loss 1.7001618146896362
iteration 0, loss 1.671524167060852
iteration 100, loss 1.6213021278381348
iteration 200, loss 1.6497552394866943
iteration 300, loss 1.6636371612548828
iteration 400, loss 1.6855930089950562
iteration 500, loss 1.7020766735076904
iteration 600, loss 1.6497453451156616
iteration 700, loss 1.6456167697906494
iteration 800, loss 1.695814609527588
iteration 0, loss 1.6869033575057983
iteration 100, loss 1.669141411781311
iteration 200, loss 1.6820173263549805
iteration 300, loss 1.7122628688812256
iteration 400, loss 1.671015739440918
iteration 500, loss 1.6906187534332275
iteration 600, loss 1.7287956476211548
iteration 700, loss 1.6987886428833008
iteration 800, loss 1.6865818500518799
iteration 0, loss 1.6545618772506714
iteration 100, loss 1.7143988609313965
iteration 200, loss 1.7283118963241577
iteration 300, loss 1.687907338142395
iteration 400, loss 1.6760703325271606
iteration 500, loss 1.7034739255905151
iteration 600, loss 1.6746920347213745
iteration 700, loss 1.6418284177780151
iteration 800, loss 1.694161295890808
iteration 0, loss 1.6901485919952393
iteration 100, loss 1.6994651556015015
iteration 200, loss 1.7142679691314697
iteration 300, loss 1.7616361379623413
iteration 400, loss 1.6806495189666748
iteration 500, loss 1.67122483253479
iteration 600, loss 1.6484941244125366
iteration 700, loss 1.6382941007614136
iteration 800, loss 1.6902827024459839
iteration 0, loss 1.6601855754852295
iteration 100, loss 1.6778838634490967
iteration 200, loss 1.715490698814392
iteration 300, loss 1.713997721672058
iteration 400, loss 1.7482942342758179
iteration 500, loss 1.7260342836380005
iteration 600, loss 1.6916165351867676
iteration 700, loss 1.6414295434951782
iteration 800, loss 1.6951606273651123
iteration 0, loss 1.6643263101577759
iteration 100, loss 1.6530815362930298
iteration 200, loss 1.717793583869934
iteration 300, loss 1.7066930532455444
iteration 400, loss 1.748255968093872
iteration 500, loss 1.6890456676483154
iteration 600, loss 1.7011691331863403
iteration 700, loss 1.7310442924499512
iteration 800, loss 1.7194560766220093
iteration 0, loss 1.5967060327529907
iteration 100, loss 1.705415964126587
iteration 200, loss 1.6634325981140137
iteration 300, loss 1.6439759731292725
iteration 400, loss 1.6385035514831543
iteration 500, loss 1.671058177947998
iteration 600, loss 1.7324405908584595
iteration 700, loss 1.75785493850708
iteration 800, loss 1.6258052587509155
iteration 0, loss 1.6678016185760498
iteration 100, loss 1.7832233905792236
iteration 200, loss 1.6749887466430664
iteration 300, loss 1.7260502576828003
iteration 400, loss 1.7165662050247192
iteration 500, loss 1.602573275566101
iteration 600, loss 1.7104754447937012
iteration 700, loss 1.7205857038497925
iteration 800, loss 1.6452308893203735
iteration 0, loss 1.6910911798477173
iteration 100, loss 1.6858898401260376
iteration 200, loss 1.6884509325027466
iteration 300, loss 1.7066234350204468
iteration 400, loss 1.7565146684646606
iteration 500, loss 1.714600682258606
iteration 600, loss 1.6419329643249512
iteration 700, loss 1.7085590362548828
iteration 800, loss 1.7305418252944946
iteration 0, loss 1.668363094329834
iteration 100, loss 1.6040847301483154
iteration 200, loss 1.7763694524765015
iteration 300, loss 1.6312181949615479
iteration 400, loss 1.6914629936218262
iteration 500, loss 1.714578628540039
iteration 600, loss 1.6939060688018799
iteration 700, loss 1.7281132936477661
iteration 800, loss 1.7075220346450806
iteration 0, loss 1.7562965154647827
iteration 100, loss 1.7216770648956299
iteration 200, loss 1.74665105342865
iteration 300, loss 1.678511381149292
iteration 400, loss 1.72926926612854
iteration 500, loss 1.7150464057922363
iteration 600, loss 1.6702532768249512
iteration 700, loss 1.7344510555267334
iteration 800, loss 1.7125848531723022
iteration 0, loss 1.7934765815734863
iteration 100, loss 1.6971492767333984
iteration 200, loss 1.7426888942718506
iteration 300, loss 1.7554067373275757
iteration 400, loss 1.6803719997406006
iteration 500, loss 1.6748828887939453
iteration 600, loss 1.6533089876174927
iteration 700, loss 1.65397047996521
iteration 800, loss 1.7166439294815063
iteration 0, loss 1.7142828702926636
iteration 100, loss 1.6552810668945312
iteration 200, loss 1.7439162731170654
iteration 300, loss 1.7337472438812256
iteration 400, loss 1.7019966840744019
iteration 500, loss 1.7172980308532715
iteration 600, loss 1.7128320932388306
iteration 700, loss 1.7302199602127075
iteration 800, loss 1.6587417125701904
iteration 0, loss 1.737477421760559
iteration 100, loss 1.6438367366790771
iteration 200, loss 1.7549878358840942
iteration 300, loss 1.6406038999557495
iteration 400, loss 1.7032002210617065
iteration 500, loss 1.7069880962371826
iteration 600, loss 1.6537691354751587
iteration 700, loss 1.7030807733535767
iteration 800, loss 1.712234377861023
iteration 0, loss 1.6352324485778809
iteration 100, loss 1.7317969799041748
iteration 200, loss 1.7577719688415527
iteration 300, loss 1.6964281797409058
iteration 400, loss 1.6481270790100098
iteration 500, loss 1.6652578115463257
iteration 600, loss 1.687156319618225
iteration 700, loss 1.6864334344863892
iteration 800, loss 1.6790239810943604
iteration 0, loss 1.6565836668014526
iteration 100, loss 1.6099294424057007
iteration 200, loss 1.7277346849441528
iteration 300, loss 1.7322345972061157
iteration 400, loss 1.7063602209091187
iteration 500, loss 1.7262004613876343
iteration 600, loss 1.6649081707000732
iteration 700, loss 1.697400450706482
iteration 800, loss 1.6053647994995117
iteration 0, loss 1.6956981420516968
iteration 100, loss 1.706360101699829
iteration 200, loss 1.6246007680892944
iteration 300, loss 1.6407032012939453
iteration 400, loss 1.7094894647598267
iteration 500, loss 1.686608076095581
iteration 600, loss 1.6752464771270752
iteration 700, loss 1.7264975309371948
iteration 800, loss 1.7157005071640015
iteration 0, loss 1.6609293222427368
iteration 100, loss 1.6944173574447632
iteration 200, loss 1.7010314464569092
iteration 300, loss 1.6478705406188965
iteration 400, loss 1.6778371334075928
iteration 500, loss 1.697001338005066
iteration 600, loss 1.6295212507247925
iteration 700, loss 1.7338359355926514
iteration 800, loss 1.6448907852172852
iteration 0, loss 1.6781907081604004
iteration 100, loss 1.7002534866333008
iteration 200, loss 1.7253732681274414
iteration 300, loss 1.7055730819702148
iteration 400, loss 1.698778748512268
iteration 500, loss 1.6923460960388184
iteration 600, loss 1.7054388523101807
iteration 700, loss 1.7205191850662231
iteration 800, loss 1.7139456272125244
iteration 0, loss 1.6028189659118652
iteration 100, loss 1.6720635890960693
iteration 200, loss 1.7013204097747803
iteration 300, loss 1.6498373746871948
iteration 400, loss 1.7426650524139404
iteration 500, loss 1.701310634613037
iteration 600, loss 1.7408709526062012
iteration 700, loss 1.688449501991272
iteration 800, loss 1.7042213678359985
iteration 0, loss 1.7065327167510986
iteration 100, loss 1.693785309791565
iteration 200, loss 1.6820687055587769
iteration 300, loss 1.6349517107009888
iteration 400, loss 1.731380820274353
iteration 500, loss 1.684038519859314
iteration 600, loss 1.6391047239303589
iteration 700, loss 1.707207202911377
iteration 800, loss 1.779517412185669
iteration 0, loss 1.6865464448928833
iteration 100, loss 1.693186640739441
iteration 200, loss 1.727170705795288
iteration 300, loss 1.6907098293304443
iteration 400, loss 1.7128324508666992
iteration 500, loss 1.6227116584777832
iteration 600, loss 1.7215726375579834
iteration 700, loss 1.7656680345535278
iteration 800, loss 1.7021819353103638
iteration 0, loss 1.7084662914276123
iteration 100, loss 1.7706983089447021
iteration 200, loss 1.6701465845108032
iteration 300, loss 1.7561975717544556
iteration 400, loss 1.7308388948440552
iteration 500, loss 1.640539526939392
iteration 600, loss 1.7150965929031372
iteration 700, loss 1.6720147132873535
iteration 800, loss 1.680762767791748
iteration 0, loss 1.7032995223999023
iteration 100, loss 1.7097694873809814
iteration 200, loss 1.7449147701263428
iteration 300, loss 1.7008812427520752
iteration 400, loss 1.6858901977539062
iteration 500, loss 1.638196587562561
iteration 600, loss 1.6745797395706177
iteration 700, loss 1.6819278001785278
iteration 800, loss 1.7127454280853271
iteration 0, loss 1.7289701700210571
iteration 100, loss 1.7853301763534546
iteration 200, loss 1.7468979358673096
iteration 300, loss 1.6599143743515015
iteration 400, loss 1.7894865274429321
iteration 500, loss 1.6673468351364136
iteration 600, loss 1.6769720315933228
iteration 700, loss 1.697811245918274
iteration 800, loss 1.7188307046890259
iteration 0, loss 1.6983929872512817
iteration 100, loss 1.6447848081588745
iteration 200, loss 1.6661373376846313
iteration 300, loss 1.7562317848205566
iteration 400, loss 1.717349886894226
iteration 500, loss 1.6920146942138672
iteration 600, loss 1.702114224433899
iteration 700, loss 1.7350265979766846
iteration 800, loss 1.614152431488037
iteration 0, loss 1.6655378341674805
iteration 100, loss 1.7077401876449585
iteration 200, loss 1.681580662727356
iteration 300, loss 1.6647262573242188
iteration 400, loss 1.694649577140808
iteration 500, loss 1.6917445659637451
iteration 600, loss 1.700439691543579
iteration 700, loss 1.6681855916976929
iteration 800, loss 1.690960168838501
iteration 0, loss 1.7589396238327026
iteration 100, loss 1.6763073205947876
iteration 200, loss 1.656720757484436
iteration 300, loss 1.762652039527893
iteration 400, loss 1.7448827028274536
iteration 500, loss 1.7253495454788208
iteration 600, loss 1.7200692892074585
iteration 700, loss 1.6895016431808472
iteration 800, loss 1.6706457138061523
iteration 0, loss 1.65437912940979
iteration 100, loss 1.6392545700073242
iteration 200, loss 1.7980973720550537
iteration 300, loss 1.6932196617126465
iteration 400, loss 1.6725517511367798
iteration 500, loss 1.675110101699829
iteration 600, loss 1.7093307971954346
iteration 700, loss 1.6714369058609009
iteration 800, loss 1.7531039714813232
iteration 0, loss 1.6627681255340576
iteration 100, loss 1.6707960367202759
iteration 200, loss 1.6558276414871216
iteration 300, loss 1.6471549272537231
iteration 400, loss 1.6711281538009644
iteration 500, loss 1.679160714149475
iteration 600, loss 1.7008239030838013
iteration 700, loss 1.7170765399932861
iteration 800, loss 1.6454353332519531
iteration 0, loss 1.634539008140564
iteration 100, loss 1.736415982246399
iteration 200, loss 1.7046462297439575
iteration 300, loss 1.6750519275665283
iteration 400, loss 1.7466073036193848
iteration 500, loss 1.7014981508255005
iteration 600, loss 1.732588768005371
iteration 700, loss 1.6745980978012085
iteration 800, loss 1.6876490116119385
iteration 0, loss 1.7180473804473877
iteration 100, loss 1.780378818511963
iteration 200, loss 1.6627522706985474
iteration 300, loss 1.6897693872451782
iteration 400, loss 1.6765456199645996
iteration 500, loss 1.7046263217926025
iteration 600, loss 1.77510666847229
iteration 700, loss 1.6484475135803223
iteration 800, loss 1.6931079626083374
iteration 0, loss 1.7021052837371826
iteration 100, loss 1.6699429750442505
iteration 200, loss 1.6997288465499878
iteration 300, loss 1.716050624847412
iteration 400, loss 1.6674516201019287
iteration 500, loss 1.7142434120178223
iteration 600, loss 1.6982529163360596
iteration 700, loss 1.7218959331512451
iteration 800, loss 1.7368063926696777
iteration 0, loss 1.6577796936035156
iteration 100, loss 1.7106884717941284
iteration 200, loss 1.7152700424194336
iteration 300, loss 1.6517729759216309
iteration 400, loss 1.6999223232269287
iteration 500, loss 1.686913251876831
iteration 600, loss 1.6815404891967773
iteration 700, loss 1.661363959312439
iteration 800, loss 1.6807682514190674
iteration 0, loss 1.6468908786773682
iteration 100, loss 1.6975613832473755
iteration 200, loss 1.6917585134506226
iteration 300, loss 1.6669220924377441
iteration 400, loss 1.6765841245651245
iteration 500, loss 1.6746582984924316
iteration 600, loss 1.674880862236023
iteration 700, loss 1.6203670501708984
iteration 800, loss 1.7166467905044556
iteration 0, loss 1.6659290790557861
iteration 100, loss 1.7101022005081177
iteration 200, loss 1.612115502357483
iteration 300, loss 1.6789171695709229
iteration 400, loss 1.6908479928970337
iteration 500, loss 1.7511482238769531
iteration 600, loss 1.6730964183807373
iteration 700, loss 1.7173612117767334
iteration 800, loss 1.6525408029556274
iteration 0, loss 1.7099946737289429
iteration 100, loss 1.6672612428665161
iteration 200, loss 1.6869499683380127
iteration 300, loss 1.6525083780288696
iteration 400, loss 1.6479493379592896
iteration 500, loss 1.669607162475586
iteration 600, loss 1.6549184322357178
iteration 700, loss 1.706331491470337
iteration 800, loss 1.684678554534912
iteration 0, loss 1.7112733125686646
iteration 100, loss 1.654672384262085
iteration 200, loss 1.700586199760437
iteration 300, loss 1.7103087902069092
iteration 400, loss 1.661170244216919
iteration 500, loss 1.6848925352096558
iteration 600, loss 1.6852796077728271
iteration 700, loss 1.681782841682434
iteration 800, loss 1.6932566165924072
iteration 0, loss 1.595508337020874
iteration 100, loss 1.6977401971817017
iteration 200, loss 1.6656782627105713
iteration 300, loss 1.6843767166137695
iteration 400, loss 1.7160335779190063
iteration 500, loss 1.7029350996017456
iteration 600, loss 1.7223833799362183
iteration 700, loss 1.7481189966201782
iteration 800, loss 1.6908559799194336
iteration 0, loss 1.6564455032348633
iteration 100, loss 1.6406376361846924
iteration 200, loss 1.7036789655685425
iteration 300, loss 1.704903483390808
iteration 400, loss 1.7418866157531738
iteration 500, loss 1.6979379653930664
iteration 600, loss 1.756066918373108
iteration 700, loss 1.681178331375122
iteration 800, loss 1.6581586599349976
iteration 0, loss 1.6719350814819336
iteration 100, loss 1.692160964012146
iteration 200, loss 1.6773630380630493
iteration 300, loss 1.7252296209335327
iteration 400, loss 1.6830580234527588
iteration 500, loss 1.6776503324508667
iteration 600, loss 1.7173972129821777
iteration 700, loss 1.673600435256958
iteration 800, loss 1.6219713687896729
iteration 0, loss 1.6937156915664673
iteration 100, loss 1.7002414464950562
iteration 200, loss 1.622904658317566
iteration 300, loss 1.6959731578826904
iteration 400, loss 1.6368454694747925
iteration 500, loss 1.6760505437850952
iteration 600, loss 1.6553906202316284
iteration 700, loss 1.7854022979736328
iteration 800, loss 1.6285498142242432
iteration 0, loss 1.6372984647750854
iteration 100, loss 1.7083309888839722
iteration 200, loss 1.654228687286377
iteration 300, loss 1.6807715892791748
iteration 400, loss 1.6545032262802124
iteration 500, loss 1.752235770225525
iteration 600, loss 1.6733999252319336
iteration 700, loss 1.736193060874939
iteration 800, loss 1.6849690675735474
iteration 0, loss 1.6925216913223267
iteration 100, loss 1.673351764678955
iteration 200, loss 1.7022855281829834
iteration 300, loss 1.6666878461837769
iteration 400, loss 1.6926411390304565
iteration 500, loss 1.6487840414047241
iteration 600, loss 1.6749080419540405
iteration 700, loss 1.6206122636795044
iteration 800, loss 1.680176019668579
iteration 0, loss 1.6421535015106201
iteration 100, loss 1.7082549333572388
iteration 200, loss 1.6371986865997314
iteration 300, loss 1.7690739631652832
iteration 400, loss 1.6026805639266968
iteration 500, loss 1.6541941165924072
iteration 600, loss 1.6874001026153564
iteration 700, loss 1.6614017486572266
iteration 800, loss 1.7473279237747192
iteration 0, loss 1.6791703701019287
iteration 100, loss 1.6665246486663818
iteration 200, loss 1.6916440725326538
iteration 300, loss 1.687683343887329
iteration 400, loss 1.6862130165100098
iteration 500, loss 1.714789867401123
iteration 600, loss 1.6258848905563354
iteration 700, loss 1.6375162601470947
iteration 800, loss 1.6891354322433472
iteration 0, loss 1.6946229934692383
iteration 100, loss 1.6900399923324585
iteration 200, loss 1.652782917022705
iteration 300, loss 1.6719589233398438
iteration 400, loss 1.6940144300460815
iteration 500, loss 1.664601445198059
iteration 600, loss 1.7033770084381104
iteration 700, loss 1.7342007160186768
iteration 800, loss 1.7497694492340088
fold 3 accuracy: 0.715
iteration 0, loss 1.6725959777832031
iteration 100, loss 1.6865266561508179
iteration 200, loss 1.686784267425537
iteration 300, loss 1.687940001487732
iteration 400, loss 1.7141183614730835
iteration 500, loss 1.7283552885055542
iteration 600, loss 1.6437798738479614
iteration 700, loss 1.6976219415664673
iteration 800, loss 1.667801856994629
iteration 0, loss 1.690477728843689
iteration 100, loss 1.583886742591858
iteration 200, loss 1.6728817224502563
iteration 300, loss 1.6550027132034302
iteration 400, loss 1.7149068117141724
iteration 500, loss 1.6666816473007202
iteration 600, loss 1.70008385181427
iteration 700, loss 1.6853809356689453
iteration 800, loss 1.661975383758545
iteration 0, loss 1.6369329690933228
iteration 100, loss 1.710608720779419
iteration 200, loss 1.6909162998199463
iteration 300, loss 1.6812595129013062
iteration 400, loss 1.6654109954833984
iteration 500, loss 1.7628769874572754
iteration 600, loss 1.7247990369796753
iteration 700, loss 1.6768945455551147
iteration 800, loss 1.663227915763855
iteration 0, loss 1.6828500032424927
iteration 100, loss 1.632481336593628
iteration 200, loss 1.6995313167572021
iteration 300, loss 1.651384711265564
iteration 400, loss 1.7802695035934448
iteration 500, loss 1.7143630981445312
iteration 600, loss 1.6670629978179932
iteration 700, loss 1.7668993473052979
iteration 800, loss 1.6452827453613281
iteration 0, loss 1.6859089136123657
iteration 100, loss 1.7604701519012451
iteration 200, loss 1.6599735021591187
iteration 300, loss 1.6632720232009888
iteration 400, loss 1.6894710063934326
iteration 500, loss 1.6954829692840576
iteration 600, loss 1.6864120960235596
iteration 700, loss 1.6227399110794067
iteration 800, loss 1.6989450454711914
iteration 0, loss 1.6585791110992432
iteration 100, loss 1.7019144296646118
iteration 200, loss 1.7418164014816284
iteration 300, loss 1.6761090755462646
iteration 400, loss 1.6565464735031128
iteration 500, loss 1.6819416284561157
iteration 600, loss 1.6513328552246094
iteration 700, loss 1.693105936050415
iteration 800, loss 1.6424223184585571
iteration 0, loss 1.7660367488861084
iteration 100, loss 1.6615904569625854
iteration 200, loss 1.6492469310760498
iteration 300, loss 1.6868140697479248
iteration 400, loss 1.701703667640686
iteration 500, loss 1.6867040395736694
iteration 600, loss 1.6536760330200195
iteration 700, loss 1.7206063270568848
iteration 800, loss 1.6040222644805908
iteration 0, loss 1.6882762908935547
iteration 100, loss 1.6849548816680908
iteration 200, loss 1.6719717979431152
iteration 300, loss 1.7155286073684692
iteration 400, loss 1.6671313047409058
iteration 500, loss 1.7139835357666016
iteration 600, loss 1.6793396472930908
iteration 700, loss 1.6494632959365845
iteration 800, loss 1.6827809810638428
iteration 0, loss 1.6760443449020386
iteration 100, loss 1.702404260635376
iteration 200, loss 1.6823714971542358
iteration 300, loss 1.657340168952942
iteration 400, loss 1.7039252519607544
iteration 500, loss 1.690649151802063
iteration 600, loss 1.6590176820755005
iteration 700, loss 1.6563552618026733
iteration 800, loss 1.7047492265701294
iteration 0, loss 1.6569010019302368
iteration 100, loss 1.6363025903701782
iteration 200, loss 1.7159851789474487
iteration 300, loss 1.7727348804473877
iteration 400, loss 1.6947669982910156
iteration 500, loss 1.651505708694458
iteration 600, loss 1.7315044403076172
iteration 700, loss 1.6512707471847534
iteration 800, loss 1.6324248313903809
iteration 0, loss 1.6630085706710815
iteration 100, loss 1.6908767223358154
iteration 200, loss 1.6476364135742188
iteration 300, loss 1.7294442653656006
iteration 400, loss 1.6764276027679443
iteration 500, loss 1.7023885250091553
iteration 600, loss 1.660822868347168
iteration 700, loss 1.7084968090057373
iteration 800, loss 1.7716801166534424
iteration 0, loss 1.7104166746139526
iteration 100, loss 1.6915943622589111
iteration 200, loss 1.7217991352081299
iteration 300, loss 1.7297941446304321
iteration 400, loss 1.705298900604248
iteration 500, loss 1.6618117094039917
iteration 600, loss 1.63044273853302
iteration 700, loss 1.6870880126953125
iteration 800, loss 1.7344892024993896
iteration 0, loss 1.6519336700439453
iteration 100, loss 1.6997110843658447
iteration 200, loss 1.8128684759140015
iteration 300, loss 1.618296504020691
iteration 400, loss 1.741053581237793
iteration 500, loss 1.6831952333450317
iteration 600, loss 1.719359040260315
iteration 700, loss 1.715017557144165
iteration 800, loss 1.7093291282653809
iteration 0, loss 1.6889593601226807
iteration 100, loss 1.7352839708328247
iteration 200, loss 1.7074209451675415
iteration 300, loss 1.7020715475082397
iteration 400, loss 1.6546962261199951
iteration 500, loss 1.6702264547348022
iteration 600, loss 1.6970716714859009
iteration 700, loss 1.6665928363800049
iteration 800, loss 1.6837950944900513
iteration 0, loss 1.7031452655792236
iteration 100, loss 1.71214759349823
iteration 200, loss 1.6962246894836426
iteration 300, loss 1.7263398170471191
iteration 400, loss 1.664417028427124
iteration 500, loss 1.7484462261199951
iteration 600, loss 1.7162753343582153
iteration 700, loss 1.6680690050125122
iteration 800, loss 1.701978325843811
iteration 0, loss 1.726283073425293
iteration 100, loss 1.7641956806182861
iteration 200, loss 1.7284027338027954
iteration 300, loss 1.6053693294525146
iteration 400, loss 1.7345575094223022
iteration 500, loss 1.7144237756729126
iteration 600, loss 1.756134271621704
iteration 700, loss 1.6530855894088745
iteration 800, loss 1.6481711864471436
iteration 0, loss 1.742348074913025
iteration 100, loss 1.6751282215118408
iteration 200, loss 1.6819162368774414
iteration 300, loss 1.7074384689331055
iteration 400, loss 1.7452350854873657
iteration 500, loss 1.6840755939483643
iteration 600, loss 1.8021538257598877
iteration 700, loss 1.6560231447219849
iteration 800, loss 1.7535351514816284
iteration 0, loss 1.6903009414672852
iteration 100, loss 1.6568480730056763
iteration 200, loss 1.6827789545059204
iteration 300, loss 1.6406620740890503
iteration 400, loss 1.6442792415618896
iteration 500, loss 1.7024002075195312
iteration 600, loss 1.6323065757751465
iteration 700, loss 1.7521424293518066
iteration 800, loss 1.610613465309143
iteration 0, loss 1.6821224689483643
iteration 100, loss 1.7226901054382324
iteration 200, loss 1.6671884059906006
iteration 300, loss 1.6905944347381592
iteration 400, loss 1.6935436725616455
iteration 500, loss 1.7361900806427002
iteration 600, loss 1.648484230041504
iteration 700, loss 1.6827600002288818
iteration 800, loss 1.7428569793701172
iteration 0, loss 1.691986322402954
iteration 100, loss 1.6231229305267334
iteration 200, loss 1.6388994455337524
iteration 300, loss 1.725455403327942
iteration 400, loss 1.7647143602371216
iteration 500, loss 1.681528925895691
iteration 600, loss 1.7016962766647339
iteration 700, loss 1.708439588546753
iteration 800, loss 1.6384397745132446
iteration 0, loss 1.6877065896987915
iteration 100, loss 1.614193320274353
iteration 200, loss 1.7632601261138916
iteration 300, loss 1.643336534500122
iteration 400, loss 1.7047736644744873
iteration 500, loss 1.677992343902588
iteration 600, loss 1.7420653104782104
iteration 700, loss 1.7116608619689941
iteration 800, loss 1.7070238590240479
iteration 0, loss 1.6902382373809814
iteration 100, loss 1.7006158828735352
iteration 200, loss 1.70259690284729
iteration 300, loss 1.6892017126083374
iteration 400, loss 1.6732739210128784
iteration 500, loss 1.6279927492141724
iteration 600, loss 1.6680811643600464
iteration 700, loss 1.73018479347229
iteration 800, loss 1.7034623622894287
iteration 0, loss 1.6448276042938232
iteration 100, loss 1.690805435180664
iteration 200, loss 1.6337372064590454
iteration 300, loss 1.7018733024597168
iteration 400, loss 1.7293983697891235
iteration 500, loss 1.7167866230010986
iteration 600, loss 1.688394546508789
iteration 700, loss 1.7041276693344116
iteration 800, loss 1.718497633934021
iteration 0, loss 1.7083122730255127
iteration 100, loss 1.695296287536621
iteration 200, loss 1.7086315155029297
iteration 300, loss 1.7114790678024292
iteration 400, loss 1.6638351678848267
iteration 500, loss 1.6808091402053833
iteration 600, loss 1.7008085250854492
iteration 700, loss 1.6880635023117065
iteration 800, loss 1.6835272312164307
iteration 0, loss 1.636540174484253
iteration 100, loss 1.7361699342727661
iteration 200, loss 1.7197415828704834
iteration 300, loss 1.703239917755127
iteration 400, loss 1.7250677347183228
iteration 500, loss 1.6238871812820435
iteration 600, loss 1.7182984352111816
iteration 700, loss 1.6895555257797241
iteration 800, loss 1.7506657838821411
iteration 0, loss 1.6932851076126099
iteration 100, loss 1.6464661359786987
iteration 200, loss 1.6973310708999634
iteration 300, loss 1.6597130298614502
iteration 400, loss 1.7219176292419434
iteration 500, loss 1.7100498676300049
iteration 600, loss 1.6319353580474854
iteration 700, loss 1.7103359699249268
iteration 800, loss 1.6724618673324585
iteration 0, loss 1.651654839515686
iteration 100, loss 1.6721757650375366
iteration 200, loss 1.684720516204834
iteration 300, loss 1.7663613557815552
iteration 400, loss 1.66978919506073
iteration 500, loss 1.6562029123306274
iteration 600, loss 1.7127656936645508
iteration 700, loss 1.7237896919250488
iteration 800, loss 1.7989038228988647
iteration 0, loss 1.6823053359985352
iteration 100, loss 1.7359508275985718
iteration 200, loss 1.6817173957824707
iteration 300, loss 1.7135722637176514
iteration 400, loss 1.676742672920227
iteration 500, loss 1.6680686473846436
iteration 600, loss 1.711148738861084
iteration 700, loss 1.6539074182510376
iteration 800, loss 1.7518597841262817
iteration 0, loss 1.7106592655181885
iteration 100, loss 1.7059639692306519
iteration 200, loss 1.7103779315948486
iteration 300, loss 1.6461060047149658
iteration 400, loss 1.6733454465866089
iteration 500, loss 1.713864803314209
iteration 600, loss 1.7606558799743652
iteration 700, loss 1.6546672582626343
iteration 800, loss 1.705017328262329
iteration 0, loss 1.7941596508026123
iteration 100, loss 1.6902421712875366
iteration 200, loss 1.6924320459365845
iteration 300, loss 1.6585568189620972
iteration 400, loss 1.7516825199127197
iteration 500, loss 1.7167613506317139
iteration 600, loss 1.6335368156433105
iteration 700, loss 1.7032208442687988
iteration 800, loss 1.795560359954834
iteration 0, loss 1.7272778749465942
iteration 100, loss 1.6148220300674438
iteration 200, loss 1.662200689315796
iteration 300, loss 1.6829414367675781
iteration 400, loss 1.7213486433029175
iteration 500, loss 1.7510807514190674
iteration 600, loss 1.6891860961914062
iteration 700, loss 1.666687250137329
iteration 800, loss 1.7422906160354614
iteration 0, loss 1.6652781963348389
iteration 100, loss 1.722840666770935
iteration 200, loss 1.6994459629058838
iteration 300, loss 1.7633167505264282
iteration 400, loss 1.7097549438476562
iteration 500, loss 1.7526466846466064
iteration 600, loss 1.6501686573028564
iteration 700, loss 1.6268184185028076
iteration 800, loss 1.6649593114852905
iteration 0, loss 1.6258444786071777
iteration 100, loss 1.6960548162460327
iteration 200, loss 1.705610990524292
iteration 300, loss 1.8019288778305054
iteration 400, loss 1.6862003803253174
iteration 500, loss 1.6937638521194458
iteration 600, loss 1.719452977180481
iteration 700, loss 1.7142975330352783
iteration 800, loss 1.6570274829864502
iteration 0, loss 1.756104826927185
iteration 100, loss 1.7017217874526978
iteration 200, loss 1.6663453578948975
iteration 300, loss 1.7046810388565063
iteration 400, loss 1.671466588973999
iteration 500, loss 1.6749484539031982
iteration 600, loss 1.665890097618103
iteration 700, loss 1.7094321250915527
iteration 800, loss 1.7425346374511719
iteration 0, loss 1.7300156354904175
iteration 100, loss 1.671116828918457
iteration 200, loss 1.6862550973892212
iteration 300, loss 1.6674576997756958
iteration 400, loss 1.710957646369934
iteration 500, loss 1.6850314140319824
iteration 600, loss 1.6842769384384155
iteration 700, loss 1.7289822101593018
iteration 800, loss 1.669052004814148
iteration 0, loss 1.685347557067871
iteration 100, loss 1.7112683057785034
iteration 200, loss 1.645345687866211
iteration 300, loss 1.7269481420516968
iteration 400, loss 1.6975067853927612
iteration 500, loss 1.708847165107727
iteration 600, loss 1.7087863683700562
iteration 700, loss 1.6592180728912354
iteration 800, loss 1.67548668384552
iteration 0, loss 1.685506820678711
iteration 100, loss 1.719882845878601
iteration 200, loss 1.6531803607940674
iteration 300, loss 1.6817197799682617
iteration 400, loss 1.6238213777542114
iteration 500, loss 1.7484800815582275
iteration 600, loss 1.6875431537628174
iteration 700, loss 1.7402013540267944
iteration 800, loss 1.642223834991455
iteration 0, loss 1.7199712991714478
iteration 100, loss 1.6660478115081787
iteration 200, loss 1.716719150543213
iteration 300, loss 1.6844758987426758
iteration 400, loss 1.727813482284546
iteration 500, loss 1.6636686325073242
iteration 600, loss 1.6804697513580322
iteration 700, loss 1.6929378509521484
iteration 800, loss 1.6580952405929565
iteration 0, loss 1.64950430393219
iteration 100, loss 1.6861357688903809
iteration 200, loss 1.657374382019043
iteration 300, loss 1.6623131036758423
iteration 400, loss 1.6723365783691406
iteration 500, loss 1.7341036796569824
iteration 600, loss 1.726464867591858
iteration 700, loss 1.6927220821380615
iteration 800, loss 1.7194406986236572
iteration 0, loss 1.6973323822021484
iteration 100, loss 1.7224138975143433
iteration 200, loss 1.5870038270950317
iteration 300, loss 1.6329785585403442
iteration 400, loss 1.6420369148254395
iteration 500, loss 1.7356568574905396
iteration 600, loss 1.653360366821289
iteration 700, loss 1.7173559665679932
iteration 800, loss 1.6824724674224854
iteration 0, loss 1.733013391494751
iteration 100, loss 1.703138828277588
iteration 200, loss 1.6661127805709839
iteration 300, loss 1.6490859985351562
iteration 400, loss 1.649978756904602
iteration 500, loss 1.7198342084884644
iteration 600, loss 1.758001446723938
iteration 700, loss 1.6947591304779053
iteration 800, loss 1.6798908710479736
iteration 0, loss 1.7071866989135742
iteration 100, loss 1.7830106019973755
iteration 200, loss 1.6463414430618286
iteration 300, loss 1.6765092611312866
iteration 400, loss 1.6731239557266235
iteration 500, loss 1.727585792541504
iteration 600, loss 1.6811718940734863
iteration 700, loss 1.7056293487548828
iteration 800, loss 1.696865439414978
iteration 0, loss 1.75288724899292
iteration 100, loss 1.6781702041625977
iteration 200, loss 1.646226406097412
iteration 300, loss 1.705730676651001
iteration 400, loss 1.6875126361846924
iteration 500, loss 1.7208476066589355
iteration 600, loss 1.7322455644607544
iteration 700, loss 1.650054693222046
iteration 800, loss 1.6640506982803345
iteration 0, loss 1.6599701642990112
iteration 100, loss 1.681626796722412
iteration 200, loss 1.6493812799453735
iteration 300, loss 1.6664791107177734
iteration 400, loss 1.6594035625457764
iteration 500, loss 1.6991772651672363
iteration 600, loss 1.6728616952896118
iteration 700, loss 1.684494972229004
iteration 800, loss 1.6394987106323242
iteration 0, loss 1.679063320159912
iteration 100, loss 1.7622838020324707
iteration 200, loss 1.6276164054870605
iteration 300, loss 1.680051565170288
iteration 400, loss 1.676119089126587
iteration 500, loss 1.7653279304504395
iteration 600, loss 1.697331190109253
iteration 700, loss 1.6284199953079224
iteration 800, loss 1.721867561340332
iteration 0, loss 1.7233641147613525
iteration 100, loss 1.6844264268875122
iteration 200, loss 1.7065961360931396
iteration 300, loss 1.6742863655090332
iteration 400, loss 1.732717752456665
iteration 500, loss 1.6310524940490723
iteration 600, loss 1.6780390739440918
iteration 700, loss 1.6791834831237793
iteration 800, loss 1.6844534873962402
iteration 0, loss 1.7214295864105225
iteration 100, loss 1.6858707666397095
iteration 200, loss 1.6502461433410645
iteration 300, loss 1.6962097883224487
iteration 400, loss 1.7165135145187378
iteration 500, loss 1.7267996072769165
iteration 600, loss 1.7623391151428223
iteration 700, loss 1.6334176063537598
iteration 800, loss 1.6753748655319214
iteration 0, loss 1.635463833808899
iteration 100, loss 1.6659619808197021
iteration 200, loss 1.6989641189575195
iteration 300, loss 1.7101144790649414
iteration 400, loss 1.6650398969650269
iteration 500, loss 1.6435546875
iteration 600, loss 1.7062015533447266
iteration 700, loss 1.6709034442901611
iteration 800, loss 1.7338355779647827
iteration 0, loss 1.7476310729980469
iteration 100, loss 1.6694401502609253
iteration 200, loss 1.6820071935653687
iteration 300, loss 1.725281834602356
iteration 400, loss 1.7167515754699707
iteration 500, loss 1.6274908781051636
iteration 600, loss 1.7365542650222778
iteration 700, loss 1.6795949935913086
iteration 800, loss 1.691224217414856
iteration 0, loss 1.837212085723877
iteration 100, loss 1.7265706062316895
iteration 200, loss 1.722969889640808
iteration 300, loss 1.633244276046753
iteration 400, loss 1.7809457778930664
iteration 500, loss 1.6646108627319336
iteration 600, loss 1.6944423913955688
iteration 700, loss 1.7144984006881714
iteration 800, loss 1.7052277326583862
fold 4 accuracy: 0.7212142857142857
[2024-02-29 01:36:52,767] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-29 01:36:52,774] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         458     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       560 MACs
fwd flops per GPU:                                                      1.12 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.12 K  
fwd latency:                                                            352.62 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.18 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '458'}
    MACs        - {'KronLayer': '560 MACs'}
    fwd latency - {'KronLayer': '352.62 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  458 = 100% Params, 560 MACs = 100% MACs, 352.62 us = 100% latency, 3.18 MFLOPS
  (kronlinear): KronLinear(458 = 100% Params, 560 MACs = 100% MACs, 258.45 us = 73.29% latency, 4.33 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 28.13 us = 7.98% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-29 01:36:52,775] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.12 K 458
iteration 0, loss 2.3295159339904785
iteration 100, loss 2.166109323501587
iteration 200, loss 1.9965569972991943
iteration 300, loss 1.9492875337600708
iteration 400, loss 1.8393642902374268
iteration 500, loss 1.8537325859069824
iteration 600, loss 1.886465311050415
iteration 700, loss 1.878572940826416
iteration 800, loss 1.8518437147140503
iteration 0, loss 1.8739395141601562
iteration 100, loss 1.8722624778747559
iteration 200, loss 1.798557996749878
iteration 300, loss 1.8899340629577637
iteration 400, loss 1.8415249586105347
iteration 500, loss 1.810685396194458
iteration 600, loss 1.78132963180542
iteration 700, loss 1.8354344367980957
iteration 800, loss 1.7945935726165771
iteration 0, loss 1.8250226974487305
iteration 100, loss 1.7960175275802612
iteration 200, loss 1.7973370552062988
iteration 300, loss 1.795138955116272
iteration 400, loss 1.7997386455535889
iteration 500, loss 1.8491586446762085
iteration 600, loss 1.7721459865570068
iteration 700, loss 1.841037392616272
iteration 800, loss 1.7645158767700195
iteration 0, loss 1.8061344623565674
iteration 100, loss 1.773709774017334
iteration 200, loss 1.7418338060379028
iteration 300, loss 1.8639378547668457
iteration 400, loss 1.809632420539856
iteration 500, loss 1.8041199445724487
iteration 600, loss 1.7094751596450806
iteration 700, loss 1.773021936416626
iteration 800, loss 1.7083953619003296
iteration 0, loss 1.759137511253357
iteration 100, loss 1.878141164779663
iteration 200, loss 1.739970326423645
iteration 300, loss 1.7645173072814941
iteration 400, loss 1.8736480474472046
iteration 500, loss 1.7226710319519043
iteration 600, loss 1.8282159566879272
iteration 700, loss 1.8102788925170898
iteration 800, loss 1.7674990892410278
iteration 0, loss 1.8155759572982788
iteration 100, loss 1.8040313720703125
iteration 200, loss 1.7282416820526123
iteration 300, loss 1.7028875350952148
iteration 400, loss 1.707803726196289
iteration 500, loss 1.8032422065734863
iteration 600, loss 1.7362412214279175
iteration 700, loss 1.7609868049621582
iteration 800, loss 1.7750918865203857
iteration 0, loss 1.7456464767456055
iteration 100, loss 1.8193371295928955
iteration 200, loss 1.768633246421814
iteration 300, loss 1.7774213552474976
iteration 400, loss 1.7842247486114502
iteration 500, loss 1.7601265907287598
iteration 600, loss 1.7275259494781494
iteration 700, loss 1.7129676342010498
iteration 800, loss 1.7321239709854126
iteration 0, loss 1.8084850311279297
iteration 100, loss 1.7095818519592285
iteration 200, loss 1.7676665782928467
iteration 300, loss 1.7786316871643066
iteration 400, loss 1.7717962265014648
iteration 500, loss 1.782050371170044
iteration 600, loss 1.7554280757904053
iteration 700, loss 1.723865270614624
iteration 800, loss 1.7705578804016113
iteration 0, loss 1.7460538148880005
iteration 100, loss 1.660261869430542
iteration 200, loss 1.7923933267593384
iteration 300, loss 1.7004317045211792
iteration 400, loss 1.8088140487670898
iteration 500, loss 1.7846037149429321
iteration 600, loss 1.7227749824523926
iteration 700, loss 1.7020509243011475
iteration 800, loss 1.7596371173858643
iteration 0, loss 1.7174606323242188
iteration 100, loss 1.8242958784103394
iteration 200, loss 1.780937910079956
iteration 300, loss 1.7881569862365723
iteration 400, loss 1.8731287717819214
iteration 500, loss 1.846328616142273
iteration 600, loss 1.7217987775802612
iteration 700, loss 1.6632497310638428
iteration 800, loss 1.7812432050704956
iteration 0, loss 1.739763617515564
iteration 100, loss 1.785292387008667
iteration 200, loss 1.7677624225616455
iteration 300, loss 1.6666251420974731
iteration 400, loss 1.822277545928955
iteration 500, loss 1.7239223718643188
iteration 600, loss 1.7178398370742798
iteration 700, loss 1.7712793350219727
iteration 800, loss 1.7831300497055054
iteration 0, loss 1.749603033065796
iteration 100, loss 1.7762621641159058
iteration 200, loss 1.772054672241211
iteration 300, loss 1.7651171684265137
iteration 400, loss 1.7716420888900757
iteration 500, loss 1.7376432418823242
iteration 600, loss 1.738818645477295
iteration 700, loss 1.7225967645645142
iteration 800, loss 1.8056695461273193
iteration 0, loss 1.7746360301971436
iteration 100, loss 1.687767505645752
iteration 200, loss 1.802689790725708
iteration 300, loss 1.7254658937454224
iteration 400, loss 1.737276554107666
iteration 500, loss 1.8506009578704834
iteration 600, loss 1.7583703994750977
iteration 700, loss 1.7858320474624634
iteration 800, loss 1.681486964225769
iteration 0, loss 1.7530970573425293
iteration 100, loss 1.6778522729873657
iteration 200, loss 1.7109392881393433
iteration 300, loss 1.7769598960876465
iteration 400, loss 1.7689247131347656
iteration 500, loss 1.6883431673049927
iteration 600, loss 1.801757574081421
iteration 700, loss 1.81067955493927
iteration 800, loss 1.782598853111267
iteration 0, loss 1.7201588153839111
iteration 100, loss 1.7337433099746704
iteration 200, loss 1.7481470108032227
iteration 300, loss 1.735056757926941
iteration 400, loss 1.7029873132705688
iteration 500, loss 1.804015874862671
iteration 600, loss 1.7296231985092163
iteration 700, loss 1.766113519668579
iteration 800, loss 1.7274330854415894
iteration 0, loss 1.789175033569336
iteration 100, loss 1.7932682037353516
iteration 200, loss 1.7812457084655762
iteration 300, loss 1.7899456024169922
iteration 400, loss 1.7469929456710815
iteration 500, loss 1.7840968370437622
iteration 600, loss 1.743842363357544
iteration 700, loss 1.7033486366271973
iteration 800, loss 1.785117506980896
iteration 0, loss 1.7275969982147217
iteration 100, loss 1.7637708187103271
iteration 200, loss 1.6838196516036987
iteration 300, loss 1.7484880685806274
iteration 400, loss 1.6985596418380737
iteration 500, loss 1.7959669828414917
iteration 600, loss 1.7475999593734741
iteration 700, loss 1.7955410480499268
iteration 800, loss 1.7371819019317627
iteration 0, loss 1.6827340126037598
iteration 100, loss 1.830411672592163
iteration 200, loss 1.788432002067566
iteration 300, loss 1.75068199634552
iteration 400, loss 1.7984315156936646
iteration 500, loss 1.6667180061340332
iteration 600, loss 1.7445005178451538
iteration 700, loss 1.7355444431304932
iteration 800, loss 1.778157114982605
iteration 0, loss 1.6973509788513184
iteration 100, loss 1.7307283878326416
iteration 200, loss 1.7580325603485107
iteration 300, loss 1.7448667287826538
iteration 400, loss 1.7864327430725098
iteration 500, loss 1.7685647010803223
iteration 600, loss 1.798350214958191
iteration 700, loss 1.7141716480255127
iteration 800, loss 1.7619740962982178
iteration 0, loss 1.7516155242919922
iteration 100, loss 1.7462942600250244
iteration 200, loss 1.7336406707763672
iteration 300, loss 1.727217435836792
iteration 400, loss 1.728322148323059
iteration 500, loss 1.7858905792236328
iteration 600, loss 1.7368061542510986
iteration 700, loss 1.804387092590332
iteration 800, loss 1.7408239841461182
iteration 0, loss 1.7706999778747559
iteration 100, loss 1.7220854759216309
iteration 200, loss 1.7625020742416382
iteration 300, loss 1.7198542356491089
iteration 400, loss 1.7680251598358154
iteration 500, loss 1.7049627304077148
iteration 600, loss 1.7615269422531128
iteration 700, loss 1.6887599229812622
iteration 800, loss 1.6765358448028564
iteration 0, loss 1.7002495527267456
iteration 100, loss 1.7123656272888184
iteration 200, loss 1.7219843864440918
iteration 300, loss 1.7362037897109985
iteration 400, loss 1.7538102865219116
iteration 500, loss 1.7222907543182373
iteration 600, loss 1.6965187788009644
iteration 700, loss 1.813835859298706
iteration 800, loss 1.7333391904830933
iteration 0, loss 1.768517255783081
iteration 100, loss 1.7666172981262207
iteration 200, loss 1.6999666690826416
iteration 300, loss 1.7414966821670532
iteration 400, loss 1.7466583251953125
iteration 500, loss 1.736082673072815
iteration 600, loss 1.8117482662200928
iteration 700, loss 1.7423864603042603
iteration 800, loss 1.7331080436706543
iteration 0, loss 1.7376912832260132
iteration 100, loss 1.7256810665130615
iteration 200, loss 1.794604778289795
iteration 300, loss 1.7373660802841187
iteration 400, loss 1.8420946598052979
iteration 500, loss 1.7467174530029297
iteration 600, loss 1.742756724357605
iteration 700, loss 1.688334584236145
iteration 800, loss 1.705510139465332
iteration 0, loss 1.750596284866333
iteration 100, loss 1.7161701917648315
iteration 200, loss 1.8073115348815918
iteration 300, loss 1.7453680038452148
iteration 400, loss 1.7137212753295898
iteration 500, loss 1.7861217260360718
iteration 600, loss 1.72469961643219
iteration 700, loss 1.6937044858932495
iteration 800, loss 1.7212111949920654
iteration 0, loss 1.687239170074463
iteration 100, loss 1.7641732692718506
iteration 200, loss 1.7581157684326172
iteration 300, loss 1.7375373840332031
iteration 400, loss 1.7280769348144531
iteration 500, loss 1.7820936441421509
iteration 600, loss 1.714951753616333
iteration 700, loss 1.7326921224594116
iteration 800, loss 1.7544173002243042
iteration 0, loss 1.7197155952453613
iteration 100, loss 1.7277684211730957
iteration 200, loss 1.7821677923202515
iteration 300, loss 1.751516342163086
iteration 400, loss 1.7696336507797241
iteration 500, loss 1.7762271165847778
iteration 600, loss 1.7223787307739258
iteration 700, loss 1.7036852836608887
iteration 800, loss 1.7518819570541382
iteration 0, loss 1.693137526512146
iteration 100, loss 1.7764298915863037
iteration 200, loss 1.6967623233795166
iteration 300, loss 1.669506549835205
iteration 400, loss 1.782002568244934
iteration 500, loss 1.7157652378082275
iteration 600, loss 1.7650145292282104
iteration 700, loss 1.7605440616607666
iteration 800, loss 1.744031548500061
iteration 0, loss 1.7470232248306274
iteration 100, loss 1.6918703317642212
iteration 200, loss 1.7933287620544434
iteration 300, loss 1.7396821975708008
iteration 400, loss 1.7184064388275146
iteration 500, loss 1.7435715198516846
iteration 600, loss 1.770892858505249
iteration 700, loss 1.7705752849578857
iteration 800, loss 1.7126778364181519
iteration 0, loss 1.714950442314148
iteration 100, loss 1.7988488674163818
iteration 200, loss 1.7656259536743164
iteration 300, loss 1.743982195854187
iteration 400, loss 1.7010581493377686
iteration 500, loss 1.6259011030197144
iteration 600, loss 1.7134569883346558
iteration 700, loss 1.6872823238372803
iteration 800, loss 1.7410672903060913
iteration 0, loss 1.7587672472000122
iteration 100, loss 1.68625009059906
iteration 200, loss 1.7989232540130615
iteration 300, loss 1.724343180656433
iteration 400, loss 1.7489261627197266
iteration 500, loss 1.6879065036773682
iteration 600, loss 1.6528935432434082
iteration 700, loss 1.7373992204666138
iteration 800, loss 1.7830158472061157
iteration 0, loss 1.7004486322402954
iteration 100, loss 1.7692731618881226
iteration 200, loss 1.706457495689392
iteration 300, loss 1.770340085029602
iteration 400, loss 1.8005881309509277
iteration 500, loss 1.7345478534698486
iteration 600, loss 1.7799142599105835
iteration 700, loss 1.7086042165756226
iteration 800, loss 1.7966487407684326
iteration 0, loss 1.7401707172393799
iteration 100, loss 1.6911652088165283
iteration 200, loss 1.753509759902954
iteration 300, loss 1.7252393960952759
iteration 400, loss 1.719232201576233
iteration 500, loss 1.6982247829437256
iteration 600, loss 1.7641044855117798
iteration 700, loss 1.8128949403762817
iteration 800, loss 1.7615830898284912
iteration 0, loss 1.7127578258514404
iteration 100, loss 1.639639973640442
iteration 200, loss 1.6844685077667236
iteration 300, loss 1.675430417060852
iteration 400, loss 1.753941535949707
iteration 500, loss 1.7797188758850098
iteration 600, loss 1.724186658859253
iteration 700, loss 1.8147258758544922
iteration 800, loss 1.730617880821228
iteration 0, loss 1.7646212577819824
iteration 100, loss 1.7130573987960815
iteration 200, loss 1.7102603912353516
iteration 300, loss 1.730721354484558
iteration 400, loss 1.695666790008545
iteration 500, loss 1.7134121656417847
iteration 600, loss 1.667424201965332
iteration 700, loss 1.6695140600204468
iteration 800, loss 1.7514457702636719
iteration 0, loss 1.6532875299453735
iteration 100, loss 1.7233301401138306
iteration 200, loss 1.7093594074249268
iteration 300, loss 1.7414246797561646
iteration 400, loss 1.7566981315612793
iteration 500, loss 1.7361711263656616
iteration 600, loss 1.7536039352416992
iteration 700, loss 1.7435603141784668
iteration 800, loss 1.8036184310913086
iteration 0, loss 1.6756144762039185
iteration 100, loss 1.6731336116790771
iteration 200, loss 1.7231829166412354
iteration 300, loss 1.7638269662857056
iteration 400, loss 1.7618907690048218
iteration 500, loss 1.747106909751892
iteration 600, loss 1.7616554498672485
iteration 700, loss 1.7173970937728882
iteration 800, loss 1.7098220586776733
iteration 0, loss 1.746605396270752
iteration 100, loss 1.8055806159973145
iteration 200, loss 1.727182149887085
iteration 300, loss 1.8057546615600586
iteration 400, loss 1.738116979598999
iteration 500, loss 1.7982491254806519
iteration 600, loss 1.7689828872680664
iteration 700, loss 1.7013320922851562
iteration 800, loss 1.6824393272399902
iteration 0, loss 1.7507505416870117
iteration 100, loss 1.6616078615188599
iteration 200, loss 1.7784229516983032
iteration 300, loss 1.7182203531265259
iteration 400, loss 1.7383171319961548
iteration 500, loss 1.7002469301223755
iteration 600, loss 1.688003659248352
iteration 700, loss 1.7514057159423828
iteration 800, loss 1.766834020614624
iteration 0, loss 1.7451268434524536
iteration 100, loss 1.7388367652893066
iteration 200, loss 1.7145946025848389
iteration 300, loss 1.7113752365112305
iteration 400, loss 1.770681381225586
iteration 500, loss 1.658275842666626
iteration 600, loss 1.6806000471115112
iteration 700, loss 1.692602276802063
iteration 800, loss 1.7459365129470825
iteration 0, loss 1.7265913486480713
iteration 100, loss 1.728402018547058
iteration 200, loss 1.7040618658065796
iteration 300, loss 1.7866286039352417
iteration 400, loss 1.7313976287841797
iteration 500, loss 1.7081239223480225
iteration 600, loss 1.7834608554840088
iteration 700, loss 1.7051516771316528
iteration 800, loss 1.7335783243179321
iteration 0, loss 1.7447021007537842
iteration 100, loss 1.7556172609329224
iteration 200, loss 1.7390435934066772
iteration 300, loss 1.6583755016326904
iteration 400, loss 1.6794929504394531
iteration 500, loss 1.7616348266601562
iteration 600, loss 1.721239686012268
iteration 700, loss 1.7386260032653809
iteration 800, loss 1.7082353830337524
iteration 0, loss 1.7171412706375122
iteration 100, loss 1.7048569917678833
iteration 200, loss 1.7706453800201416
iteration 300, loss 1.7383102178573608
iteration 400, loss 1.7775615453720093
iteration 500, loss 1.7866016626358032
iteration 600, loss 1.7314680814743042
iteration 700, loss 1.8111120462417603
iteration 800, loss 1.7198995351791382
iteration 0, loss 1.7050669193267822
iteration 100, loss 1.8302757740020752
iteration 200, loss 1.6724529266357422
iteration 300, loss 1.673488736152649
iteration 400, loss 1.7453373670578003
iteration 500, loss 1.660697102546692
iteration 600, loss 1.7440900802612305
iteration 700, loss 1.7543319463729858
iteration 800, loss 1.7238895893096924
iteration 0, loss 1.7794922590255737
iteration 100, loss 1.7091906070709229
iteration 200, loss 1.8150354623794556
iteration 300, loss 1.7625623941421509
iteration 400, loss 1.7602072954177856
iteration 500, loss 1.739916205406189
iteration 600, loss 1.702500343322754
iteration 700, loss 1.713477373123169
iteration 800, loss 1.6891391277313232
iteration 0, loss 1.7123732566833496
iteration 100, loss 1.7632203102111816
iteration 200, loss 1.7239819765090942
iteration 300, loss 1.8019349575042725
iteration 400, loss 1.78219473361969
iteration 500, loss 1.6860857009887695
iteration 600, loss 1.738930106163025
iteration 700, loss 1.7136563062667847
iteration 800, loss 1.7065273523330688
iteration 0, loss 1.6256673336029053
iteration 100, loss 1.6921924352645874
iteration 200, loss 1.723533272743225
iteration 300, loss 1.711552381515503
iteration 400, loss 1.7640453577041626
iteration 500, loss 1.7469037771224976
iteration 600, loss 1.748234748840332
iteration 700, loss 1.7288589477539062
iteration 800, loss 1.7843202352523804
iteration 0, loss 1.7576898336410522
iteration 100, loss 1.7528817653656006
iteration 200, loss 1.7517552375793457
iteration 300, loss 1.7706793546676636
iteration 400, loss 1.732913851737976
iteration 500, loss 1.7622027397155762
iteration 600, loss 1.7572449445724487
iteration 700, loss 1.7432092428207397
iteration 800, loss 1.789228916168213
iteration 0, loss 1.722999095916748
iteration 100, loss 1.747645378112793
iteration 200, loss 1.7662626504898071
iteration 300, loss 1.8094160556793213
iteration 400, loss 1.7581257820129395
iteration 500, loss 1.7462555170059204
iteration 600, loss 1.7696456909179688
iteration 700, loss 1.6773815155029297
iteration 800, loss 1.6834521293640137
iteration 0, loss 1.7433366775512695
iteration 100, loss 1.8047256469726562
iteration 200, loss 1.7129119634628296
iteration 300, loss 1.6906543970108032
iteration 400, loss 1.8276498317718506
iteration 500, loss 1.698911190032959
iteration 600, loss 1.7852113246917725
iteration 700, loss 1.73703932762146
iteration 800, loss 1.703717589378357
fold 0 accuracy: 0.6077142857142858
iteration 0, loss 1.7318271398544312
iteration 100, loss 1.744476079940796
iteration 200, loss 1.7050808668136597
iteration 300, loss 1.7551161050796509
iteration 400, loss 1.818887710571289
iteration 500, loss 1.6787582635879517
iteration 600, loss 1.7480050325393677
iteration 700, loss 1.7219030857086182
iteration 800, loss 1.7424023151397705
iteration 0, loss 1.748782753944397
iteration 100, loss 1.7502000331878662
iteration 200, loss 1.7514804601669312
iteration 300, loss 1.7536194324493408
iteration 400, loss 1.7185144424438477
iteration 500, loss 1.746884822845459
iteration 600, loss 1.6879849433898926
iteration 700, loss 1.6671448945999146
iteration 800, loss 1.7313892841339111
iteration 0, loss 1.7119066715240479
iteration 100, loss 1.8464550971984863
iteration 200, loss 1.6915911436080933
iteration 300, loss 1.74604070186615
iteration 400, loss 1.7296991348266602
iteration 500, loss 1.7811599969863892
iteration 600, loss 1.7271323204040527
iteration 700, loss 1.795765995979309
iteration 800, loss 1.7200347185134888
iteration 0, loss 1.7594648599624634
iteration 100, loss 1.7921956777572632
iteration 200, loss 1.7253340482711792
iteration 300, loss 1.706174373626709
iteration 400, loss 1.664892554283142
iteration 500, loss 1.738695502281189
iteration 600, loss 1.688105821609497
iteration 700, loss 1.7167021036148071
iteration 800, loss 1.7006723880767822
iteration 0, loss 1.7625095844268799
iteration 100, loss 1.677930474281311
iteration 200, loss 1.6776281595230103
iteration 300, loss 1.6975231170654297
iteration 400, loss 1.7536141872406006
iteration 500, loss 1.6886972188949585
iteration 600, loss 1.7265526056289673
iteration 700, loss 1.6983180046081543
iteration 800, loss 1.7596352100372314
iteration 0, loss 1.724910020828247
iteration 100, loss 1.8082811832427979
iteration 200, loss 1.6526165008544922
iteration 300, loss 1.7418742179870605
iteration 400, loss 1.7769899368286133
iteration 500, loss 1.8188374042510986
iteration 600, loss 1.7388330698013306
iteration 700, loss 1.795875906944275
iteration 800, loss 1.7320303916931152
iteration 0, loss 1.7033356428146362
iteration 100, loss 1.7121427059173584
iteration 200, loss 1.730776309967041
iteration 300, loss 1.7333208322525024
iteration 400, loss 1.718871831893921
iteration 500, loss 1.7612762451171875
iteration 600, loss 1.6690611839294434
iteration 700, loss 1.8338407278060913
iteration 800, loss 1.7891247272491455
iteration 0, loss 1.6617085933685303
iteration 100, loss 1.792608618736267
iteration 200, loss 1.7015657424926758
iteration 300, loss 1.6227482557296753
iteration 400, loss 1.725768804550171
iteration 500, loss 1.7636041641235352
iteration 600, loss 1.784486174583435
iteration 700, loss 1.788162350654602
iteration 800, loss 1.7623225450515747
iteration 0, loss 1.8029935359954834
iteration 100, loss 1.7304363250732422
iteration 200, loss 1.7428481578826904
iteration 300, loss 1.7087606191635132
iteration 400, loss 1.7120460271835327
iteration 500, loss 1.728196620941162
iteration 600, loss 1.7791330814361572
iteration 700, loss 1.72109055519104
iteration 800, loss 1.6679794788360596
iteration 0, loss 1.758665919303894
iteration 100, loss 1.778294324874878
iteration 200, loss 1.7702559232711792
iteration 300, loss 1.6998885869979858
iteration 400, loss 1.7422449588775635
iteration 500, loss 1.7734527587890625
iteration 600, loss 1.6661443710327148
iteration 700, loss 1.6870315074920654
iteration 800, loss 1.797663688659668
iteration 0, loss 1.727408766746521
iteration 100, loss 1.7273145914077759
iteration 200, loss 1.7693467140197754
iteration 300, loss 1.6958132982254028
iteration 400, loss 1.7105106115341187
iteration 500, loss 1.7379592657089233
iteration 600, loss 1.7713631391525269
iteration 700, loss 1.6732573509216309
iteration 800, loss 1.8425803184509277
iteration 0, loss 1.720333456993103
iteration 100, loss 1.750542402267456
iteration 200, loss 1.8072352409362793
iteration 300, loss 1.7385636568069458
iteration 400, loss 1.8011670112609863
iteration 500, loss 1.7084320783615112
iteration 600, loss 1.7376667261123657
iteration 700, loss 1.721584439277649
iteration 800, loss 1.7274277210235596
iteration 0, loss 1.7515162229537964
iteration 100, loss 1.7884961366653442
iteration 200, loss 1.6945607662200928
iteration 300, loss 1.6728376150131226
iteration 400, loss 1.7619284391403198
iteration 500, loss 1.6754505634307861
iteration 600, loss 1.7806586027145386
iteration 700, loss 1.7193161249160767
iteration 800, loss 1.754804253578186
iteration 0, loss 1.7118268013000488
iteration 100, loss 1.7051805257797241
iteration 200, loss 1.7425830364227295
iteration 300, loss 1.6708426475524902
iteration 400, loss 1.6276845932006836
iteration 500, loss 1.772745966911316
iteration 600, loss 1.8094379901885986
iteration 700, loss 1.762561559677124
iteration 800, loss 1.6909022331237793
iteration 0, loss 1.7642630338668823
iteration 100, loss 1.6582685708999634
iteration 200, loss 1.7456084489822388
iteration 300, loss 1.720730185508728
iteration 400, loss 1.6207317113876343
iteration 500, loss 1.7345845699310303
iteration 600, loss 1.807422161102295
iteration 700, loss 1.7092225551605225
iteration 800, loss 1.7328256368637085
iteration 0, loss 1.6591836214065552
iteration 100, loss 1.7298883199691772
iteration 200, loss 1.7949715852737427
iteration 300, loss 1.6729936599731445
iteration 400, loss 1.7069642543792725
iteration 500, loss 1.7813891172409058
iteration 600, loss 1.735944390296936
iteration 700, loss 1.772434115409851
iteration 800, loss 1.7496929168701172
iteration 0, loss 1.692206859588623
iteration 100, loss 1.7065311670303345
iteration 200, loss 1.7616369724273682
iteration 300, loss 1.7321927547454834
iteration 400, loss 1.7282384634017944
iteration 500, loss 1.7605801820755005
iteration 600, loss 1.7952368259429932
iteration 700, loss 1.699809193611145
iteration 800, loss 1.7306150197982788
iteration 0, loss 1.7119412422180176
iteration 100, loss 1.7366937398910522
iteration 200, loss 1.663811445236206
iteration 300, loss 1.7307908535003662
iteration 400, loss 1.7201358079910278
iteration 500, loss 1.704025149345398
iteration 600, loss 1.7440742254257202
iteration 700, loss 1.7325117588043213
iteration 800, loss 1.7862502336502075
iteration 0, loss 1.678512692451477
iteration 100, loss 1.6883217096328735
iteration 200, loss 1.7481112480163574
iteration 300, loss 1.7784682512283325
iteration 400, loss 1.6906752586364746
iteration 500, loss 1.74951171875
iteration 600, loss 1.788794755935669
iteration 700, loss 1.755871295928955
iteration 800, loss 1.7116972208023071
iteration 0, loss 1.6901196241378784
iteration 100, loss 1.7557225227355957
iteration 200, loss 1.725972056388855
iteration 300, loss 1.6995314359664917
iteration 400, loss 1.6320420503616333
iteration 500, loss 1.7132843732833862
iteration 600, loss 1.730307698249817
iteration 700, loss 1.6728016138076782
iteration 800, loss 1.7769051790237427
iteration 0, loss 1.742484450340271
iteration 100, loss 1.7825706005096436
iteration 200, loss 1.6691480875015259
iteration 300, loss 1.6922242641448975
iteration 400, loss 1.7456952333450317
iteration 500, loss 1.732502818107605
iteration 600, loss 1.7305419445037842
iteration 700, loss 1.7578154802322388
iteration 800, loss 1.6463384628295898
iteration 0, loss 1.6917026042938232
iteration 100, loss 1.7784247398376465
iteration 200, loss 1.7204642295837402
iteration 300, loss 1.7025548219680786
iteration 400, loss 1.7277597188949585
iteration 500, loss 1.7289941310882568
iteration 600, loss 1.6677284240722656
iteration 700, loss 1.6672948598861694
iteration 800, loss 1.6929346323013306
iteration 0, loss 1.7053773403167725
iteration 100, loss 1.7776243686676025
iteration 200, loss 1.6722992658615112
iteration 300, loss 1.688188910484314
iteration 400, loss 1.662615180015564
iteration 500, loss 1.7315007448196411
iteration 600, loss 1.7231348752975464
iteration 700, loss 1.6350469589233398
iteration 800, loss 1.6930599212646484
iteration 0, loss 1.704507827758789
iteration 100, loss 1.7087485790252686
iteration 200, loss 1.6712554693222046
iteration 300, loss 1.8529722690582275
iteration 400, loss 1.6791399717330933
iteration 500, loss 1.7203903198242188
iteration 600, loss 1.704756736755371
iteration 700, loss 1.7318496704101562
iteration 800, loss 1.7276451587677002
iteration 0, loss 1.7563400268554688
iteration 100, loss 1.6802942752838135
iteration 200, loss 1.7379631996154785
iteration 300, loss 1.6793115139007568
iteration 400, loss 1.6769306659698486
iteration 500, loss 1.7469887733459473
iteration 600, loss 1.740586757659912
iteration 700, loss 1.7015838623046875
iteration 800, loss 1.7139153480529785
iteration 0, loss 1.7097676992416382
iteration 100, loss 1.7069528102874756
iteration 200, loss 1.7486315965652466
iteration 300, loss 1.828079104423523
iteration 400, loss 1.7206872701644897
iteration 500, loss 1.726245641708374
iteration 600, loss 1.7215206623077393
iteration 700, loss 1.6329474449157715
iteration 800, loss 1.7611360549926758
iteration 0, loss 1.7344141006469727
iteration 100, loss 1.6528897285461426
iteration 200, loss 1.7459098100662231
iteration 300, loss 1.697749137878418
iteration 400, loss 1.6505544185638428
iteration 500, loss 1.7656832933425903
iteration 600, loss 1.6678717136383057
iteration 700, loss 1.7479883432388306
iteration 800, loss 1.68474280834198
iteration 0, loss 1.7526862621307373
iteration 100, loss 1.7949848175048828
iteration 200, loss 1.728311538696289
iteration 300, loss 1.7903833389282227
iteration 400, loss 1.7003387212753296
iteration 500, loss 1.7786985635757446
iteration 600, loss 1.7249221801757812
iteration 700, loss 1.7384159564971924
iteration 800, loss 1.7174112796783447
iteration 0, loss 1.731575608253479
iteration 100, loss 1.6636033058166504
iteration 200, loss 1.7234615087509155
iteration 300, loss 1.7018470764160156
iteration 400, loss 1.8250125646591187
iteration 500, loss 1.7067745923995972
iteration 600, loss 1.7173513174057007
iteration 700, loss 1.7838159799575806
iteration 800, loss 1.7073166370391846
iteration 0, loss 1.6861933469772339
iteration 100, loss 1.6991262435913086
iteration 200, loss 1.6954904794692993
iteration 300, loss 1.7035971879959106
iteration 400, loss 1.7382222414016724
iteration 500, loss 1.7655141353607178
iteration 600, loss 1.7414703369140625
iteration 700, loss 1.757856011390686
iteration 800, loss 1.7292662858963013
iteration 0, loss 1.7049474716186523
iteration 100, loss 1.6575628519058228
iteration 200, loss 1.6927316188812256
iteration 300, loss 1.7405421733856201
iteration 400, loss 1.7202939987182617
iteration 500, loss 1.6819357872009277
iteration 600, loss 1.7549792528152466
iteration 700, loss 1.680275321006775
iteration 800, loss 1.772867202758789
iteration 0, loss 1.7230713367462158
iteration 100, loss 1.7526522874832153
iteration 200, loss 1.6990783214569092
iteration 300, loss 1.7025556564331055
iteration 400, loss 1.7430684566497803
iteration 500, loss 1.7233552932739258
iteration 600, loss 1.751490831375122
iteration 700, loss 1.7288419008255005
iteration 800, loss 1.8289819955825806
iteration 0, loss 1.65090811252594
iteration 100, loss 1.7422215938568115
iteration 200, loss 1.7200276851654053
iteration 300, loss 1.6839946508407593
iteration 400, loss 1.6945024728775024
iteration 500, loss 1.7444080114364624
iteration 600, loss 1.7580538988113403
iteration 700, loss 1.759159803390503
iteration 800, loss 1.597480297088623
iteration 0, loss 1.6863055229187012
iteration 100, loss 1.7126225233078003
iteration 200, loss 1.6840475797653198
iteration 300, loss 1.7167423963546753
iteration 400, loss 1.7405973672866821
iteration 500, loss 1.7305498123168945
iteration 600, loss 1.6763861179351807
iteration 700, loss 1.7098387479782104
iteration 800, loss 1.6728943586349487
iteration 0, loss 1.7664763927459717
iteration 100, loss 1.6568553447723389
iteration 200, loss 1.7482274770736694
iteration 300, loss 1.7496427297592163
iteration 400, loss 1.7068966627120972
iteration 500, loss 1.6658921241760254
iteration 600, loss 1.7870893478393555
iteration 700, loss 1.6700514554977417
iteration 800, loss 1.6969963312149048
iteration 0, loss 1.6616380214691162
iteration 100, loss 1.726199746131897
iteration 200, loss 1.7611420154571533
iteration 300, loss 1.6836475133895874
iteration 400, loss 1.7868945598602295
iteration 500, loss 1.7194162607192993
iteration 600, loss 1.6942167282104492
iteration 700, loss 1.668561577796936
iteration 800, loss 1.633470892906189
iteration 0, loss 1.7369478940963745
iteration 100, loss 1.6719273328781128
iteration 200, loss 1.6562658548355103
iteration 300, loss 1.7885886430740356
iteration 400, loss 1.6984965801239014
iteration 500, loss 1.7595558166503906
iteration 600, loss 1.7583467960357666
iteration 700, loss 1.7890024185180664
iteration 800, loss 1.701301097869873
iteration 0, loss 1.7587474584579468
iteration 100, loss 1.6882277727127075
iteration 200, loss 1.652624487876892
iteration 300, loss 1.7473984956741333
iteration 400, loss 1.6769009828567505
iteration 500, loss 1.7464942932128906
iteration 600, loss 1.732904076576233
iteration 700, loss 1.7338618040084839
iteration 800, loss 1.806236982345581
iteration 0, loss 1.6944645643234253
iteration 100, loss 1.6934961080551147
iteration 200, loss 1.668667197227478
iteration 300, loss 1.729661464691162
iteration 400, loss 1.7682470083236694
iteration 500, loss 1.6884342432022095
iteration 600, loss 1.7052174806594849
iteration 700, loss 1.7296416759490967
iteration 800, loss 1.6836650371551514
iteration 0, loss 1.729928970336914
iteration 100, loss 1.6999667882919312
iteration 200, loss 1.7366814613342285
iteration 300, loss 1.7311660051345825
iteration 400, loss 1.6217988729476929
iteration 500, loss 1.7471086978912354
iteration 600, loss 1.7004467248916626
iteration 700, loss 1.6524178981781006
iteration 800, loss 1.669542908668518
iteration 0, loss 1.6871192455291748
iteration 100, loss 1.7194968461990356
iteration 200, loss 1.67751145362854
iteration 300, loss 1.73614501953125
iteration 400, loss 1.8385353088378906
iteration 500, loss 1.718401551246643
iteration 600, loss 1.7399810552597046
iteration 700, loss 1.7248902320861816
iteration 800, loss 1.7053637504577637
iteration 0, loss 1.6972094774246216
iteration 100, loss 1.7841832637786865
iteration 200, loss 1.7468516826629639
iteration 300, loss 1.7342102527618408
iteration 400, loss 1.710687518119812
iteration 500, loss 1.700109601020813
iteration 600, loss 1.6792652606964111
iteration 700, loss 1.7725870609283447
iteration 800, loss 1.7386337518692017
iteration 0, loss 1.7076963186264038
iteration 100, loss 1.7314627170562744
iteration 200, loss 1.7193373441696167
iteration 300, loss 1.6749058961868286
iteration 400, loss 1.7245310544967651
iteration 500, loss 1.7592414617538452
iteration 600, loss 1.7028979063034058
iteration 700, loss 1.7031314373016357
iteration 800, loss 1.6980195045471191
iteration 0, loss 1.737350583076477
iteration 100, loss 1.6859347820281982
iteration 200, loss 1.764902949333191
iteration 300, loss 1.7563315629959106
iteration 400, loss 1.690637469291687
iteration 500, loss 1.7157179117202759
iteration 600, loss 1.7362979650497437
iteration 700, loss 1.6871888637542725
iteration 800, loss 1.6685707569122314
iteration 0, loss 1.7054550647735596
iteration 100, loss 1.676080346107483
iteration 200, loss 1.7551909685134888
iteration 300, loss 1.7255079746246338
iteration 400, loss 1.7519052028656006
iteration 500, loss 1.722005844116211
iteration 600, loss 1.6921427249908447
iteration 700, loss 1.7061972618103027
iteration 800, loss 1.667519211769104
iteration 0, loss 1.7708466053009033
iteration 100, loss 1.7350988388061523
iteration 200, loss 1.6471139192581177
iteration 300, loss 1.6670093536376953
iteration 400, loss 1.7369731664657593
iteration 500, loss 1.7889677286148071
iteration 600, loss 1.784067153930664
iteration 700, loss 1.6676512956619263
iteration 800, loss 1.7463890314102173
iteration 0, loss 1.666548490524292
iteration 100, loss 1.6984273195266724
iteration 200, loss 1.7661824226379395
iteration 300, loss 1.6032110452651978
iteration 400, loss 1.7593159675598145
iteration 500, loss 1.7341505289077759
iteration 600, loss 1.7061575651168823
iteration 700, loss 1.7467072010040283
iteration 800, loss 1.6479419469833374
iteration 0, loss 1.770172119140625
iteration 100, loss 1.801859974861145
iteration 200, loss 1.7752307653427124
iteration 300, loss 1.7198776006698608
iteration 400, loss 1.7744505405426025
iteration 500, loss 1.6446120738983154
iteration 600, loss 1.702752947807312
iteration 700, loss 1.6374976634979248
iteration 800, loss 1.6721469163894653
iteration 0, loss 1.69678795337677
iteration 100, loss 1.7082222700119019
iteration 200, loss 1.75336492061615
iteration 300, loss 1.7374194860458374
iteration 400, loss 1.770734429359436
iteration 500, loss 1.700977087020874
iteration 600, loss 1.6946449279785156
iteration 700, loss 1.6386990547180176
iteration 800, loss 1.7717251777648926
iteration 0, loss 1.7208524942398071
iteration 100, loss 1.779015064239502
iteration 200, loss 1.7313190698623657
iteration 300, loss 1.7717969417572021
iteration 400, loss 1.7571663856506348
iteration 500, loss 1.686420202255249
iteration 600, loss 1.6898397207260132
iteration 700, loss 1.7286545038223267
iteration 800, loss 1.7271697521209717
fold 1 accuracy: 0.6389285714285714
iteration 0, loss 1.6921496391296387
iteration 100, loss 1.7293951511383057
iteration 200, loss 1.7160406112670898
iteration 300, loss 1.768657922744751
iteration 400, loss 1.7244632244110107
iteration 500, loss 1.7281322479248047
iteration 600, loss 1.8554298877716064
iteration 700, loss 1.694785475730896
iteration 800, loss 1.7226028442382812
iteration 0, loss 1.684543251991272
iteration 100, loss 1.6808102130889893
iteration 200, loss 1.7785372734069824
iteration 300, loss 1.7138828039169312
iteration 400, loss 1.6928166151046753
iteration 500, loss 1.669919729232788
iteration 600, loss 1.727217197418213
iteration 700, loss 1.7861558198928833
iteration 800, loss 1.721043586730957
iteration 0, loss 1.712862253189087
iteration 100, loss 1.7763763666152954
iteration 200, loss 1.7006117105484009
iteration 300, loss 1.715466022491455
iteration 400, loss 1.7488561868667603
iteration 500, loss 1.7350349426269531
iteration 600, loss 1.7368648052215576
iteration 700, loss 1.705242395401001
iteration 800, loss 1.7166651487350464
iteration 0, loss 1.683472990989685
iteration 100, loss 1.742055058479309
iteration 200, loss 1.6984901428222656
iteration 300, loss 1.6875249147415161
iteration 400, loss 1.7314660549163818
iteration 500, loss 1.7432172298431396
iteration 600, loss 1.7288529872894287
iteration 700, loss 1.7209124565124512
iteration 800, loss 1.647039771080017
iteration 0, loss 1.7140339612960815
iteration 100, loss 1.7394742965698242
iteration 200, loss 1.696535587310791
iteration 300, loss 1.7758283615112305
iteration 400, loss 1.730615496635437
iteration 500, loss 1.7449849843978882
iteration 600, loss 1.6708921194076538
iteration 700, loss 1.7429684400558472
iteration 800, loss 1.724564552307129
iteration 0, loss 1.704267978668213
iteration 100, loss 1.6911735534667969
iteration 200, loss 1.690858006477356
iteration 300, loss 1.7930233478546143
iteration 400, loss 1.649223804473877
iteration 500, loss 1.696968674659729
iteration 600, loss 1.6883835792541504
iteration 700, loss 1.707167148590088
iteration 800, loss 1.7360831499099731
iteration 0, loss 1.7296909093856812
iteration 100, loss 1.6814813613891602
iteration 200, loss 1.6718876361846924
iteration 300, loss 1.7154103517532349
iteration 400, loss 1.724609613418579
iteration 500, loss 1.798145055770874
iteration 600, loss 1.7358897924423218
iteration 700, loss 1.7182555198669434
iteration 800, loss 1.776861310005188
iteration 0, loss 1.7197515964508057
iteration 100, loss 1.6827512979507446
iteration 200, loss 1.6834709644317627
iteration 300, loss 1.7323057651519775
iteration 400, loss 1.6784213781356812
iteration 500, loss 1.7016061544418335
iteration 600, loss 1.6545989513397217
iteration 700, loss 1.7063491344451904
iteration 800, loss 1.7655164003372192
iteration 0, loss 1.6315522193908691
iteration 100, loss 1.6335062980651855
iteration 200, loss 1.7570806741714478
iteration 300, loss 1.6888275146484375
iteration 400, loss 1.6903969049453735
iteration 500, loss 1.6629374027252197
iteration 600, loss 1.7419278621673584
iteration 700, loss 1.671536922454834
iteration 800, loss 1.7772331237792969
iteration 0, loss 1.6742572784423828
iteration 100, loss 1.6990113258361816
iteration 200, loss 1.7131752967834473
iteration 300, loss 1.6780906915664673
iteration 400, loss 1.7207021713256836
iteration 500, loss 1.6963636875152588
iteration 600, loss 1.6290411949157715
iteration 700, loss 1.7276453971862793
iteration 800, loss 1.7308729887008667
iteration 0, loss 1.7432518005371094
iteration 100, loss 1.6760215759277344
iteration 200, loss 1.7397328615188599
iteration 300, loss 1.684541940689087
iteration 400, loss 1.7502150535583496
iteration 500, loss 1.764440894126892
iteration 600, loss 1.6811528205871582
iteration 700, loss 1.7227256298065186
iteration 800, loss 1.7308346033096313
iteration 0, loss 1.667007565498352
iteration 100, loss 1.7019699811935425
iteration 200, loss 1.7113065719604492
iteration 300, loss 1.664533257484436
iteration 400, loss 1.6517846584320068
iteration 500, loss 1.6470366716384888
iteration 600, loss 1.7406792640686035
iteration 700, loss 1.7167065143585205
iteration 800, loss 1.771786093711853
iteration 0, loss 1.7283761501312256
iteration 100, loss 1.7560336589813232
iteration 200, loss 1.6353973150253296
iteration 300, loss 1.7606873512268066
iteration 400, loss 1.6600130796432495
iteration 500, loss 1.7343623638153076
iteration 600, loss 1.6872347593307495
iteration 700, loss 1.698789119720459
iteration 800, loss 1.771528720855713
iteration 0, loss 1.7140554189682007
iteration 100, loss 1.7280597686767578
iteration 200, loss 1.7687718868255615
iteration 300, loss 1.7351202964782715
iteration 400, loss 1.737800121307373
iteration 500, loss 1.7406728267669678
iteration 600, loss 1.7296206951141357
iteration 700, loss 1.7273730039596558
iteration 800, loss 1.8259224891662598
iteration 0, loss 1.7202043533325195
iteration 100, loss 1.6680481433868408
iteration 200, loss 1.6118515729904175
iteration 300, loss 1.8146743774414062
iteration 400, loss 1.731758952140808
iteration 500, loss 1.7010468244552612
iteration 600, loss 1.6440787315368652
iteration 700, loss 1.7164160013198853
iteration 800, loss 1.7553056478500366
iteration 0, loss 1.6835237741470337
iteration 100, loss 1.7299013137817383
iteration 200, loss 1.765852451324463
iteration 300, loss 1.6441079378128052
iteration 400, loss 1.704680323600769
iteration 500, loss 1.7353757619857788
iteration 600, loss 1.702932596206665
iteration 700, loss 1.716831088066101
iteration 800, loss 1.6828532218933105
iteration 0, loss 1.7492436170578003
iteration 100, loss 1.7710614204406738
iteration 200, loss 1.7190325260162354
iteration 300, loss 1.6735073328018188
iteration 400, loss 1.6649861335754395
iteration 500, loss 1.6184442043304443
iteration 600, loss 1.7446774244308472
iteration 700, loss 1.7792199850082397
iteration 800, loss 1.7399537563323975
iteration 0, loss 1.710237979888916
iteration 100, loss 1.6692543029785156
iteration 200, loss 1.6254934072494507
iteration 300, loss 1.6860365867614746
iteration 400, loss 1.7159090042114258
iteration 500, loss 1.6979106664657593
iteration 600, loss 1.770121455192566
iteration 700, loss 1.7222923040390015
iteration 800, loss 1.774064302444458
iteration 0, loss 1.7201060056686401
iteration 100, loss 1.7529442310333252
iteration 200, loss 1.7066556215286255
iteration 300, loss 1.7462425231933594
iteration 400, loss 1.7167472839355469
iteration 500, loss 1.721743106842041
iteration 600, loss 1.665589451789856
iteration 700, loss 1.6899715662002563
iteration 800, loss 1.7292560338974
iteration 0, loss 1.732224464416504
iteration 100, loss 1.6860545873641968
iteration 200, loss 1.7745245695114136
iteration 300, loss 1.6952511072158813
iteration 400, loss 1.7814689874649048
iteration 500, loss 1.714151382446289
iteration 600, loss 1.7937917709350586
iteration 700, loss 1.7166517972946167
iteration 800, loss 1.7008566856384277
iteration 0, loss 1.716503381729126
iteration 100, loss 1.7768899202346802
iteration 200, loss 1.6969603300094604
iteration 300, loss 1.7834347486495972
iteration 400, loss 1.6850087642669678
iteration 500, loss 1.7944574356079102
iteration 600, loss 1.6670825481414795
iteration 700, loss 1.676911473274231
iteration 800, loss 1.7367271184921265
iteration 0, loss 1.7225993871688843
iteration 100, loss 1.7450748682022095
iteration 200, loss 1.7142459154129028
iteration 300, loss 1.7550843954086304
iteration 400, loss 1.7013728618621826
iteration 500, loss 1.7246122360229492
iteration 600, loss 1.7385225296020508
iteration 700, loss 1.6801204681396484
iteration 800, loss 1.6467872858047485
iteration 0, loss 1.7247945070266724
iteration 100, loss 1.7105748653411865
iteration 200, loss 1.7861353158950806
iteration 300, loss 1.6758784055709839
iteration 400, loss 1.789448857307434
iteration 500, loss 1.7164602279663086
iteration 600, loss 1.6949454545974731
iteration 700, loss 1.7489725351333618
iteration 800, loss 1.682664155960083
iteration 0, loss 1.73007071018219
iteration 100, loss 1.7104955911636353
iteration 200, loss 1.7036126852035522
iteration 300, loss 1.7421231269836426
iteration 400, loss 1.7722433805465698
iteration 500, loss 1.6050653457641602
iteration 600, loss 1.7159383296966553
iteration 700, loss 1.6973551511764526
iteration 800, loss 1.6850553750991821
iteration 0, loss 1.6840498447418213
iteration 100, loss 1.651672124862671
iteration 200, loss 1.6908729076385498
iteration 300, loss 1.806198000907898
iteration 400, loss 1.7406589984893799
iteration 500, loss 1.7071982622146606
iteration 600, loss 1.7628949880599976
iteration 700, loss 1.78666353225708
iteration 800, loss 1.7533241510391235
iteration 0, loss 1.7079622745513916
iteration 100, loss 1.6741375923156738
iteration 200, loss 1.8095425367355347
iteration 300, loss 1.6915103197097778
iteration 400, loss 1.717203140258789
iteration 500, loss 1.6877986192703247
iteration 600, loss 1.7510212659835815
iteration 700, loss 1.7570703029632568
iteration 800, loss 1.6506847143173218
iteration 0, loss 1.7111952304840088
iteration 100, loss 1.672334909439087
iteration 200, loss 1.7383135557174683
iteration 300, loss 1.746579885482788
iteration 400, loss 1.6929492950439453
iteration 500, loss 1.7426964044570923
iteration 600, loss 1.7254130840301514
iteration 700, loss 1.7592887878417969
iteration 800, loss 1.6701977252960205
iteration 0, loss 1.7054240703582764
iteration 100, loss 1.6886566877365112
iteration 200, loss 1.7522342205047607
iteration 300, loss 1.7999268770217896
iteration 400, loss 1.7386311292648315
iteration 500, loss 1.7336366176605225
iteration 600, loss 1.7927967309951782
iteration 700, loss 1.7301689386367798
iteration 800, loss 1.6821668148040771
iteration 0, loss 1.7032647132873535
iteration 100, loss 1.7231366634368896
iteration 200, loss 1.6676487922668457
iteration 300, loss 1.7174549102783203
iteration 400, loss 1.6619350910186768
iteration 500, loss 1.798748254776001
iteration 600, loss 1.6908679008483887
iteration 700, loss 1.7435613870620728
iteration 800, loss 1.7262611389160156
iteration 0, loss 1.7660112380981445
iteration 100, loss 1.7329257726669312
iteration 200, loss 1.7717459201812744
iteration 300, loss 1.7972779273986816
iteration 400, loss 1.6744858026504517
iteration 500, loss 1.692636489868164
iteration 600, loss 1.6762279272079468
iteration 700, loss 1.712243676185608
iteration 800, loss 1.6382167339324951
iteration 0, loss 1.643610954284668
iteration 100, loss 1.6843013763427734
iteration 200, loss 1.8101669549942017
iteration 300, loss 1.7443047761917114
iteration 400, loss 1.6752859354019165
iteration 500, loss 1.6927601099014282
iteration 600, loss 1.683349370956421
iteration 700, loss 1.6845911741256714
iteration 800, loss 1.7542660236358643
iteration 0, loss 1.7518749237060547
iteration 100, loss 1.7112890481948853
iteration 200, loss 1.7111995220184326
iteration 300, loss 1.7011066675186157
iteration 400, loss 1.6872615814208984
iteration 500, loss 1.7667347192764282
iteration 600, loss 1.7601162195205688
iteration 700, loss 1.7454458475112915
iteration 800, loss 1.6660054922103882
iteration 0, loss 1.6675188541412354
iteration 100, loss 1.745175838470459
iteration 200, loss 1.6800944805145264
iteration 300, loss 1.6943145990371704
iteration 400, loss 1.6974821090698242
iteration 500, loss 1.7021512985229492
iteration 600, loss 1.754601240158081
iteration 700, loss 1.7532709836959839
iteration 800, loss 1.7150728702545166
iteration 0, loss 1.5958292484283447
iteration 100, loss 1.7198996543884277
iteration 200, loss 1.727904200553894
iteration 300, loss 1.6914397478103638
iteration 400, loss 1.7008726596832275
iteration 500, loss 1.7678345441818237
iteration 600, loss 1.7940466403961182
iteration 700, loss 1.7659215927124023
iteration 800, loss 1.7838292121887207
iteration 0, loss 1.663524866104126
iteration 100, loss 1.7047743797302246
iteration 200, loss 1.7419179677963257
iteration 300, loss 1.7434892654418945
iteration 400, loss 1.7261054515838623
iteration 500, loss 1.7059451341629028
iteration 600, loss 1.6982711553573608
iteration 700, loss 1.7990357875823975
iteration 800, loss 1.7085344791412354
iteration 0, loss 1.8186945915222168
iteration 100, loss 1.7075600624084473
iteration 200, loss 1.8185349702835083
iteration 300, loss 1.7236642837524414
iteration 400, loss 1.7418488264083862
iteration 500, loss 1.691044807434082
iteration 600, loss 1.7454938888549805
iteration 700, loss 1.6786984205245972
iteration 800, loss 1.7015608549118042
iteration 0, loss 1.8051878213882446
iteration 100, loss 1.7144460678100586
iteration 200, loss 1.696808934211731
iteration 300, loss 1.724772572517395
iteration 400, loss 1.708177924156189
iteration 500, loss 1.693542242050171
iteration 600, loss 1.665513515472412
iteration 700, loss 1.7550595998764038
iteration 800, loss 1.7225546836853027
iteration 0, loss 1.7117477655410767
iteration 100, loss 1.7112681865692139
iteration 200, loss 1.6425530910491943
iteration 300, loss 1.7059299945831299
iteration 400, loss 1.7616196870803833
iteration 500, loss 1.7399625778198242
iteration 600, loss 1.68781316280365
iteration 700, loss 1.733681559562683
iteration 800, loss 1.6956117153167725
iteration 0, loss 1.6979787349700928
iteration 100, loss 1.6780461072921753
iteration 200, loss 1.7154641151428223
iteration 300, loss 1.7020988464355469
iteration 400, loss 1.7778581380844116
iteration 500, loss 1.8327465057373047
iteration 600, loss 1.7593871355056763
iteration 700, loss 1.691565990447998
iteration 800, loss 1.7937902212142944
iteration 0, loss 1.758209466934204
iteration 100, loss 1.7363791465759277
iteration 200, loss 1.700987458229065
iteration 300, loss 1.7449169158935547
iteration 400, loss 1.7436591386795044
iteration 500, loss 1.6674526929855347
iteration 600, loss 1.6379276514053345
iteration 700, loss 1.6542673110961914
iteration 800, loss 1.7660549879074097
iteration 0, loss 1.6693474054336548
iteration 100, loss 1.6636971235275269
iteration 200, loss 1.7006713151931763
iteration 300, loss 1.7389181852340698
iteration 400, loss 1.6990762948989868
iteration 500, loss 1.7265070676803589
iteration 600, loss 1.7329142093658447
iteration 700, loss 1.7452408075332642
iteration 800, loss 1.6722625494003296
iteration 0, loss 1.708248257637024
iteration 100, loss 1.7193557024002075
iteration 200, loss 1.777509331703186
iteration 300, loss 1.6892021894454956
iteration 400, loss 1.7380918264389038
iteration 500, loss 1.6938250064849854
iteration 600, loss 1.7123106718063354
iteration 700, loss 1.726082444190979
iteration 800, loss 1.730170726776123
iteration 0, loss 1.6910134553909302
iteration 100, loss 1.7419910430908203
iteration 200, loss 1.7571135759353638
iteration 300, loss 1.7198848724365234
iteration 400, loss 1.7105835676193237
iteration 500, loss 1.650460124015808
iteration 600, loss 1.818469524383545
iteration 700, loss 1.7194503545761108
iteration 800, loss 1.7375521659851074
iteration 0, loss 1.6506989002227783
iteration 100, loss 1.832116723060608
iteration 200, loss 1.6908763647079468
iteration 300, loss 1.663241982460022
iteration 400, loss 1.7208913564682007
iteration 500, loss 1.7028149366378784
iteration 600, loss 1.7561616897583008
iteration 700, loss 1.689493179321289
iteration 800, loss 1.7155277729034424
iteration 0, loss 1.745967149734497
iteration 100, loss 1.6164131164550781
iteration 200, loss 1.7895879745483398
iteration 300, loss 1.6916275024414062
iteration 400, loss 1.7618311643600464
iteration 500, loss 1.6663466691970825
iteration 600, loss 1.69166898727417
iteration 700, loss 1.8133093118667603
iteration 800, loss 1.7447998523712158
iteration 0, loss 1.6815671920776367
iteration 100, loss 1.6654949188232422
iteration 200, loss 1.727906346321106
iteration 300, loss 1.7808058261871338
iteration 400, loss 1.702304482460022
iteration 500, loss 1.6401481628417969
iteration 600, loss 1.7392792701721191
iteration 700, loss 1.707849144935608
iteration 800, loss 1.7019206285476685
iteration 0, loss 1.6836764812469482
iteration 100, loss 1.7653790712356567
iteration 200, loss 1.6710209846496582
iteration 300, loss 1.7262532711029053
iteration 400, loss 1.6822420358657837
iteration 500, loss 1.7975679636001587
iteration 600, loss 1.846055269241333
iteration 700, loss 1.7410566806793213
iteration 800, loss 1.7341712713241577
iteration 0, loss 1.712993860244751
iteration 100, loss 1.7520840167999268
iteration 200, loss 1.7308725118637085
iteration 300, loss 1.6727780103683472
iteration 400, loss 1.68451988697052
iteration 500, loss 1.7496281862258911
iteration 600, loss 1.6438519954681396
iteration 700, loss 1.7027112245559692
iteration 800, loss 1.7534502744674683
iteration 0, loss 1.7219069004058838
iteration 100, loss 1.7372033596038818
iteration 200, loss 1.713936686515808
iteration 300, loss 1.6514989137649536
iteration 400, loss 1.6488221883773804
iteration 500, loss 1.7237958908081055
iteration 600, loss 1.6446210145950317
iteration 700, loss 1.72258722782135
iteration 800, loss 1.7020267248153687
iteration 0, loss 1.7200678586959839
iteration 100, loss 1.6953672170639038
iteration 200, loss 1.7185779809951782
iteration 300, loss 1.7323241233825684
iteration 400, loss 1.7422902584075928
iteration 500, loss 1.7181023359298706
iteration 600, loss 1.7170815467834473
iteration 700, loss 1.6941457986831665
iteration 800, loss 1.6745108366012573
fold 2 accuracy: 0.6415
iteration 0, loss 1.7334703207015991
iteration 100, loss 1.7039697170257568
iteration 200, loss 1.7020601034164429
iteration 300, loss 1.7117193937301636
iteration 400, loss 1.6771160364151
iteration 500, loss 1.7681548595428467
iteration 600, loss 1.7247949838638306
iteration 700, loss 1.7129606008529663
iteration 800, loss 1.7436347007751465
iteration 0, loss 1.7076218128204346
iteration 100, loss 1.7415777444839478
iteration 200, loss 1.7696983814239502
iteration 300, loss 1.6731170415878296
iteration 400, loss 1.769376277923584
iteration 500, loss 1.6754159927368164
iteration 600, loss 1.7303190231323242
iteration 700, loss 1.690222978591919
iteration 800, loss 1.7007776498794556
iteration 0, loss 1.6745445728302002
iteration 100, loss 1.6974700689315796
iteration 200, loss 1.715213418006897
iteration 300, loss 1.6820510625839233
iteration 400, loss 1.7055102586746216
iteration 500, loss 1.7484617233276367
iteration 600, loss 1.6929945945739746
iteration 700, loss 1.725640892982483
iteration 800, loss 1.6943342685699463
iteration 0, loss 1.7789251804351807
iteration 100, loss 1.69339120388031
iteration 200, loss 1.7617641687393188
iteration 300, loss 1.7404955625534058
iteration 400, loss 1.6630868911743164
iteration 500, loss 1.7281839847564697
iteration 600, loss 1.7735735177993774
iteration 700, loss 1.7094206809997559
iteration 800, loss 1.8147985935211182
iteration 0, loss 1.6988248825073242
iteration 100, loss 1.7285815477371216
iteration 200, loss 1.6903703212738037
iteration 300, loss 1.689499020576477
iteration 400, loss 1.7445517778396606
iteration 500, loss 1.6626191139221191
iteration 600, loss 1.7555710077285767
iteration 700, loss 1.7004233598709106
iteration 800, loss 1.722252368927002
iteration 0, loss 1.7557549476623535
iteration 100, loss 1.7132289409637451
iteration 200, loss 1.7236173152923584
iteration 300, loss 1.7322622537612915
iteration 400, loss 1.679291844367981
iteration 500, loss 1.7322227954864502
iteration 600, loss 1.7052453756332397
iteration 700, loss 1.6627804040908813
iteration 800, loss 1.7040045261383057
iteration 0, loss 1.6876749992370605
iteration 100, loss 1.7549219131469727
iteration 200, loss 1.7109746932983398
iteration 300, loss 1.6961100101470947
iteration 400, loss 1.7483303546905518
iteration 500, loss 1.634925365447998
iteration 600, loss 1.6935148239135742
iteration 700, loss 1.7178797721862793
iteration 800, loss 1.7777916193008423
iteration 0, loss 1.6910730600357056
iteration 100, loss 1.6854760646820068
iteration 200, loss 1.7792364358901978
iteration 300, loss 1.7471168041229248
iteration 400, loss 1.728122353553772
iteration 500, loss 1.7219456434249878
iteration 600, loss 1.6542757749557495
iteration 700, loss 1.644172191619873
iteration 800, loss 1.7044624090194702
iteration 0, loss 1.6600950956344604
iteration 100, loss 1.6306160688400269
iteration 200, loss 1.8093129396438599
iteration 300, loss 1.7629551887512207
iteration 400, loss 1.7499678134918213
iteration 500, loss 1.8247956037521362
iteration 600, loss 1.6684625148773193
iteration 700, loss 1.716415524482727
iteration 800, loss 1.7260792255401611
iteration 0, loss 1.7624351978302002
iteration 100, loss 1.6573957204818726
iteration 200, loss 1.7593367099761963
iteration 300, loss 1.6752691268920898
iteration 400, loss 1.7347588539123535
iteration 500, loss 1.6640162467956543
iteration 600, loss 1.7389954328536987
iteration 700, loss 1.7001290321350098
iteration 800, loss 1.766991138458252
iteration 0, loss 1.7084323167800903
iteration 100, loss 1.6475435495376587
iteration 200, loss 1.753857970237732
iteration 300, loss 1.7327864170074463
iteration 400, loss 1.6792199611663818
iteration 500, loss 1.7032896280288696
iteration 600, loss 1.74917733669281
iteration 700, loss 1.7547839879989624
iteration 800, loss 1.7405915260314941
iteration 0, loss 1.724334478378296
iteration 100, loss 1.6847342252731323
iteration 200, loss 1.7483822107315063
iteration 300, loss 1.7342208623886108
iteration 400, loss 1.7477174997329712
iteration 500, loss 1.7506873607635498
iteration 600, loss 1.7287499904632568
iteration 700, loss 1.7128759622573853
iteration 800, loss 1.750187873840332
iteration 0, loss 1.6555513143539429
iteration 100, loss 1.725592017173767
iteration 200, loss 1.6954736709594727
iteration 300, loss 1.7270406484603882
iteration 400, loss 1.7730402946472168
iteration 500, loss 1.7182104587554932
iteration 600, loss 1.738528847694397
iteration 700, loss 1.7808167934417725
iteration 800, loss 1.7219650745391846
iteration 0, loss 1.776855707168579
iteration 100, loss 1.6426055431365967
iteration 200, loss 1.7351566553115845
iteration 300, loss 1.7138950824737549
iteration 400, loss 1.6727921962738037
iteration 500, loss 1.7357251644134521
iteration 600, loss 1.7572728395462036
iteration 700, loss 1.6717016696929932
iteration 800, loss 1.6904302835464478
iteration 0, loss 1.7273057699203491
iteration 100, loss 1.7509269714355469
iteration 200, loss 1.778889775276184
iteration 300, loss 1.7340466976165771
iteration 400, loss 1.741255283355713
iteration 500, loss 1.6864628791809082
iteration 600, loss 1.6777645349502563
iteration 700, loss 1.7156697511672974
iteration 800, loss 1.7227811813354492
iteration 0, loss 1.8107491731643677
iteration 100, loss 1.6825299263000488
iteration 200, loss 1.7622398138046265
iteration 300, loss 1.8123676776885986
iteration 400, loss 1.7085155248641968
iteration 500, loss 1.7019834518432617
iteration 600, loss 1.7467552423477173
iteration 700, loss 1.7046887874603271
iteration 800, loss 1.7323098182678223
iteration 0, loss 1.6351525783538818
iteration 100, loss 1.7732372283935547
iteration 200, loss 1.64161217212677
iteration 300, loss 1.8017157316207886
iteration 400, loss 1.752991795539856
iteration 500, loss 1.6883647441864014
iteration 600, loss 1.7354071140289307
iteration 700, loss 1.7066247463226318
iteration 800, loss 1.725149154663086
iteration 0, loss 1.6743083000183105
iteration 100, loss 1.7501940727233887
iteration 200, loss 1.7235724925994873
iteration 300, loss 1.7072323560714722
iteration 400, loss 1.7139328718185425
iteration 500, loss 1.752875804901123
iteration 600, loss 1.6978620290756226
iteration 700, loss 1.7001490592956543
iteration 800, loss 1.751318097114563
iteration 0, loss 1.7726348638534546
iteration 100, loss 1.7772643566131592
iteration 200, loss 1.7319484949111938
iteration 300, loss 1.6903585195541382
iteration 400, loss 1.6936894655227661
iteration 500, loss 1.6729212999343872
iteration 600, loss 1.700986385345459
iteration 700, loss 1.7543939352035522
iteration 800, loss 1.7795761823654175
iteration 0, loss 1.6525578498840332
iteration 100, loss 1.7232788801193237
iteration 200, loss 1.730498194694519
iteration 300, loss 1.7663062810897827
iteration 400, loss 1.749629020690918
iteration 500, loss 1.752835750579834
iteration 600, loss 1.74838125705719
iteration 700, loss 1.69863760471344
iteration 800, loss 1.6537848711013794
iteration 0, loss 1.7472662925720215
iteration 100, loss 1.7248363494873047
iteration 200, loss 1.732336401939392
iteration 300, loss 1.703885793685913
iteration 400, loss 1.702144742012024
iteration 500, loss 1.700161337852478
iteration 600, loss 1.795518398284912
iteration 700, loss 1.6938092708587646
iteration 800, loss 1.6549643278121948
iteration 0, loss 1.6669143438339233
iteration 100, loss 1.7448828220367432
iteration 200, loss 1.8095755577087402
iteration 300, loss 1.6645088195800781
iteration 400, loss 1.7565412521362305
iteration 500, loss 1.6961947679519653
iteration 600, loss 1.7467315196990967
iteration 700, loss 1.646547794342041
iteration 800, loss 1.6527842283248901
iteration 0, loss 1.8130545616149902
iteration 100, loss 1.7911896705627441
iteration 200, loss 1.648120641708374
iteration 300, loss 1.7394399642944336
iteration 400, loss 1.726345419883728
iteration 500, loss 1.6709758043289185
iteration 600, loss 1.6644887924194336
iteration 700, loss 1.7003028392791748
iteration 800, loss 1.696845531463623
iteration 0, loss 1.7132850885391235
iteration 100, loss 1.684542179107666
iteration 200, loss 1.7316738367080688
iteration 300, loss 1.7795004844665527
iteration 400, loss 1.6677309274673462
iteration 500, loss 1.7197849750518799
iteration 600, loss 1.8027509450912476
iteration 700, loss 1.7570054531097412
iteration 800, loss 1.7197246551513672
iteration 0, loss 1.6573505401611328
iteration 100, loss 1.783468246459961
iteration 200, loss 1.766977310180664
iteration 300, loss 1.7823808193206787
iteration 400, loss 1.7676531076431274
iteration 500, loss 1.7679545879364014
iteration 600, loss 1.7418910264968872
iteration 700, loss 1.6571338176727295
iteration 800, loss 1.67488694190979
iteration 0, loss 1.7243494987487793
iteration 100, loss 1.7416869401931763
iteration 200, loss 1.736668348312378
iteration 300, loss 1.6989710330963135
iteration 400, loss 1.6894440650939941
iteration 500, loss 1.7075700759887695
iteration 600, loss 1.7615407705307007
iteration 700, loss 1.7170460224151611
iteration 800, loss 1.6613709926605225
iteration 0, loss 1.7044353485107422
iteration 100, loss 1.7260878086090088
iteration 200, loss 1.7179958820343018
iteration 300, loss 1.6430660486221313
iteration 400, loss 1.7388103008270264
iteration 500, loss 1.731827974319458
iteration 600, loss 1.697901964187622
iteration 700, loss 1.6958218812942505
iteration 800, loss 1.723759651184082
iteration 0, loss 1.7341642379760742
iteration 100, loss 1.7493088245391846
iteration 200, loss 1.7861146926879883
iteration 300, loss 1.7409534454345703
iteration 400, loss 1.7801463603973389
iteration 500, loss 1.6837579011917114
iteration 600, loss 1.745459794998169
iteration 700, loss 1.7351963520050049
iteration 800, loss 1.674621820449829
iteration 0, loss 1.7170659303665161
iteration 100, loss 1.747779369354248
iteration 200, loss 1.751532793045044
iteration 300, loss 1.7459142208099365
iteration 400, loss 1.7471425533294678
iteration 500, loss 1.65565025806427
iteration 600, loss 1.7670619487762451
iteration 700, loss 1.7424488067626953
iteration 800, loss 1.6670567989349365
iteration 0, loss 1.7209744453430176
iteration 100, loss 1.700039029121399
iteration 200, loss 1.7681140899658203
iteration 300, loss 1.6992638111114502
iteration 400, loss 1.7710070610046387
iteration 500, loss 1.698326826095581
iteration 600, loss 1.7292225360870361
iteration 700, loss 1.7735172510147095
iteration 800, loss 1.7062605619430542
iteration 0, loss 1.72785222530365
iteration 100, loss 1.8085857629776
iteration 200, loss 1.7878894805908203
iteration 300, loss 1.6392701864242554
iteration 400, loss 1.761620283126831
iteration 500, loss 1.685023307800293
iteration 600, loss 1.7130849361419678
iteration 700, loss 1.7351328134536743
iteration 800, loss 1.762747049331665
iteration 0, loss 1.7405413389205933
iteration 100, loss 1.7584227323532104
iteration 200, loss 1.70640230178833
iteration 300, loss 1.755271553993225
iteration 400, loss 1.7374191284179688
iteration 500, loss 1.7068710327148438
iteration 600, loss 1.7089282274246216
iteration 700, loss 1.693269968032837
iteration 800, loss 1.6887589693069458
iteration 0, loss 1.6556873321533203
iteration 100, loss 1.672258734703064
iteration 200, loss 1.7513402700424194
iteration 300, loss 1.6666810512542725
iteration 400, loss 1.7051568031311035
iteration 500, loss 1.774165153503418
iteration 600, loss 1.748154878616333
iteration 700, loss 1.6644752025604248
iteration 800, loss 1.7054082155227661
iteration 0, loss 1.7025386095046997
iteration 100, loss 1.7898329496383667
iteration 200, loss 1.671221137046814
iteration 300, loss 1.7149827480316162
iteration 400, loss 1.6642175912857056
iteration 500, loss 1.6673685312271118
iteration 600, loss 1.6704002618789673
iteration 700, loss 1.7771823406219482
iteration 800, loss 1.7524834871292114
iteration 0, loss 1.6906909942626953
iteration 100, loss 1.7257187366485596
iteration 200, loss 1.6636898517608643
iteration 300, loss 1.6787148714065552
iteration 400, loss 1.719973087310791
iteration 500, loss 1.7747327089309692
iteration 600, loss 1.8169715404510498
iteration 700, loss 1.6858272552490234
iteration 800, loss 1.7208712100982666
iteration 0, loss 1.6289045810699463
iteration 100, loss 1.7248268127441406
iteration 200, loss 1.6525108814239502
iteration 300, loss 1.6693639755249023
iteration 400, loss 1.6370190382003784
iteration 500, loss 1.7255276441574097
iteration 600, loss 1.7764610052108765
iteration 700, loss 1.6571186780929565
iteration 800, loss 1.7183005809783936
iteration 0, loss 1.685827374458313
iteration 100, loss 1.828784465789795
iteration 200, loss 1.7952345609664917
iteration 300, loss 1.7363696098327637
iteration 400, loss 1.6859004497528076
iteration 500, loss 1.7515087127685547
iteration 600, loss 1.6609727144241333
iteration 700, loss 1.6678909063339233
iteration 800, loss 1.7217437028884888
iteration 0, loss 1.7903887033462524
iteration 100, loss 1.6744587421417236
iteration 200, loss 1.7753418684005737
iteration 300, loss 1.7110110521316528
iteration 400, loss 1.7391566038131714
iteration 500, loss 1.8117074966430664
iteration 600, loss 1.697702407836914
iteration 700, loss 1.7929915189743042
iteration 800, loss 1.7649472951889038
iteration 0, loss 1.6631734371185303
iteration 100, loss 1.6265559196472168
iteration 200, loss 1.6723806858062744
iteration 300, loss 1.7089872360229492
iteration 400, loss 1.7142492532730103
iteration 500, loss 1.7293972969055176
iteration 600, loss 1.6759886741638184
iteration 700, loss 1.641145944595337
iteration 800, loss 1.7606043815612793
iteration 0, loss 1.6488821506500244
iteration 100, loss 1.7354929447174072
iteration 200, loss 1.6899551153182983
iteration 300, loss 1.6948614120483398
iteration 400, loss 1.723334550857544
iteration 500, loss 1.7821197509765625
iteration 600, loss 1.711707353591919
iteration 700, loss 1.6952133178710938
iteration 800, loss 1.7321501970291138
iteration 0, loss 1.6926876306533813
iteration 100, loss 1.7393798828125
iteration 200, loss 1.715976357460022
iteration 300, loss 1.6968921422958374
iteration 400, loss 1.7987546920776367
iteration 500, loss 1.7418709993362427
iteration 600, loss 1.8198096752166748
iteration 700, loss 1.6877471208572388
iteration 800, loss 1.7051856517791748
iteration 0, loss 1.744672417640686
iteration 100, loss 1.7314797639846802
iteration 200, loss 1.7289494276046753
iteration 300, loss 1.7189141511917114
iteration 400, loss 1.7284079790115356
iteration 500, loss 1.7280285358428955
iteration 600, loss 1.6931415796279907
iteration 700, loss 1.7431710958480835
iteration 800, loss 1.7440111637115479
iteration 0, loss 1.6824672222137451
iteration 100, loss 1.683573842048645
iteration 200, loss 1.7403571605682373
iteration 300, loss 1.7381078004837036
iteration 400, loss 1.7759274244308472
iteration 500, loss 1.6605688333511353
iteration 600, loss 1.6876308917999268
iteration 700, loss 1.7886158227920532
iteration 800, loss 1.7072699069976807
iteration 0, loss 1.794742465019226
iteration 100, loss 1.7278273105621338
iteration 200, loss 1.7627049684524536
iteration 300, loss 1.7111831903457642
iteration 400, loss 1.7427606582641602
iteration 500, loss 1.707991123199463
iteration 600, loss 1.6128995418548584
iteration 700, loss 1.6591681241989136
iteration 800, loss 1.6813514232635498
iteration 0, loss 1.7257205247879028
iteration 100, loss 1.6999918222427368
iteration 200, loss 1.7466719150543213
iteration 300, loss 1.6661114692687988
iteration 400, loss 1.7299567461013794
iteration 500, loss 1.7665643692016602
iteration 600, loss 1.6946293115615845
iteration 700, loss 1.84945809841156
iteration 800, loss 1.7510125637054443
iteration 0, loss 1.7108136415481567
iteration 100, loss 1.7269335985183716
iteration 200, loss 1.7132922410964966
iteration 300, loss 1.7551634311676025
iteration 400, loss 1.7094861268997192
iteration 500, loss 1.7207956314086914
iteration 600, loss 1.682112693786621
iteration 700, loss 1.7247520685195923
iteration 800, loss 1.6735788583755493
iteration 0, loss 1.7442350387573242
iteration 100, loss 1.7561007738113403
iteration 200, loss 1.7009260654449463
iteration 300, loss 1.7187734842300415
iteration 400, loss 1.6931672096252441
iteration 500, loss 1.7129756212234497
iteration 600, loss 1.73444402217865
iteration 700, loss 1.745452880859375
iteration 800, loss 1.833534836769104
iteration 0, loss 1.665640115737915
iteration 100, loss 1.761073112487793
iteration 200, loss 1.6858235597610474
iteration 300, loss 1.704610824584961
iteration 400, loss 1.6180859804153442
iteration 500, loss 1.7056591510772705
iteration 600, loss 1.6629422903060913
iteration 700, loss 1.7234066724777222
iteration 800, loss 1.7945338487625122
iteration 0, loss 1.7714738845825195
iteration 100, loss 1.7170416116714478
iteration 200, loss 1.6620484590530396
iteration 300, loss 1.7227026224136353
iteration 400, loss 1.6835322380065918
iteration 500, loss 1.7010926008224487
iteration 600, loss 1.643377423286438
iteration 700, loss 1.7345552444458008
iteration 800, loss 1.733627438545227
iteration 0, loss 1.6709271669387817
iteration 100, loss 1.7509088516235352
iteration 200, loss 1.7357450723648071
iteration 300, loss 1.652955412864685
iteration 400, loss 1.751336932182312
iteration 500, loss 1.7716789245605469
iteration 600, loss 1.7199755907058716
iteration 700, loss 1.714335560798645
iteration 800, loss 1.6993407011032104
fold 3 accuracy: 0.6635
iteration 0, loss 1.6882472038269043
iteration 100, loss 1.7558293342590332
iteration 200, loss 1.7874619960784912
iteration 300, loss 1.7175434827804565
iteration 400, loss 1.7730706930160522
iteration 500, loss 1.7382819652557373
iteration 600, loss 1.7349692583084106
iteration 700, loss 1.7356114387512207
iteration 800, loss 1.6846097707748413
iteration 0, loss 1.7944352626800537
iteration 100, loss 1.7053618431091309
iteration 200, loss 1.6833677291870117
iteration 300, loss 1.7527353763580322
iteration 400, loss 1.7685692310333252
iteration 500, loss 1.676166296005249
iteration 600, loss 1.7652966976165771
iteration 700, loss 1.7252464294433594
iteration 800, loss 1.6990097761154175
iteration 0, loss 1.6452178955078125
iteration 100, loss 1.7105352878570557
iteration 200, loss 1.6823289394378662
iteration 300, loss 1.7304893732070923
iteration 400, loss 1.6704905033111572
iteration 500, loss 1.682952880859375
iteration 600, loss 1.6937131881713867
iteration 700, loss 1.76475989818573
iteration 800, loss 1.7407031059265137
iteration 0, loss 1.7185564041137695
iteration 100, loss 1.7875310182571411
iteration 200, loss 1.691744327545166
iteration 300, loss 1.7256683111190796
iteration 400, loss 1.6973295211791992
iteration 500, loss 1.6660791635513306
iteration 600, loss 1.7041436433792114
iteration 700, loss 1.7679252624511719
iteration 800, loss 1.7282617092132568
iteration 0, loss 1.7145464420318604
iteration 100, loss 1.761184573173523
iteration 200, loss 1.689785361289978
iteration 300, loss 1.7475981712341309
iteration 400, loss 1.7281885147094727
iteration 500, loss 1.7341387271881104
iteration 600, loss 1.6984846591949463
iteration 700, loss 1.703818678855896
iteration 800, loss 1.672184705734253
iteration 0, loss 1.7550429105758667
iteration 100, loss 1.7071287631988525
iteration 200, loss 1.7184419631958008
iteration 300, loss 1.7171801328659058
iteration 400, loss 1.7842012643814087
iteration 500, loss 1.7591086626052856
iteration 600, loss 1.6310200691223145
iteration 700, loss 1.7328613996505737
iteration 800, loss 1.7322847843170166
iteration 0, loss 1.7076969146728516
iteration 100, loss 1.7188347578048706
iteration 200, loss 1.7836765050888062
iteration 300, loss 1.7181384563446045
iteration 400, loss 1.6932640075683594
iteration 500, loss 1.685802936553955
iteration 600, loss 1.6593475341796875
iteration 700, loss 1.7444229125976562
iteration 800, loss 1.6351855993270874
iteration 0, loss 1.7652417421340942
iteration 100, loss 1.706449270248413
iteration 200, loss 1.6898777484893799
iteration 300, loss 1.7077512741088867
iteration 400, loss 1.6416015625
iteration 500, loss 1.6848784685134888
iteration 600, loss 1.7274391651153564
iteration 700, loss 1.7796387672424316
iteration 800, loss 1.7511684894561768
iteration 0, loss 1.639904260635376
iteration 100, loss 1.6340415477752686
iteration 200, loss 1.803843379020691
iteration 300, loss 1.6546499729156494
iteration 400, loss 1.6890066862106323
iteration 500, loss 1.7454532384872437
iteration 600, loss 1.7130001783370972
iteration 700, loss 1.6917343139648438
iteration 800, loss 1.806266188621521
iteration 0, loss 1.7468966245651245
iteration 100, loss 1.7345932722091675
iteration 200, loss 1.6815274953842163
iteration 300, loss 1.7127659320831299
iteration 400, loss 1.710585355758667
iteration 500, loss 1.635621428489685
iteration 600, loss 1.8190006017684937
iteration 700, loss 1.7469292879104614
iteration 800, loss 1.668445110321045
iteration 0, loss 1.7803959846496582
iteration 100, loss 1.7357585430145264
iteration 200, loss 1.698525071144104
iteration 300, loss 1.6623833179473877
iteration 400, loss 1.6776217222213745
iteration 500, loss 1.747033953666687
iteration 600, loss 1.7388553619384766
iteration 700, loss 1.6685400009155273
iteration 800, loss 1.739985466003418
iteration 0, loss 1.653662085533142
iteration 100, loss 1.7195208072662354
iteration 200, loss 1.8015888929367065
iteration 300, loss 1.6361255645751953
iteration 400, loss 1.7418094873428345
iteration 500, loss 1.8237335681915283
iteration 600, loss 1.6373505592346191
iteration 700, loss 1.6796762943267822
iteration 800, loss 1.6993420124053955
iteration 0, loss 1.749729037284851
iteration 100, loss 1.7038854360580444
iteration 200, loss 1.847674012184143
iteration 300, loss 1.6679357290267944
iteration 400, loss 1.7105261087417603
iteration 500, loss 1.7630025148391724
iteration 600, loss 1.632863163948059
iteration 700, loss 1.6641557216644287
iteration 800, loss 1.7714976072311401
iteration 0, loss 1.623030185699463
iteration 100, loss 1.6938318014144897
iteration 200, loss 1.6881812810897827
iteration 300, loss 1.6979663372039795
iteration 400, loss 1.7673110961914062
iteration 500, loss 1.7171753644943237
iteration 600, loss 1.649503469467163
iteration 700, loss 1.7708903551101685
iteration 800, loss 1.704079508781433
iteration 0, loss 1.6877590417861938
iteration 100, loss 1.7164177894592285
iteration 200, loss 1.7170889377593994
iteration 300, loss 1.6885054111480713
iteration 400, loss 1.6722825765609741
iteration 500, loss 1.7550108432769775
iteration 600, loss 1.6975971460342407
iteration 700, loss 1.654386281967163
iteration 800, loss 1.7315844297409058
iteration 0, loss 1.655852198600769
iteration 100, loss 1.7142133712768555
iteration 200, loss 1.6606301069259644
iteration 300, loss 1.7144640684127808
iteration 400, loss 1.7224235534667969
iteration 500, loss 1.7933361530303955
iteration 600, loss 1.7231354713439941
iteration 700, loss 1.77484929561615
iteration 800, loss 1.697637915611267
iteration 0, loss 1.7470921277999878
iteration 100, loss 1.6540347337722778
iteration 200, loss 1.7370514869689941
iteration 300, loss 1.7559869289398193
iteration 400, loss 1.742822289466858
iteration 500, loss 1.6762982606887817
iteration 600, loss 1.7200506925582886
iteration 700, loss 1.7866238355636597
iteration 800, loss 1.6502102613449097
iteration 0, loss 1.7111847400665283
iteration 100, loss 1.7138197422027588
iteration 200, loss 1.7464650869369507
iteration 300, loss 1.7809561491012573
iteration 400, loss 1.7355268001556396
iteration 500, loss 1.7293260097503662
iteration 600, loss 1.7508128881454468
iteration 700, loss 1.7511876821517944
iteration 800, loss 1.835289478302002
iteration 0, loss 1.734849214553833
iteration 100, loss 1.6666762828826904
iteration 200, loss 1.775799036026001
iteration 300, loss 1.7073748111724854
iteration 400, loss 1.7314645051956177
iteration 500, loss 1.670737862586975
iteration 600, loss 1.6162559986114502
iteration 700, loss 1.7264395952224731
iteration 800, loss 1.734871745109558
iteration 0, loss 1.7367604970932007
iteration 100, loss 1.7386900186538696
iteration 200, loss 1.697656273841858
iteration 300, loss 1.8161472082138062
iteration 400, loss 1.7010903358459473
iteration 500, loss 1.7946467399597168
iteration 600, loss 1.7595688104629517
iteration 700, loss 1.6701245307922363
iteration 800, loss 1.7588953971862793
iteration 0, loss 1.6891589164733887
iteration 100, loss 1.8178467750549316
iteration 200, loss 1.7150639295578003
iteration 300, loss 1.7393444776535034
iteration 400, loss 1.7147607803344727
iteration 500, loss 1.6495438814163208
iteration 600, loss 1.7141610383987427
iteration 700, loss 1.7447428703308105
iteration 800, loss 1.6582602262496948
iteration 0, loss 1.6762995719909668
iteration 100, loss 1.7472233772277832
iteration 200, loss 1.6871336698532104
iteration 300, loss 1.7503539323806763
iteration 400, loss 1.786968469619751
iteration 500, loss 1.7716883420944214
iteration 600, loss 1.7416102886199951
iteration 700, loss 1.7751766443252563
iteration 800, loss 1.6826320886611938
iteration 0, loss 1.7619630098342896
iteration 100, loss 1.7017202377319336
iteration 200, loss 1.7318364381790161
iteration 300, loss 1.7691324949264526
iteration 400, loss 1.7230606079101562
iteration 500, loss 1.6848924160003662
iteration 600, loss 1.661572813987732
iteration 700, loss 1.7471553087234497
iteration 800, loss 1.7208143472671509
iteration 0, loss 1.6701358556747437
iteration 100, loss 1.7085456848144531
iteration 200, loss 1.73381769657135
iteration 300, loss 1.7520265579223633
iteration 400, loss 1.7394479513168335
iteration 500, loss 1.7234876155853271
iteration 600, loss 1.6532061100006104
iteration 700, loss 1.7599080801010132
iteration 800, loss 1.7040725946426392
iteration 0, loss 1.66575288772583
iteration 100, loss 1.6904799938201904
iteration 200, loss 1.7120397090911865
iteration 300, loss 1.7780686616897583
iteration 400, loss 1.7658649682998657
iteration 500, loss 1.704261064529419
iteration 600, loss 1.7005308866500854
iteration 700, loss 1.741219401359558
iteration 800, loss 1.6941112279891968
iteration 0, loss 1.6689255237579346
iteration 100, loss 1.7208815813064575
iteration 200, loss 1.7420026063919067
iteration 300, loss 1.6614325046539307
iteration 400, loss 1.7784111499786377
iteration 500, loss 1.718085765838623
iteration 600, loss 1.658158779144287
iteration 700, loss 1.6946790218353271
iteration 800, loss 1.6712878942489624
iteration 0, loss 1.6931840181350708
iteration 100, loss 1.7889846563339233
iteration 200, loss 1.661538004875183
iteration 300, loss 1.696721076965332
iteration 400, loss 1.726599097251892
iteration 500, loss 1.7448137998580933
iteration 600, loss 1.7241650819778442
iteration 700, loss 1.738523244857788
iteration 800, loss 1.7043620347976685
iteration 0, loss 1.743865966796875
iteration 100, loss 1.720587968826294
iteration 200, loss 1.721453070640564
iteration 300, loss 1.7593938112258911
iteration 400, loss 1.639410138130188
iteration 500, loss 1.7429777383804321
iteration 600, loss 1.7705256938934326
iteration 700, loss 1.704612135887146
iteration 800, loss 1.7259876728057861
iteration 0, loss 1.6456124782562256
iteration 100, loss 1.715824007987976
iteration 200, loss 1.735954999923706
iteration 300, loss 1.7020623683929443
iteration 400, loss 1.7020376920700073
iteration 500, loss 1.7933101654052734
iteration 600, loss 1.7169631719589233
iteration 700, loss 1.720595121383667
iteration 800, loss 1.6718637943267822
iteration 0, loss 1.7112038135528564
iteration 100, loss 1.7261565923690796
iteration 200, loss 1.7778233289718628
iteration 300, loss 1.737931489944458
iteration 400, loss 1.7487682104110718
iteration 500, loss 1.6920362710952759
iteration 600, loss 1.7183245420455933
iteration 700, loss 1.739145278930664
iteration 800, loss 1.8273307085037231
iteration 0, loss 1.707243800163269
iteration 100, loss 1.7295572757720947
iteration 200, loss 1.669238805770874
iteration 300, loss 1.7271103858947754
iteration 400, loss 1.7340766191482544
iteration 500, loss 1.7104389667510986
iteration 600, loss 1.737763524055481
iteration 700, loss 1.7237937450408936
iteration 800, loss 1.773825764656067
iteration 0, loss 1.7733664512634277
iteration 100, loss 1.7732383012771606
iteration 200, loss 1.7872536182403564
iteration 300, loss 1.7190332412719727
iteration 400, loss 1.786690354347229
iteration 500, loss 1.6929209232330322
iteration 600, loss 1.7125306129455566
iteration 700, loss 1.749210000038147
iteration 800, loss 1.7366743087768555
iteration 0, loss 1.6961195468902588
iteration 100, loss 1.7666829824447632
iteration 200, loss 1.8153448104858398
iteration 300, loss 1.72605299949646
iteration 400, loss 1.6671123504638672
iteration 500, loss 1.72311532497406
iteration 600, loss 1.7043954133987427
iteration 700, loss 1.6560906171798706
iteration 800, loss 1.6266179084777832
iteration 0, loss 1.7336251735687256
iteration 100, loss 1.7237460613250732
iteration 200, loss 1.6929795742034912
iteration 300, loss 1.658846378326416
iteration 400, loss 1.7496041059494019
iteration 500, loss 1.7505980730056763
iteration 600, loss 1.6876860857009888
iteration 700, loss 1.713402271270752
iteration 800, loss 1.669349193572998
iteration 0, loss 1.7256697416305542
iteration 100, loss 1.7067508697509766
iteration 200, loss 1.6693496704101562
iteration 300, loss 1.7255771160125732
iteration 400, loss 1.692101001739502
iteration 500, loss 1.6640821695327759
iteration 600, loss 1.7786074876785278
iteration 700, loss 1.752461314201355
iteration 800, loss 1.7307443618774414
iteration 0, loss 1.7160608768463135
iteration 100, loss 1.71416175365448
iteration 200, loss 1.6879057884216309
iteration 300, loss 1.7202973365783691
iteration 400, loss 1.6591876745224
iteration 500, loss 1.6836174726486206
iteration 600, loss 1.7060737609863281
iteration 700, loss 1.6724889278411865
iteration 800, loss 1.734168529510498
iteration 0, loss 1.7285881042480469
iteration 100, loss 1.6801578998565674
iteration 200, loss 1.7376259565353394
iteration 300, loss 1.7602276802062988
iteration 400, loss 1.7348499298095703
iteration 500, loss 1.7862098217010498
iteration 600, loss 1.7098102569580078
iteration 700, loss 1.7453508377075195
iteration 800, loss 1.7903388738632202
iteration 0, loss 1.7374391555786133
iteration 100, loss 1.6651445627212524
iteration 200, loss 1.735837459564209
iteration 300, loss 1.708396077156067
iteration 400, loss 1.7729970216751099
iteration 500, loss 1.7048043012619019
iteration 600, loss 1.7152659893035889
iteration 700, loss 1.7972400188446045
iteration 800, loss 1.7072761058807373
iteration 0, loss 1.7598984241485596
iteration 100, loss 1.7255951166152954
iteration 200, loss 1.7069056034088135
iteration 300, loss 1.693609595298767
iteration 400, loss 1.7116732597351074
iteration 500, loss 1.7269091606140137
iteration 600, loss 1.6869198083877563
iteration 700, loss 1.7083117961883545
iteration 800, loss 1.6641254425048828
iteration 0, loss 1.7654023170471191
iteration 100, loss 1.6956090927124023
iteration 200, loss 1.7284835577011108
iteration 300, loss 1.7000999450683594
iteration 400, loss 1.6990207433700562
iteration 500, loss 1.7727668285369873
iteration 600, loss 1.7263754606246948
iteration 700, loss 1.7001272439956665
iteration 800, loss 1.676835536956787
iteration 0, loss 1.7356106042861938
iteration 100, loss 1.7070956230163574
iteration 200, loss 1.8221873044967651
iteration 300, loss 1.7525030374526978
iteration 400, loss 1.6739856004714966
iteration 500, loss 1.7102012634277344
iteration 600, loss 1.722899317741394
iteration 700, loss 1.769988775253296
iteration 800, loss 1.6930314302444458
iteration 0, loss 1.7646018266677856
iteration 100, loss 1.6783381700515747
iteration 200, loss 1.6724631786346436
iteration 300, loss 1.7020878791809082
iteration 400, loss 1.6623855829238892
iteration 500, loss 1.7441260814666748
iteration 600, loss 1.6995621919631958
iteration 700, loss 1.6999040842056274
iteration 800, loss 1.7377028465270996
iteration 0, loss 1.6753052473068237
iteration 100, loss 1.776158332824707
iteration 200, loss 1.6670163869857788
iteration 300, loss 1.7436050176620483
iteration 400, loss 1.6693068742752075
iteration 500, loss 1.7078816890716553
iteration 600, loss 1.7097532749176025
iteration 700, loss 1.7619531154632568
iteration 800, loss 1.7046711444854736
iteration 0, loss 1.7479103803634644
iteration 100, loss 1.699265480041504
iteration 200, loss 1.7546216249465942
iteration 300, loss 1.8180168867111206
iteration 400, loss 1.6596122980117798
iteration 500, loss 1.7176717519760132
iteration 600, loss 1.7272024154663086
iteration 700, loss 1.6795979738235474
iteration 800, loss 1.8067941665649414
iteration 0, loss 1.6906802654266357
iteration 100, loss 1.710585117340088
iteration 200, loss 1.7054880857467651
iteration 300, loss 1.6829681396484375
iteration 400, loss 1.793408989906311
iteration 500, loss 1.6511645317077637
iteration 600, loss 1.6729228496551514
iteration 700, loss 1.6299306154251099
iteration 800, loss 1.6874139308929443
iteration 0, loss 1.8474559783935547
iteration 100, loss 1.6865452527999878
iteration 200, loss 1.662473201751709
iteration 300, loss 1.5905038118362427
iteration 400, loss 1.6653177738189697
iteration 500, loss 1.6806961297988892
iteration 600, loss 1.6846249103546143
iteration 700, loss 1.739906668663025
iteration 800, loss 1.6556963920593262
iteration 0, loss 1.7169451713562012
iteration 100, loss 1.7228440046310425
iteration 200, loss 1.71753990650177
iteration 300, loss 1.7983460426330566
iteration 400, loss 1.6621495485305786
iteration 500, loss 1.691622257232666
iteration 600, loss 1.729884386062622
iteration 700, loss 1.6733263731002808
iteration 800, loss 1.684005856513977
iteration 0, loss 1.684913992881775
iteration 100, loss 1.7344826459884644
iteration 200, loss 1.7282508611679077
iteration 300, loss 1.7207896709442139
iteration 400, loss 1.6986432075500488
iteration 500, loss 1.7223416566848755
iteration 600, loss 1.7601845264434814
iteration 700, loss 1.6677638292312622
iteration 800, loss 1.6791272163391113
iteration 0, loss 1.6861422061920166
iteration 100, loss 1.743685007095337
iteration 200, loss 1.7296912670135498
iteration 300, loss 1.6642025709152222
iteration 400, loss 1.704459309577942
iteration 500, loss 1.7455806732177734
iteration 600, loss 1.8157403469085693
iteration 700, loss 1.7401504516601562
iteration 800, loss 1.7211135625839233
iteration 0, loss 1.7422388792037964
iteration 100, loss 1.7231636047363281
iteration 200, loss 1.7351956367492676
iteration 300, loss 1.7327289581298828
iteration 400, loss 1.6693603992462158
iteration 500, loss 1.7671836614608765
iteration 600, loss 1.7315137386322021
iteration 700, loss 1.7340582609176636
iteration 800, loss 1.831529974937439
fold 4 accuracy: 0.6526428571428572
[2024-02-29 01:58:23,642] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-29 01:58:23,643] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         458     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       560 MACs
fwd flops per GPU:                                                      1.12 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.12 K  
fwd latency:                                                            344.04 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.26 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '458'}
    MACs        - {'KronLayer': '560 MACs'}
    fwd latency - {'KronLayer': '344.04 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  458 = 100% Params, 560 MACs = 100% MACs, 344.04 us = 100% latency, 3.26 MFLOPS
  (kronlinear): KronLinear(458 = 100% Params, 560 MACs = 100% MACs, 249.86 us = 72.63% latency, 4.48 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 28.37 us = 8.25% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-29 01:58:23,644] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.12 K 458
iteration 0, loss 2.308563470840454
iteration 100, loss 2.234560966491699
iteration 200, loss 2.1711878776550293
iteration 300, loss 2.0696887969970703
iteration 400, loss 2.002711057662964
iteration 500, loss 1.8879104852676392
iteration 600, loss 1.9020475149154663
iteration 700, loss 1.9537187814712524
iteration 800, loss 1.9246023893356323
iteration 0, loss 1.9106320142745972
iteration 100, loss 1.9188123941421509
iteration 200, loss 1.8491085767745972
iteration 300, loss 1.840323567390442
iteration 400, loss 1.8137620687484741
iteration 500, loss 1.830999493598938
iteration 600, loss 1.8288359642028809
iteration 700, loss 1.788806676864624
iteration 800, loss 1.8072738647460938
iteration 0, loss 1.8615533113479614
iteration 100, loss 1.7851084470748901
iteration 200, loss 1.7756474018096924
iteration 300, loss 1.7300745248794556
iteration 400, loss 1.7583162784576416
iteration 500, loss 1.8157776594161987
iteration 600, loss 1.8111920356750488
iteration 700, loss 1.784533143043518
iteration 800, loss 1.7465119361877441
iteration 0, loss 1.7358815670013428
iteration 100, loss 1.757365345954895
iteration 200, loss 1.759110689163208
iteration 300, loss 1.8565236330032349
iteration 400, loss 1.7903417348861694
iteration 500, loss 1.7169839143753052
iteration 600, loss 1.7780520915985107
iteration 700, loss 1.8220534324645996
iteration 800, loss 1.7682654857635498
iteration 0, loss 1.7779014110565186
iteration 100, loss 1.7729806900024414
iteration 200, loss 1.8506264686584473
iteration 300, loss 1.7881759405136108
iteration 400, loss 1.7479535341262817
iteration 500, loss 1.8146789073944092
iteration 600, loss 1.7349454164505005
iteration 700, loss 1.7277648448944092
iteration 800, loss 1.8496235609054565
iteration 0, loss 1.731177806854248
iteration 100, loss 1.7665283679962158
iteration 200, loss 1.8028297424316406
iteration 300, loss 1.750072717666626
iteration 400, loss 1.7733360528945923
iteration 500, loss 1.8310632705688477
iteration 600, loss 1.7522927522659302
iteration 700, loss 1.7362215518951416
iteration 800, loss 1.7454304695129395
iteration 0, loss 1.7740392684936523
iteration 100, loss 1.7959221601486206
iteration 200, loss 1.7421391010284424
iteration 300, loss 1.785764217376709
iteration 400, loss 1.7470694780349731
iteration 500, loss 1.731762170791626
iteration 600, loss 1.7842333316802979
iteration 700, loss 1.7977417707443237
iteration 800, loss 1.708964467048645
iteration 0, loss 1.7499518394470215
iteration 100, loss 1.7421587705612183
iteration 200, loss 1.8572652339935303
iteration 300, loss 1.7008687257766724
iteration 400, loss 1.7451468706130981
iteration 500, loss 1.7412917613983154
iteration 600, loss 1.7635928392410278
iteration 700, loss 1.7252984046936035
iteration 800, loss 1.7803316116333008
iteration 0, loss 1.780164361000061
iteration 100, loss 1.7763251066207886
iteration 200, loss 1.701646327972412
iteration 300, loss 1.740709662437439
iteration 400, loss 1.7568970918655396
iteration 500, loss 1.6775758266448975
iteration 600, loss 1.7780314683914185
iteration 700, loss 1.7572836875915527
iteration 800, loss 1.708584189414978
iteration 0, loss 1.7930034399032593
iteration 100, loss 1.7420198917388916
iteration 200, loss 1.736390471458435
iteration 300, loss 1.7693698406219482
iteration 400, loss 1.7171529531478882
iteration 500, loss 1.7832489013671875
iteration 600, loss 1.8269000053405762
iteration 700, loss 1.7054568529129028
iteration 800, loss 1.7435346841812134
iteration 0, loss 1.7987399101257324
iteration 100, loss 1.6746245622634888
iteration 200, loss 1.8105093240737915
iteration 300, loss 1.764711856842041
iteration 400, loss 1.7293784618377686
iteration 500, loss 1.6861907243728638
iteration 600, loss 1.7584714889526367
iteration 700, loss 1.7519605159759521
iteration 800, loss 1.750507116317749
iteration 0, loss 1.6640524864196777
iteration 100, loss 1.7554184198379517
iteration 200, loss 1.71615731716156
iteration 300, loss 1.8149663209915161
iteration 400, loss 1.7330249547958374
iteration 500, loss 1.788514256477356
iteration 600, loss 1.706215262413025
iteration 700, loss 1.7592159509658813
iteration 800, loss 1.7769955396652222
iteration 0, loss 1.7433468103408813
iteration 100, loss 1.785283088684082
iteration 200, loss 1.7542449235916138
iteration 300, loss 1.7567890882492065
iteration 400, loss 1.7026240825653076
iteration 500, loss 1.7378766536712646
iteration 600, loss 1.703108549118042
iteration 700, loss 1.725030779838562
iteration 800, loss 1.841064214706421
iteration 0, loss 1.721239447593689
iteration 100, loss 1.7396053075790405
iteration 200, loss 1.7599515914916992
iteration 300, loss 1.734236717224121
iteration 400, loss 1.796121597290039
iteration 500, loss 1.70901620388031
iteration 600, loss 1.7522199153900146
iteration 700, loss 1.7324154376983643
iteration 800, loss 1.7546937465667725
iteration 0, loss 1.7507573366165161
iteration 100, loss 1.6594754457473755
iteration 200, loss 1.7558422088623047
iteration 300, loss 1.7892612218856812
iteration 400, loss 1.742732286453247
iteration 500, loss 1.7164727449417114
iteration 600, loss 1.7550878524780273
iteration 700, loss 1.7533154487609863
iteration 800, loss 1.7425110340118408
iteration 0, loss 1.7181392908096313
iteration 100, loss 1.7000638246536255
iteration 200, loss 1.8072764873504639
iteration 300, loss 1.6837044954299927
iteration 400, loss 1.8271907567977905
iteration 500, loss 1.7434991598129272
iteration 600, loss 1.7484753131866455
iteration 700, loss 1.7543749809265137
iteration 800, loss 1.677325963973999
iteration 0, loss 1.7379928827285767
iteration 100, loss 1.6976691484451294
iteration 200, loss 1.765541434288025
iteration 300, loss 1.775076150894165
iteration 400, loss 1.7180465459823608
iteration 500, loss 1.7288724184036255
iteration 600, loss 1.7284116744995117
iteration 700, loss 1.7615151405334473
iteration 800, loss 1.7311172485351562
iteration 0, loss 1.7138395309448242
iteration 100, loss 1.703676700592041
iteration 200, loss 1.7574585676193237
iteration 300, loss 1.8160237073898315
iteration 400, loss 1.7579076290130615
iteration 500, loss 1.7506811618804932
iteration 600, loss 1.7343193292617798
iteration 700, loss 1.7722176313400269
iteration 800, loss 1.7305963039398193
iteration 0, loss 1.7404944896697998
iteration 100, loss 1.7526272535324097
iteration 200, loss 1.6878045797348022
iteration 300, loss 1.751800537109375
iteration 400, loss 1.7282440662384033
iteration 500, loss 1.7058449983596802
iteration 600, loss 1.73995041847229
iteration 700, loss 1.7376134395599365
iteration 800, loss 1.7710832357406616
iteration 0, loss 1.7279099225997925
iteration 100, loss 1.7282323837280273
iteration 200, loss 1.7508882284164429
iteration 300, loss 1.7315312623977661
iteration 400, loss 1.7206634283065796
iteration 500, loss 1.7105032205581665
iteration 600, loss 1.7133188247680664
iteration 700, loss 1.719668984413147
iteration 800, loss 1.7482898235321045
iteration 0, loss 1.704046368598938
iteration 100, loss 1.6944270133972168
iteration 200, loss 1.707582950592041
iteration 300, loss 1.7484198808670044
iteration 400, loss 1.7844990491867065
iteration 500, loss 1.753814935684204
iteration 600, loss 1.6918798685073853
iteration 700, loss 1.7318354845046997
iteration 800, loss 1.7227425575256348
iteration 0, loss 1.822765588760376
iteration 100, loss 1.6854126453399658
iteration 200, loss 1.724593997001648
iteration 300, loss 1.7050143480300903
iteration 400, loss 1.7203364372253418
iteration 500, loss 1.7524867057800293
iteration 600, loss 1.8381905555725098
iteration 700, loss 1.6846263408660889
iteration 800, loss 1.7098082304000854
iteration 0, loss 1.7228871583938599
iteration 100, loss 1.782088279724121
iteration 200, loss 1.7408363819122314
iteration 300, loss 1.715022087097168
iteration 400, loss 1.8239219188690186
iteration 500, loss 1.7550204992294312
iteration 600, loss 1.7985014915466309
iteration 700, loss 1.7607958316802979
iteration 800, loss 1.7967511415481567
iteration 0, loss 1.7822316884994507
iteration 100, loss 1.746835708618164
iteration 200, loss 1.7090904712677002
iteration 300, loss 1.7945809364318848
iteration 400, loss 1.6710485219955444
iteration 500, loss 1.686392068862915
iteration 600, loss 1.7988687753677368
iteration 700, loss 1.7355979681015015
iteration 800, loss 1.726439356803894
iteration 0, loss 1.7691783905029297
iteration 100, loss 1.7637450695037842
iteration 200, loss 1.7613427639007568
iteration 300, loss 1.6980714797973633
iteration 400, loss 1.7753055095672607
iteration 500, loss 1.7722703218460083
iteration 600, loss 1.7068071365356445
iteration 700, loss 1.750011682510376
iteration 800, loss 1.7688136100769043
iteration 0, loss 1.792928695678711
iteration 100, loss 1.7375240325927734
iteration 200, loss 1.8329620361328125
iteration 300, loss 1.7553030252456665
iteration 400, loss 1.820520043373108
iteration 500, loss 1.7628154754638672
iteration 600, loss 1.6927714347839355
iteration 700, loss 1.7455904483795166
iteration 800, loss 1.7063974142074585
iteration 0, loss 1.7079973220825195
iteration 100, loss 1.6752701997756958
iteration 200, loss 1.7295366525650024
iteration 300, loss 1.7122313976287842
iteration 400, loss 1.6786274909973145
iteration 500, loss 1.7135761976242065
iteration 600, loss 1.7181328535079956
iteration 700, loss 1.7909963130950928
iteration 800, loss 1.7695610523223877
iteration 0, loss 1.7484077215194702
iteration 100, loss 1.7000361680984497
iteration 200, loss 1.7025870084762573
iteration 300, loss 1.7756226062774658
iteration 400, loss 1.7268537282943726
iteration 500, loss 1.747636079788208
iteration 600, loss 1.8098573684692383
iteration 700, loss 1.8144192695617676
iteration 800, loss 1.703262448310852
iteration 0, loss 1.7760190963745117
iteration 100, loss 1.7549934387207031
iteration 200, loss 1.6965701580047607
iteration 300, loss 1.7628475427627563
iteration 400, loss 1.7652740478515625
iteration 500, loss 1.6793934106826782
iteration 600, loss 1.6943531036376953
iteration 700, loss 1.785516381263733
iteration 800, loss 1.6799451112747192
iteration 0, loss 1.77641761302948
iteration 100, loss 1.694487452507019
iteration 200, loss 1.7892770767211914
iteration 300, loss 1.7008347511291504
iteration 400, loss 1.7624192237854004
iteration 500, loss 1.7253551483154297
iteration 600, loss 1.727171778678894
iteration 700, loss 1.7182891368865967
iteration 800, loss 1.7078986167907715
iteration 0, loss 1.7603225708007812
iteration 100, loss 1.7165600061416626
iteration 200, loss 1.7621642351150513
iteration 300, loss 1.7260940074920654
iteration 400, loss 1.7547624111175537
iteration 500, loss 1.773219347000122
iteration 600, loss 1.7382898330688477
iteration 700, loss 1.7020065784454346
iteration 800, loss 1.7620314359664917
iteration 0, loss 1.7672386169433594
iteration 100, loss 1.745851993560791
iteration 200, loss 1.7302618026733398
iteration 300, loss 1.7381377220153809
iteration 400, loss 1.7108161449432373
iteration 500, loss 1.6993281841278076
iteration 600, loss 1.7556126117706299
iteration 700, loss 1.6755493879318237
iteration 800, loss 1.7254410982131958
iteration 0, loss 1.7352670431137085
iteration 100, loss 1.7170716524124146
iteration 200, loss 1.7468292713165283
iteration 300, loss 1.7007638216018677
iteration 400, loss 1.6859149932861328
iteration 500, loss 1.750805139541626
iteration 600, loss 1.7217527627944946
iteration 700, loss 1.7644919157028198
iteration 800, loss 1.7018969058990479
iteration 0, loss 1.7402803897857666
iteration 100, loss 1.754097819328308
iteration 200, loss 1.7328499555587769
iteration 300, loss 1.733130693435669
iteration 400, loss 1.7899131774902344
iteration 500, loss 1.7056663036346436
iteration 600, loss 1.7632067203521729
iteration 700, loss 1.714118480682373
iteration 800, loss 1.6583631038665771
iteration 0, loss 1.6672823429107666
iteration 100, loss 1.7161191701889038
iteration 200, loss 1.7048230171203613
iteration 300, loss 1.7221500873565674
iteration 400, loss 1.7936952114105225
iteration 500, loss 1.7440059185028076
iteration 600, loss 1.684537410736084
iteration 700, loss 1.7368478775024414
iteration 800, loss 1.7150872945785522
iteration 0, loss 1.715490460395813
iteration 100, loss 1.735967755317688
iteration 200, loss 1.737050175666809
iteration 300, loss 1.8604737520217896
iteration 400, loss 1.7481791973114014
iteration 500, loss 1.7240101099014282
iteration 600, loss 1.7172608375549316
iteration 700, loss 1.7618696689605713
iteration 800, loss 1.758362054824829
iteration 0, loss 1.713655710220337
iteration 100, loss 1.7139590978622437
iteration 200, loss 1.6562507152557373
iteration 300, loss 1.7401096820831299
iteration 400, loss 1.7463257312774658
iteration 500, loss 1.7710790634155273
iteration 600, loss 1.782192587852478
iteration 700, loss 1.7887682914733887
iteration 800, loss 1.739571213722229
iteration 0, loss 1.7549446821212769
iteration 100, loss 1.730568289756775
iteration 200, loss 1.6948539018630981
iteration 300, loss 1.702367901802063
iteration 400, loss 1.7706698179244995
iteration 500, loss 1.662408709526062
iteration 600, loss 1.7704463005065918
iteration 700, loss 1.7966572046279907
iteration 800, loss 1.7307323217391968
iteration 0, loss 1.7356590032577515
iteration 100, loss 1.7400617599487305
iteration 200, loss 1.7663211822509766
iteration 300, loss 1.762884259223938
iteration 400, loss 1.7773182392120361
iteration 500, loss 1.7635865211486816
iteration 600, loss 1.7394402027130127
iteration 700, loss 1.81720769405365
iteration 800, loss 1.749466896057129
iteration 0, loss 1.697985053062439
iteration 100, loss 1.7798657417297363
iteration 200, loss 1.7272710800170898
iteration 300, loss 1.705927848815918
iteration 400, loss 1.7556923627853394
iteration 500, loss 1.792630672454834
iteration 600, loss 1.7474459409713745
iteration 700, loss 1.7239909172058105
iteration 800, loss 1.7332018613815308
iteration 0, loss 1.714655876159668
iteration 100, loss 1.7509620189666748
iteration 200, loss 1.7659636735916138
iteration 300, loss 1.739240288734436
iteration 400, loss 1.7120035886764526
iteration 500, loss 1.7000079154968262
iteration 600, loss 1.7370764017105103
iteration 700, loss 1.7253215312957764
iteration 800, loss 1.7758429050445557
iteration 0, loss 1.7760909795761108
iteration 100, loss 1.740962266921997
iteration 200, loss 1.6973053216934204
iteration 300, loss 1.8054893016815186
iteration 400, loss 1.735850214958191
iteration 500, loss 1.6916062831878662
iteration 600, loss 1.6759059429168701
iteration 700, loss 1.7683539390563965
iteration 800, loss 1.7258442640304565
iteration 0, loss 1.7108502388000488
iteration 100, loss 1.7342078685760498
iteration 200, loss 1.7630234956741333
iteration 300, loss 1.6514413356781006
iteration 400, loss 1.7299480438232422
iteration 500, loss 1.7706964015960693
iteration 600, loss 1.717722773551941
iteration 700, loss 1.7491039037704468
iteration 800, loss 1.7198526859283447
iteration 0, loss 1.7321070432662964
iteration 100, loss 1.707741379737854
iteration 200, loss 1.7715628147125244
iteration 300, loss 1.7564165592193604
iteration 400, loss 1.6892982721328735
iteration 500, loss 1.7020820379257202
iteration 600, loss 1.720749855041504
iteration 700, loss 1.7388174533843994
iteration 800, loss 1.7829762697219849
iteration 0, loss 1.843146800994873
iteration 100, loss 1.6967122554779053
iteration 200, loss 1.6800177097320557
iteration 300, loss 1.6377382278442383
iteration 400, loss 1.7476361989974976
iteration 500, loss 1.7020200490951538
iteration 600, loss 1.7841755151748657
iteration 700, loss 1.7749913930892944
iteration 800, loss 1.7607929706573486
iteration 0, loss 1.70882248878479
iteration 100, loss 1.691268801689148
iteration 200, loss 1.7026081085205078
iteration 300, loss 1.8044925928115845
iteration 400, loss 1.7420568466186523
iteration 500, loss 1.7838908433914185
iteration 600, loss 1.7577983140945435
iteration 700, loss 1.7504836320877075
iteration 800, loss 1.7108091115951538
iteration 0, loss 1.6912446022033691
iteration 100, loss 1.7827659845352173
iteration 200, loss 1.7623895406723022
iteration 300, loss 1.7319233417510986
iteration 400, loss 1.6966145038604736
iteration 500, loss 1.7750763893127441
iteration 600, loss 1.7839494943618774
iteration 700, loss 1.7390328645706177
iteration 800, loss 1.6997183561325073
iteration 0, loss 1.7400001287460327
iteration 100, loss 1.7451051473617554
iteration 200, loss 1.694973349571228
iteration 300, loss 1.661710500717163
iteration 400, loss 1.7442712783813477
iteration 500, loss 1.7265591621398926
iteration 600, loss 1.7103849649429321
iteration 700, loss 1.7002593278884888
iteration 800, loss 1.749589443206787
iteration 0, loss 1.7229151725769043
iteration 100, loss 1.7187118530273438
iteration 200, loss 1.656700611114502
iteration 300, loss 1.6928365230560303
iteration 400, loss 1.6737289428710938
iteration 500, loss 1.7082509994506836
iteration 600, loss 1.7634822130203247
iteration 700, loss 1.7624200582504272
iteration 800, loss 1.7478970289230347
iteration 0, loss 1.6966159343719482
iteration 100, loss 1.733181357383728
iteration 200, loss 1.715987205505371
iteration 300, loss 1.7695895433425903
iteration 400, loss 1.7111680507659912
iteration 500, loss 1.7228517532348633
iteration 600, loss 1.7282449007034302
iteration 700, loss 1.7566241025924683
iteration 800, loss 1.6650608777999878
fold 0 accuracy: 0.5947142857142858
iteration 0, loss 1.779748797416687
iteration 100, loss 1.74050772190094
iteration 200, loss 1.7254385948181152
iteration 300, loss 1.7480639219284058
iteration 400, loss 1.7674916982650757
iteration 500, loss 1.7575238943099976
iteration 600, loss 1.7752538919448853
iteration 700, loss 1.7302325963974
iteration 800, loss 1.7156622409820557
iteration 0, loss 1.7464200258255005
iteration 100, loss 1.6884770393371582
iteration 200, loss 1.6933202743530273
iteration 300, loss 1.7391645908355713
iteration 400, loss 1.675927996635437
iteration 500, loss 1.715333342552185
iteration 600, loss 1.7153103351593018
iteration 700, loss 1.7290387153625488
iteration 800, loss 1.7184679508209229
iteration 0, loss 1.6517950296401978
iteration 100, loss 1.7501018047332764
iteration 200, loss 1.7411638498306274
iteration 300, loss 1.7559293508529663
iteration 400, loss 1.760006308555603
iteration 500, loss 1.7232249975204468
iteration 600, loss 1.7721338272094727
iteration 700, loss 1.6682981252670288
iteration 800, loss 1.6793887615203857
iteration 0, loss 1.7411561012268066
iteration 100, loss 1.7896606922149658
iteration 200, loss 1.749603509902954
iteration 300, loss 1.6520448923110962
iteration 400, loss 1.7225220203399658
iteration 500, loss 1.734628677368164
iteration 600, loss 1.681460976600647
iteration 700, loss 1.7804921865463257
iteration 800, loss 1.6867802143096924
iteration 0, loss 1.7342894077301025
iteration 100, loss 1.839874029159546
iteration 200, loss 1.6839076280593872
iteration 300, loss 1.665691614151001
iteration 400, loss 1.7525243759155273
iteration 500, loss 1.7328691482543945
iteration 600, loss 1.6691672801971436
iteration 700, loss 1.7928818464279175
iteration 800, loss 1.6411982774734497
iteration 0, loss 1.7460061311721802
iteration 100, loss 1.7253398895263672
iteration 200, loss 1.7538422346115112
iteration 300, loss 1.7991970777511597
iteration 400, loss 1.7953628301620483
iteration 500, loss 1.7251402139663696
iteration 600, loss 1.7249385118484497
iteration 700, loss 1.6780462265014648
iteration 800, loss 1.7005095481872559
iteration 0, loss 1.7070465087890625
iteration 100, loss 1.787605881690979
iteration 200, loss 1.738221526145935
iteration 300, loss 1.7617428302764893
iteration 400, loss 1.6613523960113525
iteration 500, loss 1.784905195236206
iteration 600, loss 1.7940938472747803
iteration 700, loss 1.765265703201294
iteration 800, loss 1.7418333292007446
iteration 0, loss 1.8239381313323975
iteration 100, loss 1.718645691871643
iteration 200, loss 1.701716423034668
iteration 300, loss 1.6799837350845337
iteration 400, loss 1.6738207340240479
iteration 500, loss 1.7993606328964233
iteration 600, loss 1.7376964092254639
iteration 700, loss 1.734632134437561
iteration 800, loss 1.7531604766845703
iteration 0, loss 1.6968352794647217
iteration 100, loss 1.6531013250350952
iteration 200, loss 1.7207986116409302
iteration 300, loss 1.7319974899291992
iteration 400, loss 1.675135850906372
iteration 500, loss 1.6768006086349487
iteration 600, loss 1.7060483694076538
iteration 700, loss 1.6897143125534058
iteration 800, loss 1.7661139965057373
iteration 0, loss 1.745235800743103
iteration 100, loss 1.7344318628311157
iteration 200, loss 1.741121768951416
iteration 300, loss 1.6931129693984985
iteration 400, loss 1.742673635482788
iteration 500, loss 1.7617816925048828
iteration 600, loss 1.7170530557632446
iteration 700, loss 1.742935299873352
iteration 800, loss 1.7648613452911377
iteration 0, loss 1.7222998142242432
iteration 100, loss 1.6574634313583374
iteration 200, loss 1.7443976402282715
iteration 300, loss 1.7238048315048218
iteration 400, loss 1.68064284324646
iteration 500, loss 1.7244019508361816
iteration 600, loss 1.7221894264221191
iteration 700, loss 1.7013345956802368
iteration 800, loss 1.784501075744629
iteration 0, loss 1.7987959384918213
iteration 100, loss 1.795751929283142
iteration 200, loss 1.6556713581085205
iteration 300, loss 1.8179558515548706
iteration 400, loss 1.7111070156097412
iteration 500, loss 1.7121524810791016
iteration 600, loss 1.7617915868759155
iteration 700, loss 1.7129809856414795
iteration 800, loss 1.7899560928344727
iteration 0, loss 1.7254269123077393
iteration 100, loss 1.7149724960327148
iteration 200, loss 1.6923186779022217
iteration 300, loss 1.7105281352996826
iteration 400, loss 1.7069765329360962
iteration 500, loss 1.6929762363433838
iteration 600, loss 1.73370361328125
iteration 700, loss 1.6702252626419067
iteration 800, loss 1.7602297067642212
iteration 0, loss 1.8153399229049683
iteration 100, loss 1.726881504058838
iteration 200, loss 1.708211898803711
iteration 300, loss 1.7590948343276978
iteration 400, loss 1.696725845336914
iteration 500, loss 1.7381349802017212
iteration 600, loss 1.7576854228973389
iteration 700, loss 1.7632263898849487
iteration 800, loss 1.7338300943374634
iteration 0, loss 1.717086672782898
iteration 100, loss 1.6591507196426392
iteration 200, loss 1.6759995222091675
iteration 300, loss 1.7051303386688232
iteration 400, loss 1.805161714553833
iteration 500, loss 1.7495135068893433
iteration 600, loss 1.734714388847351
iteration 700, loss 1.694962501525879
iteration 800, loss 1.7116963863372803
iteration 0, loss 1.7228782176971436
iteration 100, loss 1.7734506130218506
iteration 200, loss 1.7344452142715454
iteration 300, loss 1.7483412027359009
iteration 400, loss 1.7438410520553589
iteration 500, loss 1.7759263515472412
iteration 600, loss 1.7292159795761108
iteration 700, loss 1.7508039474487305
iteration 800, loss 1.6891400814056396
iteration 0, loss 1.7228349447250366
iteration 100, loss 1.6738436222076416
iteration 200, loss 1.6974214315414429
iteration 300, loss 1.7533924579620361
iteration 400, loss 1.7583438158035278
iteration 500, loss 1.7585335969924927
iteration 600, loss 1.7015564441680908
iteration 700, loss 1.6740232706069946
iteration 800, loss 1.7119354009628296
iteration 0, loss 1.7123299837112427
iteration 100, loss 1.6678752899169922
iteration 200, loss 1.7089380025863647
iteration 300, loss 1.6658775806427002
iteration 400, loss 1.733201265335083
iteration 500, loss 1.6780650615692139
iteration 600, loss 1.743297815322876
iteration 700, loss 1.7204134464263916
iteration 800, loss 1.776699185371399
iteration 0, loss 1.873286485671997
iteration 100, loss 1.7564510107040405
iteration 200, loss 1.7338002920150757
iteration 300, loss 1.7544324398040771
iteration 400, loss 1.7464302778244019
iteration 500, loss 1.7363580465316772
iteration 600, loss 1.6844385862350464
iteration 700, loss 1.7460235357284546
iteration 800, loss 1.693968415260315
iteration 0, loss 1.7325955629348755
iteration 100, loss 1.767772912979126
iteration 200, loss 1.7177189588546753
iteration 300, loss 1.7369624376296997
iteration 400, loss 1.7591532468795776
iteration 500, loss 1.7019495964050293
iteration 600, loss 1.6450474262237549
iteration 700, loss 1.7296041250228882
iteration 800, loss 1.646697759628296
iteration 0, loss 1.7427926063537598
iteration 100, loss 1.6895009279251099
iteration 200, loss 1.6751184463500977
iteration 300, loss 1.7551138401031494
iteration 400, loss 1.765772819519043
iteration 500, loss 1.6951051950454712
iteration 600, loss 1.7684730291366577
iteration 700, loss 1.7784464359283447
iteration 800, loss 1.7018331289291382
iteration 0, loss 1.7655222415924072
iteration 100, loss 1.7213728427886963
iteration 200, loss 1.6587493419647217
iteration 300, loss 1.810741901397705
iteration 400, loss 1.7564713954925537
iteration 500, loss 1.7557754516601562
iteration 600, loss 1.7135357856750488
iteration 700, loss 1.7510097026824951
iteration 800, loss 1.6969329118728638
iteration 0, loss 1.777931809425354
iteration 100, loss 1.7031221389770508
iteration 200, loss 1.7509160041809082
iteration 300, loss 1.6816784143447876
iteration 400, loss 1.6904356479644775
iteration 500, loss 1.7231603860855103
iteration 600, loss 1.6807279586791992
iteration 700, loss 1.7363213300704956
iteration 800, loss 1.7079404592514038
iteration 0, loss 1.7021232843399048
iteration 100, loss 1.7682034969329834
iteration 200, loss 1.7191567420959473
iteration 300, loss 1.7656919956207275
iteration 400, loss 1.6984481811523438
iteration 500, loss 1.792892336845398
iteration 600, loss 1.7087960243225098
iteration 700, loss 1.7334959506988525
iteration 800, loss 1.7611157894134521
iteration 0, loss 1.693366527557373
iteration 100, loss 1.7562007904052734
iteration 200, loss 1.741763949394226
iteration 300, loss 1.6940805912017822
iteration 400, loss 1.803709864616394
iteration 500, loss 1.6922273635864258
iteration 600, loss 1.698178768157959
iteration 700, loss 1.6840394735336304
iteration 800, loss 1.740496277809143
iteration 0, loss 1.7402993440628052
iteration 100, loss 1.7237787246704102
iteration 200, loss 1.6776862144470215
iteration 300, loss 1.7883546352386475
iteration 400, loss 1.7121574878692627
iteration 500, loss 1.6879467964172363
iteration 600, loss 1.8169317245483398
iteration 700, loss 1.7905902862548828
iteration 800, loss 1.7664244174957275
iteration 0, loss 1.7300504446029663
iteration 100, loss 1.7048712968826294
iteration 200, loss 1.668874979019165
iteration 300, loss 1.7084002494812012
iteration 400, loss 1.7439610958099365
iteration 500, loss 1.7256062030792236
iteration 600, loss 1.7252745628356934
iteration 700, loss 1.748038411140442
iteration 800, loss 1.7126368284225464
iteration 0, loss 1.7439820766448975
iteration 100, loss 1.8237217664718628
iteration 200, loss 1.7532042264938354
iteration 300, loss 1.7986737489700317
iteration 400, loss 1.7621461153030396
iteration 500, loss 1.7509112358093262
iteration 600, loss 1.7806915044784546
iteration 700, loss 1.7464377880096436
iteration 800, loss 1.7523276805877686
iteration 0, loss 1.7296878099441528
iteration 100, loss 1.682651162147522
iteration 200, loss 1.690932035446167
iteration 300, loss 1.70424485206604
iteration 400, loss 1.742862343788147
iteration 500, loss 1.6530866622924805
iteration 600, loss 1.699070930480957
iteration 700, loss 1.646957278251648
iteration 800, loss 1.6900312900543213
iteration 0, loss 1.7578741312026978
iteration 100, loss 1.696010947227478
iteration 200, loss 1.7801687717437744
iteration 300, loss 1.6990532875061035
iteration 400, loss 1.7158616781234741
iteration 500, loss 1.7163540124893188
iteration 600, loss 1.6792603731155396
iteration 700, loss 1.7310705184936523
iteration 800, loss 1.6432560682296753
iteration 0, loss 1.7478065490722656
iteration 100, loss 1.6705106496810913
iteration 200, loss 1.7043883800506592
iteration 300, loss 1.7261821031570435
iteration 400, loss 1.7072174549102783
iteration 500, loss 1.6801228523254395
iteration 600, loss 1.7009074687957764
iteration 700, loss 1.7462135553359985
iteration 800, loss 1.696685791015625
iteration 0, loss 1.68606436252594
iteration 100, loss 1.768365502357483
iteration 200, loss 1.7363665103912354
iteration 300, loss 1.6612820625305176
iteration 400, loss 1.71988844871521
iteration 500, loss 1.7444937229156494
iteration 600, loss 1.7303272485733032
iteration 700, loss 1.733796238899231
iteration 800, loss 1.6713902950286865
iteration 0, loss 1.749909520149231
iteration 100, loss 1.736816644668579
iteration 200, loss 1.7272197008132935
iteration 300, loss 1.7332148551940918
iteration 400, loss 1.6843597888946533
iteration 500, loss 1.6444029808044434
iteration 600, loss 1.761049747467041
iteration 700, loss 1.7197895050048828
iteration 800, loss 1.770579218864441
iteration 0, loss 1.7460525035858154
iteration 100, loss 1.6992326974868774
iteration 200, loss 1.6722866296768188
iteration 300, loss 1.6850674152374268
iteration 400, loss 1.7630653381347656
iteration 500, loss 1.7480552196502686
iteration 600, loss 1.73264741897583
iteration 700, loss 1.7217800617218018
iteration 800, loss 1.7111191749572754
iteration 0, loss 1.6907252073287964
iteration 100, loss 1.6946353912353516
iteration 200, loss 1.751860499382019
iteration 300, loss 1.670204997062683
iteration 400, loss 1.745403528213501
iteration 500, loss 1.8144571781158447
iteration 600, loss 1.7263208627700806
iteration 700, loss 1.8077723979949951
iteration 800, loss 1.6925283670425415
iteration 0, loss 1.7120246887207031
iteration 100, loss 1.7052531242370605
iteration 200, loss 1.7212395668029785
iteration 300, loss 1.7095258235931396
iteration 400, loss 1.6969624757766724
iteration 500, loss 1.7105909585952759
iteration 600, loss 1.6837493181228638
iteration 700, loss 1.678864598274231
iteration 800, loss 1.7119548320770264
iteration 0, loss 1.7938159704208374
iteration 100, loss 1.7079648971557617
iteration 200, loss 1.6928478479385376
iteration 300, loss 1.706947922706604
iteration 400, loss 1.764367699623108
iteration 500, loss 1.726906180381775
iteration 600, loss 1.6958377361297607
iteration 700, loss 1.7333006858825684
iteration 800, loss 1.7854375839233398
iteration 0, loss 1.7645715475082397
iteration 100, loss 1.7353641986846924
iteration 200, loss 1.668997883796692
iteration 300, loss 1.7412986755371094
iteration 400, loss 1.749101996421814
iteration 500, loss 1.748749852180481
iteration 600, loss 1.6983472108840942
iteration 700, loss 1.7960201501846313
iteration 800, loss 1.7840393781661987
iteration 0, loss 1.6922662258148193
iteration 100, loss 1.643317461013794
iteration 200, loss 1.7218642234802246
iteration 300, loss 1.6917330026626587
iteration 400, loss 1.6842762231826782
iteration 500, loss 1.701517105102539
iteration 600, loss 1.7556138038635254
iteration 700, loss 1.6761901378631592
iteration 800, loss 1.7588169574737549
iteration 0, loss 1.7368836402893066
iteration 100, loss 1.7653119564056396
iteration 200, loss 1.751491904258728
iteration 300, loss 1.673958659172058
iteration 400, loss 1.6446501016616821
iteration 500, loss 1.6506351232528687
iteration 600, loss 1.7203476428985596
iteration 700, loss 1.7531994581222534
iteration 800, loss 1.7695837020874023
iteration 0, loss 1.6980489492416382
iteration 100, loss 1.694097876548767
iteration 200, loss 1.7314176559448242
iteration 300, loss 1.6809636354446411
iteration 400, loss 1.7272125482559204
iteration 500, loss 1.7061803340911865
iteration 600, loss 1.655724287033081
iteration 700, loss 1.6955292224884033
iteration 800, loss 1.7075480222702026
iteration 0, loss 1.691933035850525
iteration 100, loss 1.788422703742981
iteration 200, loss 1.7301511764526367
iteration 300, loss 1.7370307445526123
iteration 400, loss 1.6740425825119019
iteration 500, loss 1.7158640623092651
iteration 600, loss 1.7235008478164673
iteration 700, loss 1.7195247411727905
iteration 800, loss 1.7269014120101929
iteration 0, loss 1.7679312229156494
iteration 100, loss 1.7242701053619385
iteration 200, loss 1.6886554956436157
iteration 300, loss 1.7499852180480957
iteration 400, loss 1.635745882987976
iteration 500, loss 1.7049423456192017
iteration 600, loss 1.707146167755127
iteration 700, loss 1.780583143234253
iteration 800, loss 1.7077480554580688
iteration 0, loss 1.729079246520996
iteration 100, loss 1.766548752784729
iteration 200, loss 1.7245241403579712
iteration 300, loss 1.7677996158599854
iteration 400, loss 1.7282298803329468
iteration 500, loss 1.7483205795288086
iteration 600, loss 1.75730562210083
iteration 700, loss 1.6868609189987183
iteration 800, loss 1.700860857963562
iteration 0, loss 1.6731990575790405
iteration 100, loss 1.7286360263824463
iteration 200, loss 1.7513090372085571
iteration 300, loss 1.7519038915634155
iteration 400, loss 1.6712760925292969
iteration 500, loss 1.731099009513855
iteration 600, loss 1.6292392015457153
iteration 700, loss 1.7035831212997437
iteration 800, loss 1.7130887508392334
iteration 0, loss 1.7420628070831299
iteration 100, loss 1.746464729309082
iteration 200, loss 1.689265251159668
iteration 300, loss 1.69916832447052
iteration 400, loss 1.739553689956665
iteration 500, loss 1.7364434003829956
iteration 600, loss 1.7271268367767334
iteration 700, loss 1.732870101928711
iteration 800, loss 1.6818066835403442
iteration 0, loss 1.6908667087554932
iteration 100, loss 1.739152431488037
iteration 200, loss 1.7903178930282593
iteration 300, loss 1.814314842224121
iteration 400, loss 1.7359211444854736
iteration 500, loss 1.7225933074951172
iteration 600, loss 1.7690496444702148
iteration 700, loss 1.6930040121078491
iteration 800, loss 1.7210898399353027
iteration 0, loss 1.7464635372161865
iteration 100, loss 1.6877602338790894
iteration 200, loss 1.7602477073669434
iteration 300, loss 1.721320629119873
iteration 400, loss 1.705243468284607
iteration 500, loss 1.6885032653808594
iteration 600, loss 1.7025032043457031
iteration 700, loss 1.8519021272659302
iteration 800, loss 1.7481532096862793
iteration 0, loss 1.81557035446167
iteration 100, loss 1.7068872451782227
iteration 200, loss 1.7112233638763428
iteration 300, loss 1.6998368501663208
iteration 400, loss 1.7523380517959595
iteration 500, loss 1.7138235569000244
iteration 600, loss 1.7032431364059448
iteration 700, loss 1.6605771780014038
iteration 800, loss 1.7486751079559326
iteration 0, loss 1.7492955923080444
iteration 100, loss 1.6969037055969238
iteration 200, loss 1.7475587129592896
iteration 300, loss 1.733994722366333
iteration 400, loss 1.649882197380066
iteration 500, loss 1.7452130317687988
iteration 600, loss 1.7690483331680298
iteration 700, loss 1.7017464637756348
iteration 800, loss 1.7214627265930176
fold 1 accuracy: 0.6229285714285714
iteration 0, loss 1.6972227096557617
iteration 100, loss 1.7872990369796753
iteration 200, loss 1.693029522895813
iteration 300, loss 1.705820918083191
iteration 400, loss 1.7550537586212158
iteration 500, loss 1.7625943422317505
iteration 600, loss 1.6882247924804688
iteration 700, loss 1.7560497522354126
iteration 800, loss 1.7308101654052734
iteration 0, loss 1.8053771257400513
iteration 100, loss 1.7096184492111206
iteration 200, loss 1.7131925821304321
iteration 300, loss 1.6828057765960693
iteration 400, loss 1.6825189590454102
iteration 500, loss 1.6439831256866455
iteration 600, loss 1.737842082977295
iteration 700, loss 1.666224479675293
iteration 800, loss 1.7516875267028809
iteration 0, loss 1.7244912385940552
iteration 100, loss 1.7786656618118286
iteration 200, loss 1.742370367050171
iteration 300, loss 1.648244857788086
iteration 400, loss 1.7341806888580322
iteration 500, loss 1.7666677236557007
iteration 600, loss 1.732733964920044
iteration 700, loss 1.7575578689575195
iteration 800, loss 1.6487758159637451
iteration 0, loss 1.6533679962158203
iteration 100, loss 1.7421157360076904
iteration 200, loss 1.7143781185150146
iteration 300, loss 1.6807277202606201
iteration 400, loss 1.6443428993225098
iteration 500, loss 1.7851319313049316
iteration 600, loss 1.7193206548690796
iteration 700, loss 1.7424054145812988
iteration 800, loss 1.6724673509597778
iteration 0, loss 1.7452175617218018
iteration 100, loss 1.6834427118301392
iteration 200, loss 1.7227290868759155
iteration 300, loss 1.671061635017395
iteration 400, loss 1.721647024154663
iteration 500, loss 1.6914644241333008
iteration 600, loss 1.6669328212738037
iteration 700, loss 1.6414958238601685
iteration 800, loss 1.6526679992675781
iteration 0, loss 1.720771312713623
iteration 100, loss 1.691628336906433
iteration 200, loss 1.7203863859176636
iteration 300, loss 1.6805751323699951
iteration 400, loss 1.721617341041565
iteration 500, loss 1.7377444505691528
iteration 600, loss 1.6969542503356934
iteration 700, loss 1.7344483137130737
iteration 800, loss 1.691692590713501
iteration 0, loss 1.6758315563201904
iteration 100, loss 1.7004139423370361
iteration 200, loss 1.6167107820510864
iteration 300, loss 1.7613409757614136
iteration 400, loss 1.6863906383514404
iteration 500, loss 1.7165499925613403
iteration 600, loss 1.7394388914108276
iteration 700, loss 1.6927484273910522
iteration 800, loss 1.7109534740447998
iteration 0, loss 1.6937376260757446
iteration 100, loss 1.680348515510559
iteration 200, loss 1.8040803670883179
iteration 300, loss 1.762027621269226
iteration 400, loss 1.7841960191726685
iteration 500, loss 1.7628623247146606
iteration 600, loss 1.7181906700134277
iteration 700, loss 1.7214441299438477
iteration 800, loss 1.6796574592590332
iteration 0, loss 1.6616058349609375
iteration 100, loss 1.6615731716156006
iteration 200, loss 1.639974594116211
iteration 300, loss 1.7463880777359009
iteration 400, loss 1.7702736854553223
iteration 500, loss 1.6729785203933716
iteration 600, loss 1.726090669631958
iteration 700, loss 1.7460256814956665
iteration 800, loss 1.66920804977417
iteration 0, loss 1.7644935846328735
iteration 100, loss 1.7252758741378784
iteration 200, loss 1.7094517946243286
iteration 300, loss 1.7246438264846802
iteration 400, loss 1.7082358598709106
iteration 500, loss 1.6681653261184692
iteration 600, loss 1.7158771753311157
iteration 700, loss 1.7378062009811401
iteration 800, loss 1.6711491346359253
iteration 0, loss 1.734851598739624
iteration 100, loss 1.737471103668213
iteration 200, loss 1.7025030851364136
iteration 300, loss 1.703588843345642
iteration 400, loss 1.7701489925384521
iteration 500, loss 1.7349166870117188
iteration 600, loss 1.6965903043746948
iteration 700, loss 1.673205018043518
iteration 800, loss 1.7298810482025146
iteration 0, loss 1.7655225992202759
iteration 100, loss 1.68070387840271
iteration 200, loss 1.7656477689743042
iteration 300, loss 1.6937165260314941
iteration 400, loss 1.7176450490951538
iteration 500, loss 1.7411901950836182
iteration 600, loss 1.6812114715576172
iteration 700, loss 1.7245292663574219
iteration 800, loss 1.728285789489746
iteration 0, loss 1.7608548402786255
iteration 100, loss 1.7692333459854126
iteration 200, loss 1.6743383407592773
iteration 300, loss 1.7856394052505493
iteration 400, loss 1.7566866874694824
iteration 500, loss 1.7949668169021606
iteration 600, loss 1.708067536354065
iteration 700, loss 1.740113615989685
iteration 800, loss 1.6668421030044556
iteration 0, loss 1.672993779182434
iteration 100, loss 1.7198855876922607
iteration 200, loss 1.7373228073120117
iteration 300, loss 1.7846863269805908
iteration 400, loss 1.7241461277008057
iteration 500, loss 1.761102557182312
iteration 600, loss 1.6741654872894287
iteration 700, loss 1.6679500341415405
iteration 800, loss 1.698716402053833
iteration 0, loss 1.7655149698257446
iteration 100, loss 1.7853541374206543
iteration 200, loss 1.7346093654632568
iteration 300, loss 1.7154457569122314
iteration 400, loss 1.7659326791763306
iteration 500, loss 1.6745895147323608
iteration 600, loss 1.7109060287475586
iteration 700, loss 1.734553575515747
iteration 800, loss 1.7697672843933105
iteration 0, loss 1.7006193399429321
iteration 100, loss 1.6948493719100952
iteration 200, loss 1.7614949941635132
iteration 300, loss 1.6761385202407837
iteration 400, loss 1.7171761989593506
iteration 500, loss 1.7650169134140015
iteration 600, loss 1.7682133913040161
iteration 700, loss 1.712918758392334
iteration 800, loss 1.6607574224472046
iteration 0, loss 1.7426382303237915
iteration 100, loss 1.7153677940368652
iteration 200, loss 1.7315380573272705
iteration 300, loss 1.7335811853408813
iteration 400, loss 1.744530200958252
iteration 500, loss 1.7224775552749634
iteration 600, loss 1.740203857421875
iteration 700, loss 1.7210983037948608
iteration 800, loss 1.684927225112915
iteration 0, loss 1.7010623216629028
iteration 100, loss 1.7038319110870361
iteration 200, loss 1.7545666694641113
iteration 300, loss 1.7463182210922241
iteration 400, loss 1.6655519008636475
iteration 500, loss 1.6984914541244507
iteration 600, loss 1.7109639644622803
iteration 700, loss 1.7236770391464233
iteration 800, loss 1.747831106185913
iteration 0, loss 1.7078455686569214
iteration 100, loss 1.6870917081832886
iteration 200, loss 1.7363427877426147
iteration 300, loss 1.738368034362793
iteration 400, loss 1.6684212684631348
iteration 500, loss 1.7524511814117432
iteration 600, loss 1.6766611337661743
iteration 700, loss 1.6793910264968872
iteration 800, loss 1.7413028478622437
iteration 0, loss 1.7308552265167236
iteration 100, loss 1.6812734603881836
iteration 200, loss 1.765108585357666
iteration 300, loss 1.711931586265564
iteration 400, loss 1.6800025701522827
iteration 500, loss 1.7280676364898682
iteration 600, loss 1.7016642093658447
iteration 700, loss 1.7890616655349731
iteration 800, loss 1.7044576406478882
iteration 0, loss 1.7768269777297974
iteration 100, loss 1.6710405349731445
iteration 200, loss 1.688598394393921
iteration 300, loss 1.7032415866851807
iteration 400, loss 1.7318525314331055
iteration 500, loss 1.6633796691894531
iteration 600, loss 1.6876423358917236
iteration 700, loss 1.6780345439910889
iteration 800, loss 1.71402907371521
iteration 0, loss 1.7742624282836914
iteration 100, loss 1.7214754819869995
iteration 200, loss 1.680174469947815
iteration 300, loss 1.7058213949203491
iteration 400, loss 1.7254725694656372
iteration 500, loss 1.6975300312042236
iteration 600, loss 1.7189750671386719
iteration 700, loss 1.7104191780090332
iteration 800, loss 1.710342288017273
iteration 0, loss 1.7338746786117554
iteration 100, loss 1.7025245428085327
iteration 200, loss 1.6844667196273804
iteration 300, loss 1.7650312185287476
iteration 400, loss 1.7377039194107056
iteration 500, loss 1.7275447845458984
iteration 600, loss 1.704280138015747
iteration 700, loss 1.7323451042175293
iteration 800, loss 1.8058314323425293
iteration 0, loss 1.7086204290390015
iteration 100, loss 1.6601182222366333
iteration 200, loss 1.751288652420044
iteration 300, loss 1.7569888830184937
iteration 400, loss 1.7301757335662842
iteration 500, loss 1.682246208190918
iteration 600, loss 1.7309329509735107
iteration 700, loss 1.6772453784942627
iteration 800, loss 1.8010863065719604
iteration 0, loss 1.7434579133987427
iteration 100, loss 1.682849407196045
iteration 200, loss 1.7576255798339844
iteration 300, loss 1.686361312866211
iteration 400, loss 1.7426365613937378
iteration 500, loss 1.7494834661483765
iteration 600, loss 1.6811881065368652
iteration 700, loss 1.7506422996520996
iteration 800, loss 1.7260677814483643
iteration 0, loss 1.693334937095642
iteration 100, loss 1.6075173616409302
iteration 200, loss 1.6928014755249023
iteration 300, loss 1.7184391021728516
iteration 400, loss 1.6909840106964111
iteration 500, loss 1.7235527038574219
iteration 600, loss 1.795322060585022
iteration 700, loss 1.678954005241394
iteration 800, loss 1.7831865549087524
iteration 0, loss 1.6560033559799194
iteration 100, loss 1.650248646736145
iteration 200, loss 1.6631520986557007
iteration 300, loss 1.7080358266830444
iteration 400, loss 1.7399896383285522
iteration 500, loss 1.7644747495651245
iteration 600, loss 1.771139144897461
iteration 700, loss 1.671893835067749
iteration 800, loss 1.725579023361206
iteration 0, loss 1.6313245296478271
iteration 100, loss 1.7613660097122192
iteration 200, loss 1.810966968536377
iteration 300, loss 1.7825924158096313
iteration 400, loss 1.7115479707717896
iteration 500, loss 1.7136502265930176
iteration 600, loss 1.7227078676223755
iteration 700, loss 1.743491291999817
iteration 800, loss 1.7848843336105347
iteration 0, loss 1.8250904083251953
iteration 100, loss 1.7314494848251343
iteration 200, loss 1.6743499040603638
iteration 300, loss 1.6783603429794312
iteration 400, loss 1.7505797147750854
iteration 500, loss 1.7413122653961182
iteration 600, loss 1.7041871547698975
iteration 700, loss 1.7415554523468018
iteration 800, loss 1.7169691324234009
iteration 0, loss 1.710440993309021
iteration 100, loss 1.760683298110962
iteration 200, loss 1.6789745092391968
iteration 300, loss 1.7107818126678467
iteration 400, loss 1.6790307760238647
iteration 500, loss 1.7256723642349243
iteration 600, loss 1.6866084337234497
iteration 700, loss 1.6796993017196655
iteration 800, loss 1.7200759649276733
iteration 0, loss 1.7008693218231201
iteration 100, loss 1.694156527519226
iteration 200, loss 1.675115704536438
iteration 300, loss 1.6767741441726685
iteration 400, loss 1.662316083908081
iteration 500, loss 1.7184427976608276
iteration 600, loss 1.7612017393112183
iteration 700, loss 1.7246443033218384
iteration 800, loss 1.6732378005981445
iteration 0, loss 1.7312345504760742
iteration 100, loss 1.7997350692749023
iteration 200, loss 1.7647547721862793
iteration 300, loss 1.719459056854248
iteration 400, loss 1.7370274066925049
iteration 500, loss 1.7846522331237793
iteration 600, loss 1.7666256427764893
iteration 700, loss 1.6772421598434448
iteration 800, loss 1.653574824333191
iteration 0, loss 1.684211254119873
iteration 100, loss 1.7798911333084106
iteration 200, loss 1.7504380941390991
iteration 300, loss 1.710977554321289
iteration 400, loss 1.703305959701538
iteration 500, loss 1.7068867683410645
iteration 600, loss 1.6950352191925049
iteration 700, loss 1.7542879581451416
iteration 800, loss 1.6314888000488281
iteration 0, loss 1.7076749801635742
iteration 100, loss 1.6989516019821167
iteration 200, loss 1.7516276836395264
iteration 300, loss 1.68361234664917
iteration 400, loss 1.7161067724227905
iteration 500, loss 1.7403277158737183
iteration 600, loss 1.7710126638412476
iteration 700, loss 1.7681889533996582
iteration 800, loss 1.7061036825180054
iteration 0, loss 1.6639387607574463
iteration 100, loss 1.7077332735061646
iteration 200, loss 1.7374826669692993
iteration 300, loss 1.7543870210647583
iteration 400, loss 1.7538139820098877
iteration 500, loss 1.695846676826477
iteration 600, loss 1.6801085472106934
iteration 700, loss 1.6898871660232544
iteration 800, loss 1.7667689323425293
iteration 0, loss 1.7418715953826904
iteration 100, loss 1.7691171169281006
iteration 200, loss 1.7218210697174072
iteration 300, loss 1.7190051078796387
iteration 400, loss 1.6968700885772705
iteration 500, loss 1.7136138677597046
iteration 600, loss 1.714341163635254
iteration 700, loss 1.7254576683044434
iteration 800, loss 1.6768152713775635
iteration 0, loss 1.7480554580688477
iteration 100, loss 1.730688452720642
iteration 200, loss 1.699702262878418
iteration 300, loss 1.7114315032958984
iteration 400, loss 1.8131093978881836
iteration 500, loss 1.7303307056427002
iteration 600, loss 1.6684739589691162
iteration 700, loss 1.7167351245880127
iteration 800, loss 1.7479175329208374
iteration 0, loss 1.7140947580337524
iteration 100, loss 1.7009114027023315
iteration 200, loss 1.7528042793273926
iteration 300, loss 1.818918228149414
iteration 400, loss 1.7606834173202515
iteration 500, loss 1.76120126247406
iteration 600, loss 1.7560768127441406
iteration 700, loss 1.761962890625
iteration 800, loss 1.7336536645889282
iteration 0, loss 1.7216719388961792
iteration 100, loss 1.7092158794403076
iteration 200, loss 1.694871425628662
iteration 300, loss 1.7422055006027222
iteration 400, loss 1.7285891771316528
iteration 500, loss 1.7244237661361694
iteration 600, loss 1.7185090780258179
iteration 700, loss 1.6883171796798706
iteration 800, loss 1.7499465942382812
iteration 0, loss 1.7502471208572388
iteration 100, loss 1.7130624055862427
iteration 200, loss 1.7017502784729004
iteration 300, loss 1.7190680503845215
iteration 400, loss 1.7593315839767456
iteration 500, loss 1.655348777770996
iteration 600, loss 1.7002148628234863
iteration 700, loss 1.7106629610061646
iteration 800, loss 1.7517330646514893
iteration 0, loss 1.7440699338912964
iteration 100, loss 1.7287737131118774
iteration 200, loss 1.7524044513702393
iteration 300, loss 1.7774579524993896
iteration 400, loss 1.696499228477478
iteration 500, loss 1.7075756788253784
iteration 600, loss 1.7175441980361938
iteration 700, loss 1.7383031845092773
iteration 800, loss 1.692481279373169
iteration 0, loss 1.7829171419143677
iteration 100, loss 1.6994085311889648
iteration 200, loss 1.7136294841766357
iteration 300, loss 1.7703866958618164
iteration 400, loss 1.69575035572052
iteration 500, loss 1.678352952003479
iteration 600, loss 1.6940443515777588
iteration 700, loss 1.7319408655166626
iteration 800, loss 1.7728909254074097
iteration 0, loss 1.758834958076477
iteration 100, loss 1.6746265888214111
iteration 200, loss 1.7126777172088623
iteration 300, loss 1.6816389560699463
iteration 400, loss 1.71034836769104
iteration 500, loss 1.7162882089614868
iteration 600, loss 1.7891210317611694
iteration 700, loss 1.7091097831726074
iteration 800, loss 1.6565901041030884
iteration 0, loss 1.7107857465744019
iteration 100, loss 1.744041919708252
iteration 200, loss 1.7372081279754639
iteration 300, loss 1.7521138191223145
iteration 400, loss 1.707322359085083
iteration 500, loss 1.7104464769363403
iteration 600, loss 1.7588001489639282
iteration 700, loss 1.7369515895843506
iteration 800, loss 1.6899076700210571
iteration 0, loss 1.6513887643814087
iteration 100, loss 1.6959444284439087
iteration 200, loss 1.699751377105713
iteration 300, loss 1.7388269901275635
iteration 400, loss 1.7461228370666504
iteration 500, loss 1.7650266885757446
iteration 600, loss 1.7185503244400024
iteration 700, loss 1.6520923376083374
iteration 800, loss 1.6860326528549194
iteration 0, loss 1.7206854820251465
iteration 100, loss 1.6927357912063599
iteration 200, loss 1.7329965829849243
iteration 300, loss 1.7361947298049927
iteration 400, loss 1.7452343702316284
iteration 500, loss 1.720716118812561
iteration 600, loss 1.6868607997894287
iteration 700, loss 1.7345390319824219
iteration 800, loss 1.7234995365142822
iteration 0, loss 1.7353633642196655
iteration 100, loss 1.721327781677246
iteration 200, loss 1.6772651672363281
iteration 300, loss 1.7492296695709229
iteration 400, loss 1.7841482162475586
iteration 500, loss 1.7343084812164307
iteration 600, loss 1.7444391250610352
iteration 700, loss 1.6910710334777832
iteration 800, loss 1.6435456275939941
iteration 0, loss 1.7294179201126099
iteration 100, loss 1.7580024003982544
iteration 200, loss 1.7316546440124512
iteration 300, loss 1.7233061790466309
iteration 400, loss 1.697059154510498
iteration 500, loss 1.7072274684906006
iteration 600, loss 1.6864264011383057
iteration 700, loss 1.6935135126113892
iteration 800, loss 1.720002293586731
iteration 0, loss 1.6570284366607666
iteration 100, loss 1.6585235595703125
iteration 200, loss 1.6863930225372314
iteration 300, loss 1.758238673210144
iteration 400, loss 1.7635082006454468
iteration 500, loss 1.6908568143844604
iteration 600, loss 1.7052218914031982
iteration 700, loss 1.7316725254058838
iteration 800, loss 1.7217482328414917
iteration 0, loss 1.6503852605819702
iteration 100, loss 1.7255562543869019
iteration 200, loss 1.6766101121902466
iteration 300, loss 1.663478970527649
iteration 400, loss 1.7023248672485352
iteration 500, loss 1.6780688762664795
iteration 600, loss 1.6838679313659668
iteration 700, loss 1.679703712463379
iteration 800, loss 1.7766377925872803
fold 2 accuracy: 0.6452142857142857
iteration 0, loss 1.769734263420105
iteration 100, loss 1.7163667678833008
iteration 200, loss 1.7121596336364746
iteration 300, loss 1.7829841375350952
iteration 400, loss 1.7006362676620483
iteration 500, loss 1.7908942699432373
iteration 600, loss 1.6839591264724731
iteration 700, loss 1.7855812311172485
iteration 800, loss 1.738035798072815
iteration 0, loss 1.6540802717208862
iteration 100, loss 1.7421534061431885
iteration 200, loss 1.6999518871307373
iteration 300, loss 1.743424654006958
iteration 400, loss 1.7179555892944336
iteration 500, loss 1.731727123260498
iteration 600, loss 1.7020254135131836
iteration 700, loss 1.7172261476516724
iteration 800, loss 1.7221043109893799
iteration 0, loss 1.7247151136398315
iteration 100, loss 1.7265785932540894
iteration 200, loss 1.7405911684036255
iteration 300, loss 1.6697958707809448
iteration 400, loss 1.716133952140808
iteration 500, loss 1.703484058380127
iteration 600, loss 1.6798536777496338
iteration 700, loss 1.6994662284851074
iteration 800, loss 1.7164653539657593
iteration 0, loss 1.7544896602630615
iteration 100, loss 1.7242684364318848
iteration 200, loss 1.7165316343307495
iteration 300, loss 1.7255300283432007
iteration 400, loss 1.7096531391143799
iteration 500, loss 1.7059078216552734
iteration 600, loss 1.6751025915145874
iteration 700, loss 1.7359799146652222
iteration 800, loss 1.6767956018447876
iteration 0, loss 1.7365751266479492
iteration 100, loss 1.6868616342544556
iteration 200, loss 1.7189736366271973
iteration 300, loss 1.6982529163360596
iteration 400, loss 1.685187578201294
iteration 500, loss 1.6609071493148804
iteration 600, loss 1.7751801013946533
iteration 700, loss 1.7508435249328613
iteration 800, loss 1.6406899690628052
iteration 0, loss 1.7156009674072266
iteration 100, loss 1.756568431854248
iteration 200, loss 1.7578203678131104
iteration 300, loss 1.7341228723526
iteration 400, loss 1.8351439237594604
iteration 500, loss 1.6582207679748535
iteration 600, loss 1.7724549770355225
iteration 700, loss 1.7203541994094849
iteration 800, loss 1.7537868022918701
iteration 0, loss 1.680762767791748
iteration 100, loss 1.7081109285354614
iteration 200, loss 1.7063696384429932
iteration 300, loss 1.7739224433898926
iteration 400, loss 1.6700485944747925
iteration 500, loss 1.6730940341949463
iteration 600, loss 1.7431720495224
iteration 700, loss 1.7232894897460938
iteration 800, loss 1.665291428565979
iteration 0, loss 1.7185560464859009
iteration 100, loss 1.7944689989089966
iteration 200, loss 1.7040094137191772
iteration 300, loss 1.7317291498184204
iteration 400, loss 1.7864304780960083
iteration 500, loss 1.7280982732772827
iteration 600, loss 1.7158268690109253
iteration 700, loss 1.7593050003051758
iteration 800, loss 1.72890305519104
iteration 0, loss 1.7333426475524902
iteration 100, loss 1.7300411462783813
iteration 200, loss 1.7376961708068848
iteration 300, loss 1.7114050388336182
iteration 400, loss 1.6994870901107788
iteration 500, loss 1.7643837928771973
iteration 600, loss 1.7060256004333496
iteration 700, loss 1.6695072650909424
iteration 800, loss 1.7157793045043945
iteration 0, loss 1.6888368129730225
iteration 100, loss 1.6927701234817505
iteration 200, loss 1.7258539199829102
iteration 300, loss 1.6783336400985718
iteration 400, loss 1.6653525829315186
iteration 500, loss 1.6348567008972168
iteration 600, loss 1.7614359855651855
iteration 700, loss 1.685546636581421
iteration 800, loss 1.7019236087799072
iteration 0, loss 1.7157294750213623
iteration 100, loss 1.698203206062317
iteration 200, loss 1.7044419050216675
iteration 300, loss 1.6716761589050293
iteration 400, loss 1.73494553565979
iteration 500, loss 1.6637978553771973
iteration 600, loss 1.7438740730285645
iteration 700, loss 1.7002966403961182
iteration 800, loss 1.711753010749817
iteration 0, loss 1.7490140199661255
iteration 100, loss 1.7240492105484009
iteration 200, loss 1.6639560461044312
iteration 300, loss 1.6600199937820435
iteration 400, loss 1.7093420028686523
iteration 500, loss 1.694408893585205
iteration 600, loss 1.7622671127319336
iteration 700, loss 1.7460646629333496
iteration 800, loss 1.7130252122879028
iteration 0, loss 1.7133885622024536
iteration 100, loss 1.7653396129608154
iteration 200, loss 1.7511203289031982
iteration 300, loss 1.7161800861358643
iteration 400, loss 1.8168264627456665
iteration 500, loss 1.7750316858291626
iteration 600, loss 1.695685863494873
iteration 700, loss 1.6921831369400024
iteration 800, loss 1.6840052604675293
iteration 0, loss 1.7092448472976685
iteration 100, loss 1.7118446826934814
iteration 200, loss 1.7724496126174927
iteration 300, loss 1.6967447996139526
iteration 400, loss 1.7234901189804077
iteration 500, loss 1.6969399452209473
iteration 600, loss 1.73399019241333
iteration 700, loss 1.7088584899902344
iteration 800, loss 1.7474026679992676
iteration 0, loss 1.7401739358901978
iteration 100, loss 1.6734085083007812
iteration 200, loss 1.657014012336731
iteration 300, loss 1.7031975984573364
iteration 400, loss 1.776774525642395
iteration 500, loss 1.7489032745361328
iteration 600, loss 1.7530920505523682
iteration 700, loss 1.6704744100570679
iteration 800, loss 1.6923807859420776
iteration 0, loss 1.740417242050171
iteration 100, loss 1.713111400604248
iteration 200, loss 1.7972338199615479
iteration 300, loss 1.7115600109100342
iteration 400, loss 1.6658071279525757
iteration 500, loss 1.7169132232666016
iteration 600, loss 1.7580137252807617
iteration 700, loss 1.766627550125122
iteration 800, loss 1.7223949432373047
iteration 0, loss 1.7107605934143066
iteration 100, loss 1.7211101055145264
iteration 200, loss 1.8034226894378662
iteration 300, loss 1.699754238128662
iteration 400, loss 1.6996060609817505
iteration 500, loss 1.6942312717437744
iteration 600, loss 1.7147853374481201
iteration 700, loss 1.6852076053619385
iteration 800, loss 1.6994496583938599
iteration 0, loss 1.6906400918960571
iteration 100, loss 1.774190068244934
iteration 200, loss 1.785742998123169
iteration 300, loss 1.7266857624053955
iteration 400, loss 1.738650918006897
iteration 500, loss 1.7358100414276123
iteration 600, loss 1.6842551231384277
iteration 700, loss 1.767665147781372
iteration 800, loss 1.7128970623016357
iteration 0, loss 1.7075951099395752
iteration 100, loss 1.731606125831604
iteration 200, loss 1.7323631048202515
iteration 300, loss 1.7620515823364258
iteration 400, loss 1.7162830829620361
iteration 500, loss 1.7716825008392334
iteration 600, loss 1.7090411186218262
iteration 700, loss 1.6272021532058716
iteration 800, loss 1.7116737365722656
iteration 0, loss 1.7740964889526367
iteration 100, loss 1.6850507259368896
iteration 200, loss 1.6844172477722168
iteration 300, loss 1.7727384567260742
iteration 400, loss 1.710097312927246
iteration 500, loss 1.764611005783081
iteration 600, loss 1.7034497261047363
iteration 700, loss 1.6777292490005493
iteration 800, loss 1.6852145195007324
iteration 0, loss 1.723875641822815
iteration 100, loss 1.745285153388977
iteration 200, loss 1.7434788942337036
iteration 300, loss 1.7486852407455444
iteration 400, loss 1.7560889720916748
iteration 500, loss 1.7995719909667969
iteration 600, loss 1.756504774093628
iteration 700, loss 1.6658750772476196
iteration 800, loss 1.6812242269515991
iteration 0, loss 1.730546474456787
iteration 100, loss 1.782739281654358
iteration 200, loss 1.6715420484542847
iteration 300, loss 1.6601234674453735
iteration 400, loss 1.682694435119629
iteration 500, loss 1.669024109840393
iteration 600, loss 1.750975251197815
iteration 700, loss 1.695683479309082
iteration 800, loss 1.7281559705734253
iteration 0, loss 1.776688814163208
iteration 100, loss 1.7640328407287598
iteration 200, loss 1.6847317218780518
iteration 300, loss 1.7345854043960571
iteration 400, loss 1.7541232109069824
iteration 500, loss 1.7248640060424805
iteration 600, loss 1.6924424171447754
iteration 700, loss 1.6646416187286377
iteration 800, loss 1.76539945602417
iteration 0, loss 1.6913630962371826
iteration 100, loss 1.7102972269058228
iteration 200, loss 1.7440699338912964
iteration 300, loss 1.722069501876831
iteration 400, loss 1.736581802368164
iteration 500, loss 1.6767572164535522
iteration 600, loss 1.6892915964126587
iteration 700, loss 1.7112833261489868
iteration 800, loss 1.7543944120407104
iteration 0, loss 1.7202410697937012
iteration 100, loss 1.7428160905838013
iteration 200, loss 1.6541833877563477
iteration 300, loss 1.7530709505081177
iteration 400, loss 1.7776440382003784
iteration 500, loss 1.7596116065979004
iteration 600, loss 1.7669260501861572
iteration 700, loss 1.7537890672683716
iteration 800, loss 1.78200101852417
iteration 0, loss 1.7328327894210815
iteration 100, loss 1.6434653997421265
iteration 200, loss 1.6889197826385498
iteration 300, loss 1.7316008806228638
iteration 400, loss 1.6970632076263428
iteration 500, loss 1.7073125839233398
iteration 600, loss 1.670681357383728
iteration 700, loss 1.7032123804092407
iteration 800, loss 1.7115271091461182
iteration 0, loss 1.7213692665100098
iteration 100, loss 1.6476949453353882
iteration 200, loss 1.7520146369934082
iteration 300, loss 1.7180302143096924
iteration 400, loss 1.7019051313400269
iteration 500, loss 1.7317590713500977
iteration 600, loss 1.7285898923873901
iteration 700, loss 1.725418210029602
iteration 800, loss 1.7733190059661865
iteration 0, loss 1.7537394762039185
iteration 100, loss 1.6936242580413818
iteration 200, loss 1.6606732606887817
iteration 300, loss 1.6912052631378174
iteration 400, loss 1.6832636594772339
iteration 500, loss 1.7321594953536987
iteration 600, loss 1.6875920295715332
iteration 700, loss 1.742821216583252
iteration 800, loss 1.7188984155654907
iteration 0, loss 1.7007081508636475
iteration 100, loss 1.6818703413009644
iteration 200, loss 1.7329076528549194
iteration 300, loss 1.7192035913467407
iteration 400, loss 1.7517642974853516
iteration 500, loss 1.7505654096603394
iteration 600, loss 1.670403242111206
iteration 700, loss 1.6932734251022339
iteration 800, loss 1.7693347930908203
iteration 0, loss 1.7716710567474365
iteration 100, loss 1.7045551538467407
iteration 200, loss 1.7237602472305298
iteration 300, loss 1.6918113231658936
iteration 400, loss 1.7032440900802612
iteration 500, loss 1.6984878778457642
iteration 600, loss 1.6972829103469849
iteration 700, loss 1.6990758180618286
iteration 800, loss 1.7043784856796265
iteration 0, loss 1.747275948524475
iteration 100, loss 1.8742940425872803
iteration 200, loss 1.6392823457717896
iteration 300, loss 1.6751537322998047
iteration 400, loss 1.7307069301605225
iteration 500, loss 1.7386304140090942
iteration 600, loss 1.7131009101867676
iteration 700, loss 1.7255101203918457
iteration 800, loss 1.7498137950897217
iteration 0, loss 1.6604421138763428
iteration 100, loss 1.7300881147384644
iteration 200, loss 1.7256826162338257
iteration 300, loss 1.7034494876861572
iteration 400, loss 1.6665483713150024
iteration 500, loss 1.6768803596496582
iteration 600, loss 1.6939488649368286
iteration 700, loss 1.7219511270523071
iteration 800, loss 1.7751740217208862
iteration 0, loss 1.7272467613220215
iteration 100, loss 1.711756944656372
iteration 200, loss 1.7158360481262207
iteration 300, loss 1.72075355052948
iteration 400, loss 1.6935690641403198
iteration 500, loss 1.7195006608963013
iteration 600, loss 1.644192099571228
iteration 700, loss 1.7740312814712524
iteration 800, loss 1.7089992761611938
iteration 0, loss 1.7055482864379883
iteration 100, loss 1.8183366060256958
iteration 200, loss 1.704976201057434
iteration 300, loss 1.6888775825500488
iteration 400, loss 1.7091751098632812
iteration 500, loss 1.687927007675171
iteration 600, loss 1.694563865661621
iteration 700, loss 1.6571769714355469
iteration 800, loss 1.722061038017273
iteration 0, loss 1.7401535511016846
iteration 100, loss 1.7477033138275146
iteration 200, loss 1.681972622871399
iteration 300, loss 1.6947788000106812
iteration 400, loss 1.6867996454238892
iteration 500, loss 1.686222791671753
iteration 600, loss 1.7152812480926514
iteration 700, loss 1.7020995616912842
iteration 800, loss 1.6786601543426514
iteration 0, loss 1.6685458421707153
iteration 100, loss 1.68533456325531
iteration 200, loss 1.7739558219909668
iteration 300, loss 1.7703900337219238
iteration 400, loss 1.6878587007522583
iteration 500, loss 1.7099719047546387
iteration 600, loss 1.759223461151123
iteration 700, loss 1.6664512157440186
iteration 800, loss 1.6433966159820557
iteration 0, loss 1.6802856922149658
iteration 100, loss 1.641554355621338
iteration 200, loss 1.6989058256149292
iteration 300, loss 1.7120387554168701
iteration 400, loss 1.6597015857696533
iteration 500, loss 1.6769763231277466
iteration 600, loss 1.722116231918335
iteration 700, loss 1.710282325744629
iteration 800, loss 1.753227949142456
iteration 0, loss 1.7212576866149902
iteration 100, loss 1.6733852624893188
iteration 200, loss 1.7287025451660156
iteration 300, loss 1.729421615600586
iteration 400, loss 1.7645528316497803
iteration 500, loss 1.6538159847259521
iteration 600, loss 1.71845543384552
iteration 700, loss 1.6558728218078613
iteration 800, loss 1.732815146446228
iteration 0, loss 1.7436805963516235
iteration 100, loss 1.8461620807647705
iteration 200, loss 1.722503900527954
iteration 300, loss 1.7361794710159302
iteration 400, loss 1.6879239082336426
iteration 500, loss 1.7101730108261108
iteration 600, loss 1.782219409942627
iteration 700, loss 1.6872878074645996
iteration 800, loss 1.828269600868225
iteration 0, loss 1.6884175539016724
iteration 100, loss 1.756852626800537
iteration 200, loss 1.7348021268844604
iteration 300, loss 1.7181785106658936
iteration 400, loss 1.7411894798278809
iteration 500, loss 1.7496719360351562
iteration 600, loss 1.6961263418197632
iteration 700, loss 1.7249555587768555
iteration 800, loss 1.7324891090393066
iteration 0, loss 1.7282720804214478
iteration 100, loss 1.737972617149353
iteration 200, loss 1.6944191455841064
iteration 300, loss 1.6844632625579834
iteration 400, loss 1.767685055732727
iteration 500, loss 1.6625783443450928
iteration 600, loss 1.7170578241348267
iteration 700, loss 1.6993815898895264
iteration 800, loss 1.716740608215332
iteration 0, loss 1.697481632232666
iteration 100, loss 1.7461316585540771
iteration 200, loss 1.671751856803894
iteration 300, loss 1.7281498908996582
iteration 400, loss 1.7649880647659302
iteration 500, loss 1.7600066661834717
iteration 600, loss 1.661385416984558
iteration 700, loss 1.6836491823196411
iteration 800, loss 1.7005900144577026
iteration 0, loss 1.738484263420105
iteration 100, loss 1.6359889507293701
iteration 200, loss 1.6503981351852417
iteration 300, loss 1.7402372360229492
iteration 400, loss 1.681146502494812
iteration 500, loss 1.756017804145813
iteration 600, loss 1.6770045757293701
iteration 700, loss 1.686511516571045
iteration 800, loss 1.6673227548599243
iteration 0, loss 1.7614649534225464
iteration 100, loss 1.7463752031326294
iteration 200, loss 1.7035785913467407
iteration 300, loss 1.722529411315918
iteration 400, loss 1.7506117820739746
iteration 500, loss 1.7111469507217407
iteration 600, loss 1.7087823152542114
iteration 700, loss 1.737152338027954
iteration 800, loss 1.6710782051086426
iteration 0, loss 1.722122311592102
iteration 100, loss 1.7075778245925903
iteration 200, loss 1.7429636716842651
iteration 300, loss 1.690443515777588
iteration 400, loss 1.7581653594970703
iteration 500, loss 1.6908289194107056
iteration 600, loss 1.6925164461135864
iteration 700, loss 1.7225149869918823
iteration 800, loss 1.739349365234375
iteration 0, loss 1.7331570386886597
iteration 100, loss 1.6753292083740234
iteration 200, loss 1.68143630027771
iteration 300, loss 1.785000205039978
iteration 400, loss 1.7212005853652954
iteration 500, loss 1.7002246379852295
iteration 600, loss 1.662173867225647
iteration 700, loss 1.6502257585525513
iteration 800, loss 1.7043644189834595
iteration 0, loss 1.6461176872253418
iteration 100, loss 1.7796579599380493
iteration 200, loss 1.6508042812347412
iteration 300, loss 1.6631951332092285
iteration 400, loss 1.6915104389190674
iteration 500, loss 1.7424838542938232
iteration 600, loss 1.6637250185012817
iteration 700, loss 1.6261364221572876
iteration 800, loss 1.7698394060134888
iteration 0, loss 1.7255209684371948
iteration 100, loss 1.6861488819122314
iteration 200, loss 1.6903104782104492
iteration 300, loss 1.7545132637023926
iteration 400, loss 1.677932858467102
iteration 500, loss 1.7379320859909058
iteration 600, loss 1.6805168390274048
iteration 700, loss 1.7038477659225464
iteration 800, loss 1.7264713048934937
iteration 0, loss 1.6498883962631226
iteration 100, loss 1.696354627609253
iteration 200, loss 1.7330235242843628
iteration 300, loss 1.779508352279663
iteration 400, loss 1.6948641538619995
iteration 500, loss 1.746172547340393
iteration 600, loss 1.72968327999115
iteration 700, loss 1.6990792751312256
iteration 800, loss 1.688611626625061
iteration 0, loss 1.816906213760376
iteration 100, loss 1.7047011852264404
iteration 200, loss 1.6905611753463745
iteration 300, loss 1.684874415397644
iteration 400, loss 1.773856282234192
iteration 500, loss 1.6317026615142822
iteration 600, loss 1.65702486038208
iteration 700, loss 1.6994982957839966
iteration 800, loss 1.7260242700576782
fold 3 accuracy: 0.6687857142857143
iteration 0, loss 1.7128241062164307
iteration 100, loss 1.7084858417510986
iteration 200, loss 1.6624670028686523
iteration 300, loss 1.6309276819229126
iteration 400, loss 1.7805176973342896
iteration 500, loss 1.7762176990509033
iteration 600, loss 1.7125015258789062
iteration 700, loss 1.824228048324585
iteration 800, loss 1.686488389968872
iteration 0, loss 1.691697359085083
iteration 100, loss 1.7132059335708618
iteration 200, loss 1.763338327407837
iteration 300, loss 1.6444333791732788
iteration 400, loss 1.7517917156219482
iteration 500, loss 1.6871453523635864
iteration 600, loss 1.7099230289459229
iteration 700, loss 1.689170002937317
iteration 800, loss 1.6734992265701294
iteration 0, loss 1.7332088947296143
iteration 100, loss 1.7605146169662476
iteration 200, loss 1.676154375076294
iteration 300, loss 1.7725837230682373
iteration 400, loss 1.7108749151229858
iteration 500, loss 1.660660982131958
iteration 600, loss 1.733249306678772
iteration 700, loss 1.7037712335586548
iteration 800, loss 1.7445175647735596
iteration 0, loss 1.7319867610931396
iteration 100, loss 1.687905192375183
iteration 200, loss 1.763493537902832
iteration 300, loss 1.8411341905593872
iteration 400, loss 1.732071042060852
iteration 500, loss 1.6749659776687622
iteration 600, loss 1.7144557237625122
iteration 700, loss 1.7376394271850586
iteration 800, loss 1.656018853187561
iteration 0, loss 1.622419834136963
iteration 100, loss 1.7252472639083862
iteration 200, loss 1.6454577445983887
iteration 300, loss 1.6709743738174438
iteration 400, loss 1.6556679010391235
iteration 500, loss 1.6888296604156494
iteration 600, loss 1.7182170152664185
iteration 700, loss 1.7743759155273438
iteration 800, loss 1.677467703819275
iteration 0, loss 1.7411538362503052
iteration 100, loss 1.7511425018310547
iteration 200, loss 1.7438607215881348
iteration 300, loss 1.7069652080535889
iteration 400, loss 1.7345432043075562
iteration 500, loss 1.726562261581421
iteration 600, loss 1.7308076620101929
iteration 700, loss 1.7713674306869507
iteration 800, loss 1.6837269067764282
iteration 0, loss 1.668742299079895
iteration 100, loss 1.7479015588760376
iteration 200, loss 1.7079837322235107
iteration 300, loss 1.698716402053833
iteration 400, loss 1.7501685619354248
iteration 500, loss 1.7618894577026367
iteration 600, loss 1.6303746700286865
iteration 700, loss 1.6647591590881348
iteration 800, loss 1.7099452018737793
iteration 0, loss 1.7139332294464111
iteration 100, loss 1.7230266332626343
iteration 200, loss 1.7232036590576172
iteration 300, loss 1.6977756023406982
iteration 400, loss 1.7355561256408691
iteration 500, loss 1.7265934944152832
iteration 600, loss 1.6787375211715698
iteration 700, loss 1.7758859395980835
iteration 800, loss 1.7268126010894775
iteration 0, loss 1.7170443534851074
iteration 100, loss 1.7294162511825562
iteration 200, loss 1.6769834756851196
iteration 300, loss 1.7021738290786743
iteration 400, loss 1.6582157611846924
iteration 500, loss 1.7155861854553223
iteration 600, loss 1.6999555826187134
iteration 700, loss 1.670686960220337
iteration 800, loss 1.7355860471725464
iteration 0, loss 1.7460761070251465
iteration 100, loss 1.6805105209350586
iteration 200, loss 1.732564091682434
iteration 300, loss 1.730042815208435
iteration 400, loss 1.770400881767273
iteration 500, loss 1.7045010328292847
iteration 600, loss 1.7466429471969604
iteration 700, loss 1.7774008512496948
iteration 800, loss 1.7217354774475098
iteration 0, loss 1.6565101146697998
iteration 100, loss 1.7354869842529297
iteration 200, loss 1.666050910949707
iteration 300, loss 1.6857342720031738
iteration 400, loss 1.7251280546188354
iteration 500, loss 1.7688043117523193
iteration 600, loss 1.7729778289794922
iteration 700, loss 1.6816561222076416
iteration 800, loss 1.741467833518982
iteration 0, loss 1.7052556276321411
iteration 100, loss 1.7416843175888062
iteration 200, loss 1.8174558877944946
iteration 300, loss 1.6373168230056763
iteration 400, loss 1.722834825515747
iteration 500, loss 1.731233835220337
iteration 600, loss 1.6633421182632446
iteration 700, loss 1.7601301670074463
iteration 800, loss 1.7217397689819336
iteration 0, loss 1.706299066543579
iteration 100, loss 1.6585109233856201
iteration 200, loss 1.7203569412231445
iteration 300, loss 1.7259939908981323
iteration 400, loss 1.7721861600875854
iteration 500, loss 1.7075104713439941
iteration 600, loss 1.7131779193878174
iteration 700, loss 1.6706260442733765
iteration 800, loss 1.739414930343628
iteration 0, loss 1.6662614345550537
iteration 100, loss 1.7353508472442627
iteration 200, loss 1.727824330329895
iteration 300, loss 1.6845101118087769
iteration 400, loss 1.7129710912704468
iteration 500, loss 1.6735000610351562
iteration 600, loss 1.6842197179794312
iteration 700, loss 1.6939888000488281
iteration 800, loss 1.720862865447998
iteration 0, loss 1.7476158142089844
iteration 100, loss 1.6565279960632324
iteration 200, loss 1.6671222448349
iteration 300, loss 1.6935945749282837
iteration 400, loss 1.7305279970169067
iteration 500, loss 1.7463490962982178
iteration 600, loss 1.698125958442688
iteration 700, loss 1.6583830118179321
iteration 800, loss 1.7173560857772827
iteration 0, loss 1.8176636695861816
iteration 100, loss 1.703482985496521
iteration 200, loss 1.7191119194030762
iteration 300, loss 1.6885833740234375
iteration 400, loss 1.7294305562973022
iteration 500, loss 1.7156394720077515
iteration 600, loss 1.6621896028518677
iteration 700, loss 1.6947907209396362
iteration 800, loss 1.7261073589324951
iteration 0, loss 1.6833266019821167
iteration 100, loss 1.7162902355194092
iteration 200, loss 1.7571465969085693
iteration 300, loss 1.7485276460647583
iteration 400, loss 1.7223610877990723
iteration 500, loss 1.703829288482666
iteration 600, loss 1.735574722290039
iteration 700, loss 1.6714363098144531
iteration 800, loss 1.743644118309021
iteration 0, loss 1.7232439517974854
iteration 100, loss 1.6866148710250854
iteration 200, loss 1.6752501726150513
iteration 300, loss 1.7635068893432617
iteration 400, loss 1.7214837074279785
iteration 500, loss 1.7282863855361938
iteration 600, loss 1.7348898649215698
iteration 700, loss 1.6360846757888794
iteration 800, loss 1.701389193534851
iteration 0, loss 1.6740065813064575
iteration 100, loss 1.6901206970214844
iteration 200, loss 1.6785149574279785
iteration 300, loss 1.795037031173706
iteration 400, loss 1.7500354051589966
iteration 500, loss 1.768235683441162
iteration 600, loss 1.6960631608963013
iteration 700, loss 1.727739691734314
iteration 800, loss 1.7210005521774292
iteration 0, loss 1.694053292274475
iteration 100, loss 1.7286962270736694
iteration 200, loss 1.6871888637542725
iteration 300, loss 1.6783326864242554
iteration 400, loss 1.773633360862732
iteration 500, loss 1.7008733749389648
iteration 600, loss 1.7594352960586548
iteration 700, loss 1.7173283100128174
iteration 800, loss 1.7653332948684692
iteration 0, loss 1.7750074863433838
iteration 100, loss 1.7447327375411987
iteration 200, loss 1.7123546600341797
iteration 300, loss 1.7360680103302002
iteration 400, loss 1.7009559869766235
iteration 500, loss 1.6878193616867065
iteration 600, loss 1.7226437330245972
iteration 700, loss 1.6817532777786255
iteration 800, loss 1.7811931371688843
iteration 0, loss 1.7574777603149414
iteration 100, loss 1.734989881515503
iteration 200, loss 1.6970170736312866
iteration 300, loss 1.7227177619934082
iteration 400, loss 1.6689155101776123
iteration 500, loss 1.7518771886825562
iteration 600, loss 1.7307422161102295
iteration 700, loss 1.7790350914001465
iteration 800, loss 1.6784331798553467
iteration 0, loss 1.6953821182250977
iteration 100, loss 1.7568753957748413
iteration 200, loss 1.7959798574447632
iteration 300, loss 1.6787070035934448
iteration 400, loss 1.6461738348007202
iteration 500, loss 1.719689965248108
iteration 600, loss 1.6851650476455688
iteration 700, loss 1.728973388671875
iteration 800, loss 1.7562053203582764
iteration 0, loss 1.7218174934387207
iteration 100, loss 1.7299622297286987
iteration 200, loss 1.7508227825164795
iteration 300, loss 1.7293448448181152
iteration 400, loss 1.717868685722351
iteration 500, loss 1.6479120254516602
iteration 600, loss 1.719841718673706
iteration 700, loss 1.6785926818847656
iteration 800, loss 1.6458351612091064
iteration 0, loss 1.7633821964263916
iteration 100, loss 1.672239065170288
iteration 200, loss 1.709659457206726
iteration 300, loss 1.7466583251953125
iteration 400, loss 1.6981533765792847
iteration 500, loss 1.7327553033828735
iteration 600, loss 1.7153390645980835
iteration 700, loss 1.6997014284133911
iteration 800, loss 1.7421303987503052
iteration 0, loss 1.7318021059036255
iteration 100, loss 1.722778558731079
iteration 200, loss 1.6602903604507446
iteration 300, loss 1.716423511505127
iteration 400, loss 1.6975396871566772
iteration 500, loss 1.7361695766448975
iteration 600, loss 1.7546237707138062
iteration 700, loss 1.7682576179504395
iteration 800, loss 1.755486011505127
iteration 0, loss 1.741019606590271
iteration 100, loss 1.7190743684768677
iteration 200, loss 1.6942780017852783
iteration 300, loss 1.6827621459960938
iteration 400, loss 1.683508038520813
iteration 500, loss 1.7869057655334473
iteration 600, loss 1.70949387550354
iteration 700, loss 1.7223832607269287
iteration 800, loss 1.7404216527938843
iteration 0, loss 1.672761082649231
iteration 100, loss 1.6935546398162842
iteration 200, loss 1.6714791059494019
iteration 300, loss 1.674163579940796
iteration 400, loss 1.706589937210083
iteration 500, loss 1.7239680290222168
iteration 600, loss 1.699336051940918
iteration 700, loss 1.705465316772461
iteration 800, loss 1.7605003118515015
iteration 0, loss 1.6711304187774658
iteration 100, loss 1.7184100151062012
iteration 200, loss 1.6645771265029907
iteration 300, loss 1.7062751054763794
iteration 400, loss 1.6935161352157593
iteration 500, loss 1.7133821249008179
iteration 600, loss 1.690862774848938
iteration 700, loss 1.6369216442108154
iteration 800, loss 1.7072960138320923
iteration 0, loss 1.6714282035827637
iteration 100, loss 1.747620940208435
iteration 200, loss 1.7223846912384033
iteration 300, loss 1.6696962118148804
iteration 400, loss 1.7045286893844604
iteration 500, loss 1.767689824104309
iteration 600, loss 1.7178797721862793
iteration 700, loss 1.6976977586746216
iteration 800, loss 1.6500288248062134
iteration 0, loss 1.7245534658432007
iteration 100, loss 1.682801365852356
iteration 200, loss 1.6823437213897705
iteration 300, loss 1.676741123199463
iteration 400, loss 1.710601568222046
iteration 500, loss 1.6860109567642212
iteration 600, loss 1.652754783630371
iteration 700, loss 1.6880625486373901
iteration 800, loss 1.7494699954986572
iteration 0, loss 1.6720656156539917
iteration 100, loss 1.6646987199783325
iteration 200, loss 1.699403166770935
iteration 300, loss 1.6666008234024048
iteration 400, loss 1.7555427551269531
iteration 500, loss 1.6813468933105469
iteration 600, loss 1.7037912607192993
iteration 700, loss 1.6272104978561401
iteration 800, loss 1.695927381515503
iteration 0, loss 1.7131913900375366
iteration 100, loss 1.67368483543396
iteration 200, loss 1.6413012742996216
iteration 300, loss 1.7160401344299316
iteration 400, loss 1.7396491765975952
iteration 500, loss 1.6736539602279663
iteration 600, loss 1.6835756301879883
iteration 700, loss 1.7100903987884521
iteration 800, loss 1.7507109642028809
iteration 0, loss 1.674803614616394
iteration 100, loss 1.7223671674728394
iteration 200, loss 1.7645397186279297
iteration 300, loss 1.7164156436920166
iteration 400, loss 1.7327319383621216
iteration 500, loss 1.6897592544555664
iteration 600, loss 1.7242178916931152
iteration 700, loss 1.7440732717514038
iteration 800, loss 1.6923466920852661
iteration 0, loss 1.6632273197174072
iteration 100, loss 1.6880524158477783
iteration 200, loss 1.6345727443695068
iteration 300, loss 1.7186540365219116
iteration 400, loss 1.727151870727539
iteration 500, loss 1.734212875366211
iteration 600, loss 1.7127835750579834
iteration 700, loss 1.7364808320999146
iteration 800, loss 1.671129584312439
iteration 0, loss 1.7323217391967773
iteration 100, loss 1.679639458656311
iteration 200, loss 1.760027527809143
iteration 300, loss 1.6620135307312012
iteration 400, loss 1.7522046566009521
iteration 500, loss 1.6525737047195435
iteration 600, loss 1.663533091545105
iteration 700, loss 1.6689530611038208
iteration 800, loss 1.7563767433166504
iteration 0, loss 1.7207486629486084
iteration 100, loss 1.740600824356079
iteration 200, loss 1.6584886312484741
iteration 300, loss 1.773038387298584
iteration 400, loss 1.7272204160690308
iteration 500, loss 1.6675008535385132
iteration 600, loss 1.743343710899353
iteration 700, loss 1.672818660736084
iteration 800, loss 1.7011260986328125
iteration 0, loss 1.6531744003295898
iteration 100, loss 1.7037239074707031
iteration 200, loss 1.677772879600525
iteration 300, loss 1.7372888326644897
iteration 400, loss 1.7133674621582031
iteration 500, loss 1.6942903995513916
iteration 600, loss 1.6976354122161865
iteration 700, loss 1.7462111711502075
iteration 800, loss 1.7108793258666992
iteration 0, loss 1.7278356552124023
iteration 100, loss 1.7635235786437988
iteration 200, loss 1.7139935493469238
iteration 300, loss 1.7252371311187744
iteration 400, loss 1.6257877349853516
iteration 500, loss 1.763551950454712
iteration 600, loss 1.7113250494003296
iteration 700, loss 1.6969435214996338
iteration 800, loss 1.7480502128601074
iteration 0, loss 1.7609305381774902
iteration 100, loss 1.7128677368164062
iteration 200, loss 1.6991281509399414
iteration 300, loss 1.7531603574752808
iteration 400, loss 1.6674128770828247
iteration 500, loss 1.724341630935669
iteration 600, loss 1.710302710533142
iteration 700, loss 1.74899423122406
iteration 800, loss 1.640214443206787
iteration 0, loss 1.690386414527893
iteration 100, loss 1.6809055805206299
iteration 200, loss 1.7307358980178833
iteration 300, loss 1.6896984577178955
iteration 400, loss 1.6790432929992676
iteration 500, loss 1.6840829849243164
iteration 600, loss 1.8093370199203491
iteration 700, loss 1.664529800415039
iteration 800, loss 1.732323169708252
iteration 0, loss 1.6957135200500488
iteration 100, loss 1.7009508609771729
iteration 200, loss 1.7236028909683228
iteration 300, loss 1.6880812644958496
iteration 400, loss 1.699967861175537
iteration 500, loss 1.6913881301879883
iteration 600, loss 1.7015658617019653
iteration 700, loss 1.730333685874939
iteration 800, loss 1.6984494924545288
iteration 0, loss 1.6899563074111938
iteration 100, loss 1.7245864868164062
iteration 200, loss 1.7245848178863525
iteration 300, loss 1.7040352821350098
iteration 400, loss 1.6686744689941406
iteration 500, loss 1.6848280429840088
iteration 600, loss 1.6962041854858398
iteration 700, loss 1.7492001056671143
iteration 800, loss 1.7468961477279663
iteration 0, loss 1.7116544246673584
iteration 100, loss 1.7199479341506958
iteration 200, loss 1.7143828868865967
iteration 300, loss 1.6648764610290527
iteration 400, loss 1.7169244289398193
iteration 500, loss 1.724588394165039
iteration 600, loss 1.6953885555267334
iteration 700, loss 1.6113001108169556
iteration 800, loss 1.6792762279510498
iteration 0, loss 1.6906471252441406
iteration 100, loss 1.6820213794708252
iteration 200, loss 1.7047408819198608
iteration 300, loss 1.6810024976730347
iteration 400, loss 1.6679426431655884
iteration 500, loss 1.7283823490142822
iteration 600, loss 1.6617670059204102
iteration 700, loss 1.701098918914795
iteration 800, loss 1.6546891927719116
iteration 0, loss 1.763512134552002
iteration 100, loss 1.7212116718292236
iteration 200, loss 1.6706552505493164
iteration 300, loss 1.6680994033813477
iteration 400, loss 1.6828627586364746
iteration 500, loss 1.6851845979690552
iteration 600, loss 1.6821560859680176
iteration 700, loss 1.6899131536483765
iteration 800, loss 1.7073776721954346
iteration 0, loss 1.738940715789795
iteration 100, loss 1.7329710721969604
iteration 200, loss 1.7351596355438232
iteration 300, loss 1.7245659828186035
iteration 400, loss 1.6602121591567993
iteration 500, loss 1.7315922975540161
iteration 600, loss 1.6721285581588745
iteration 700, loss 1.6783721446990967
iteration 800, loss 1.6844624280929565
iteration 0, loss 1.71148681640625
iteration 100, loss 1.6897709369659424
iteration 200, loss 1.6754941940307617
iteration 300, loss 1.7509715557098389
iteration 400, loss 1.6666935682296753
iteration 500, loss 1.7381426095962524
iteration 600, loss 1.7559568881988525
iteration 700, loss 1.7104215621948242
iteration 800, loss 1.6608622074127197
iteration 0, loss 1.6944957971572876
iteration 100, loss 1.7625877857208252
iteration 200, loss 1.6981009244918823
iteration 300, loss 1.6322147846221924
iteration 400, loss 1.7387261390686035
iteration 500, loss 1.6918723583221436
iteration 600, loss 1.705365777015686
iteration 700, loss 1.7163364887237549
iteration 800, loss 1.688604712486267
iteration 0, loss 1.6776577234268188
iteration 100, loss 1.7001274824142456
iteration 200, loss 1.7536709308624268
iteration 300, loss 1.7186821699142456
iteration 400, loss 1.7257075309753418
iteration 500, loss 1.698514461517334
iteration 600, loss 1.6620900630950928
iteration 700, loss 1.7292557954788208
iteration 800, loss 1.7266539335250854
fold 4 accuracy: 0.7109285714285715
[2024-02-29 02:19:22,608] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-29 02:19:22,609] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         458     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       560 MACs
fwd flops per GPU:                                                      1.12 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.12 K  
fwd latency:                                                            343.32 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.26 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '458'}
    MACs        - {'KronLayer': '560 MACs'}
    fwd latency - {'KronLayer': '343.32 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  458 = 100% Params, 560 MACs = 100% MACs, 343.32 us = 100% latency, 3.26 MFLOPS
  (kronlinear): KronLinear(458 = 100% Params, 560 MACs = 100% MACs, 248.43 us = 72.36% latency, 4.51 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 28.13 us = 8.19% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-29 02:19:22,613] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.12 K 458
iteration 0, loss 2.303715467453003
iteration 100, loss 2.221163749694824
iteration 200, loss 2.085134506225586
iteration 300, loss 1.9807567596435547
iteration 400, loss 1.9477639198303223
iteration 500, loss 1.8974708318710327
iteration 600, loss 1.9252519607543945
iteration 700, loss 1.7814663648605347
iteration 800, loss 1.8720418214797974
iteration 0, loss 1.8349336385726929
iteration 100, loss 1.8286569118499756
iteration 200, loss 1.847419261932373
iteration 300, loss 1.8043543100357056
iteration 400, loss 1.7419425249099731
iteration 500, loss 1.8217952251434326
iteration 600, loss 1.7771683931350708
iteration 700, loss 1.8100316524505615
iteration 800, loss 1.815420150756836
iteration 0, loss 1.8617655038833618
iteration 100, loss 1.7377243041992188
iteration 200, loss 1.7715771198272705
iteration 300, loss 1.7644649744033813
iteration 400, loss 1.7986135482788086
iteration 500, loss 1.7885093688964844
iteration 600, loss 1.7661705017089844
iteration 700, loss 1.8294814825057983
iteration 800, loss 1.7706447839736938
iteration 0, loss 1.7712559700012207
iteration 100, loss 1.747266411781311
iteration 200, loss 1.7725999355316162
iteration 300, loss 1.77057683467865
iteration 400, loss 1.7398810386657715
iteration 500, loss 1.7607930898666382
iteration 600, loss 1.7623623609542847
iteration 700, loss 1.7357546091079712
iteration 800, loss 1.7415797710418701
iteration 0, loss 1.7239723205566406
iteration 100, loss 1.816920518875122
iteration 200, loss 1.7798179388046265
iteration 300, loss 1.679502248764038
iteration 400, loss 1.8411394357681274
iteration 500, loss 1.7363157272338867
iteration 600, loss 1.6955219507217407
iteration 700, loss 1.6993813514709473
iteration 800, loss 1.778496265411377
iteration 0, loss 1.8205575942993164
iteration 100, loss 1.7837560176849365
iteration 200, loss 1.6749283075332642
iteration 300, loss 1.8029265403747559
iteration 400, loss 1.7408008575439453
iteration 500, loss 1.7172596454620361
iteration 600, loss 1.7241219282150269
iteration 700, loss 1.7792991399765015
iteration 800, loss 1.759452223777771
iteration 0, loss 1.7638905048370361
iteration 100, loss 1.7980048656463623
iteration 200, loss 1.736626386642456
iteration 300, loss 1.7152107954025269
iteration 400, loss 1.7612794637680054
iteration 500, loss 1.7328729629516602
iteration 600, loss 1.7347933053970337
iteration 700, loss 1.7697762250900269
iteration 800, loss 1.6905001401901245
iteration 0, loss 1.7192144393920898
iteration 100, loss 1.7469611167907715
iteration 200, loss 1.7730798721313477
iteration 300, loss 1.7779245376586914
iteration 400, loss 1.7593586444854736
iteration 500, loss 1.7447335720062256
iteration 600, loss 1.7204017639160156
iteration 700, loss 1.8396351337432861
iteration 800, loss 1.7148381471633911
iteration 0, loss 1.7449533939361572
iteration 100, loss 1.7457233667373657
iteration 200, loss 1.7479465007781982
iteration 300, loss 1.7040451765060425
iteration 400, loss 1.712364673614502
iteration 500, loss 1.6975359916687012
iteration 600, loss 1.8354365825653076
iteration 700, loss 1.701020359992981
iteration 800, loss 1.7708191871643066
iteration 0, loss 1.792907476425171
iteration 100, loss 1.7184597253799438
iteration 200, loss 1.6862146854400635
iteration 300, loss 1.7395546436309814
iteration 400, loss 1.7732982635498047
iteration 500, loss 1.7603883743286133
iteration 600, loss 1.6991424560546875
iteration 700, loss 1.7567920684814453
iteration 800, loss 1.6774744987487793
iteration 0, loss 1.8101330995559692
iteration 100, loss 1.701019525527954
iteration 200, loss 1.7707144021987915
iteration 300, loss 1.7715619802474976
iteration 400, loss 1.6894452571868896
iteration 500, loss 1.7975093126296997
iteration 600, loss 1.794862151145935
iteration 700, loss 1.681880235671997
iteration 800, loss 1.7785181999206543
iteration 0, loss 1.6624932289123535
iteration 100, loss 1.667439579963684
iteration 200, loss 1.7902008295059204
iteration 300, loss 1.6965248584747314
iteration 400, loss 1.7344410419464111
iteration 500, loss 1.7093642950057983
iteration 600, loss 1.709949254989624
iteration 700, loss 1.7191836833953857
iteration 800, loss 1.7407095432281494
iteration 0, loss 1.6908867359161377
iteration 100, loss 1.7377394437789917
iteration 200, loss 1.766632080078125
iteration 300, loss 1.7343559265136719
iteration 400, loss 1.7234926223754883
iteration 500, loss 1.7840112447738647
iteration 600, loss 1.7526026964187622
iteration 700, loss 1.7093939781188965
iteration 800, loss 1.7665820121765137
iteration 0, loss 1.723738193511963
iteration 100, loss 1.7852658033370972
iteration 200, loss 1.7627863883972168
iteration 300, loss 1.7433326244354248
iteration 400, loss 1.7353829145431519
iteration 500, loss 1.8443865776062012
iteration 600, loss 1.7542579174041748
iteration 700, loss 1.7475358247756958
iteration 800, loss 1.7241393327713013
iteration 0, loss 1.7201316356658936
iteration 100, loss 1.6946907043457031
iteration 200, loss 1.7637302875518799
iteration 300, loss 1.720629096031189
iteration 400, loss 1.766501545906067
iteration 500, loss 1.7287530899047852
iteration 600, loss 1.7639497518539429
iteration 700, loss 1.8357073068618774
iteration 800, loss 1.7056152820587158
iteration 0, loss 1.7007462978363037
iteration 100, loss 1.7479634284973145
iteration 200, loss 1.7838001251220703
iteration 300, loss 1.8054744005203247
iteration 400, loss 1.7055555582046509
iteration 500, loss 1.691290259361267
iteration 600, loss 1.78292977809906
iteration 700, loss 1.7711389064788818
iteration 800, loss 1.7784889936447144
iteration 0, loss 1.7350107431411743
iteration 100, loss 1.7981181144714355
iteration 200, loss 1.6807700395584106
iteration 300, loss 1.812667727470398
iteration 400, loss 1.7310707569122314
iteration 500, loss 1.7590466737747192
iteration 600, loss 1.7275595664978027
iteration 700, loss 1.7786016464233398
iteration 800, loss 1.7378888130187988
iteration 0, loss 1.74583101272583
iteration 100, loss 1.748878836631775
iteration 200, loss 1.7383408546447754
iteration 300, loss 1.7534143924713135
iteration 400, loss 1.7737138271331787
iteration 500, loss 1.6972500085830688
iteration 600, loss 1.7415798902511597
iteration 700, loss 1.6765214204788208
iteration 800, loss 1.7551730871200562
iteration 0, loss 1.7371114492416382
iteration 100, loss 1.7726106643676758
iteration 200, loss 1.7673207521438599
iteration 300, loss 1.7280522584915161
iteration 400, loss 1.752282977104187
iteration 500, loss 1.7587213516235352
iteration 600, loss 1.7118393182754517
iteration 700, loss 1.7611066102981567
iteration 800, loss 1.7274374961853027
iteration 0, loss 1.7215549945831299
iteration 100, loss 1.7179464101791382
iteration 200, loss 1.7831095457077026
iteration 300, loss 1.6810050010681152
iteration 400, loss 1.6882166862487793
iteration 500, loss 1.7482184171676636
iteration 600, loss 1.7274361848831177
iteration 700, loss 1.737624168395996
iteration 800, loss 1.8109445571899414
iteration 0, loss 1.7597720623016357
iteration 100, loss 1.8291888236999512
iteration 200, loss 1.7199851274490356
iteration 300, loss 1.6932839155197144
iteration 400, loss 1.8091421127319336
iteration 500, loss 1.7361088991165161
iteration 600, loss 1.743263840675354
iteration 700, loss 1.731058120727539
iteration 800, loss 1.7007094621658325
iteration 0, loss 1.7967039346694946
iteration 100, loss 1.715696096420288
iteration 200, loss 1.7806415557861328
iteration 300, loss 1.6897523403167725
iteration 400, loss 1.7294608354568481
iteration 500, loss 1.709049105644226
iteration 600, loss 1.7011233568191528
iteration 700, loss 1.7842981815338135
iteration 800, loss 1.6818702220916748
iteration 0, loss 1.792944312095642
iteration 100, loss 1.7940332889556885
iteration 200, loss 1.727466106414795
iteration 300, loss 1.649835228919983
iteration 400, loss 1.7582792043685913
iteration 500, loss 1.7192169427871704
iteration 600, loss 1.7149157524108887
iteration 700, loss 1.7835038900375366
iteration 800, loss 1.7650172710418701
iteration 0, loss 1.704150676727295
iteration 100, loss 1.7764184474945068
iteration 200, loss 1.7462211847305298
iteration 300, loss 1.710300326347351
iteration 400, loss 1.7843153476715088
iteration 500, loss 1.696028470993042
iteration 600, loss 1.7314752340316772
iteration 700, loss 1.7308324575424194
iteration 800, loss 1.7105681896209717
iteration 0, loss 1.7327667474746704
iteration 100, loss 1.6709895133972168
iteration 200, loss 1.7100616693496704
iteration 300, loss 1.6720457077026367
iteration 400, loss 1.6840404272079468
iteration 500, loss 1.7423789501190186
iteration 600, loss 1.7293107509613037
iteration 700, loss 1.7240450382232666
iteration 800, loss 1.7011977434158325
iteration 0, loss 1.709929347038269
iteration 100, loss 1.7487329244613647
iteration 200, loss 1.7719942331314087
iteration 300, loss 1.7126450538635254
iteration 400, loss 1.8144949674606323
iteration 500, loss 1.73340904712677
iteration 600, loss 1.7357113361358643
iteration 700, loss 1.6990967988967896
iteration 800, loss 1.689939260482788
iteration 0, loss 1.7345879077911377
iteration 100, loss 1.7431102991104126
iteration 200, loss 1.7346774339675903
iteration 300, loss 1.7027678489685059
iteration 400, loss 1.7382748126983643
iteration 500, loss 1.7048012018203735
iteration 600, loss 1.75068998336792
iteration 700, loss 1.7575223445892334
iteration 800, loss 1.7391393184661865
iteration 0, loss 1.7153650522232056
iteration 100, loss 1.7233541011810303
iteration 200, loss 1.6599321365356445
iteration 300, loss 1.7291945219039917
iteration 400, loss 1.742364525794983
iteration 500, loss 1.719952940940857
iteration 600, loss 1.6406934261322021
iteration 700, loss 1.7765792608261108
iteration 800, loss 1.7053145170211792
iteration 0, loss 1.735111117362976
iteration 100, loss 1.7072594165802002
iteration 200, loss 1.7230663299560547
iteration 300, loss 1.6916072368621826
iteration 400, loss 1.7305071353912354
iteration 500, loss 1.7112159729003906
iteration 600, loss 1.7126833200454712
iteration 700, loss 1.7162728309631348
iteration 800, loss 1.7323616743087769
iteration 0, loss 1.7084063291549683
iteration 100, loss 1.7230980396270752
iteration 200, loss 1.7302448749542236
iteration 300, loss 1.7879408597946167
iteration 400, loss 1.6933461427688599
iteration 500, loss 1.744802713394165
iteration 600, loss 1.7416293621063232
iteration 700, loss 1.7010873556137085
iteration 800, loss 1.7730298042297363
iteration 0, loss 1.7694669961929321
iteration 100, loss 1.6915920972824097
iteration 200, loss 1.7275851964950562
iteration 300, loss 1.8283021450042725
iteration 400, loss 1.8030551671981812
iteration 500, loss 1.7150381803512573
iteration 600, loss 1.7926867008209229
iteration 700, loss 1.7294648885726929
iteration 800, loss 1.7667152881622314
iteration 0, loss 1.738459825515747
iteration 100, loss 1.7663533687591553
iteration 200, loss 1.796079158782959
iteration 300, loss 1.7307281494140625
iteration 400, loss 1.8101286888122559
iteration 500, loss 1.7609403133392334
iteration 600, loss 1.7041246891021729
iteration 700, loss 1.7262909412384033
iteration 800, loss 1.7519309520721436
iteration 0, loss 1.7804399728775024
iteration 100, loss 1.7173221111297607
iteration 200, loss 1.7070627212524414
iteration 300, loss 1.6828851699829102
iteration 400, loss 1.7160617113113403
iteration 500, loss 1.807796835899353
iteration 600, loss 1.7389912605285645
iteration 700, loss 1.7369487285614014
iteration 800, loss 1.7572154998779297
iteration 0, loss 1.7293546199798584
iteration 100, loss 1.7128379344940186
iteration 200, loss 1.7334671020507812
iteration 300, loss 1.7772209644317627
iteration 400, loss 1.7116931676864624
iteration 500, loss 1.7449145317077637
iteration 600, loss 1.6771708726882935
iteration 700, loss 1.799065351486206
iteration 800, loss 1.7236766815185547
iteration 0, loss 1.723413109779358
iteration 100, loss 1.7378904819488525
iteration 200, loss 1.7767142057418823
iteration 300, loss 1.7535746097564697
iteration 400, loss 1.6676619052886963
iteration 500, loss 1.7042970657348633
iteration 600, loss 1.7277063131332397
iteration 700, loss 1.7778306007385254
iteration 800, loss 1.6771502494812012
iteration 0, loss 1.6717499494552612
iteration 100, loss 1.764981985092163
iteration 200, loss 1.7433176040649414
iteration 300, loss 1.657996654510498
iteration 400, loss 1.7278034687042236
iteration 500, loss 1.7558220624923706
iteration 600, loss 1.716038465499878
iteration 700, loss 1.7055763006210327
iteration 800, loss 1.7571717500686646
iteration 0, loss 1.7688181400299072
iteration 100, loss 1.698828935623169
iteration 200, loss 1.788985013961792
iteration 300, loss 1.7432224750518799
iteration 400, loss 1.6732203960418701
iteration 500, loss 1.7760165929794312
iteration 600, loss 1.678703784942627
iteration 700, loss 1.7663472890853882
iteration 800, loss 1.689589262008667
iteration 0, loss 1.7565182447433472
iteration 100, loss 1.7272906303405762
iteration 200, loss 1.7150967121124268
iteration 300, loss 1.7781982421875
iteration 400, loss 1.7155680656433105
iteration 500, loss 1.7232683897018433
iteration 600, loss 1.7580476999282837
iteration 700, loss 1.7025699615478516
iteration 800, loss 1.709063172340393
iteration 0, loss 1.7177932262420654
iteration 100, loss 1.7154347896575928
iteration 200, loss 1.7303271293640137
iteration 300, loss 1.7465397119522095
iteration 400, loss 1.7796043157577515
iteration 500, loss 1.7188854217529297
iteration 600, loss 1.7701478004455566
iteration 700, loss 1.7314260005950928
iteration 800, loss 1.7053189277648926
iteration 0, loss 1.7373422384262085
iteration 100, loss 1.7532312870025635
iteration 200, loss 1.6528704166412354
iteration 300, loss 1.717219352722168
iteration 400, loss 1.832872748374939
iteration 500, loss 1.7041980028152466
iteration 600, loss 1.7373799085617065
iteration 700, loss 1.692416787147522
iteration 800, loss 1.741452932357788
iteration 0, loss 1.6779437065124512
iteration 100, loss 1.6932499408721924
iteration 200, loss 1.7012888193130493
iteration 300, loss 1.772304892539978
iteration 400, loss 1.6987862586975098
iteration 500, loss 1.6917963027954102
iteration 600, loss 1.7633824348449707
iteration 700, loss 1.7324565649032593
iteration 800, loss 1.7862695455551147
iteration 0, loss 1.7536189556121826
iteration 100, loss 1.705526351928711
iteration 200, loss 1.7230663299560547
iteration 300, loss 1.7663378715515137
iteration 400, loss 1.7552522420883179
iteration 500, loss 1.801987886428833
iteration 600, loss 1.7516825199127197
iteration 700, loss 1.6596934795379639
iteration 800, loss 1.7376675605773926
iteration 0, loss 1.722307801246643
iteration 100, loss 1.7695492506027222
iteration 200, loss 1.7575650215148926
iteration 300, loss 1.7376656532287598
iteration 400, loss 1.7232650518417358
iteration 500, loss 1.760316252708435
iteration 600, loss 1.7109317779541016
iteration 700, loss 1.7225404977798462
iteration 800, loss 1.7669517993927002
iteration 0, loss 1.7403653860092163
iteration 100, loss 1.7413067817687988
iteration 200, loss 1.7613468170166016
iteration 300, loss 1.7644425630569458
iteration 400, loss 1.7029812335968018
iteration 500, loss 1.7067837715148926
iteration 600, loss 1.7389452457427979
iteration 700, loss 1.7620857954025269
iteration 800, loss 1.7293202877044678
iteration 0, loss 1.752875566482544
iteration 100, loss 1.772826910018921
iteration 200, loss 1.7262609004974365
iteration 300, loss 1.7283260822296143
iteration 400, loss 1.7834787368774414
iteration 500, loss 1.706737756729126
iteration 600, loss 1.7752516269683838
iteration 700, loss 1.7193576097488403
iteration 800, loss 1.6957318782806396
iteration 0, loss 1.7135086059570312
iteration 100, loss 1.7686033248901367
iteration 200, loss 1.8192819356918335
iteration 300, loss 1.8054312467575073
iteration 400, loss 1.7180086374282837
iteration 500, loss 1.752705693244934
iteration 600, loss 1.716404914855957
iteration 700, loss 1.7073031663894653
iteration 800, loss 1.708001971244812
iteration 0, loss 1.796420693397522
iteration 100, loss 1.7042524814605713
iteration 200, loss 1.743660807609558
iteration 300, loss 1.722804069519043
iteration 400, loss 1.6856995820999146
iteration 500, loss 1.752562165260315
iteration 600, loss 1.7282383441925049
iteration 700, loss 1.7234768867492676
iteration 800, loss 1.7048883438110352
iteration 0, loss 1.6585981845855713
iteration 100, loss 1.7477638721466064
iteration 200, loss 1.7786953449249268
iteration 300, loss 1.7339630126953125
iteration 400, loss 1.7310912609100342
iteration 500, loss 1.7283927202224731
iteration 600, loss 1.673314094543457
iteration 700, loss 1.799381971359253
iteration 800, loss 1.7344589233398438
iteration 0, loss 1.713173508644104
iteration 100, loss 1.6980901956558228
iteration 200, loss 1.755785584449768
iteration 300, loss 1.7867215871810913
iteration 400, loss 1.688310980796814
iteration 500, loss 1.7367122173309326
iteration 600, loss 1.7195559740066528
iteration 700, loss 1.7577402591705322
iteration 800, loss 1.7446693181991577
iteration 0, loss 1.6594587564468384
iteration 100, loss 1.7614021301269531
iteration 200, loss 1.6492602825164795
iteration 300, loss 1.767242431640625
iteration 400, loss 1.7429866790771484
iteration 500, loss 1.766123652458191
iteration 600, loss 1.7658278942108154
iteration 700, loss 1.702520728111267
iteration 800, loss 1.688319444656372
fold 0 accuracy: 0.6305714285714286
iteration 0, loss 1.7178623676300049
iteration 100, loss 1.7497104406356812
iteration 200, loss 1.7367764711380005
iteration 300, loss 1.755767583847046
iteration 400, loss 1.710877776145935
iteration 500, loss 1.7308313846588135
iteration 600, loss 1.6702245473861694
iteration 700, loss 1.7071139812469482
iteration 800, loss 1.7536389827728271
iteration 0, loss 1.6855497360229492
iteration 100, loss 1.7159180641174316
iteration 200, loss 1.6883972883224487
iteration 300, loss 1.8124750852584839
iteration 400, loss 1.7359317541122437
iteration 500, loss 1.7254645824432373
iteration 600, loss 1.7197935581207275
iteration 700, loss 1.7859408855438232
iteration 800, loss 1.746206283569336
iteration 0, loss 1.734183669090271
iteration 100, loss 1.794195294380188
iteration 200, loss 1.7355518341064453
iteration 300, loss 1.701827883720398
iteration 400, loss 1.7388625144958496
iteration 500, loss 1.7359910011291504
iteration 600, loss 1.7999682426452637
iteration 700, loss 1.7051514387130737
iteration 800, loss 1.7746113538742065
iteration 0, loss 1.7534915208816528
iteration 100, loss 1.7022150754928589
iteration 200, loss 1.7241899967193604
iteration 300, loss 1.791541576385498
iteration 400, loss 1.7175629138946533
iteration 500, loss 1.7053334712982178
iteration 600, loss 1.7091374397277832
iteration 700, loss 1.6761924028396606
iteration 800, loss 1.7142270803451538
iteration 0, loss 1.7768176794052124
iteration 100, loss 1.7278281450271606
iteration 200, loss 1.7776470184326172
iteration 300, loss 1.7206647396087646
iteration 400, loss 1.7192914485931396
iteration 500, loss 1.7122581005096436
iteration 600, loss 1.7197304964065552
iteration 700, loss 1.681878685951233
iteration 800, loss 1.7060121297836304
iteration 0, loss 1.809234380722046
iteration 100, loss 1.7127691507339478
iteration 200, loss 1.7973984479904175
iteration 300, loss 1.6638386249542236
iteration 400, loss 1.709978699684143
iteration 500, loss 1.7559189796447754
iteration 600, loss 1.785644769668579
iteration 700, loss 1.7917739152908325
iteration 800, loss 1.7693521976470947
iteration 0, loss 1.733672022819519
iteration 100, loss 1.6837034225463867
iteration 200, loss 1.6991199254989624
iteration 300, loss 1.7070578336715698
iteration 400, loss 1.7052040100097656
iteration 500, loss 1.7279080152511597
iteration 600, loss 1.7192310094833374
iteration 700, loss 1.7719089984893799
iteration 800, loss 1.6809940338134766
iteration 0, loss 1.6995266675949097
iteration 100, loss 1.6985609531402588
iteration 200, loss 1.715010404586792
iteration 300, loss 1.7239658832550049
iteration 400, loss 1.7938607931137085
iteration 500, loss 1.7710908651351929
iteration 600, loss 1.6656891107559204
iteration 700, loss 1.842360258102417
iteration 800, loss 1.7525585889816284
iteration 0, loss 1.7948459386825562
iteration 100, loss 1.7464324235916138
iteration 200, loss 1.700217843055725
iteration 300, loss 1.6625206470489502
iteration 400, loss 1.7968813180923462
iteration 500, loss 1.6849228143692017
iteration 600, loss 1.755583643913269
iteration 700, loss 1.7135299444198608
iteration 800, loss 1.7155115604400635
iteration 0, loss 1.7382876873016357
iteration 100, loss 1.700864553451538
iteration 200, loss 1.7502113580703735
iteration 300, loss 1.7863659858703613
iteration 400, loss 1.753617286682129
iteration 500, loss 1.7204502820968628
iteration 600, loss 1.7317856550216675
iteration 700, loss 1.7213656902313232
iteration 800, loss 1.7740377187728882
iteration 0, loss 1.7254747152328491
iteration 100, loss 1.7424200773239136
iteration 200, loss 1.7280676364898682
iteration 300, loss 1.7424544095993042
iteration 400, loss 1.7212274074554443
iteration 500, loss 1.6570571660995483
iteration 600, loss 1.73319411277771
iteration 700, loss 1.77689790725708
iteration 800, loss 1.7043975591659546
iteration 0, loss 1.7259453535079956
iteration 100, loss 1.8177673816680908
iteration 200, loss 1.7508965730667114
iteration 300, loss 1.7457228899002075
iteration 400, loss 1.698659896850586
iteration 500, loss 1.7805827856063843
iteration 600, loss 1.7780619859695435
iteration 700, loss 1.7695772647857666
iteration 800, loss 1.7073787450790405
iteration 0, loss 1.737847924232483
iteration 100, loss 1.681180477142334
iteration 200, loss 1.6936283111572266
iteration 300, loss 1.6750725507736206
iteration 400, loss 1.695802092552185
iteration 500, loss 1.7679842710494995
iteration 600, loss 1.674682855606079
iteration 700, loss 1.8089426755905151
iteration 800, loss 1.7301673889160156
iteration 0, loss 1.6763490438461304
iteration 100, loss 1.7428232431411743
iteration 200, loss 1.7482584714889526
iteration 300, loss 1.6768754720687866
iteration 400, loss 1.6813539266586304
iteration 500, loss 1.7145771980285645
iteration 600, loss 1.6849881410598755
iteration 700, loss 1.7492693662643433
iteration 800, loss 1.7220237255096436
iteration 0, loss 1.669800043106079
iteration 100, loss 1.7353413105010986
iteration 200, loss 1.7333378791809082
iteration 300, loss 1.721110224723816
iteration 400, loss 1.740591287612915
iteration 500, loss 1.7737609148025513
iteration 600, loss 1.680031180381775
iteration 700, loss 1.6895952224731445
iteration 800, loss 1.7011057138442993
iteration 0, loss 1.7833694219589233
iteration 100, loss 1.6826704740524292
iteration 200, loss 1.7146904468536377
iteration 300, loss 1.6988492012023926
iteration 400, loss 1.7257319688796997
iteration 500, loss 1.7358059883117676
iteration 600, loss 1.7766962051391602
iteration 700, loss 1.751368522644043
iteration 800, loss 1.7252390384674072
iteration 0, loss 1.6966968774795532
iteration 100, loss 1.6903694868087769
iteration 200, loss 1.7758623361587524
iteration 300, loss 1.6607612371444702
iteration 400, loss 1.7653995752334595
iteration 500, loss 1.7659690380096436
iteration 600, loss 1.7974042892456055
iteration 700, loss 1.7194263935089111
iteration 800, loss 1.7096819877624512
iteration 0, loss 1.6751022338867188
iteration 100, loss 1.7522259950637817
iteration 200, loss 1.7275307178497314
iteration 300, loss 1.7314575910568237
iteration 400, loss 1.6941715478897095
iteration 500, loss 1.7256088256835938
iteration 600, loss 1.6724008321762085
iteration 700, loss 1.7064287662506104
iteration 800, loss 1.7279120683670044
iteration 0, loss 1.7614881992340088
iteration 100, loss 1.6752431392669678
iteration 200, loss 1.7687827348709106
iteration 300, loss 1.7248640060424805
iteration 400, loss 1.723311424255371
iteration 500, loss 1.7157243490219116
iteration 600, loss 1.7244871854782104
iteration 700, loss 1.7121691703796387
iteration 800, loss 1.6745339632034302
iteration 0, loss 1.702699899673462
iteration 100, loss 1.717278242111206
iteration 200, loss 1.7496376037597656
iteration 300, loss 1.7004824876785278
iteration 400, loss 1.725592851638794
iteration 500, loss 1.7352793216705322
iteration 600, loss 1.6997233629226685
iteration 700, loss 1.7258543968200684
iteration 800, loss 1.7002737522125244
iteration 0, loss 1.6787829399108887
iteration 100, loss 1.6733858585357666
iteration 200, loss 1.7834128141403198
iteration 300, loss 1.6677180528640747
iteration 400, loss 1.6799441576004028
iteration 500, loss 1.7010905742645264
iteration 600, loss 1.7119890451431274
iteration 700, loss 1.6619279384613037
iteration 800, loss 1.7136517763137817
iteration 0, loss 1.7277119159698486
iteration 100, loss 1.7363152503967285
iteration 200, loss 1.6967616081237793
iteration 300, loss 1.7284350395202637
iteration 400, loss 1.7798787355422974
iteration 500, loss 1.7110121250152588
iteration 600, loss 1.702284574508667
iteration 700, loss 1.7546712160110474
iteration 800, loss 1.729828119277954
iteration 0, loss 1.7304177284240723
iteration 100, loss 1.7179937362670898
iteration 200, loss 1.7421061992645264
iteration 300, loss 1.722191572189331
iteration 400, loss 1.7359955310821533
iteration 500, loss 1.7082345485687256
iteration 600, loss 1.7838324308395386
iteration 700, loss 1.6817582845687866
iteration 800, loss 1.687096118927002
iteration 0, loss 1.6911864280700684
iteration 100, loss 1.7117695808410645
iteration 200, loss 1.7226426601409912
iteration 300, loss 1.6569336652755737
iteration 400, loss 1.7251218557357788
iteration 500, loss 1.7458703517913818
iteration 600, loss 1.712732195854187
iteration 700, loss 1.7413209676742554
iteration 800, loss 1.7545616626739502
iteration 0, loss 1.756492018699646
iteration 100, loss 1.7774232625961304
iteration 200, loss 1.7069807052612305
iteration 300, loss 1.6707539558410645
iteration 400, loss 1.751561164855957
iteration 500, loss 1.7695603370666504
iteration 600, loss 1.7690470218658447
iteration 700, loss 1.717879295349121
iteration 800, loss 1.7212475538253784
iteration 0, loss 1.718587040901184
iteration 100, loss 1.783585786819458
iteration 200, loss 1.69221830368042
iteration 300, loss 1.782208800315857
iteration 400, loss 1.808133602142334
iteration 500, loss 1.7365615367889404
iteration 600, loss 1.6895190477371216
iteration 700, loss 1.703958511352539
iteration 800, loss 1.7104427814483643
iteration 0, loss 1.7190277576446533
iteration 100, loss 1.7505329847335815
iteration 200, loss 1.741491675376892
iteration 300, loss 1.8135448694229126
iteration 400, loss 1.7151753902435303
iteration 500, loss 1.7463213205337524
iteration 600, loss 1.7499573230743408
iteration 700, loss 1.6837371587753296
iteration 800, loss 1.719402551651001
iteration 0, loss 1.7234729528427124
iteration 100, loss 1.6757481098175049
iteration 200, loss 1.7080657482147217
iteration 300, loss 1.6691232919692993
iteration 400, loss 1.702913522720337
iteration 500, loss 1.7154983282089233
iteration 600, loss 1.7288326025009155
iteration 700, loss 1.658026933670044
iteration 800, loss 1.7415187358856201
iteration 0, loss 1.7190494537353516
iteration 100, loss 1.7516589164733887
iteration 200, loss 1.692765712738037
iteration 300, loss 1.727880597114563
iteration 400, loss 1.772169828414917
iteration 500, loss 1.731431484222412
iteration 600, loss 1.740476369857788
iteration 700, loss 1.6971441507339478
iteration 800, loss 1.7174235582351685
iteration 0, loss 1.685353398323059
iteration 100, loss 1.72611403465271
iteration 200, loss 1.7033692598342896
iteration 300, loss 1.7520673274993896
iteration 400, loss 1.7440441846847534
iteration 500, loss 1.7828550338745117
iteration 600, loss 1.7091717720031738
iteration 700, loss 1.7675650119781494
iteration 800, loss 1.7826074361801147
iteration 0, loss 1.742114782333374
iteration 100, loss 1.764593482017517
iteration 200, loss 1.7301054000854492
iteration 300, loss 1.6628596782684326
iteration 400, loss 1.742552399635315
iteration 500, loss 1.6928865909576416
iteration 600, loss 1.716390609741211
iteration 700, loss 1.7105551958084106
iteration 800, loss 1.696066975593567
iteration 0, loss 1.7265875339508057
iteration 100, loss 1.7424041032791138
iteration 200, loss 1.693227767944336
iteration 300, loss 1.7814462184906006
iteration 400, loss 1.7994495630264282
iteration 500, loss 1.7764787673950195
iteration 600, loss 1.7451976537704468
iteration 700, loss 1.7083381414413452
iteration 800, loss 1.7284127473831177
iteration 0, loss 1.7069106101989746
iteration 100, loss 1.6848605871200562
iteration 200, loss 1.677374005317688
iteration 300, loss 1.7497888803482056
iteration 400, loss 1.7706393003463745
iteration 500, loss 1.7258284091949463
iteration 600, loss 1.7191542387008667
iteration 700, loss 1.7483000755310059
iteration 800, loss 1.6823554039001465
iteration 0, loss 1.728921890258789
iteration 100, loss 1.74774169921875
iteration 200, loss 1.7375136613845825
iteration 300, loss 1.7392958402633667
iteration 400, loss 1.7688963413238525
iteration 500, loss 1.6822278499603271
iteration 600, loss 1.6978588104248047
iteration 700, loss 1.7430598735809326
iteration 800, loss 1.683651328086853
iteration 0, loss 1.6952548027038574
iteration 100, loss 1.7010430097579956
iteration 200, loss 1.7191238403320312
iteration 300, loss 1.6651209592819214
iteration 400, loss 1.7546014785766602
iteration 500, loss 1.774809718132019
iteration 600, loss 1.7550294399261475
iteration 700, loss 1.779712200164795
iteration 800, loss 1.7004388570785522
iteration 0, loss 1.7562353610992432
iteration 100, loss 1.7274701595306396
iteration 200, loss 1.7753667831420898
iteration 300, loss 1.7288403511047363
iteration 400, loss 1.7121495008468628
iteration 500, loss 1.715869426727295
iteration 600, loss 1.7467880249023438
iteration 700, loss 1.7080754041671753
iteration 800, loss 1.6617848873138428
iteration 0, loss 1.7327172756195068
iteration 100, loss 1.685397982597351
iteration 200, loss 1.7467947006225586
iteration 300, loss 1.7214034795761108
iteration 400, loss 1.764234185218811
iteration 500, loss 1.7902445793151855
iteration 600, loss 1.742232322692871
iteration 700, loss 1.750864028930664
iteration 800, loss 1.6851718425750732
iteration 0, loss 1.7260913848876953
iteration 100, loss 1.685836672782898
iteration 200, loss 1.709965467453003
iteration 300, loss 1.8045785427093506
iteration 400, loss 1.7447501420974731
iteration 500, loss 1.7099111080169678
iteration 600, loss 1.7132127285003662
iteration 700, loss 1.6949257850646973
iteration 800, loss 1.7086530923843384
iteration 0, loss 1.7422113418579102
iteration 100, loss 1.7236436605453491
iteration 200, loss 1.7111581563949585
iteration 300, loss 1.7330578565597534
iteration 400, loss 1.718385934829712
iteration 500, loss 1.715652346611023
iteration 600, loss 1.8048845529556274
iteration 700, loss 1.7470546960830688
iteration 800, loss 1.757361650466919
iteration 0, loss 1.8004870414733887
iteration 100, loss 1.8028167486190796
iteration 200, loss 1.7224400043487549
iteration 300, loss 1.7185957431793213
iteration 400, loss 1.7029283046722412
iteration 500, loss 1.6741489171981812
iteration 600, loss 1.6783350706100464
iteration 700, loss 1.7006906270980835
iteration 800, loss 1.7133852243423462
iteration 0, loss 1.7257455587387085
iteration 100, loss 1.6908377408981323
iteration 200, loss 1.7799712419509888
iteration 300, loss 1.7530440092086792
iteration 400, loss 1.8127524852752686
iteration 500, loss 1.722367525100708
iteration 600, loss 1.6883426904678345
iteration 700, loss 1.761558175086975
iteration 800, loss 1.703292727470398
iteration 0, loss 1.699838638305664
iteration 100, loss 1.704672932624817
iteration 200, loss 1.7418190240859985
iteration 300, loss 1.6660974025726318
iteration 400, loss 1.7025095224380493
iteration 500, loss 1.7676712274551392
iteration 600, loss 1.7216888666152954
iteration 700, loss 1.6804150342941284
iteration 800, loss 1.712843894958496
iteration 0, loss 1.7014518976211548
iteration 100, loss 1.6704820394515991
iteration 200, loss 1.6900616884231567
iteration 300, loss 1.7363053560256958
iteration 400, loss 1.8126840591430664
iteration 500, loss 1.7333941459655762
iteration 600, loss 1.7055180072784424
iteration 700, loss 1.7558566331863403
iteration 800, loss 1.731055498123169
iteration 0, loss 1.6831128597259521
iteration 100, loss 1.7144008874893188
iteration 200, loss 1.7391437292099
iteration 300, loss 1.6893413066864014
iteration 400, loss 1.7527996301651
iteration 500, loss 1.7745784521102905
iteration 600, loss 1.6971375942230225
iteration 700, loss 1.7231580018997192
iteration 800, loss 1.686211109161377
iteration 0, loss 1.6922094821929932
iteration 100, loss 1.8331636190414429
iteration 200, loss 1.7179558277130127
iteration 300, loss 1.7435246706008911
iteration 400, loss 1.7089051008224487
iteration 500, loss 1.7334229946136475
iteration 600, loss 1.8145323991775513
iteration 700, loss 1.7134166955947876
iteration 800, loss 1.6970244646072388
iteration 0, loss 1.6926921606063843
iteration 100, loss 1.6777452230453491
iteration 200, loss 1.7057271003723145
iteration 300, loss 1.6890438795089722
iteration 400, loss 1.7427470684051514
iteration 500, loss 1.764390230178833
iteration 600, loss 1.6981395483016968
iteration 700, loss 1.672371506690979
iteration 800, loss 1.765442132949829
iteration 0, loss 1.7380356788635254
iteration 100, loss 1.7347396612167358
iteration 200, loss 1.689218521118164
iteration 300, loss 1.7476969957351685
iteration 400, loss 1.6932282447814941
iteration 500, loss 1.6927573680877686
iteration 600, loss 1.7937285900115967
iteration 700, loss 1.8190230131149292
iteration 800, loss 1.7286158800125122
iteration 0, loss 1.69675874710083
iteration 100, loss 1.6695592403411865
iteration 200, loss 1.6955920457839966
iteration 300, loss 1.7324169874191284
iteration 400, loss 1.6695899963378906
iteration 500, loss 1.7375162839889526
iteration 600, loss 1.7502031326293945
iteration 700, loss 1.7018494606018066
iteration 800, loss 1.7168011665344238
iteration 0, loss 1.751799464225769
iteration 100, loss 1.7618972063064575
iteration 200, loss 1.737408995628357
iteration 300, loss 1.7383421659469604
iteration 400, loss 1.6910425424575806
iteration 500, loss 1.7590813636779785
iteration 600, loss 1.7455439567565918
iteration 700, loss 1.7808120250701904
iteration 800, loss 1.6929445266723633
iteration 0, loss 1.7043781280517578
iteration 100, loss 1.7245863676071167
iteration 200, loss 1.7294998168945312
iteration 300, loss 1.7102352380752563
iteration 400, loss 1.7340645790100098
iteration 500, loss 1.7468628883361816
iteration 600, loss 1.7492473125457764
iteration 700, loss 1.7210856676101685
iteration 800, loss 1.7388755083084106
fold 1 accuracy: 0.6497857142857143
iteration 0, loss 1.661375880241394
iteration 100, loss 1.7691811323165894
iteration 200, loss 1.7804635763168335
iteration 300, loss 1.7189555168151855
iteration 400, loss 1.7439039945602417
iteration 500, loss 1.7539070844650269
iteration 600, loss 1.743420124053955
iteration 700, loss 1.6899763345718384
iteration 800, loss 1.712620496749878
iteration 0, loss 1.7989449501037598
iteration 100, loss 1.6954352855682373
iteration 200, loss 1.7122210264205933
iteration 300, loss 1.6441222429275513
iteration 400, loss 1.6753175258636475
iteration 500, loss 1.724745750427246
iteration 600, loss 1.7359577417373657
iteration 700, loss 1.7050623893737793
iteration 800, loss 1.6734269857406616
iteration 0, loss 1.7402538061141968
iteration 100, loss 1.7546982765197754
iteration 200, loss 1.7506742477416992
iteration 300, loss 1.7999722957611084
iteration 400, loss 1.7820103168487549
iteration 500, loss 1.7175681591033936
iteration 600, loss 1.7515075206756592
iteration 700, loss 1.6827327013015747
iteration 800, loss 1.6997817754745483
iteration 0, loss 1.7578479051589966
iteration 100, loss 1.6689313650131226
iteration 200, loss 1.7075722217559814
iteration 300, loss 1.7211962938308716
iteration 400, loss 1.6955996751785278
iteration 500, loss 1.69683837890625
iteration 600, loss 1.6863893270492554
iteration 700, loss 1.7701539993286133
iteration 800, loss 1.735931396484375
iteration 0, loss 1.7242670059204102
iteration 100, loss 1.6987888813018799
iteration 200, loss 1.792242407798767
iteration 300, loss 1.6681030988693237
iteration 400, loss 1.7534502744674683
iteration 500, loss 1.7487106323242188
iteration 600, loss 1.7301892042160034
iteration 700, loss 1.7667204141616821
iteration 800, loss 1.682267189025879
iteration 0, loss 1.675441026687622
iteration 100, loss 1.7868034839630127
iteration 200, loss 1.7077786922454834
iteration 300, loss 1.7533400058746338
iteration 400, loss 1.6991286277770996
iteration 500, loss 1.6751813888549805
iteration 600, loss 1.6341122388839722
iteration 700, loss 1.7355188131332397
iteration 800, loss 1.7209794521331787
iteration 0, loss 1.6806755065917969
iteration 100, loss 1.7443832159042358
iteration 200, loss 1.6665184497833252
iteration 300, loss 1.7028086185455322
iteration 400, loss 1.7316269874572754
iteration 500, loss 1.7432633638381958
iteration 600, loss 1.7250396013259888
iteration 700, loss 1.703428030014038
iteration 800, loss 1.7234532833099365
iteration 0, loss 1.7782613039016724
iteration 100, loss 1.6868864297866821
iteration 200, loss 1.782027006149292
iteration 300, loss 1.7071696519851685
iteration 400, loss 1.718882441520691
iteration 500, loss 1.747870922088623
iteration 600, loss 1.7333025932312012
iteration 700, loss 1.7984050512313843
iteration 800, loss 1.6937397718429565
iteration 0, loss 1.6709914207458496
iteration 100, loss 1.7039926052093506
iteration 200, loss 1.71566641330719
iteration 300, loss 1.7190619707107544
iteration 400, loss 1.6963337659835815
iteration 500, loss 1.7201749086380005
iteration 600, loss 1.6768112182617188
iteration 700, loss 1.6740727424621582
iteration 800, loss 1.7065929174423218
iteration 0, loss 1.7094205617904663
iteration 100, loss 1.7425427436828613
iteration 200, loss 1.777815818786621
iteration 300, loss 1.7295902967453003
iteration 400, loss 1.6820369958877563
iteration 500, loss 1.7295352220535278
iteration 600, loss 1.755639910697937
iteration 700, loss 1.7511813640594482
iteration 800, loss 1.7232149839401245
iteration 0, loss 1.7389436960220337
iteration 100, loss 1.7309246063232422
iteration 200, loss 1.7188522815704346
iteration 300, loss 1.7222018241882324
iteration 400, loss 1.6717236042022705
iteration 500, loss 1.7487413883209229
iteration 600, loss 1.7191109657287598
iteration 700, loss 1.7816550731658936
iteration 800, loss 1.7062751054763794
iteration 0, loss 1.7181023359298706
iteration 100, loss 1.6366791725158691
iteration 200, loss 1.731594443321228
iteration 300, loss 1.741870641708374
iteration 400, loss 1.738821268081665
iteration 500, loss 1.7272471189498901
iteration 600, loss 1.7937324047088623
iteration 700, loss 1.7330541610717773
iteration 800, loss 1.6539005041122437
iteration 0, loss 1.733557105064392
iteration 100, loss 1.711323618888855
iteration 200, loss 1.7586352825164795
iteration 300, loss 1.7110307216644287
iteration 400, loss 1.7289607524871826
iteration 500, loss 1.7807663679122925
iteration 600, loss 1.6856356859207153
iteration 700, loss 1.776559829711914
iteration 800, loss 1.7361764907836914
iteration 0, loss 1.773194432258606
iteration 100, loss 1.7294683456420898
iteration 200, loss 1.7113674879074097
iteration 300, loss 1.7577790021896362
iteration 400, loss 1.7295409440994263
iteration 500, loss 1.6927320957183838
iteration 600, loss 1.7224425077438354
iteration 700, loss 1.7226568460464478
iteration 800, loss 1.7486741542816162
iteration 0, loss 1.7933998107910156
iteration 100, loss 1.7167179584503174
iteration 200, loss 1.8004947900772095
iteration 300, loss 1.7940013408660889
iteration 400, loss 1.7506351470947266
iteration 500, loss 1.7507745027542114
iteration 600, loss 1.7557417154312134
iteration 700, loss 1.6796824932098389
iteration 800, loss 1.7407951354980469
iteration 0, loss 1.6556538343429565
iteration 100, loss 1.713950753211975
iteration 200, loss 1.6616841554641724
iteration 300, loss 1.6839542388916016
iteration 400, loss 1.646422028541565
iteration 500, loss 1.7583752870559692
iteration 600, loss 1.6712913513183594
iteration 700, loss 1.7157317399978638
iteration 800, loss 1.6747883558273315
iteration 0, loss 1.7742382287979126
iteration 100, loss 1.673494815826416
iteration 200, loss 1.7146892547607422
iteration 300, loss 1.7099906206130981
iteration 400, loss 1.7670100927352905
iteration 500, loss 1.710504174232483
iteration 600, loss 1.7833154201507568
iteration 700, loss 1.782578945159912
iteration 800, loss 1.6711112260818481
iteration 0, loss 1.6658565998077393
iteration 100, loss 1.718725323677063
iteration 200, loss 1.6914780139923096
iteration 300, loss 1.7147409915924072
iteration 400, loss 1.7383698225021362
iteration 500, loss 1.726802945137024
iteration 600, loss 1.7265559434890747
iteration 700, loss 1.7467905282974243
iteration 800, loss 1.7597301006317139
iteration 0, loss 1.7310760021209717
iteration 100, loss 1.7970677614212036
iteration 200, loss 1.6473268270492554
iteration 300, loss 1.6991714239120483
iteration 400, loss 1.7012356519699097
iteration 500, loss 1.6804594993591309
iteration 600, loss 1.718327522277832
iteration 700, loss 1.7344965934753418
iteration 800, loss 1.6951987743377686
iteration 0, loss 1.7262746095657349
iteration 100, loss 1.7280128002166748
iteration 200, loss 1.7843650579452515
iteration 300, loss 1.665972352027893
iteration 400, loss 1.6889632940292358
iteration 500, loss 1.7495574951171875
iteration 600, loss 1.7576804161071777
iteration 700, loss 1.8303427696228027
iteration 800, loss 1.670444369316101
iteration 0, loss 1.6719880104064941
iteration 100, loss 1.7416003942489624
iteration 200, loss 1.7736810445785522
iteration 300, loss 1.7618829011917114
iteration 400, loss 1.6637111902236938
iteration 500, loss 1.6666756868362427
iteration 600, loss 1.7900816202163696
iteration 700, loss 1.755139708518982
iteration 800, loss 1.7381229400634766
iteration 0, loss 1.714089274406433
iteration 100, loss 1.7413110733032227
iteration 200, loss 1.7232235670089722
iteration 300, loss 1.7657701969146729
iteration 400, loss 1.718392252922058
iteration 500, loss 1.7697933912277222
iteration 600, loss 1.7838115692138672
iteration 700, loss 1.709762454032898
iteration 800, loss 1.7348445653915405
iteration 0, loss 1.712655782699585
iteration 100, loss 1.7832257747650146
iteration 200, loss 1.6631340980529785
iteration 300, loss 1.7447339296340942
iteration 400, loss 1.761579155921936
iteration 500, loss 1.70071542263031
iteration 600, loss 1.751755952835083
iteration 700, loss 1.694661259651184
iteration 800, loss 1.6968433856964111
iteration 0, loss 1.7220379114151
iteration 100, loss 1.7433539628982544
iteration 200, loss 1.7376141548156738
iteration 300, loss 1.7247384786605835
iteration 400, loss 1.734328269958496
iteration 500, loss 1.825347661972046
iteration 600, loss 1.7086923122406006
iteration 700, loss 1.7818994522094727
iteration 800, loss 1.7038148641586304
iteration 0, loss 1.6468710899353027
iteration 100, loss 1.764424204826355
iteration 200, loss 1.7330169677734375
iteration 300, loss 1.7468533515930176
iteration 400, loss 1.7301671504974365
iteration 500, loss 1.7604750394821167
iteration 600, loss 1.660622000694275
iteration 700, loss 1.6520644426345825
iteration 800, loss 1.6948306560516357
iteration 0, loss 1.6873753070831299
iteration 100, loss 1.7073701620101929
iteration 200, loss 1.6604799032211304
iteration 300, loss 1.7182140350341797
iteration 400, loss 1.6689203977584839
iteration 500, loss 1.714066982269287
iteration 600, loss 1.7117552757263184
iteration 700, loss 1.7128032445907593
iteration 800, loss 1.7205235958099365
iteration 0, loss 1.7419772148132324
iteration 100, loss 1.716223120689392
iteration 200, loss 1.7549430131912231
iteration 300, loss 1.6623975038528442
iteration 400, loss 1.6791083812713623
iteration 500, loss 1.7208778858184814
iteration 600, loss 1.6685081720352173
iteration 700, loss 1.790966272354126
iteration 800, loss 1.7124418020248413
iteration 0, loss 1.7109332084655762
iteration 100, loss 1.7659692764282227
iteration 200, loss 1.7073640823364258
iteration 300, loss 1.7057090997695923
iteration 400, loss 1.7652299404144287
iteration 500, loss 1.7305381298065186
iteration 600, loss 1.6956263780593872
iteration 700, loss 1.633003830909729
iteration 800, loss 1.7321544885635376
iteration 0, loss 1.714392066001892
iteration 100, loss 1.7342350482940674
iteration 200, loss 1.7193520069122314
iteration 300, loss 1.7151182889938354
iteration 400, loss 1.7248693704605103
iteration 500, loss 1.6742619276046753
iteration 600, loss 1.684846043586731
iteration 700, loss 1.747436285018921
iteration 800, loss 1.7200510501861572
iteration 0, loss 1.71077561378479
iteration 100, loss 1.7242608070373535
iteration 200, loss 1.6824723482131958
iteration 300, loss 1.6856275796890259
iteration 400, loss 1.6857879161834717
iteration 500, loss 1.6591070890426636
iteration 600, loss 1.6934880018234253
iteration 700, loss 1.7897396087646484
iteration 800, loss 1.757236361503601
iteration 0, loss 1.7302254438400269
iteration 100, loss 1.750732660293579
iteration 200, loss 1.7288883924484253
iteration 300, loss 1.8144421577453613
iteration 400, loss 1.6893759965896606
iteration 500, loss 1.7797399759292603
iteration 600, loss 1.7262625694274902
iteration 700, loss 1.7580457925796509
iteration 800, loss 1.7317038774490356
iteration 0, loss 1.7535284757614136
iteration 100, loss 1.775636911392212
iteration 200, loss 1.696649432182312
iteration 300, loss 1.6720962524414062
iteration 400, loss 1.7254632711410522
iteration 500, loss 1.6976746320724487
iteration 600, loss 1.7051435708999634
iteration 700, loss 1.7640844583511353
iteration 800, loss 1.6888011693954468
iteration 0, loss 1.7635955810546875
iteration 100, loss 1.7822301387786865
iteration 200, loss 1.6696181297302246
iteration 300, loss 1.72676682472229
iteration 400, loss 1.745058536529541
iteration 500, loss 1.779770016670227
iteration 600, loss 1.7261600494384766
iteration 700, loss 1.7708567380905151
iteration 800, loss 1.7563080787658691
iteration 0, loss 1.778713583946228
iteration 100, loss 1.700980544090271
iteration 200, loss 1.644144058227539
iteration 300, loss 1.7703524827957153
iteration 400, loss 1.7731512784957886
iteration 500, loss 1.7348089218139648
iteration 600, loss 1.7341339588165283
iteration 700, loss 1.6885958909988403
iteration 800, loss 1.7425193786621094
iteration 0, loss 1.734574317932129
iteration 100, loss 1.735939383506775
iteration 200, loss 1.7172346115112305
iteration 300, loss 1.7572851181030273
iteration 400, loss 1.6958292722702026
iteration 500, loss 1.671355962753296
iteration 600, loss 1.7083481550216675
iteration 700, loss 1.6868387460708618
iteration 800, loss 1.698960542678833
iteration 0, loss 1.8051484823226929
iteration 100, loss 1.7089546918869019
iteration 200, loss 1.6691054105758667
iteration 300, loss 1.697550892829895
iteration 400, loss 1.762956976890564
iteration 500, loss 1.744877815246582
iteration 600, loss 1.738237977027893
iteration 700, loss 1.707854986190796
iteration 800, loss 1.6549409627914429
iteration 0, loss 1.6377348899841309
iteration 100, loss 1.6962244510650635
iteration 200, loss 1.799717903137207
iteration 300, loss 1.679300308227539
iteration 400, loss 1.6985059976577759
iteration 500, loss 1.7516316175460815
iteration 600, loss 1.688192367553711
iteration 700, loss 1.6323819160461426
iteration 800, loss 1.763983964920044
iteration 0, loss 1.7994120121002197
iteration 100, loss 1.7021212577819824
iteration 200, loss 1.7484327554702759
iteration 300, loss 1.635394811630249
iteration 400, loss 1.7371681928634644
iteration 500, loss 1.7150052785873413
iteration 600, loss 1.6733368635177612
iteration 700, loss 1.6916000843048096
iteration 800, loss 1.6873880624771118
iteration 0, loss 1.7102729082107544
iteration 100, loss 1.675302267074585
iteration 200, loss 1.69480562210083
iteration 300, loss 1.7137713432312012
iteration 400, loss 1.6941630840301514
iteration 500, loss 1.739045262336731
iteration 600, loss 1.7069413661956787
iteration 700, loss 1.7135686874389648
iteration 800, loss 1.7088079452514648
iteration 0, loss 1.6770521402359009
iteration 100, loss 1.7077277898788452
iteration 200, loss 1.740870475769043
iteration 300, loss 1.7427352666854858
iteration 400, loss 1.721793532371521
iteration 500, loss 1.7615164518356323
iteration 600, loss 1.8120909929275513
iteration 700, loss 1.674329400062561
iteration 800, loss 1.7168711423873901
iteration 0, loss 1.7548805475234985
iteration 100, loss 1.720466136932373
iteration 200, loss 1.725754737854004
iteration 300, loss 1.7684121131896973
iteration 400, loss 1.754168152809143
iteration 500, loss 1.6794421672821045
iteration 600, loss 1.740713357925415
iteration 700, loss 1.6949297189712524
iteration 800, loss 1.767225980758667
iteration 0, loss 1.7576884031295776
iteration 100, loss 1.713326334953308
iteration 200, loss 1.6903733015060425
iteration 300, loss 1.7248975038528442
iteration 400, loss 1.7210346460342407
iteration 500, loss 1.728715419769287
iteration 600, loss 1.7720081806182861
iteration 700, loss 1.7427966594696045
iteration 800, loss 1.7006657123565674
iteration 0, loss 1.6494067907333374
iteration 100, loss 1.7216339111328125
iteration 200, loss 1.7180447578430176
iteration 300, loss 1.6832634210586548
iteration 400, loss 1.7208300828933716
iteration 500, loss 1.7056541442871094
iteration 600, loss 1.6916677951812744
iteration 700, loss 1.7752060890197754
iteration 800, loss 1.738646388053894
iteration 0, loss 1.7836740016937256
iteration 100, loss 1.780225157737732
iteration 200, loss 1.8011689186096191
iteration 300, loss 1.768056869506836
iteration 400, loss 1.714929461479187
iteration 500, loss 1.677606463432312
iteration 600, loss 1.743973970413208
iteration 700, loss 1.755359172821045
iteration 800, loss 1.7196884155273438
iteration 0, loss 1.7055177688598633
iteration 100, loss 1.6919500827789307
iteration 200, loss 1.7001813650131226
iteration 300, loss 1.7300293445587158
iteration 400, loss 1.7402647733688354
iteration 500, loss 1.803548812866211
iteration 600, loss 1.6836717128753662
iteration 700, loss 1.708217978477478
iteration 800, loss 1.7015273571014404
iteration 0, loss 1.6732800006866455
iteration 100, loss 1.7196264266967773
iteration 200, loss 1.762274146080017
iteration 300, loss 1.7868988513946533
iteration 400, loss 1.7117258310317993
iteration 500, loss 1.7483344078063965
iteration 600, loss 1.7733910083770752
iteration 700, loss 1.8186110258102417
iteration 800, loss 1.662811517715454
iteration 0, loss 1.675750494003296
iteration 100, loss 1.670567274093628
iteration 200, loss 1.7748444080352783
iteration 300, loss 1.7153323888778687
iteration 400, loss 1.7637102603912354
iteration 500, loss 1.7378555536270142
iteration 600, loss 1.6963926553726196
iteration 700, loss 1.77094304561615
iteration 800, loss 1.744492530822754
iteration 0, loss 1.722068428993225
iteration 100, loss 1.7117630243301392
iteration 200, loss 1.7344892024993896
iteration 300, loss 1.6702368259429932
iteration 400, loss 1.6587810516357422
iteration 500, loss 1.713688850402832
iteration 600, loss 1.6120829582214355
iteration 700, loss 1.6516696214675903
iteration 800, loss 1.7595244646072388
iteration 0, loss 1.7512506246566772
iteration 100, loss 1.7146857976913452
iteration 200, loss 1.7238248586654663
iteration 300, loss 1.7041637897491455
iteration 400, loss 1.7053102254867554
iteration 500, loss 1.791419506072998
iteration 600, loss 1.7162258625030518
iteration 700, loss 1.7109235525131226
iteration 800, loss 1.7634522914886475
iteration 0, loss 1.7259939908981323
iteration 100, loss 1.7354958057403564
iteration 200, loss 1.7296401262283325
iteration 300, loss 1.6535446643829346
iteration 400, loss 1.6661359071731567
iteration 500, loss 1.7119622230529785
iteration 600, loss 1.7258485555648804
iteration 700, loss 1.7265777587890625
iteration 800, loss 1.6602072715759277
fold 2 accuracy: 0.6588571428571428
iteration 0, loss 1.7507686614990234
iteration 100, loss 1.6625453233718872
iteration 200, loss 1.7252869606018066
iteration 300, loss 1.6832566261291504
iteration 400, loss 1.724860668182373
iteration 500, loss 1.7248153686523438
iteration 600, loss 1.7086515426635742
iteration 700, loss 1.6835755109786987
iteration 800, loss 1.7500914335250854
iteration 0, loss 1.721817970275879
iteration 100, loss 1.7087332010269165
iteration 200, loss 1.7685569524765015
iteration 300, loss 1.7193098068237305
iteration 400, loss 1.7793840169906616
iteration 500, loss 1.8050140142440796
iteration 600, loss 1.656061053276062
iteration 700, loss 1.703126072883606
iteration 800, loss 1.6925113201141357
iteration 0, loss 1.6489711999893188
iteration 100, loss 1.7144896984100342
iteration 200, loss 1.7410962581634521
iteration 300, loss 1.6928542852401733
iteration 400, loss 1.7117596864700317
iteration 500, loss 1.72640860080719
iteration 600, loss 1.6790106296539307
iteration 700, loss 1.6977999210357666
iteration 800, loss 1.8116401433944702
iteration 0, loss 1.68167245388031
iteration 100, loss 1.7457427978515625
iteration 200, loss 1.7231558561325073
iteration 300, loss 1.754920482635498
iteration 400, loss 1.6923197507858276
iteration 500, loss 1.7046098709106445
iteration 600, loss 1.7518601417541504
iteration 700, loss 1.74041748046875
iteration 800, loss 1.717874526977539
iteration 0, loss 1.6829508543014526
iteration 100, loss 1.687589406967163
iteration 200, loss 1.7365632057189941
iteration 300, loss 1.7812217473983765
iteration 400, loss 1.7375916242599487
iteration 500, loss 1.7964951992034912
iteration 600, loss 1.774233341217041
iteration 700, loss 1.684267520904541
iteration 800, loss 1.6282097101211548
iteration 0, loss 1.7015875577926636
iteration 100, loss 1.708540678024292
iteration 200, loss 1.697923183441162
iteration 300, loss 1.707490086555481
iteration 400, loss 1.7309023141860962
iteration 500, loss 1.6998223066329956
iteration 600, loss 1.6235920190811157
iteration 700, loss 1.7155667543411255
iteration 800, loss 1.7177623510360718
iteration 0, loss 1.710888385772705
iteration 100, loss 1.6967952251434326
iteration 200, loss 1.6679000854492188
iteration 300, loss 1.778362512588501
iteration 400, loss 1.7293150424957275
iteration 500, loss 1.7355172634124756
iteration 600, loss 1.6871669292449951
iteration 700, loss 1.7502098083496094
iteration 800, loss 1.691114902496338
iteration 0, loss 1.7204241752624512
iteration 100, loss 1.7615985870361328
iteration 200, loss 1.794427752494812
iteration 300, loss 1.7044214010238647
iteration 400, loss 1.747457504272461
iteration 500, loss 1.6892472505569458
iteration 600, loss 1.8207604885101318
iteration 700, loss 1.7080974578857422
iteration 800, loss 1.7121955156326294
iteration 0, loss 1.7240523099899292
iteration 100, loss 1.6806026697158813
iteration 200, loss 1.683890700340271
iteration 300, loss 1.7421239614486694
iteration 400, loss 1.7411928176879883
iteration 500, loss 1.6956679821014404
iteration 600, loss 1.7275384664535522
iteration 700, loss 1.7866346836090088
iteration 800, loss 1.6889749765396118
iteration 0, loss 1.7607803344726562
iteration 100, loss 1.6591520309448242
iteration 200, loss 1.7457149028778076
iteration 300, loss 1.7081729173660278
iteration 400, loss 1.7282038927078247
iteration 500, loss 1.7103615999221802
iteration 600, loss 1.7533786296844482
iteration 700, loss 1.674993634223938
iteration 800, loss 1.7395973205566406
iteration 0, loss 1.7445014715194702
iteration 100, loss 1.668088674545288
iteration 200, loss 1.6834120750427246
iteration 300, loss 1.6960387229919434
iteration 400, loss 1.7290534973144531
iteration 500, loss 1.682378888130188
iteration 600, loss 1.684830904006958
iteration 700, loss 1.6550387144088745
iteration 800, loss 1.7429383993148804
iteration 0, loss 1.690368890762329
iteration 100, loss 1.708947777748108
iteration 200, loss 1.7438350915908813
iteration 300, loss 1.7199974060058594
iteration 400, loss 1.739617109298706
iteration 500, loss 1.6964592933654785
iteration 600, loss 1.6966010332107544
iteration 700, loss 1.6641039848327637
iteration 800, loss 1.7111811637878418
iteration 0, loss 1.7341734170913696
iteration 100, loss 1.7470723390579224
iteration 200, loss 1.702802300453186
iteration 300, loss 1.7553186416625977
iteration 400, loss 1.6382637023925781
iteration 500, loss 1.6701422929763794
iteration 600, loss 1.7492549419403076
iteration 700, loss 1.7112023830413818
iteration 800, loss 1.7603377103805542
iteration 0, loss 1.7470347881317139
iteration 100, loss 1.658765435218811
iteration 200, loss 1.7316768169403076
iteration 300, loss 1.7211869955062866
iteration 400, loss 1.7858002185821533
iteration 500, loss 1.7117407321929932
iteration 600, loss 1.7325453758239746
iteration 700, loss 1.6971267461776733
iteration 800, loss 1.7795114517211914
iteration 0, loss 1.6500705480575562
iteration 100, loss 1.738877534866333
iteration 200, loss 1.6639750003814697
iteration 300, loss 1.6808956861495972
iteration 400, loss 1.7122180461883545
iteration 500, loss 1.6710551977157593
iteration 600, loss 1.7058645486831665
iteration 700, loss 1.758953332901001
iteration 800, loss 1.6657887697219849
iteration 0, loss 1.6473833322525024
iteration 100, loss 1.732797622680664
iteration 200, loss 1.7661532163619995
iteration 300, loss 1.7146896123886108
iteration 400, loss 1.6997754573822021
iteration 500, loss 1.7666569948196411
iteration 600, loss 1.7167710065841675
iteration 700, loss 1.7571278810501099
iteration 800, loss 1.7401238679885864
iteration 0, loss 1.7063252925872803
iteration 100, loss 1.69240140914917
iteration 200, loss 1.7276859283447266
iteration 300, loss 1.7353515625
iteration 400, loss 1.7022197246551514
iteration 500, loss 1.6734296083450317
iteration 600, loss 1.7226179838180542
iteration 700, loss 1.726405143737793
iteration 800, loss 1.720420002937317
iteration 0, loss 1.7160494327545166
iteration 100, loss 1.672948956489563
iteration 200, loss 1.6784917116165161
iteration 300, loss 1.7637195587158203
iteration 400, loss 1.756784439086914
iteration 500, loss 1.7214897871017456
iteration 600, loss 1.711744785308838
iteration 700, loss 1.6960108280181885
iteration 800, loss 1.7059916257858276
iteration 0, loss 1.7268673181533813
iteration 100, loss 1.6705197095870972
iteration 200, loss 1.7102515697479248
iteration 300, loss 1.65071702003479
iteration 400, loss 1.7387975454330444
iteration 500, loss 1.7489964962005615
iteration 600, loss 1.6899722814559937
iteration 700, loss 1.7093796730041504
iteration 800, loss 1.6559964418411255
iteration 0, loss 1.744384527206421
iteration 100, loss 1.7501925230026245
iteration 200, loss 1.7582224607467651
iteration 300, loss 1.6891483068466187
iteration 400, loss 1.6986416578292847
iteration 500, loss 1.6635833978652954
iteration 600, loss 1.6937397718429565
iteration 700, loss 1.6631962060928345
iteration 800, loss 1.7559248208999634
iteration 0, loss 1.7451035976409912
iteration 100, loss 1.693570852279663
iteration 200, loss 1.686457633972168
iteration 300, loss 1.7200837135314941
iteration 400, loss 1.7111823558807373
iteration 500, loss 1.7187421321868896
iteration 600, loss 1.6837525367736816
iteration 700, loss 1.7593271732330322
iteration 800, loss 1.801876187324524
iteration 0, loss 1.7033635377883911
iteration 100, loss 1.7112218141555786
iteration 200, loss 1.6979254484176636
iteration 300, loss 1.7809867858886719
iteration 400, loss 1.7016321420669556
iteration 500, loss 1.69241201877594
iteration 600, loss 1.681275725364685
iteration 700, loss 1.7091797590255737
iteration 800, loss 1.7249369621276855
iteration 0, loss 1.7253928184509277
iteration 100, loss 1.7164533138275146
iteration 200, loss 1.6937599182128906
iteration 300, loss 1.7335681915283203
iteration 400, loss 1.6914803981781006
iteration 500, loss 1.6707161664962769
iteration 600, loss 1.6544548273086548
iteration 700, loss 1.7325634956359863
iteration 800, loss 1.7599499225616455
iteration 0, loss 1.606559157371521
iteration 100, loss 1.6696910858154297
iteration 200, loss 1.6749533414840698
iteration 300, loss 1.6900752782821655
iteration 400, loss 1.7115235328674316
iteration 500, loss 1.7304754257202148
iteration 600, loss 1.7449544668197632
iteration 700, loss 1.7136601209640503
iteration 800, loss 1.7106280326843262
iteration 0, loss 1.6920479536056519
iteration 100, loss 1.6848098039627075
iteration 200, loss 1.7399544715881348
iteration 300, loss 1.7130663394927979
iteration 400, loss 1.6793479919433594
iteration 500, loss 1.7172681093215942
iteration 600, loss 1.7077256441116333
iteration 700, loss 1.6837419271469116
iteration 800, loss 1.731008768081665
iteration 0, loss 1.7132322788238525
iteration 100, loss 1.7105380296707153
iteration 200, loss 1.6572182178497314
iteration 300, loss 1.7211532592773438
iteration 400, loss 1.64827561378479
iteration 500, loss 1.7690876722335815
iteration 600, loss 1.7592169046401978
iteration 700, loss 1.7061704397201538
iteration 800, loss 1.6680597066879272
iteration 0, loss 1.7437856197357178
iteration 100, loss 1.736019253730774
iteration 200, loss 1.721890926361084
iteration 300, loss 1.7692991495132446
iteration 400, loss 1.6928552389144897
iteration 500, loss 1.6931841373443604
iteration 600, loss 1.711683988571167
iteration 700, loss 1.7444679737091064
iteration 800, loss 1.684136152267456
iteration 0, loss 1.6696343421936035
iteration 100, loss 1.6809430122375488
iteration 200, loss 1.7570571899414062
iteration 300, loss 1.7333616018295288
iteration 400, loss 1.7001255750656128
iteration 500, loss 1.7465332746505737
iteration 600, loss 1.7560782432556152
iteration 700, loss 1.6857956647872925
iteration 800, loss 1.7905802726745605
iteration 0, loss 1.7023108005523682
iteration 100, loss 1.6779464483261108
iteration 200, loss 1.6500415802001953
iteration 300, loss 1.7322072982788086
iteration 400, loss 1.680650234222412
iteration 500, loss 1.7329572439193726
iteration 600, loss 1.708512783050537
iteration 700, loss 1.7294971942901611
iteration 800, loss 1.6698917150497437
iteration 0, loss 1.6991078853607178
iteration 100, loss 1.678289771080017
iteration 200, loss 1.7183955907821655
iteration 300, loss 1.675831913948059
iteration 400, loss 1.7397911548614502
iteration 500, loss 1.710282325744629
iteration 600, loss 1.7491759061813354
iteration 700, loss 1.678997278213501
iteration 800, loss 1.7535343170166016
iteration 0, loss 1.6712703704833984
iteration 100, loss 1.7420448064804077
iteration 200, loss 1.730241298675537
iteration 300, loss 1.6856855154037476
iteration 400, loss 1.6794477701187134
iteration 500, loss 1.7075941562652588
iteration 600, loss 1.7463605403900146
iteration 700, loss 1.6869299411773682
iteration 800, loss 1.7033326625823975
iteration 0, loss 1.7330986261367798
iteration 100, loss 1.7033010721206665
iteration 200, loss 1.666919469833374
iteration 300, loss 1.7257919311523438
iteration 400, loss 1.760144829750061
iteration 500, loss 1.7007596492767334
iteration 600, loss 1.6519211530685425
iteration 700, loss 1.6944305896759033
iteration 800, loss 1.7217940092086792
iteration 0, loss 1.703283429145813
iteration 100, loss 1.6454271078109741
iteration 200, loss 1.6882288455963135
iteration 300, loss 1.6976279020309448
iteration 400, loss 1.6916677951812744
iteration 500, loss 1.7618441581726074
iteration 600, loss 1.7123467922210693
iteration 700, loss 1.7006839513778687
iteration 800, loss 1.7418028116226196
iteration 0, loss 1.7383017539978027
iteration 100, loss 1.7060039043426514
iteration 200, loss 1.7233480215072632
iteration 300, loss 1.7428416013717651
iteration 400, loss 1.6522669792175293
iteration 500, loss 1.6711227893829346
iteration 600, loss 1.6587508916854858
iteration 700, loss 1.744018316268921
iteration 800, loss 1.765742540359497
iteration 0, loss 1.8084992170333862
iteration 100, loss 1.6772804260253906
iteration 200, loss 1.7104653120040894
iteration 300, loss 1.6598553657531738
iteration 400, loss 1.6892098188400269
iteration 500, loss 1.742891550064087
iteration 600, loss 1.684040904045105
iteration 700, loss 1.7662826776504517
iteration 800, loss 1.7753711938858032
iteration 0, loss 1.713027000427246
iteration 100, loss 1.6798723936080933
iteration 200, loss 1.7118661403656006
iteration 300, loss 1.7308269739151
iteration 400, loss 1.7107758522033691
iteration 500, loss 1.7418241500854492
iteration 600, loss 1.71879243850708
iteration 700, loss 1.729830026626587
iteration 800, loss 1.7496592998504639
iteration 0, loss 1.7649610042572021
iteration 100, loss 1.6904582977294922
iteration 200, loss 1.7312264442443848
iteration 300, loss 1.7080339193344116
iteration 400, loss 1.6674532890319824
iteration 500, loss 1.723258137702942
iteration 600, loss 1.6913554668426514
iteration 700, loss 1.7325336933135986
iteration 800, loss 1.7499475479125977
iteration 0, loss 1.7227267026901245
iteration 100, loss 1.7053017616271973
iteration 200, loss 1.6597756147384644
iteration 300, loss 1.7312954664230347
iteration 400, loss 1.774408221244812
iteration 500, loss 1.7002875804901123
iteration 600, loss 1.6867215633392334
iteration 700, loss 1.6916849613189697
iteration 800, loss 1.6527023315429688
iteration 0, loss 1.7874360084533691
iteration 100, loss 1.7451857328414917
iteration 200, loss 1.7886277437210083
iteration 300, loss 1.7028800249099731
iteration 400, loss 1.6523061990737915
iteration 500, loss 1.67725670337677
iteration 600, loss 1.6710938215255737
iteration 700, loss 1.8037110567092896
iteration 800, loss 1.7251750230789185
iteration 0, loss 1.6580475568771362
iteration 100, loss 1.6869874000549316
iteration 200, loss 1.7434440851211548
iteration 300, loss 1.7209460735321045
iteration 400, loss 1.7033426761627197
iteration 500, loss 1.6710426807403564
iteration 600, loss 1.655310034751892
iteration 700, loss 1.716634750366211
iteration 800, loss 1.6923598051071167
iteration 0, loss 1.7478517293930054
iteration 100, loss 1.7238743305206299
iteration 200, loss 1.670939326286316
iteration 300, loss 1.7333825826644897
iteration 400, loss 1.7245192527770996
iteration 500, loss 1.7303218841552734
iteration 600, loss 1.7324546575546265
iteration 700, loss 1.675064206123352
iteration 800, loss 1.6928952932357788
iteration 0, loss 1.704197645187378
iteration 100, loss 1.7390190362930298
iteration 200, loss 1.748484492301941
iteration 300, loss 1.7428977489471436
iteration 400, loss 1.7256966829299927
iteration 500, loss 1.7136260271072388
iteration 600, loss 1.8025718927383423
iteration 700, loss 1.745668888092041
iteration 800, loss 1.7023365497589111
iteration 0, loss 1.6594955921173096
iteration 100, loss 1.7342342138290405
iteration 200, loss 1.6567072868347168
iteration 300, loss 1.8152601718902588
iteration 400, loss 1.7220661640167236
iteration 500, loss 1.6356096267700195
iteration 600, loss 1.7271249294281006
iteration 700, loss 1.7119396924972534
iteration 800, loss 1.7596884965896606
iteration 0, loss 1.7156503200531006
iteration 100, loss 1.6958633661270142
iteration 200, loss 1.739331603050232
iteration 300, loss 1.8244138956069946
iteration 400, loss 1.6826171875
iteration 500, loss 1.721761703491211
iteration 600, loss 1.6896438598632812
iteration 700, loss 1.7001292705535889
iteration 800, loss 1.7142927646636963
iteration 0, loss 1.7453364133834839
iteration 100, loss 1.7131608724594116
iteration 200, loss 1.682034969329834
iteration 300, loss 1.7221903800964355
iteration 400, loss 1.7442117929458618
iteration 500, loss 1.7691251039505005
iteration 600, loss 1.7548801898956299
iteration 700, loss 1.778775930404663
iteration 800, loss 1.7150311470031738
iteration 0, loss 1.7190489768981934
iteration 100, loss 1.7197462320327759
iteration 200, loss 1.6788630485534668
iteration 300, loss 1.701684594154358
iteration 400, loss 1.76877760887146
iteration 500, loss 1.7128900289535522
iteration 600, loss 1.6788074970245361
iteration 700, loss 1.7746168375015259
iteration 800, loss 1.6913193464279175
iteration 0, loss 1.6700763702392578
iteration 100, loss 1.668463110923767
iteration 200, loss 1.7655388116836548
iteration 300, loss 1.706699013710022
iteration 400, loss 1.6882436275482178
iteration 500, loss 1.6886273622512817
iteration 600, loss 1.678088903427124
iteration 700, loss 1.719574213027954
iteration 800, loss 1.700918436050415
iteration 0, loss 1.7019883394241333
iteration 100, loss 1.6897603273391724
iteration 200, loss 1.721299171447754
iteration 300, loss 1.767338514328003
iteration 400, loss 1.7492470741271973
iteration 500, loss 1.7166800498962402
iteration 600, loss 1.755873441696167
iteration 700, loss 1.6824719905853271
iteration 800, loss 1.7175192832946777
iteration 0, loss 1.7399734258651733
iteration 100, loss 1.7078981399536133
iteration 200, loss 1.6862276792526245
iteration 300, loss 1.759563684463501
iteration 400, loss 1.715104103088379
iteration 500, loss 1.6691625118255615
iteration 600, loss 1.7706186771392822
iteration 700, loss 1.723346471786499
iteration 800, loss 1.7298887968063354
iteration 0, loss 1.6760170459747314
iteration 100, loss 1.746953010559082
iteration 200, loss 1.6601588726043701
iteration 300, loss 1.778745174407959
iteration 400, loss 1.688292145729065
iteration 500, loss 1.7556159496307373
iteration 600, loss 1.7707879543304443
iteration 700, loss 1.7223293781280518
iteration 800, loss 1.7061352729797363
fold 3 accuracy: 0.679
iteration 0, loss 1.6849584579467773
iteration 100, loss 1.7159485816955566
iteration 200, loss 1.663750171661377
iteration 300, loss 1.6684321165084839
iteration 400, loss 1.6926348209381104
iteration 500, loss 1.700156331062317
iteration 600, loss 1.708298683166504
iteration 700, loss 1.6463667154312134
iteration 800, loss 1.7355057001113892
iteration 0, loss 1.7177635431289673
iteration 100, loss 1.7258358001708984
iteration 200, loss 1.6895251274108887
iteration 300, loss 1.6947767734527588
iteration 400, loss 1.6859233379364014
iteration 500, loss 1.7177786827087402
iteration 600, loss 1.7340494394302368
iteration 700, loss 1.6942174434661865
iteration 800, loss 1.693835735321045
iteration 0, loss 1.7110624313354492
iteration 100, loss 1.682113766670227
iteration 200, loss 1.7233641147613525
iteration 300, loss 1.7058806419372559
iteration 400, loss 1.7491878271102905
iteration 500, loss 1.7451575994491577
iteration 600, loss 1.764661192893982
iteration 700, loss 1.7365597486495972
iteration 800, loss 1.7113933563232422
iteration 0, loss 1.6940162181854248
iteration 100, loss 1.676963448524475
iteration 200, loss 1.657217264175415
iteration 300, loss 1.7950263023376465
iteration 400, loss 1.769002079963684
iteration 500, loss 1.6774616241455078
iteration 600, loss 1.6553136110305786
iteration 700, loss 1.7094658613204956
iteration 800, loss 1.7482401132583618
iteration 0, loss 1.6985983848571777
iteration 100, loss 1.70308256149292
iteration 200, loss 1.696373462677002
iteration 300, loss 1.7392451763153076
iteration 400, loss 1.6967447996139526
iteration 500, loss 1.6902779340744019
iteration 600, loss 1.7339386940002441
iteration 700, loss 1.6845734119415283
iteration 800, loss 1.6707947254180908
iteration 0, loss 1.6830148696899414
iteration 100, loss 1.701166033744812
iteration 200, loss 1.7845473289489746
iteration 300, loss 1.7617079019546509
iteration 400, loss 1.7253553867340088
iteration 500, loss 1.7048583030700684
iteration 600, loss 1.7112751007080078
iteration 700, loss 1.6444416046142578
iteration 800, loss 1.7099131345748901
iteration 0, loss 1.6861642599105835
iteration 100, loss 1.6975486278533936
iteration 200, loss 1.701075553894043
iteration 300, loss 1.6668217182159424
iteration 400, loss 1.708940863609314
iteration 500, loss 1.6920713186264038
iteration 600, loss 1.686566948890686
iteration 700, loss 1.8255953788757324
iteration 800, loss 1.739375352859497
iteration 0, loss 1.6444921493530273
iteration 100, loss 1.7020329236984253
iteration 200, loss 1.7056937217712402
iteration 300, loss 1.632033348083496
iteration 400, loss 1.7380937337875366
iteration 500, loss 1.6965882778167725
iteration 600, loss 1.7057558298110962
iteration 700, loss 1.7081185579299927
iteration 800, loss 1.662300944328308
iteration 0, loss 1.7594865560531616
iteration 100, loss 1.677544116973877
iteration 200, loss 1.7132501602172852
iteration 300, loss 1.70613431930542
iteration 400, loss 1.658913254737854
iteration 500, loss 1.6695287227630615
iteration 600, loss 1.7225593328475952
iteration 700, loss 1.7140134572982788
iteration 800, loss 1.6590602397918701
iteration 0, loss 1.746254563331604
iteration 100, loss 1.7537509202957153
iteration 200, loss 1.6974672079086304
iteration 300, loss 1.680909514427185
iteration 400, loss 1.6721446514129639
iteration 500, loss 1.6760778427124023
iteration 600, loss 1.6958837509155273
iteration 700, loss 1.7328779697418213
iteration 800, loss 1.6552040576934814
iteration 0, loss 1.74698805809021
iteration 100, loss 1.6499449014663696
iteration 200, loss 1.7090176343917847
iteration 300, loss 1.7093174457550049
iteration 400, loss 1.6758573055267334
iteration 500, loss 1.7457774877548218
iteration 600, loss 1.6674493551254272
iteration 700, loss 1.7346022129058838
iteration 800, loss 1.7583391666412354
iteration 0, loss 1.6785639524459839
iteration 100, loss 1.7322561740875244
iteration 200, loss 1.7264635562896729
iteration 300, loss 1.7062479257583618
iteration 400, loss 1.725761890411377
iteration 500, loss 1.7550588846206665
iteration 600, loss 1.715439796447754
iteration 700, loss 1.7321263551712036
iteration 800, loss 1.6769895553588867
iteration 0, loss 1.7118581533432007
iteration 100, loss 1.7629988193511963
iteration 200, loss 1.653778076171875
iteration 300, loss 1.666602611541748
iteration 400, loss 1.7084102630615234
iteration 500, loss 1.6837135553359985
iteration 600, loss 1.65174400806427
iteration 700, loss 1.5949667692184448
iteration 800, loss 1.7213712930679321
iteration 0, loss 1.7012522220611572
iteration 100, loss 1.7018218040466309
iteration 200, loss 1.6855268478393555
iteration 300, loss 1.7326501607894897
iteration 400, loss 1.728683352470398
iteration 500, loss 1.7136425971984863
iteration 600, loss 1.730261206626892
iteration 700, loss 1.7537217140197754
iteration 800, loss 1.6783833503723145
iteration 0, loss 1.6866884231567383
iteration 100, loss 1.7016983032226562
iteration 200, loss 1.6792855262756348
iteration 300, loss 1.6845005750656128
iteration 400, loss 1.6486583948135376
iteration 500, loss 1.731929063796997
iteration 600, loss 1.6645063161849976
iteration 700, loss 1.7227530479431152
iteration 800, loss 1.7172623872756958
iteration 0, loss 1.7252297401428223
iteration 100, loss 1.699729323387146
iteration 200, loss 1.6928812265396118
iteration 300, loss 1.7293424606323242
iteration 400, loss 1.7016758918762207
iteration 500, loss 1.784818410873413
iteration 600, loss 1.7261857986450195
iteration 700, loss 1.7213071584701538
iteration 800, loss 1.69597327709198
iteration 0, loss 1.6901357173919678
iteration 100, loss 1.6581579446792603
iteration 200, loss 1.6855255365371704
iteration 300, loss 1.7597301006317139
iteration 400, loss 1.6953623294830322
iteration 500, loss 1.652986764907837
iteration 600, loss 1.757912278175354
iteration 700, loss 1.7467458248138428
iteration 800, loss 1.7129263877868652
iteration 0, loss 1.6730021238327026
iteration 100, loss 1.7272069454193115
iteration 200, loss 1.7295180559158325
iteration 300, loss 1.6372321844100952
iteration 400, loss 1.6824865341186523
iteration 500, loss 1.6795917749404907
iteration 600, loss 1.7111161947250366
iteration 700, loss 1.6974518299102783
iteration 800, loss 1.6950676441192627
iteration 0, loss 1.658620834350586
iteration 100, loss 1.7025398015975952
iteration 200, loss 1.6824798583984375
iteration 300, loss 1.6830767393112183
iteration 400, loss 1.7213388681411743
iteration 500, loss 1.6767182350158691
iteration 600, loss 1.7321009635925293
iteration 700, loss 1.8013396263122559
iteration 800, loss 1.6875770092010498
iteration 0, loss 1.745519757270813
iteration 100, loss 1.7246909141540527
iteration 200, loss 1.7023918628692627
iteration 300, loss 1.7093324661254883
iteration 400, loss 1.717966079711914
iteration 500, loss 1.6259807348251343
iteration 600, loss 1.706709623336792
iteration 700, loss 1.697237491607666
iteration 800, loss 1.6633250713348389
iteration 0, loss 1.6816015243530273
iteration 100, loss 1.6658682823181152
iteration 200, loss 1.722404956817627
iteration 300, loss 1.6926383972167969
iteration 400, loss 1.728584885597229
iteration 500, loss 1.649495005607605
iteration 600, loss 1.6622506380081177
iteration 700, loss 1.737605333328247
iteration 800, loss 1.6623029708862305
iteration 0, loss 1.6742044687271118
iteration 100, loss 1.68743896484375
iteration 200, loss 1.7156963348388672
iteration 300, loss 1.6969643831253052
iteration 400, loss 1.7003555297851562
iteration 500, loss 1.7330249547958374
iteration 600, loss 1.7015438079833984
iteration 700, loss 1.6494836807250977
iteration 800, loss 1.722445011138916
iteration 0, loss 1.6585137844085693
iteration 100, loss 1.7050790786743164
iteration 200, loss 1.7410104274749756
iteration 300, loss 1.7314040660858154
iteration 400, loss 1.6796646118164062
iteration 500, loss 1.7693064212799072
iteration 600, loss 1.6947190761566162
iteration 700, loss 1.727502703666687
iteration 800, loss 1.7042734622955322
iteration 0, loss 1.6935999393463135
iteration 100, loss 1.6472266912460327
iteration 200, loss 1.7651665210723877
iteration 300, loss 1.7213126420974731
iteration 400, loss 1.6823607683181763
iteration 500, loss 1.747422456741333
iteration 600, loss 1.7106568813323975
iteration 700, loss 1.7251712083816528
iteration 800, loss 1.6658823490142822
iteration 0, loss 1.7084678411483765
iteration 100, loss 1.7063769102096558
iteration 200, loss 1.7037335634231567
iteration 300, loss 1.7707765102386475
iteration 400, loss 1.735476016998291
iteration 500, loss 1.7160382270812988
iteration 600, loss 1.7155638933181763
iteration 700, loss 1.703446865081787
iteration 800, loss 1.718685507774353
iteration 0, loss 1.7325363159179688
iteration 100, loss 1.6776533126831055
iteration 200, loss 1.7206714153289795
iteration 300, loss 1.6638532876968384
iteration 400, loss 1.7110083103179932
iteration 500, loss 1.7077139616012573
iteration 600, loss 1.6880968809127808
iteration 700, loss 1.7629343271255493
iteration 800, loss 1.6962425708770752
iteration 0, loss 1.7046304941177368
iteration 100, loss 1.7207891941070557
iteration 200, loss 1.6602007150650024
iteration 300, loss 1.698439598083496
iteration 400, loss 1.6443980932235718
iteration 500, loss 1.7114779949188232
iteration 600, loss 1.691112995147705
iteration 700, loss 1.7088619470596313
iteration 800, loss 1.684921145439148
iteration 0, loss 1.6773350238800049
iteration 100, loss 1.703980803489685
iteration 200, loss 1.6780281066894531
iteration 300, loss 1.6987272500991821
iteration 400, loss 1.744195818901062
iteration 500, loss 1.7153576612472534
iteration 600, loss 1.7096498012542725
iteration 700, loss 1.7207117080688477
iteration 800, loss 1.6866185665130615
iteration 0, loss 1.7462042570114136
iteration 100, loss 1.7650796175003052
iteration 200, loss 1.73219633102417
iteration 300, loss 1.6408512592315674
iteration 400, loss 1.8075846433639526
iteration 500, loss 1.7404245138168335
iteration 600, loss 1.748368263244629
iteration 700, loss 1.686303973197937
iteration 800, loss 1.7475965023040771
iteration 0, loss 1.6530777215957642
iteration 100, loss 1.7505401372909546
iteration 200, loss 1.6399931907653809
iteration 300, loss 1.7166359424591064
iteration 400, loss 1.7028836011886597
iteration 500, loss 1.7378462553024292
iteration 600, loss 1.6959528923034668
iteration 700, loss 1.724748134613037
iteration 800, loss 1.6979273557662964
iteration 0, loss 1.701332449913025
iteration 100, loss 1.7026163339614868
iteration 200, loss 1.7208499908447266
iteration 300, loss 1.749692440032959
iteration 400, loss 1.70647132396698
iteration 500, loss 1.6487020254135132
iteration 600, loss 1.7234724760055542
iteration 700, loss 1.656810998916626
iteration 800, loss 1.740496277809143
iteration 0, loss 1.7386661767959595
iteration 100, loss 1.6693499088287354
iteration 200, loss 1.7251026630401611
iteration 300, loss 1.782413363456726
iteration 400, loss 1.6733434200286865
iteration 500, loss 1.727186918258667
iteration 600, loss 1.7032281160354614
iteration 700, loss 1.6868215799331665
iteration 800, loss 1.7020736932754517
iteration 0, loss 1.7609812021255493
iteration 100, loss 1.711601972579956
iteration 200, loss 1.6800436973571777
iteration 300, loss 1.648808479309082
iteration 400, loss 1.7142974138259888
iteration 500, loss 1.7084877490997314
iteration 600, loss 1.8042865991592407
iteration 700, loss 1.678309679031372
iteration 800, loss 1.7517118453979492
iteration 0, loss 1.7028412818908691
iteration 100, loss 1.7295246124267578
iteration 200, loss 1.7467634677886963
iteration 300, loss 1.7236194610595703
iteration 400, loss 1.7808680534362793
iteration 500, loss 1.7047102451324463
iteration 600, loss 1.7302976846694946
iteration 700, loss 1.6923332214355469
iteration 800, loss 1.6371103525161743
iteration 0, loss 1.6609514951705933
iteration 100, loss 1.711285948753357
iteration 200, loss 1.76567804813385
iteration 300, loss 1.6806401014328003
iteration 400, loss 1.711715817451477
iteration 500, loss 1.7174874544143677
iteration 600, loss 1.7203723192214966
iteration 700, loss 1.7557305097579956
iteration 800, loss 1.6941722631454468
iteration 0, loss 1.6879959106445312
iteration 100, loss 1.7577685117721558
iteration 200, loss 1.7063095569610596
iteration 300, loss 1.697661280632019
iteration 400, loss 1.667687177658081
iteration 500, loss 1.694024920463562
iteration 600, loss 1.6717520952224731
iteration 700, loss 1.7028650045394897
iteration 800, loss 1.6999984979629517
iteration 0, loss 1.7110284566879272
iteration 100, loss 1.7373602390289307
iteration 200, loss 1.6951292753219604
iteration 300, loss 1.743268609046936
iteration 400, loss 1.732775092124939
iteration 500, loss 1.701155662536621
iteration 600, loss 1.6776593923568726
iteration 700, loss 1.6477309465408325
iteration 800, loss 1.7544152736663818
iteration 0, loss 1.6686067581176758
iteration 100, loss 1.719130277633667
iteration 200, loss 1.7484567165374756
iteration 300, loss 1.6749602556228638
iteration 400, loss 1.6785762310028076
iteration 500, loss 1.644528865814209
iteration 600, loss 1.7110686302185059
iteration 700, loss 1.7149255275726318
iteration 800, loss 1.7095056772232056
iteration 0, loss 1.7185667753219604
iteration 100, loss 1.7231857776641846
iteration 200, loss 1.7173199653625488
iteration 300, loss 1.6853920221328735
iteration 400, loss 1.6665544509887695
iteration 500, loss 1.6815159320831299
iteration 600, loss 1.6899011135101318
iteration 700, loss 1.7226091623306274
iteration 800, loss 1.6524910926818848
iteration 0, loss 1.7029623985290527
iteration 100, loss 1.6450330018997192
iteration 200, loss 1.7185620069503784
iteration 300, loss 1.6557363271713257
iteration 400, loss 1.7202131748199463
iteration 500, loss 1.7521440982818604
iteration 600, loss 1.7781134843826294
iteration 700, loss 1.6935087442398071
iteration 800, loss 1.7789180278778076
iteration 0, loss 1.7190016508102417
iteration 100, loss 1.6359281539916992
iteration 200, loss 1.7118548154830933
iteration 300, loss 1.6521285772323608
iteration 400, loss 1.7161215543746948
iteration 500, loss 1.6133763790130615
iteration 600, loss 1.6712182760238647
iteration 700, loss 1.7809545993804932
iteration 800, loss 1.7091560363769531
iteration 0, loss 1.7223352193832397
iteration 100, loss 1.7010568380355835
iteration 200, loss 1.7479467391967773
iteration 300, loss 1.7271023988723755
iteration 400, loss 1.710522174835205
iteration 500, loss 1.6815961599349976
iteration 600, loss 1.6715881824493408
iteration 700, loss 1.6580660343170166
iteration 800, loss 1.7167049646377563
iteration 0, loss 1.7883834838867188
iteration 100, loss 1.6435428857803345
iteration 200, loss 1.7454568147659302
iteration 300, loss 1.7179540395736694
iteration 400, loss 1.6830188035964966
iteration 500, loss 1.6295890808105469
iteration 600, loss 1.7482068538665771
iteration 700, loss 1.727962851524353
iteration 800, loss 1.7191200256347656
iteration 0, loss 1.699209451675415
iteration 100, loss 1.6419094800949097
iteration 200, loss 1.7537983655929565
iteration 300, loss 1.659096360206604
iteration 400, loss 1.7262276411056519
iteration 500, loss 1.653686761856079
iteration 600, loss 1.7270811796188354
iteration 700, loss 1.7052518129348755
iteration 800, loss 1.6853245496749878
iteration 0, loss 1.6642810106277466
iteration 100, loss 1.697182297706604
iteration 200, loss 1.7393693923950195
iteration 300, loss 1.7389521598815918
iteration 400, loss 1.7250431776046753
iteration 500, loss 1.7325918674468994
iteration 600, loss 1.6812604665756226
iteration 700, loss 1.6894415616989136
iteration 800, loss 1.7625123262405396
iteration 0, loss 1.6814682483673096
iteration 100, loss 1.7070159912109375
iteration 200, loss 1.6439794301986694
iteration 300, loss 1.7822473049163818
iteration 400, loss 1.6715574264526367
iteration 500, loss 1.7185449600219727
iteration 600, loss 1.6981611251831055
iteration 700, loss 1.6623318195343018
iteration 800, loss 1.7245558500289917
iteration 0, loss 1.7194658517837524
iteration 100, loss 1.7383772134780884
iteration 200, loss 1.6973679065704346
iteration 300, loss 1.6707994937896729
iteration 400, loss 1.727880835533142
iteration 500, loss 1.6394476890563965
iteration 600, loss 1.6597400903701782
iteration 700, loss 1.693278193473816
iteration 800, loss 1.7274726629257202
iteration 0, loss 1.715389609336853
iteration 100, loss 1.701021432876587
iteration 200, loss 1.697127342224121
iteration 300, loss 1.6725051403045654
iteration 400, loss 1.7134168148040771
iteration 500, loss 1.6782913208007812
iteration 600, loss 1.681024193763733
iteration 700, loss 1.684262752532959
iteration 800, loss 1.675549030303955
iteration 0, loss 1.7262052297592163
iteration 100, loss 1.6696032285690308
iteration 200, loss 1.7123342752456665
iteration 300, loss 1.7130041122436523
iteration 400, loss 1.6682461500167847
iteration 500, loss 1.6699764728546143
iteration 600, loss 1.7269906997680664
iteration 700, loss 1.7246558666229248
iteration 800, loss 1.6177810430526733
iteration 0, loss 1.699831247329712
iteration 100, loss 1.6775963306427002
iteration 200, loss 1.7058370113372803
iteration 300, loss 1.6635010242462158
iteration 400, loss 1.7084418535232544
iteration 500, loss 1.669131875038147
iteration 600, loss 1.6831425428390503
iteration 700, loss 1.7387200593948364
iteration 800, loss 1.7018078565597534
fold 4 accuracy: 0.7105
[2024-02-29 02:38:52,534] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-29 02:38:52,535] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         458     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       560 MACs
fwd flops per GPU:                                                      1.12 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.12 K  
fwd latency:                                                            461.1 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    2.43 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '458'}
    MACs        - {'KronLayer': '560 MACs'}
    fwd latency - {'KronLayer': '461.1 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  458 = 100% Params, 560 MACs = 100% MACs, 461.1 us = 100% latency, 2.43 MFLOPS
  (kronlinear): KronLinear(458 = 100% Params, 560 MACs = 100% MACs, 366.69 us = 79.52% latency, 3.05 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 27.66 us = 6% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-29 02:38:52,543] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.12 K 458
iteration 0, loss 2.325209617614746
iteration 100, loss 2.299056053161621
iteration 200, loss 2.1551830768585205
iteration 300, loss 2.1177618503570557
iteration 400, loss 2.0317158699035645
iteration 500, loss 1.9365822076797485
iteration 600, loss 1.9363065958023071
iteration 700, loss 1.8484033346176147
iteration 800, loss 1.847621202468872
iteration 0, loss 1.8703323602676392
iteration 100, loss 1.9057886600494385
iteration 200, loss 1.8548569679260254
iteration 300, loss 1.9058775901794434
iteration 400, loss 1.8843692541122437
iteration 500, loss 1.8410333395004272
iteration 600, loss 1.7907543182373047
iteration 700, loss 1.8846004009246826
iteration 800, loss 1.7790030241012573
iteration 0, loss 1.837073564529419
iteration 100, loss 1.8640127182006836
iteration 200, loss 1.834265947341919
iteration 300, loss 1.7749594449996948
iteration 400, loss 1.7884167432785034
iteration 500, loss 1.7099121809005737
iteration 600, loss 1.7819985151290894
iteration 700, loss 1.8094033002853394
iteration 800, loss 1.7800233364105225
iteration 0, loss 1.7775399684906006
iteration 100, loss 1.8336141109466553
iteration 200, loss 1.7595200538635254
iteration 300, loss 1.794678807258606
iteration 400, loss 1.7738337516784668
iteration 500, loss 1.8202452659606934
iteration 600, loss 1.7797869443893433
iteration 700, loss 1.7978591918945312
iteration 800, loss 1.8069920539855957
iteration 0, loss 1.863847017288208
iteration 100, loss 1.7302305698394775
iteration 200, loss 1.7933223247528076
iteration 300, loss 1.7801988124847412
iteration 400, loss 1.7160844802856445
iteration 500, loss 1.758934736251831
iteration 600, loss 1.7774850130081177
iteration 700, loss 1.744829773902893
iteration 800, loss 1.8209809064865112
iteration 0, loss 1.733656406402588
iteration 100, loss 1.7326862812042236
iteration 200, loss 1.7232792377471924
iteration 300, loss 1.7404630184173584
iteration 400, loss 1.7160154581069946
iteration 500, loss 1.7636948823928833
iteration 600, loss 1.7642302513122559
iteration 700, loss 1.747166395187378
iteration 800, loss 1.7723417282104492
iteration 0, loss 1.7397966384887695
iteration 100, loss 1.7193803787231445
iteration 200, loss 1.7438374757766724
iteration 300, loss 1.7051764726638794
iteration 400, loss 1.7025015354156494
iteration 500, loss 1.8001880645751953
iteration 600, loss 1.7757388353347778
iteration 700, loss 1.817621111869812
iteration 800, loss 1.7299073934555054
iteration 0, loss 1.7668213844299316
iteration 100, loss 1.7531251907348633
iteration 200, loss 1.7539188861846924
iteration 300, loss 1.7290339469909668
iteration 400, loss 1.7675222158432007
iteration 500, loss 1.7106856107711792
iteration 600, loss 1.7576918601989746
iteration 700, loss 1.7849117517471313
iteration 800, loss 1.7582786083221436
iteration 0, loss 1.7067312002182007
iteration 100, loss 1.810284972190857
iteration 200, loss 1.7811436653137207
iteration 300, loss 1.682137131690979
iteration 400, loss 1.742236852645874
iteration 500, loss 1.7566956281661987
iteration 600, loss 1.7211943864822388
iteration 700, loss 1.7459070682525635
iteration 800, loss 1.7302485704421997
iteration 0, loss 1.7826082706451416
iteration 100, loss 1.728898286819458
iteration 200, loss 1.7361263036727905
iteration 300, loss 1.7689063549041748
iteration 400, loss 1.7360575199127197
iteration 500, loss 1.6826801300048828
iteration 600, loss 1.6928495168685913
iteration 700, loss 1.7453745603561401
iteration 800, loss 1.7257474660873413
iteration 0, loss 1.720718264579773
iteration 100, loss 1.6818031072616577
iteration 200, loss 1.7310240268707275
iteration 300, loss 1.7404530048370361
iteration 400, loss 1.7854455709457397
iteration 500, loss 1.6939988136291504
iteration 600, loss 1.6902471780776978
iteration 700, loss 1.6817377805709839
iteration 800, loss 1.7608327865600586
iteration 0, loss 1.6530888080596924
iteration 100, loss 1.7484196424484253
iteration 200, loss 1.7644374370574951
iteration 300, loss 1.7421071529388428
iteration 400, loss 1.6868815422058105
iteration 500, loss 1.7132303714752197
iteration 600, loss 1.8323901891708374
iteration 700, loss 1.7894290685653687
iteration 800, loss 1.7403905391693115
iteration 0, loss 1.751242995262146
iteration 100, loss 1.713260531425476
iteration 200, loss 1.7161073684692383
iteration 300, loss 1.6606507301330566
iteration 400, loss 1.7054455280303955
iteration 500, loss 1.680004596710205
iteration 600, loss 1.6991373300552368
iteration 700, loss 1.8013147115707397
iteration 800, loss 1.715218424797058
iteration 0, loss 1.7999650239944458
iteration 100, loss 1.6470478773117065
iteration 200, loss 1.7376564741134644
iteration 300, loss 1.749444603919983
iteration 400, loss 1.7226510047912598
iteration 500, loss 1.754019021987915
iteration 600, loss 1.7250254154205322
iteration 700, loss 1.7271777391433716
iteration 800, loss 1.7071053981781006
iteration 0, loss 1.7142863273620605
iteration 100, loss 1.7105389833450317
iteration 200, loss 1.7493925094604492
iteration 300, loss 1.7036830186843872
iteration 400, loss 1.7630643844604492
iteration 500, loss 1.7148020267486572
iteration 600, loss 1.853543996810913
iteration 700, loss 1.7371859550476074
iteration 800, loss 1.736499309539795
iteration 0, loss 1.705370545387268
iteration 100, loss 1.7042624950408936
iteration 200, loss 1.7755879163742065
iteration 300, loss 1.7141611576080322
iteration 400, loss 1.7262200117111206
iteration 500, loss 1.7302978038787842
iteration 600, loss 1.822157859802246
iteration 700, loss 1.7417123317718506
iteration 800, loss 1.6809942722320557
iteration 0, loss 1.7267873287200928
iteration 100, loss 1.7605626583099365
iteration 200, loss 1.739395260810852
iteration 300, loss 1.706581711769104
iteration 400, loss 1.6686986684799194
iteration 500, loss 1.7612333297729492
iteration 600, loss 1.7922154664993286
iteration 700, loss 1.787297010421753
iteration 800, loss 1.6834354400634766
iteration 0, loss 1.737674593925476
iteration 100, loss 1.7593427896499634
iteration 200, loss 1.6693545579910278
iteration 300, loss 1.7387712001800537
iteration 400, loss 1.6722593307495117
iteration 500, loss 1.7233058214187622
iteration 600, loss 1.6900904178619385
iteration 700, loss 1.6843411922454834
iteration 800, loss 1.7862657308578491
iteration 0, loss 1.68912935256958
iteration 100, loss 1.7317681312561035
iteration 200, loss 1.7649881839752197
iteration 300, loss 1.6787320375442505
iteration 400, loss 1.72007417678833
iteration 500, loss 1.756447196006775
iteration 600, loss 1.729248046875
iteration 700, loss 1.7122159004211426
iteration 800, loss 1.693798542022705
iteration 0, loss 1.6776927709579468
iteration 100, loss 1.7604596614837646
iteration 200, loss 1.700660228729248
iteration 300, loss 1.665247917175293
iteration 400, loss 1.6849641799926758
iteration 500, loss 1.6874172687530518
iteration 600, loss 1.7039438486099243
iteration 700, loss 1.7290364503860474
iteration 800, loss 1.768188714981079
iteration 0, loss 1.7235416173934937
iteration 100, loss 1.693042516708374
iteration 200, loss 1.778566598892212
iteration 300, loss 1.7230112552642822
iteration 400, loss 1.7013312578201294
iteration 500, loss 1.712807297706604
iteration 600, loss 1.7497392892837524
iteration 700, loss 1.7367641925811768
iteration 800, loss 1.6404383182525635
iteration 0, loss 1.6952115297317505
iteration 100, loss 1.6943997144699097
iteration 200, loss 1.646756887435913
iteration 300, loss 1.7216473817825317
iteration 400, loss 1.686903715133667
iteration 500, loss 1.7178407907485962
iteration 600, loss 1.7243441343307495
iteration 700, loss 1.6740113496780396
iteration 800, loss 1.7413175106048584
iteration 0, loss 1.7669802904129028
iteration 100, loss 1.7565417289733887
iteration 200, loss 1.6992965936660767
iteration 300, loss 1.7127666473388672
iteration 400, loss 1.7981171607971191
iteration 500, loss 1.6266859769821167
iteration 600, loss 1.7533819675445557
iteration 700, loss 1.765977382659912
iteration 800, loss 1.6333754062652588
iteration 0, loss 1.6851252317428589
iteration 100, loss 1.7174077033996582
iteration 200, loss 1.7652074098587036
iteration 300, loss 1.6689962148666382
iteration 400, loss 1.6446001529693604
iteration 500, loss 1.7331488132476807
iteration 600, loss 1.6976838111877441
iteration 700, loss 1.7387446165084839
iteration 800, loss 1.6892180442810059
iteration 0, loss 1.6719800233840942
iteration 100, loss 1.6729035377502441
iteration 200, loss 1.7683219909667969
iteration 300, loss 1.7899081707000732
iteration 400, loss 1.7056922912597656
iteration 500, loss 1.7026417255401611
iteration 600, loss 1.6716561317443848
iteration 700, loss 1.6902116537094116
iteration 800, loss 1.6749069690704346
iteration 0, loss 1.6613349914550781
iteration 100, loss 1.6942739486694336
iteration 200, loss 1.6893093585968018
iteration 300, loss 1.724645972251892
iteration 400, loss 1.7200777530670166
iteration 500, loss 1.7268579006195068
iteration 600, loss 1.7546982765197754
iteration 700, loss 1.7616188526153564
iteration 800, loss 1.7177176475524902
iteration 0, loss 1.7924138307571411
iteration 100, loss 1.764578938484192
iteration 200, loss 1.7391818761825562
iteration 300, loss 1.6713496446609497
iteration 400, loss 1.7438440322875977
iteration 500, loss 1.7319731712341309
iteration 600, loss 1.7288552522659302
iteration 700, loss 1.7100718021392822
iteration 800, loss 1.7411080598831177
iteration 0, loss 1.7066881656646729
iteration 100, loss 1.7168986797332764
iteration 200, loss 1.8303755521774292
iteration 300, loss 1.7457270622253418
iteration 400, loss 1.7506848573684692
iteration 500, loss 1.6825294494628906
iteration 600, loss 1.703628420829773
iteration 700, loss 1.7219974994659424
iteration 800, loss 1.666067123413086
iteration 0, loss 1.6456998586654663
iteration 100, loss 1.734195590019226
iteration 200, loss 1.7249637842178345
iteration 300, loss 1.7318572998046875
iteration 400, loss 1.7677581310272217
iteration 500, loss 1.7017916440963745
iteration 600, loss 1.7272523641586304
iteration 700, loss 1.6964833736419678
iteration 800, loss 1.691832423210144
iteration 0, loss 1.8400728702545166
iteration 100, loss 1.7116706371307373
iteration 200, loss 1.7696044445037842
iteration 300, loss 1.7092925310134888
iteration 400, loss 1.694616675376892
iteration 500, loss 1.6648468971252441
iteration 600, loss 1.7203869819641113
iteration 700, loss 1.6849877834320068
iteration 800, loss 1.6803524494171143
iteration 0, loss 1.68220055103302
iteration 100, loss 1.7436124086380005
iteration 200, loss 1.6745870113372803
iteration 300, loss 1.6403725147247314
iteration 400, loss 1.676409125328064
iteration 500, loss 1.6638216972351074
iteration 600, loss 1.705899953842163
iteration 700, loss 1.6837401390075684
iteration 800, loss 1.7420016527175903
iteration 0, loss 1.726261019706726
iteration 100, loss 1.695031762123108
iteration 200, loss 1.6765609979629517
iteration 300, loss 1.6949687004089355
iteration 400, loss 1.7120376825332642
iteration 500, loss 1.737722396850586
iteration 600, loss 1.6470752954483032
iteration 700, loss 1.7628408670425415
iteration 800, loss 1.7451457977294922
iteration 0, loss 1.6737347841262817
iteration 100, loss 1.7285778522491455
iteration 200, loss 1.7765638828277588
iteration 300, loss 1.6818008422851562
iteration 400, loss 1.6767164468765259
iteration 500, loss 1.7238317728042603
iteration 600, loss 1.6492294073104858
iteration 700, loss 1.7314091920852661
iteration 800, loss 1.7659809589385986
iteration 0, loss 1.7460445165634155
iteration 100, loss 1.7147809267044067
iteration 200, loss 1.7551963329315186
iteration 300, loss 1.7184888124465942
iteration 400, loss 1.6920833587646484
iteration 500, loss 1.682600736618042
iteration 600, loss 1.7354435920715332
iteration 700, loss 1.7536317110061646
iteration 800, loss 1.7547264099121094
iteration 0, loss 1.7610101699829102
iteration 100, loss 1.7246307134628296
iteration 200, loss 1.7204275131225586
iteration 300, loss 1.6921818256378174
iteration 400, loss 1.7471225261688232
iteration 500, loss 1.7006958723068237
iteration 600, loss 1.690260410308838
iteration 700, loss 1.642813801765442
iteration 800, loss 1.7225703001022339
iteration 0, loss 1.7176158428192139
iteration 100, loss 1.6911736726760864
iteration 200, loss 1.6590144634246826
iteration 300, loss 1.7023698091506958
iteration 400, loss 1.7808868885040283
iteration 500, loss 1.687355399131775
iteration 600, loss 1.6754399538040161
iteration 700, loss 1.6856322288513184
iteration 800, loss 1.7237969636917114
iteration 0, loss 1.7056514024734497
iteration 100, loss 1.6827508211135864
iteration 200, loss 1.6773629188537598
iteration 300, loss 1.6751295328140259
iteration 400, loss 1.7306737899780273
iteration 500, loss 1.7339825630187988
iteration 600, loss 1.7418725490570068
iteration 700, loss 1.7310324907302856
iteration 800, loss 1.723500370979309
iteration 0, loss 1.7065277099609375
iteration 100, loss 1.7749476432800293
iteration 200, loss 1.7278298139572144
iteration 300, loss 1.6892544031143188
iteration 400, loss 1.6663501262664795
iteration 500, loss 1.705953598022461
iteration 600, loss 1.6998032331466675
iteration 700, loss 1.801889181137085
iteration 800, loss 1.6900060176849365
iteration 0, loss 1.7286945581436157
iteration 100, loss 1.7319523096084595
iteration 200, loss 1.726312518119812
iteration 300, loss 1.7596389055252075
iteration 400, loss 1.7881968021392822
iteration 500, loss 1.7308945655822754
iteration 600, loss 1.7364749908447266
iteration 700, loss 1.6464438438415527
iteration 800, loss 1.7271794080734253
iteration 0, loss 1.6849980354309082
iteration 100, loss 1.74207603931427
iteration 200, loss 1.6320760250091553
iteration 300, loss 1.6743165254592896
iteration 400, loss 1.6664835214614868
iteration 500, loss 1.7316405773162842
iteration 600, loss 1.6613380908966064
iteration 700, loss 1.6940255165100098
iteration 800, loss 1.6990612745285034
iteration 0, loss 1.744897723197937
iteration 100, loss 1.7079063653945923
iteration 200, loss 1.7302908897399902
iteration 300, loss 1.7143908739089966
iteration 400, loss 1.7630019187927246
iteration 500, loss 1.6837722063064575
iteration 600, loss 1.7149444818496704
iteration 700, loss 1.6982421875
iteration 800, loss 1.7077770233154297
iteration 0, loss 1.7175291776657104
iteration 100, loss 1.7535803318023682
iteration 200, loss 1.6555728912353516
iteration 300, loss 1.6739839315414429
iteration 400, loss 1.7931820154190063
iteration 500, loss 1.748612880706787
iteration 600, loss 1.6816301345825195
iteration 700, loss 1.7439053058624268
iteration 800, loss 1.7515780925750732
iteration 0, loss 1.810913324356079
iteration 100, loss 1.7666655778884888
iteration 200, loss 1.6992602348327637
iteration 300, loss 1.717786431312561
iteration 400, loss 1.7064241170883179
iteration 500, loss 1.7055624723434448
iteration 600, loss 1.787885069847107
iteration 700, loss 1.7580983638763428
iteration 800, loss 1.7419157028198242
iteration 0, loss 1.726296305656433
iteration 100, loss 1.6648575067520142
iteration 200, loss 1.667264699935913
iteration 300, loss 1.714093804359436
iteration 400, loss 1.671331763267517
iteration 500, loss 1.6862213611602783
iteration 600, loss 1.6579045057296753
iteration 700, loss 1.795609712600708
iteration 800, loss 1.6551380157470703
iteration 0, loss 1.7247415781021118
iteration 100, loss 1.7120718955993652
iteration 200, loss 1.7126692533493042
iteration 300, loss 1.6939564943313599
iteration 400, loss 1.7181870937347412
iteration 500, loss 1.712066650390625
iteration 600, loss 1.7526836395263672
iteration 700, loss 1.6871141195297241
iteration 800, loss 1.6643145084381104
iteration 0, loss 1.6794145107269287
iteration 100, loss 1.6308444738388062
iteration 200, loss 1.7236385345458984
iteration 300, loss 1.6855272054672241
iteration 400, loss 1.6806557178497314
iteration 500, loss 1.6916313171386719
iteration 600, loss 1.6762815713882446
iteration 700, loss 1.6903963088989258
iteration 800, loss 1.7667723894119263
iteration 0, loss 1.675945520401001
iteration 100, loss 1.7380975484848022
iteration 200, loss 1.7125288248062134
iteration 300, loss 1.7287677526474
iteration 400, loss 1.823799729347229
iteration 500, loss 1.7771576642990112
iteration 600, loss 1.6476380825042725
iteration 700, loss 1.7477757930755615
iteration 800, loss 1.7438302040100098
iteration 0, loss 1.7719076871871948
iteration 100, loss 1.749994158744812
iteration 200, loss 1.799473524093628
iteration 300, loss 1.7657321691513062
iteration 400, loss 1.6572778224945068
iteration 500, loss 1.7285126447677612
iteration 600, loss 1.7114112377166748
iteration 700, loss 1.7186402082443237
iteration 800, loss 1.6769132614135742
iteration 0, loss 1.7148761749267578
iteration 100, loss 1.7966209650039673
iteration 200, loss 1.658217430114746
iteration 300, loss 1.6914201974868774
iteration 400, loss 1.7158408164978027
iteration 500, loss 1.727953553199768
iteration 600, loss 1.7148693799972534
iteration 700, loss 1.7246696949005127
iteration 800, loss 1.721590518951416
iteration 0, loss 1.6643742322921753
iteration 100, loss 1.717734694480896
iteration 200, loss 1.714895248413086
iteration 300, loss 1.71249520778656
iteration 400, loss 1.7301833629608154
iteration 500, loss 1.7486850023269653
iteration 600, loss 1.7921552658081055
iteration 700, loss 1.7113308906555176
iteration 800, loss 1.7118676900863647
fold 0 accuracy: 0.6981428571428572
iteration 0, loss 1.7151780128479004
iteration 100, loss 1.7173364162445068
iteration 200, loss 1.7405575513839722
iteration 300, loss 1.733291745185852
iteration 400, loss 1.702648401260376
iteration 500, loss 1.7442692518234253
iteration 600, loss 1.619296908378601
iteration 700, loss 1.7068670988082886
iteration 800, loss 1.6450505256652832
iteration 0, loss 1.7060595750808716
iteration 100, loss 1.7334874868392944
iteration 200, loss 1.668427586555481
iteration 300, loss 1.71633780002594
iteration 400, loss 1.678356409072876
iteration 500, loss 1.7279433012008667
iteration 600, loss 1.6481894254684448
iteration 700, loss 1.686086654663086
iteration 800, loss 1.7516920566558838
iteration 0, loss 1.7340604066848755
iteration 100, loss 1.7675740718841553
iteration 200, loss 1.729291319847107
iteration 300, loss 1.6143230199813843
iteration 400, loss 1.6683967113494873
iteration 500, loss 1.7216403484344482
iteration 600, loss 1.703428030014038
iteration 700, loss 1.6918115615844727
iteration 800, loss 1.7580927610397339
iteration 0, loss 1.6775928735733032
iteration 100, loss 1.7258813381195068
iteration 200, loss 1.7668770551681519
iteration 300, loss 1.6814603805541992
iteration 400, loss 1.6808345317840576
iteration 500, loss 1.6735029220581055
iteration 600, loss 1.726426362991333
iteration 700, loss 1.6214929819107056
iteration 800, loss 1.772861123085022
iteration 0, loss 1.7225860357284546
iteration 100, loss 1.717171549797058
iteration 200, loss 1.748719334602356
iteration 300, loss 1.6552367210388184
iteration 400, loss 1.6947718858718872
iteration 500, loss 1.6874234676361084
iteration 600, loss 1.7049894332885742
iteration 700, loss 1.663135290145874
iteration 800, loss 1.672966718673706
iteration 0, loss 1.694105863571167
iteration 100, loss 1.6904160976409912
iteration 200, loss 1.796250581741333
iteration 300, loss 1.6566210985183716
iteration 400, loss 1.7123202085494995
iteration 500, loss 1.6803324222564697
iteration 600, loss 1.7567460536956787
iteration 700, loss 1.7274971008300781
iteration 800, loss 1.668247938156128
iteration 0, loss 1.6664186716079712
iteration 100, loss 1.7548236846923828
iteration 200, loss 1.629858374595642
iteration 300, loss 1.7183140516281128
iteration 400, loss 1.6984269618988037
iteration 500, loss 1.7258687019348145
iteration 600, loss 1.6756922006607056
iteration 700, loss 1.6859774589538574
iteration 800, loss 1.6985952854156494
iteration 0, loss 1.7467706203460693
iteration 100, loss 1.698021650314331
iteration 200, loss 1.6872235536575317
iteration 300, loss 1.6231205463409424
iteration 400, loss 1.7201550006866455
iteration 500, loss 1.6424089670181274
iteration 600, loss 1.6324493885040283
iteration 700, loss 1.6728630065917969
iteration 800, loss 1.6911332607269287
iteration 0, loss 1.6895015239715576
iteration 100, loss 1.7172473669052124
iteration 200, loss 1.6809643507003784
iteration 300, loss 1.6922415494918823
iteration 400, loss 1.7025829553604126
iteration 500, loss 1.6809579133987427
iteration 600, loss 1.7169418334960938
iteration 700, loss 1.819542646408081
iteration 800, loss 1.717208743095398
iteration 0, loss 1.716449499130249
iteration 100, loss 1.6954504251480103
iteration 200, loss 1.6694273948669434
iteration 300, loss 1.703015923500061
iteration 400, loss 1.6599148511886597
iteration 500, loss 1.6812788248062134
iteration 600, loss 1.7735241651535034
iteration 700, loss 1.7540287971496582
iteration 800, loss 1.7270407676696777
iteration 0, loss 1.669834852218628
iteration 100, loss 1.7103465795516968
iteration 200, loss 1.7525748014450073
iteration 300, loss 1.804212212562561
iteration 400, loss 1.7653532028198242
iteration 500, loss 1.7074165344238281
iteration 600, loss 1.7569829225540161
iteration 700, loss 1.6683486700057983
iteration 800, loss 1.6791083812713623
iteration 0, loss 1.6876484155654907
iteration 100, loss 1.6569266319274902
iteration 200, loss 1.7020364999771118
iteration 300, loss 1.6677600145339966
iteration 400, loss 1.7120438814163208
iteration 500, loss 1.6632803678512573
iteration 600, loss 1.719119668006897
iteration 700, loss 1.7396868467330933
iteration 800, loss 1.7401484251022339
iteration 0, loss 1.6693456172943115
iteration 100, loss 1.686490774154663
iteration 200, loss 1.6972960233688354
iteration 300, loss 1.6686140298843384
iteration 400, loss 1.6607369184494019
iteration 500, loss 1.6599161624908447
iteration 600, loss 1.7166796922683716
iteration 700, loss 1.7217985391616821
iteration 800, loss 1.7566072940826416
iteration 0, loss 1.7586948871612549
iteration 100, loss 1.6661937236785889
iteration 200, loss 1.7407101392745972
iteration 300, loss 1.7288328409194946
iteration 400, loss 1.7440862655639648
iteration 500, loss 1.7029173374176025
iteration 600, loss 1.7800372838974
iteration 700, loss 1.762707233428955
iteration 800, loss 1.6877862215042114
iteration 0, loss 1.7512046098709106
iteration 100, loss 1.690706729888916
iteration 200, loss 1.6881632804870605
iteration 300, loss 1.6465163230895996
iteration 400, loss 1.7206653356552124
iteration 500, loss 1.724137306213379
iteration 600, loss 1.6230603456497192
iteration 700, loss 1.7239290475845337
iteration 800, loss 1.6609208583831787
iteration 0, loss 1.6916629076004028
iteration 100, loss 1.6662989854812622
iteration 200, loss 1.6738314628601074
iteration 300, loss 1.6478111743927002
iteration 400, loss 1.7351428270339966
iteration 500, loss 1.667210578918457
iteration 600, loss 1.658820390701294
iteration 700, loss 1.644661784172058
iteration 800, loss 1.659854769706726
iteration 0, loss 1.6815760135650635
iteration 100, loss 1.751009464263916
iteration 200, loss 1.721039056777954
iteration 300, loss 1.6776845455169678
iteration 400, loss 1.6709879636764526
iteration 500, loss 1.7499901056289673
iteration 600, loss 1.6623501777648926
iteration 700, loss 1.6709591150283813
iteration 800, loss 1.7702116966247559
iteration 0, loss 1.676274061203003
iteration 100, loss 1.7258208990097046
iteration 200, loss 1.6399240493774414
iteration 300, loss 1.6726855039596558
iteration 400, loss 1.7270811796188354
iteration 500, loss 1.7243248224258423
iteration 600, loss 1.742759108543396
iteration 700, loss 1.7157965898513794
iteration 800, loss 1.7692220211029053
iteration 0, loss 1.6786653995513916
iteration 100, loss 1.6571016311645508
iteration 200, loss 1.7072941064834595
iteration 300, loss 1.703557014465332
iteration 400, loss 1.7302805185317993
iteration 500, loss 1.6893088817596436
iteration 600, loss 1.683741807937622
iteration 700, loss 1.7173141241073608
iteration 800, loss 1.7030092477798462
iteration 0, loss 1.7124139070510864
iteration 100, loss 1.6621665954589844
iteration 200, loss 1.7163541316986084
iteration 300, loss 1.660111665725708
iteration 400, loss 1.6829456090927124
iteration 500, loss 1.7058064937591553
iteration 600, loss 1.7063814401626587
iteration 700, loss 1.6152418851852417
iteration 800, loss 1.7331753969192505
iteration 0, loss 1.6844723224639893
iteration 100, loss 1.6895936727523804
iteration 200, loss 1.7280160188674927
iteration 300, loss 1.7088594436645508
iteration 400, loss 1.6796942949295044
iteration 500, loss 1.632917881011963
iteration 600, loss 1.7152814865112305
iteration 700, loss 1.6641383171081543
iteration 800, loss 1.6369831562042236
iteration 0, loss 1.6952391862869263
iteration 100, loss 1.715247631072998
iteration 200, loss 1.6805520057678223
iteration 300, loss 1.762400507926941
iteration 400, loss 1.7054080963134766
iteration 500, loss 1.6881084442138672
iteration 600, loss 1.7584607601165771
iteration 700, loss 1.7573256492614746
iteration 800, loss 1.697007417678833
iteration 0, loss 1.7300866842269897
iteration 100, loss 1.6478570699691772
iteration 200, loss 1.7069523334503174
iteration 300, loss 1.6949836015701294
iteration 400, loss 1.6612548828125
iteration 500, loss 1.7744081020355225
iteration 600, loss 1.680616021156311
iteration 700, loss 1.7171317338943481
iteration 800, loss 1.7063555717468262
iteration 0, loss 1.6619350910186768
iteration 100, loss 1.7246882915496826
iteration 200, loss 1.7246583700180054
iteration 300, loss 1.7310922145843506
iteration 400, loss 1.7087985277175903
iteration 500, loss 1.7335859537124634
iteration 600, loss 1.7245107889175415
iteration 700, loss 1.6989378929138184
iteration 800, loss 1.7220369577407837
iteration 0, loss 1.6616405248641968
iteration 100, loss 1.6450004577636719
iteration 200, loss 1.7459639310836792
iteration 300, loss 1.7156838178634644
iteration 400, loss 1.7567352056503296
iteration 500, loss 1.6640836000442505
iteration 600, loss 1.7239079475402832
iteration 700, loss 1.6663384437561035
iteration 800, loss 1.6701143980026245
iteration 0, loss 1.7040437459945679
iteration 100, loss 1.694334626197815
iteration 200, loss 1.6818691492080688
iteration 300, loss 1.7230950593948364
iteration 400, loss 1.7269612550735474
iteration 500, loss 1.6787128448486328
iteration 600, loss 1.692413330078125
iteration 700, loss 1.7515909671783447
iteration 800, loss 1.7192668914794922
iteration 0, loss 1.764793872833252
iteration 100, loss 1.717966914176941
iteration 200, loss 1.7792396545410156
iteration 300, loss 1.6559340953826904
iteration 400, loss 1.733979344367981
iteration 500, loss 1.7608869075775146
iteration 600, loss 1.6980717182159424
iteration 700, loss 1.658042311668396
iteration 800, loss 1.7934563159942627
iteration 0, loss 1.671417236328125
iteration 100, loss 1.745225429534912
iteration 200, loss 1.6414259672164917
iteration 300, loss 1.698499083518982
iteration 400, loss 1.6594769954681396
iteration 500, loss 1.7310056686401367
iteration 600, loss 1.6729700565338135
iteration 700, loss 1.6795804500579834
iteration 800, loss 1.6760938167572021
iteration 0, loss 1.6624069213867188
iteration 100, loss 1.7387815713882446
iteration 200, loss 1.6952627897262573
iteration 300, loss 1.733203411102295
iteration 400, loss 1.740073561668396
iteration 500, loss 1.7112854719161987
iteration 600, loss 1.7722536325454712
iteration 700, loss 1.7270258665084839
iteration 800, loss 1.709406852722168
iteration 0, loss 1.672126054763794
iteration 100, loss 1.7269392013549805
iteration 200, loss 1.6912810802459717
iteration 300, loss 1.7024569511413574
iteration 400, loss 1.6853705644607544
iteration 500, loss 1.6481437683105469
iteration 600, loss 1.7194656133651733
iteration 700, loss 1.7252955436706543
iteration 800, loss 1.6846792697906494
iteration 0, loss 1.7202788591384888
iteration 100, loss 1.759552240371704
iteration 200, loss 1.7412736415863037
iteration 300, loss 1.6884338855743408
iteration 400, loss 1.7036030292510986
iteration 500, loss 1.7219445705413818
iteration 600, loss 1.7510547637939453
iteration 700, loss 1.7248353958129883
iteration 800, loss 1.7390857934951782
iteration 0, loss 1.6800854206085205
iteration 100, loss 1.7218459844589233
iteration 200, loss 1.6419923305511475
iteration 300, loss 1.7114111185073853
iteration 400, loss 1.6786954402923584
iteration 500, loss 1.6958128213882446
iteration 600, loss 1.6972134113311768
iteration 700, loss 1.7126444578170776
iteration 800, loss 1.651190161705017
iteration 0, loss 1.741584062576294
iteration 100, loss 1.7458269596099854
iteration 200, loss 1.6641185283660889
iteration 300, loss 1.6844148635864258
iteration 400, loss 1.6851046085357666
iteration 500, loss 1.723071575164795
iteration 600, loss 1.6209025382995605
iteration 700, loss 1.7011263370513916
iteration 800, loss 1.6914594173431396
iteration 0, loss 1.7685350179672241
iteration 100, loss 1.6994513273239136
iteration 200, loss 1.6334792375564575
iteration 300, loss 1.704943299293518
iteration 400, loss 1.724108099937439
iteration 500, loss 1.7345314025878906
iteration 600, loss 1.691806674003601
iteration 700, loss 1.6893867254257202
iteration 800, loss 1.688575029373169
iteration 0, loss 1.6903032064437866
iteration 100, loss 1.6981405019760132
iteration 200, loss 1.7777600288391113
iteration 300, loss 1.6503421068191528
iteration 400, loss 1.6721410751342773
iteration 500, loss 1.7238168716430664
iteration 600, loss 1.684449553489685
iteration 700, loss 1.6814848184585571
iteration 800, loss 1.6984227895736694
iteration 0, loss 1.709150791168213
iteration 100, loss 1.712583065032959
iteration 200, loss 1.730107307434082
iteration 300, loss 1.7364089488983154
iteration 400, loss 1.6776360273361206
iteration 500, loss 1.6626633405685425
iteration 600, loss 1.7362934350967407
iteration 700, loss 1.6931219100952148
iteration 800, loss 1.7521092891693115
iteration 0, loss 1.655754566192627
iteration 100, loss 1.6917260885238647
iteration 200, loss 1.717974066734314
iteration 300, loss 1.6472442150115967
iteration 400, loss 1.6984236240386963
iteration 500, loss 1.6530842781066895
iteration 600, loss 1.6870173215866089
iteration 700, loss 1.7001433372497559
iteration 800, loss 1.7348932027816772
iteration 0, loss 1.7216808795928955
iteration 100, loss 1.648018717765808
iteration 200, loss 1.6583106517791748
iteration 300, loss 1.6661393642425537
iteration 400, loss 1.6710448265075684
iteration 500, loss 1.7321525812149048
iteration 600, loss 1.6999075412750244
iteration 700, loss 1.720961332321167
iteration 800, loss 1.681234359741211
iteration 0, loss 1.6812679767608643
iteration 100, loss 1.666736364364624
iteration 200, loss 1.691704273223877
iteration 300, loss 1.689780831336975
iteration 400, loss 1.6928796768188477
iteration 500, loss 1.6925954818725586
iteration 600, loss 1.717839241027832
iteration 700, loss 1.6794524192810059
iteration 800, loss 1.655426025390625
iteration 0, loss 1.7116860151290894
iteration 100, loss 1.7101091146469116
iteration 200, loss 1.7923213243484497
iteration 300, loss 1.6499944925308228
iteration 400, loss 1.6473673582077026
iteration 500, loss 1.710078239440918
iteration 600, loss 1.7044073343276978
iteration 700, loss 1.7187871932983398
iteration 800, loss 1.6870893239974976
iteration 0, loss 1.695553183555603
iteration 100, loss 1.7347655296325684
iteration 200, loss 1.766770362854004
iteration 300, loss 1.7615845203399658
iteration 400, loss 1.7298275232315063
iteration 500, loss 1.723070502281189
iteration 600, loss 1.6821235418319702
iteration 700, loss 1.67080819606781
iteration 800, loss 1.6615227460861206
iteration 0, loss 1.66971755027771
iteration 100, loss 1.7691503763198853
iteration 200, loss 1.6960200071334839
iteration 300, loss 1.7134120464324951
iteration 400, loss 1.678383231163025
iteration 500, loss 1.6273932456970215
iteration 600, loss 1.7232321500778198
iteration 700, loss 1.6824134588241577
iteration 800, loss 1.7577015161514282
iteration 0, loss 1.688002109527588
iteration 100, loss 1.6621108055114746
iteration 200, loss 1.6628855466842651
iteration 300, loss 1.7128455638885498
iteration 400, loss 1.6693381071090698
iteration 500, loss 1.687992811203003
iteration 600, loss 1.7097558975219727
iteration 700, loss 1.641313076019287
iteration 800, loss 1.6470361948013306
iteration 0, loss 1.598177194595337
iteration 100, loss 1.703604817390442
iteration 200, loss 1.739678144454956
iteration 300, loss 1.691965103149414
iteration 400, loss 1.687089443206787
iteration 500, loss 1.7170274257659912
iteration 600, loss 1.70878005027771
iteration 700, loss 1.7340492010116577
iteration 800, loss 1.7200372219085693
iteration 0, loss 1.6690878868103027
iteration 100, loss 1.7089855670928955
iteration 200, loss 1.6603482961654663
iteration 300, loss 1.707665205001831
iteration 400, loss 1.7250460386276245
iteration 500, loss 1.7433212995529175
iteration 600, loss 1.6822857856750488
iteration 700, loss 1.7432434558868408
iteration 800, loss 1.6799339056015015
iteration 0, loss 1.7572661638259888
iteration 100, loss 1.7073020935058594
iteration 200, loss 1.6863502264022827
iteration 300, loss 1.6800340414047241
iteration 400, loss 1.6763145923614502
iteration 500, loss 1.668756127357483
iteration 600, loss 1.7288589477539062
iteration 700, loss 1.6800849437713623
iteration 800, loss 1.6805716753005981
iteration 0, loss 1.7805442810058594
iteration 100, loss 1.7742565870285034
iteration 200, loss 1.7054438591003418
iteration 300, loss 1.6931742429733276
iteration 400, loss 1.6809452772140503
iteration 500, loss 1.668309211730957
iteration 600, loss 1.7110579013824463
iteration 700, loss 1.7242517471313477
iteration 800, loss 1.6982871294021606
iteration 0, loss 1.6654982566833496
iteration 100, loss 1.7010334730148315
iteration 200, loss 1.769901156425476
iteration 300, loss 1.6762903928756714
iteration 400, loss 1.7002958059310913
iteration 500, loss 1.6881740093231201
iteration 600, loss 1.69923734664917
iteration 700, loss 1.7340059280395508
iteration 800, loss 1.7430974245071411
iteration 0, loss 1.6231465339660645
iteration 100, loss 1.7009706497192383
iteration 200, loss 1.7183239459991455
iteration 300, loss 1.661502718925476
iteration 400, loss 1.7318090200424194
iteration 500, loss 1.7334904670715332
iteration 600, loss 1.6625447273254395
iteration 700, loss 1.6039685010910034
iteration 800, loss 1.694970965385437
iteration 0, loss 1.6891096830368042
iteration 100, loss 1.6739124059677124
iteration 200, loss 1.6241872310638428
iteration 300, loss 1.7587354183197021
iteration 400, loss 1.7142329216003418
iteration 500, loss 1.7559778690338135
iteration 600, loss 1.7082674503326416
iteration 700, loss 1.6903488636016846
iteration 800, loss 1.6862839460372925
fold 1 accuracy: 0.7101428571428572
iteration 0, loss 1.7236921787261963
iteration 100, loss 1.6608774662017822
iteration 200, loss 1.7118511199951172
iteration 300, loss 1.6890801191329956
iteration 400, loss 1.6689201593399048
iteration 500, loss 1.7094590663909912
iteration 600, loss 1.666885495185852
iteration 700, loss 1.76558256149292
iteration 800, loss 1.6196008920669556
iteration 0, loss 1.7692136764526367
iteration 100, loss 1.67719566822052
iteration 200, loss 1.7346832752227783
iteration 300, loss 1.7222425937652588
iteration 400, loss 1.7112360000610352
iteration 500, loss 1.6924302577972412
iteration 600, loss 1.6954163312911987
iteration 700, loss 1.6843962669372559
iteration 800, loss 1.7305970191955566
iteration 0, loss 1.6944671869277954
iteration 100, loss 1.7636213302612305
iteration 200, loss 1.6590737104415894
iteration 300, loss 1.6662797927856445
iteration 400, loss 1.767397403717041
iteration 500, loss 1.694546103477478
iteration 600, loss 1.7197951078414917
iteration 700, loss 1.6734135150909424
iteration 800, loss 1.7317478656768799
iteration 0, loss 1.685107946395874
iteration 100, loss 1.7278516292572021
iteration 200, loss 1.7557826042175293
iteration 300, loss 1.683485984802246
iteration 400, loss 1.663832426071167
iteration 500, loss 1.7403013706207275
iteration 600, loss 1.8150634765625
iteration 700, loss 1.6966971158981323
iteration 800, loss 1.7038055658340454
iteration 0, loss 1.7435259819030762
iteration 100, loss 1.7264248132705688
iteration 200, loss 1.7029263973236084
iteration 300, loss 1.6953970193862915
iteration 400, loss 1.6917909383773804
iteration 500, loss 1.7525403499603271
iteration 600, loss 1.6493040323257446
iteration 700, loss 1.7245519161224365
iteration 800, loss 1.6768357753753662
iteration 0, loss 1.7105894088745117
iteration 100, loss 1.6476588249206543
iteration 200, loss 1.7201178073883057
iteration 300, loss 1.664795994758606
iteration 400, loss 1.7123427391052246
iteration 500, loss 1.637563705444336
iteration 600, loss 1.6857701539993286
iteration 700, loss 1.6871858835220337
iteration 800, loss 1.718115210533142
iteration 0, loss 1.7159326076507568
iteration 100, loss 1.6687352657318115
iteration 200, loss 1.7233394384384155
iteration 300, loss 1.70003342628479
iteration 400, loss 1.735366940498352
iteration 500, loss 1.7215874195098877
iteration 600, loss 1.7399163246154785
iteration 700, loss 1.6895266771316528
iteration 800, loss 1.7046359777450562
iteration 0, loss 1.7014278173446655
iteration 100, loss 1.6656808853149414
iteration 200, loss 1.6813362836837769
iteration 300, loss 1.705127477645874
iteration 400, loss 1.6972413063049316
iteration 500, loss 1.7503381967544556
iteration 600, loss 1.7085734605789185
iteration 700, loss 1.6272838115692139
iteration 800, loss 1.6911349296569824
iteration 0, loss 1.7312966585159302
iteration 100, loss 1.6261106729507446
iteration 200, loss 1.7205671072006226
iteration 300, loss 1.7443851232528687
iteration 400, loss 1.683667778968811
iteration 500, loss 1.6810859441757202
iteration 600, loss 1.6747034788131714
iteration 700, loss 1.6726551055908203
iteration 800, loss 1.6796443462371826
iteration 0, loss 1.6809381246566772
iteration 100, loss 1.7454146146774292
iteration 200, loss 1.7020857334136963
iteration 300, loss 1.7230902910232544
iteration 400, loss 1.6972870826721191
iteration 500, loss 1.7228891849517822
iteration 600, loss 1.6876966953277588
iteration 700, loss 1.7473572492599487
iteration 800, loss 1.7603282928466797
iteration 0, loss 1.7033166885375977
iteration 100, loss 1.6567915678024292
iteration 200, loss 1.6690375804901123
iteration 300, loss 1.7326171398162842
iteration 400, loss 1.681921362876892
iteration 500, loss 1.630457878112793
iteration 600, loss 1.6633696556091309
iteration 700, loss 1.6732773780822754
iteration 800, loss 1.7005714178085327
iteration 0, loss 1.709439754486084
iteration 100, loss 1.6985559463500977
iteration 200, loss 1.7110927104949951
iteration 300, loss 1.7334904670715332
iteration 400, loss 1.706778645515442
iteration 500, loss 1.6660332679748535
iteration 600, loss 1.7286769151687622
iteration 700, loss 1.7283645868301392
iteration 800, loss 1.6708347797393799
iteration 0, loss 1.715529203414917
iteration 100, loss 1.6997820138931274
iteration 200, loss 1.729579210281372
iteration 300, loss 1.7299398183822632
iteration 400, loss 1.6906453371047974
iteration 500, loss 1.7150678634643555
iteration 600, loss 1.6840572357177734
iteration 700, loss 1.658463954925537
iteration 800, loss 1.731916069984436
iteration 0, loss 1.6487442255020142
iteration 100, loss 1.778151512145996
iteration 200, loss 1.6571049690246582
iteration 300, loss 1.7080059051513672
iteration 400, loss 1.679182767868042
iteration 500, loss 1.721808910369873
iteration 600, loss 1.6992361545562744
iteration 700, loss 1.6773934364318848
iteration 800, loss 1.7224243879318237
iteration 0, loss 1.6821205615997314
iteration 100, loss 1.6721625328063965
iteration 200, loss 1.746654987335205
iteration 300, loss 1.6992521286010742
iteration 400, loss 1.7023646831512451
iteration 500, loss 1.6404553651809692
iteration 600, loss 1.6405396461486816
iteration 700, loss 1.678331971168518
iteration 800, loss 1.6645458936691284
iteration 0, loss 1.7113852500915527
iteration 100, loss 1.6742267608642578
iteration 200, loss 1.707015037536621
iteration 300, loss 1.7939449548721313
iteration 400, loss 1.6761950254440308
iteration 500, loss 1.669527530670166
iteration 600, loss 1.6948823928833008
iteration 700, loss 1.6590745449066162
iteration 800, loss 1.7467615604400635
iteration 0, loss 1.703894853591919
iteration 100, loss 1.663962483406067
iteration 200, loss 1.7461603879928589
iteration 300, loss 1.7579245567321777
iteration 400, loss 1.696628451347351
iteration 500, loss 1.749618411064148
iteration 600, loss 1.8134979009628296
iteration 700, loss 1.691762924194336
iteration 800, loss 1.7378411293029785
iteration 0, loss 1.6668833494186401
iteration 100, loss 1.7137982845306396
iteration 200, loss 1.6379326581954956
iteration 300, loss 1.6502705812454224
iteration 400, loss 1.6786270141601562
iteration 500, loss 1.702251672744751
iteration 600, loss 1.708805799484253
iteration 700, loss 1.660979151725769
iteration 800, loss 1.6843183040618896
iteration 0, loss 1.7214425802230835
iteration 100, loss 1.67194402217865
iteration 200, loss 1.6356737613677979
iteration 300, loss 1.7838983535766602
iteration 400, loss 1.673789143562317
iteration 500, loss 1.7638036012649536
iteration 600, loss 1.7375152111053467
iteration 700, loss 1.7111849784851074
iteration 800, loss 1.6827261447906494
iteration 0, loss 1.656599998474121
iteration 100, loss 1.686407446861267
iteration 200, loss 1.6937347650527954
iteration 300, loss 1.717796802520752
iteration 400, loss 1.6455743312835693
iteration 500, loss 1.674827218055725
iteration 600, loss 1.6651078462600708
iteration 700, loss 1.6483550071716309
iteration 800, loss 1.7157295942306519
iteration 0, loss 1.7255359888076782
iteration 100, loss 1.6891627311706543
iteration 200, loss 1.7275317907333374
iteration 300, loss 1.726418137550354
iteration 400, loss 1.67047917842865
iteration 500, loss 1.745391607284546
iteration 600, loss 1.6899617910385132
iteration 700, loss 1.702232003211975
iteration 800, loss 1.6778805255889893
iteration 0, loss 1.6450107097625732
iteration 100, loss 1.6922069787979126
iteration 200, loss 1.7617967128753662
iteration 300, loss 1.7358620166778564
iteration 400, loss 1.6939584016799927
iteration 500, loss 1.647634506225586
iteration 600, loss 1.7769635915756226
iteration 700, loss 1.6935532093048096
iteration 800, loss 1.6690939664840698
iteration 0, loss 1.7456490993499756
iteration 100, loss 1.747941017150879
iteration 200, loss 1.6423537731170654
iteration 300, loss 1.7094796895980835
iteration 400, loss 1.7085566520690918
iteration 500, loss 1.6767432689666748
iteration 600, loss 1.7120898962020874
iteration 700, loss 1.6878951787948608
iteration 800, loss 1.7010481357574463
iteration 0, loss 1.6827802658081055
iteration 100, loss 1.6894477605819702
iteration 200, loss 1.7141048908233643
iteration 300, loss 1.7344094514846802
iteration 400, loss 1.7038427591323853
iteration 500, loss 1.7011115550994873
iteration 600, loss 1.7608274221420288
iteration 700, loss 1.7543630599975586
iteration 800, loss 1.6725614070892334
iteration 0, loss 1.7229208946228027
iteration 100, loss 1.6849266290664673
iteration 200, loss 1.6888587474822998
iteration 300, loss 1.6989715099334717
iteration 400, loss 1.6978713274002075
iteration 500, loss 1.7559599876403809
iteration 600, loss 1.6466624736785889
iteration 700, loss 1.6205413341522217
iteration 800, loss 1.6669437885284424
iteration 0, loss 1.7291256189346313
iteration 100, loss 1.6801856756210327
iteration 200, loss 1.769661545753479
iteration 300, loss 1.7066491842269897
iteration 400, loss 1.6809860467910767
iteration 500, loss 1.6908329725265503
iteration 600, loss 1.7341399192810059
iteration 700, loss 1.6759451627731323
iteration 800, loss 1.706416130065918
iteration 0, loss 1.68184232711792
iteration 100, loss 1.6982734203338623
iteration 200, loss 1.7322313785552979
iteration 300, loss 1.6951861381530762
iteration 400, loss 1.6410611867904663
iteration 500, loss 1.728994369506836
iteration 600, loss 1.7190662622451782
iteration 700, loss 1.6759363412857056
iteration 800, loss 1.6670945882797241
iteration 0, loss 1.6630160808563232
iteration 100, loss 1.7599178552627563
iteration 200, loss 1.6842401027679443
iteration 300, loss 1.740912914276123
iteration 400, loss 1.6779346466064453
iteration 500, loss 1.8032267093658447
iteration 600, loss 1.6772874593734741
iteration 700, loss 1.702781319618225
iteration 800, loss 1.6813331842422485
iteration 0, loss 1.679570198059082
iteration 100, loss 1.6846731901168823
iteration 200, loss 1.719931721687317
iteration 300, loss 1.6984012126922607
iteration 400, loss 1.7604217529296875
iteration 500, loss 1.650437831878662
iteration 600, loss 1.6864140033721924
iteration 700, loss 1.7539631128311157
iteration 800, loss 1.6492526531219482
iteration 0, loss 1.6548479795455933
iteration 100, loss 1.6519083976745605
iteration 200, loss 1.7137645483016968
iteration 300, loss 1.7230873107910156
iteration 400, loss 1.720550775527954
iteration 500, loss 1.6821447610855103
iteration 600, loss 1.6386512517929077
iteration 700, loss 1.6963884830474854
iteration 800, loss 1.6880125999450684
iteration 0, loss 1.7246686220169067
iteration 100, loss 1.6700423955917358
iteration 200, loss 1.7194393873214722
iteration 300, loss 1.7287758588790894
iteration 400, loss 1.603751540184021
iteration 500, loss 1.6927160024642944
iteration 600, loss 1.7401721477508545
iteration 700, loss 1.7426780462265015
iteration 800, loss 1.7071670293807983
iteration 0, loss 1.7380846738815308
iteration 100, loss 1.6707173585891724
iteration 200, loss 1.655017375946045
iteration 300, loss 1.6242954730987549
iteration 400, loss 1.6607147455215454
iteration 500, loss 1.7095719575881958
iteration 600, loss 1.6381253004074097
iteration 700, loss 1.7585959434509277
iteration 800, loss 1.7085825204849243
iteration 0, loss 1.7241482734680176
iteration 100, loss 1.6952723264694214
iteration 200, loss 1.691813349723816
iteration 300, loss 1.6168688535690308
iteration 400, loss 1.6772221326828003
iteration 500, loss 1.6888511180877686
iteration 600, loss 1.7079408168792725
iteration 700, loss 1.644117832183838
iteration 800, loss 1.7108205556869507
iteration 0, loss 1.682065725326538
iteration 100, loss 1.6849110126495361
iteration 200, loss 1.6676502227783203
iteration 300, loss 1.7189947366714478
iteration 400, loss 1.6824816465377808
iteration 500, loss 1.7500264644622803
iteration 600, loss 1.7152057886123657
iteration 700, loss 1.7546300888061523
iteration 800, loss 1.725547432899475
iteration 0, loss 1.7532882690429688
iteration 100, loss 1.6772606372833252
iteration 200, loss 1.6751686334609985
iteration 300, loss 1.6712989807128906
iteration 400, loss 1.6456043720245361
iteration 500, loss 1.6975507736206055
iteration 600, loss 1.6886098384857178
iteration 700, loss 1.6609405279159546
iteration 800, loss 1.7134379148483276
iteration 0, loss 1.7003728151321411
iteration 100, loss 1.7100474834442139
iteration 200, loss 1.6972190141677856
iteration 300, loss 1.6912765502929688
iteration 400, loss 1.7333499193191528
iteration 500, loss 1.6876695156097412
iteration 600, loss 1.6692688465118408
iteration 700, loss 1.7627218961715698
iteration 800, loss 1.7025065422058105
iteration 0, loss 1.7017194032669067
iteration 100, loss 1.699944257736206
iteration 200, loss 1.7066365480422974
iteration 300, loss 1.7677427530288696
iteration 400, loss 1.7618181705474854
iteration 500, loss 1.6654975414276123
iteration 600, loss 1.7313487529754639
iteration 700, loss 1.7369359731674194
iteration 800, loss 1.6327780485153198
iteration 0, loss 1.6980512142181396
iteration 100, loss 1.7295533418655396
iteration 200, loss 1.6995985507965088
iteration 300, loss 1.6967442035675049
iteration 400, loss 1.6749727725982666
iteration 500, loss 1.6629159450531006
iteration 600, loss 1.710152268409729
iteration 700, loss 1.6648015975952148
iteration 800, loss 1.7178981304168701
iteration 0, loss 1.684644341468811
iteration 100, loss 1.7130759954452515
iteration 200, loss 1.7612844705581665
iteration 300, loss 1.630937099456787
iteration 400, loss 1.6742212772369385
iteration 500, loss 1.7895326614379883
iteration 600, loss 1.707391381263733
iteration 700, loss 1.7096573114395142
iteration 800, loss 1.683812141418457
iteration 0, loss 1.6627751588821411
iteration 100, loss 1.6744641065597534
iteration 200, loss 1.6538710594177246
iteration 300, loss 1.7256451845169067
iteration 400, loss 1.6972473859786987
iteration 500, loss 1.7193397283554077
iteration 600, loss 1.654598355293274
iteration 700, loss 1.6965264081954956
iteration 800, loss 1.6861557960510254
iteration 0, loss 1.693951964378357
iteration 100, loss 1.6718724966049194
iteration 200, loss 1.7011504173278809
iteration 300, loss 1.7304905652999878
iteration 400, loss 1.6956182718276978
iteration 500, loss 1.6669483184814453
iteration 600, loss 1.7003509998321533
iteration 700, loss 1.6888123750686646
iteration 800, loss 1.730013370513916
iteration 0, loss 1.6879205703735352
iteration 100, loss 1.6660220623016357
iteration 200, loss 1.7509920597076416
iteration 300, loss 1.6111977100372314
iteration 400, loss 1.712254524230957
iteration 500, loss 1.7424262762069702
iteration 600, loss 1.7357394695281982
iteration 700, loss 1.7192893028259277
iteration 800, loss 1.719207525253296
iteration 0, loss 1.6511530876159668
iteration 100, loss 1.7585809230804443
iteration 200, loss 1.6230273246765137
iteration 300, loss 1.6947604417800903
iteration 400, loss 1.71243417263031
iteration 500, loss 1.7144923210144043
iteration 600, loss 1.7053241729736328
iteration 700, loss 1.626523494720459
iteration 800, loss 1.6529418230056763
iteration 0, loss 1.6764353513717651
iteration 100, loss 1.7037765979766846
iteration 200, loss 1.6001771688461304
iteration 300, loss 1.7382690906524658
iteration 400, loss 1.7533727884292603
iteration 500, loss 1.6361916065216064
iteration 600, loss 1.6341787576675415
iteration 700, loss 1.7257765531539917
iteration 800, loss 1.7076005935668945
iteration 0, loss 1.7028743028640747
iteration 100, loss 1.7128127813339233
iteration 200, loss 1.7207151651382446
iteration 300, loss 1.6369379758834839
iteration 400, loss 1.6875468492507935
iteration 500, loss 1.7744135856628418
iteration 600, loss 1.73129403591156
iteration 700, loss 1.688403606414795
iteration 800, loss 1.7677935361862183
iteration 0, loss 1.6900891065597534
iteration 100, loss 1.6646335124969482
iteration 200, loss 1.7148102521896362
iteration 300, loss 1.6467005014419556
iteration 400, loss 1.6883141994476318
iteration 500, loss 1.7304604053497314
iteration 600, loss 1.6686309576034546
iteration 700, loss 1.7279456853866577
iteration 800, loss 1.655959129333496
iteration 0, loss 1.6765000820159912
iteration 100, loss 1.6990108489990234
iteration 200, loss 1.7549903392791748
iteration 300, loss 1.6667734384536743
iteration 400, loss 1.6523199081420898
iteration 500, loss 1.6832009553909302
iteration 600, loss 1.7095341682434082
iteration 700, loss 1.6849151849746704
iteration 800, loss 1.702848196029663
iteration 0, loss 1.7400118112564087
iteration 100, loss 1.7346001863479614
iteration 200, loss 1.7605538368225098
iteration 300, loss 1.6913747787475586
iteration 400, loss 1.7232002019882202
iteration 500, loss 1.6694499254226685
iteration 600, loss 1.7065895795822144
iteration 700, loss 1.7637805938720703
iteration 800, loss 1.7391518354415894
iteration 0, loss 1.6821048259735107
iteration 100, loss 1.7126884460449219
iteration 200, loss 1.7184956073760986
iteration 300, loss 1.7192469835281372
iteration 400, loss 1.7179107666015625
iteration 500, loss 1.7606300115585327
iteration 600, loss 1.6690073013305664
iteration 700, loss 1.6332929134368896
iteration 800, loss 1.6290302276611328
iteration 0, loss 1.6185399293899536
iteration 100, loss 1.713468313217163
iteration 200, loss 1.7511261701583862
iteration 300, loss 1.6944218873977661
iteration 400, loss 1.6594306230545044
iteration 500, loss 1.7761600017547607
iteration 600, loss 1.6828460693359375
iteration 700, loss 1.6870518922805786
iteration 800, loss 1.649954915046692
fold 2 accuracy: 0.7108571428571429
iteration 0, loss 1.7413980960845947
iteration 100, loss 1.6817210912704468
iteration 200, loss 1.7456209659576416
iteration 300, loss 1.6886118650436401
iteration 400, loss 1.6909995079040527
iteration 500, loss 1.6890199184417725
iteration 600, loss 1.7608850002288818
iteration 700, loss 1.6865814924240112
iteration 800, loss 1.6500182151794434
iteration 0, loss 1.6762025356292725
iteration 100, loss 1.6364151239395142
iteration 200, loss 1.7075977325439453
iteration 300, loss 1.7140361070632935
iteration 400, loss 1.6389036178588867
iteration 500, loss 1.7175081968307495
iteration 600, loss 1.7546367645263672
iteration 700, loss 1.6638318300247192
iteration 800, loss 1.6305195093154907
iteration 0, loss 1.692610740661621
iteration 100, loss 1.7020182609558105
iteration 200, loss 1.7230684757232666
iteration 300, loss 1.665382981300354
iteration 400, loss 1.7030179500579834
iteration 500, loss 1.650773525238037
iteration 600, loss 1.6401286125183105
iteration 700, loss 1.7059887647628784
iteration 800, loss 1.6853644847869873
iteration 0, loss 1.6744903326034546
iteration 100, loss 1.6722792387008667
iteration 200, loss 1.7089728116989136
iteration 300, loss 1.6524436473846436
iteration 400, loss 1.685620665550232
iteration 500, loss 1.7393286228179932
iteration 600, loss 1.7061271667480469
iteration 700, loss 1.672585368156433
iteration 800, loss 1.6935571432113647
iteration 0, loss 1.7422014474868774
iteration 100, loss 1.712897539138794
iteration 200, loss 1.7108663320541382
iteration 300, loss 1.6854524612426758
iteration 400, loss 1.7107585668563843
iteration 500, loss 1.733467936515808
iteration 600, loss 1.6812275648117065
iteration 700, loss 1.7164450883865356
iteration 800, loss 1.7300602197647095
iteration 0, loss 1.7438273429870605
iteration 100, loss 1.653650164604187
iteration 200, loss 1.697352647781372
iteration 300, loss 1.6654425859451294
iteration 400, loss 1.6549029350280762
iteration 500, loss 1.6699280738830566
iteration 600, loss 1.6990115642547607
iteration 700, loss 1.7173500061035156
iteration 800, loss 1.6646978855133057
iteration 0, loss 1.7049516439437866
iteration 100, loss 1.7002403736114502
iteration 200, loss 1.6281898021697998
iteration 300, loss 1.708990454673767
iteration 400, loss 1.6231489181518555
iteration 500, loss 1.6299967765808105
iteration 600, loss 1.680456519126892
iteration 700, loss 1.6728078126907349
iteration 800, loss 1.7785563468933105
iteration 0, loss 1.6896212100982666
iteration 100, loss 1.720943808555603
iteration 200, loss 1.6721467971801758
iteration 300, loss 1.6954982280731201
iteration 400, loss 1.7315764427185059
iteration 500, loss 1.6534063816070557
iteration 600, loss 1.631409764289856
iteration 700, loss 1.6612074375152588
iteration 800, loss 1.6408271789550781
iteration 0, loss 1.7020387649536133
iteration 100, loss 1.7053637504577637
iteration 200, loss 1.6505776643753052
iteration 300, loss 1.7395943403244019
iteration 400, loss 1.6926921606063843
iteration 500, loss 1.6694210767745972
iteration 600, loss 1.701432704925537
iteration 700, loss 1.6894009113311768
iteration 800, loss 1.6036971807479858
iteration 0, loss 1.7106999158859253
iteration 100, loss 1.6772528886795044
iteration 200, loss 1.6606553792953491
iteration 300, loss 1.6134846210479736
iteration 400, loss 1.6503345966339111
iteration 500, loss 1.6807628870010376
iteration 600, loss 1.6669005155563354
iteration 700, loss 1.7023504972457886
iteration 800, loss 1.7276850938796997
iteration 0, loss 1.6572868824005127
iteration 100, loss 1.6679459810256958
iteration 200, loss 1.7031540870666504
iteration 300, loss 1.731203556060791
iteration 400, loss 1.6968878507614136
iteration 500, loss 1.7394622564315796
iteration 600, loss 1.6618915796279907
iteration 700, loss 1.690515398979187
iteration 800, loss 1.6835687160491943
iteration 0, loss 1.638166904449463
iteration 100, loss 1.7214924097061157
iteration 200, loss 1.6893317699432373
iteration 300, loss 1.7207107543945312
iteration 400, loss 1.719470500946045
iteration 500, loss 1.6581288576126099
iteration 600, loss 1.64285409450531
iteration 700, loss 1.7322496175765991
iteration 800, loss 1.770864486694336
iteration 0, loss 1.7346620559692383
iteration 100, loss 1.7316575050354004
iteration 200, loss 1.7034364938735962
iteration 300, loss 1.7083147764205933
iteration 400, loss 1.7239891290664673
iteration 500, loss 1.6574028730392456
iteration 600, loss 1.7031960487365723
iteration 700, loss 1.744655728340149
iteration 800, loss 1.6891319751739502
iteration 0, loss 1.791022539138794
iteration 100, loss 1.686606764793396
iteration 200, loss 1.638929843902588
iteration 300, loss 1.647809386253357
iteration 400, loss 1.7129262685775757
iteration 500, loss 1.7411367893218994
iteration 600, loss 1.6448767185211182
iteration 700, loss 1.6779687404632568
iteration 800, loss 1.7087056636810303
iteration 0, loss 1.7240080833435059
iteration 100, loss 1.7141406536102295
iteration 200, loss 1.6936609745025635
iteration 300, loss 1.6909534931182861
iteration 400, loss 1.6654409170150757
iteration 500, loss 1.638406753540039
iteration 600, loss 1.7409155368804932
iteration 700, loss 1.8092173337936401
iteration 800, loss 1.6815897226333618
iteration 0, loss 1.7530162334442139
iteration 100, loss 1.6958352327346802
iteration 200, loss 1.6910759210586548
iteration 300, loss 1.7355111837387085
iteration 400, loss 1.7333619594573975
iteration 500, loss 1.6986534595489502
iteration 600, loss 1.7417879104614258
iteration 700, loss 1.7078341245651245
iteration 800, loss 1.625150203704834
iteration 0, loss 1.6823556423187256
iteration 100, loss 1.6280772686004639
iteration 200, loss 1.7332454919815063
iteration 300, loss 1.6907174587249756
iteration 400, loss 1.6579445600509644
iteration 500, loss 1.6260682344436646
iteration 600, loss 1.7119293212890625
iteration 700, loss 1.688448190689087
iteration 800, loss 1.7215746641159058
iteration 0, loss 1.6404615640640259
iteration 100, loss 1.7328218221664429
iteration 200, loss 1.6990091800689697
iteration 300, loss 1.715606927871704
iteration 400, loss 1.7141261100769043
iteration 500, loss 1.726793885231018
iteration 600, loss 1.6812843084335327
iteration 700, loss 1.7435965538024902
iteration 800, loss 1.6866698265075684
iteration 0, loss 1.7232201099395752
iteration 100, loss 1.7899010181427002
iteration 200, loss 1.7087702751159668
iteration 300, loss 1.6941466331481934
iteration 400, loss 1.7833918333053589
iteration 500, loss 1.7006949186325073
iteration 600, loss 1.7080029249191284
iteration 700, loss 1.6970134973526
iteration 800, loss 1.6950228214263916
iteration 0, loss 1.798237919807434
iteration 100, loss 1.7068530321121216
iteration 200, loss 1.7257814407348633
iteration 300, loss 1.7363663911819458
iteration 400, loss 1.7702369689941406
iteration 500, loss 1.6690690517425537
iteration 600, loss 1.7423510551452637
iteration 700, loss 1.6794788837432861
iteration 800, loss 1.6942391395568848
iteration 0, loss 1.7299294471740723
iteration 100, loss 1.7486975193023682
iteration 200, loss 1.669906497001648
iteration 300, loss 1.7007780075073242
iteration 400, loss 1.6069483757019043
iteration 500, loss 1.7133394479751587
iteration 600, loss 1.6727643013000488
iteration 700, loss 1.6836539506912231
iteration 800, loss 1.7286746501922607
iteration 0, loss 1.7696712017059326
iteration 100, loss 1.6663081645965576
iteration 200, loss 1.700234055519104
iteration 300, loss 1.73683500289917
iteration 400, loss 1.6868529319763184
iteration 500, loss 1.607887864112854
iteration 600, loss 1.6761294603347778
iteration 700, loss 1.7331523895263672
iteration 800, loss 1.7529534101486206
iteration 0, loss 1.7122446298599243
iteration 100, loss 1.6494953632354736
iteration 200, loss 1.7380870580673218
iteration 300, loss 1.701014757156372
iteration 400, loss 1.7189788818359375
iteration 500, loss 1.7923113107681274
iteration 600, loss 1.6720699071884155
iteration 700, loss 1.700944423675537
iteration 800, loss 1.6603028774261475
iteration 0, loss 1.6517473459243774
iteration 100, loss 1.6658304929733276
iteration 200, loss 1.683304786682129
iteration 300, loss 1.7264655828475952
iteration 400, loss 1.752273440361023
iteration 500, loss 1.6963858604431152
iteration 600, loss 1.7278028726577759
iteration 700, loss 1.6680712699890137
iteration 800, loss 1.744288444519043
iteration 0, loss 1.6942806243896484
iteration 100, loss 1.6820391416549683
iteration 200, loss 1.6282731294631958
iteration 300, loss 1.6654026508331299
iteration 400, loss 1.7526808977127075
iteration 500, loss 1.6646082401275635
iteration 600, loss 1.7378413677215576
iteration 700, loss 1.720481038093567
iteration 800, loss 1.6658035516738892
iteration 0, loss 1.6726800203323364
iteration 100, loss 1.6500591039657593
iteration 200, loss 1.686953067779541
iteration 300, loss 1.7303838729858398
iteration 400, loss 1.6679342985153198
iteration 500, loss 1.6975631713867188
iteration 600, loss 1.7400912046432495
iteration 700, loss 1.6561875343322754
iteration 800, loss 1.6668674945831299
iteration 0, loss 1.6869641542434692
iteration 100, loss 1.680264949798584
iteration 200, loss 1.7208168506622314
iteration 300, loss 1.645298719406128
iteration 400, loss 1.7390556335449219
iteration 500, loss 1.708553671836853
iteration 600, loss 1.6817559003829956
iteration 700, loss 1.6910407543182373
iteration 800, loss 1.6793187856674194
iteration 0, loss 1.6783288717269897
iteration 100, loss 1.7105402946472168
iteration 200, loss 1.685445785522461
iteration 300, loss 1.6835390329360962
iteration 400, loss 1.6923097372055054
iteration 500, loss 1.66548752784729
iteration 600, loss 1.7484214305877686
iteration 700, loss 1.6875908374786377
iteration 800, loss 1.6532464027404785
iteration 0, loss 1.6816843748092651
iteration 100, loss 1.6960227489471436
iteration 200, loss 1.7458823919296265
iteration 300, loss 1.7055511474609375
iteration 400, loss 1.634907841682434
iteration 500, loss 1.7067008018493652
iteration 600, loss 1.709713101387024
iteration 700, loss 1.6268794536590576
iteration 800, loss 1.6742275953292847
iteration 0, loss 1.641160011291504
iteration 100, loss 1.7037997245788574
iteration 200, loss 1.6735605001449585
iteration 300, loss 1.665278673171997
iteration 400, loss 1.6839895248413086
iteration 500, loss 1.7312134504318237
iteration 600, loss 1.7326236963272095
iteration 700, loss 1.7283307313919067
iteration 800, loss 1.6104037761688232
iteration 0, loss 1.7386618852615356
iteration 100, loss 1.7259747982025146
iteration 200, loss 1.6771272420883179
iteration 300, loss 1.6278396844863892
iteration 400, loss 1.6780494451522827
iteration 500, loss 1.7393149137496948
iteration 600, loss 1.6937700510025024
iteration 700, loss 1.665525197982788
iteration 800, loss 1.6602866649627686
iteration 0, loss 1.6355468034744263
iteration 100, loss 1.6864893436431885
iteration 200, loss 1.6706498861312866
iteration 300, loss 1.6983851194381714
iteration 400, loss 1.6628408432006836
iteration 500, loss 1.7298990488052368
iteration 600, loss 1.6356650590896606
iteration 700, loss 1.669687271118164
iteration 800, loss 1.7025504112243652
iteration 0, loss 1.6690936088562012
iteration 100, loss 1.6423368453979492
iteration 200, loss 1.7170977592468262
iteration 300, loss 1.7056727409362793
iteration 400, loss 1.7525581121444702
iteration 500, loss 1.6539729833602905
iteration 600, loss 1.6716217994689941
iteration 700, loss 1.6890084743499756
iteration 800, loss 1.6784690618515015
iteration 0, loss 1.7458338737487793
iteration 100, loss 1.6769003868103027
iteration 200, loss 1.7381898164749146
iteration 300, loss 1.7376958131790161
iteration 400, loss 1.7050316333770752
iteration 500, loss 1.6815088987350464
iteration 600, loss 1.723902702331543
iteration 700, loss 1.7017788887023926
iteration 800, loss 1.6843664646148682
iteration 0, loss 1.6094446182250977
iteration 100, loss 1.6620413064956665
iteration 200, loss 1.6914421319961548
iteration 300, loss 1.687247395515442
iteration 400, loss 1.6878963708877563
iteration 500, loss 1.704535961151123
iteration 600, loss 1.7027894258499146
iteration 700, loss 1.6870062351226807
iteration 800, loss 1.6507428884506226
iteration 0, loss 1.7066925764083862
iteration 100, loss 1.6804261207580566
iteration 200, loss 1.7065781354904175
iteration 300, loss 1.6614727973937988
iteration 400, loss 1.75627863407135
iteration 500, loss 1.7389189004898071
iteration 600, loss 1.6613426208496094
iteration 700, loss 1.6902140378952026
iteration 800, loss 1.6530086994171143
iteration 0, loss 1.6658592224121094
iteration 100, loss 1.6699765920639038
iteration 200, loss 1.6464473009109497
iteration 300, loss 1.721238136291504
iteration 400, loss 1.69887113571167
iteration 500, loss 1.7391029596328735
iteration 600, loss 1.6922816038131714
iteration 700, loss 1.704656958580017
iteration 800, loss 1.6804468631744385
iteration 0, loss 1.6656290292739868
iteration 100, loss 1.6744260787963867
iteration 200, loss 1.6490856409072876
iteration 300, loss 1.6758739948272705
iteration 400, loss 1.6766420602798462
iteration 500, loss 1.71067476272583
iteration 600, loss 1.6851511001586914
iteration 700, loss 1.6867072582244873
iteration 800, loss 1.699223518371582
iteration 0, loss 1.6667749881744385
iteration 100, loss 1.7007997035980225
iteration 200, loss 1.71229887008667
iteration 300, loss 1.7096322774887085
iteration 400, loss 1.6703628301620483
iteration 500, loss 1.691597580909729
iteration 600, loss 1.7599847316741943
iteration 700, loss 1.5918700695037842
iteration 800, loss 1.7158427238464355
iteration 0, loss 1.7085342407226562
iteration 100, loss 1.7732042074203491
iteration 200, loss 1.7097066640853882
iteration 300, loss 1.655012845993042
iteration 400, loss 1.7283399105072021
iteration 500, loss 1.6930822134017944
iteration 600, loss 1.7002284526824951
iteration 700, loss 1.6657408475875854
iteration 800, loss 1.6811128854751587
iteration 0, loss 1.6810863018035889
iteration 100, loss 1.7465311288833618
iteration 200, loss 1.6650701761245728
iteration 300, loss 1.737729787826538
iteration 400, loss 1.6978315114974976
iteration 500, loss 1.6368589401245117
iteration 600, loss 1.6895617246627808
iteration 700, loss 1.6528855562210083
iteration 800, loss 1.741938591003418
iteration 0, loss 1.68881356716156
iteration 100, loss 1.6174404621124268
iteration 200, loss 1.702541708946228
iteration 300, loss 1.6660902500152588
iteration 400, loss 1.685941457748413
iteration 500, loss 1.7024729251861572
iteration 600, loss 1.6987470388412476
iteration 700, loss 1.7396245002746582
iteration 800, loss 1.639768362045288
iteration 0, loss 1.700991153717041
iteration 100, loss 1.6941208839416504
iteration 200, loss 1.6820614337921143
iteration 300, loss 1.6960363388061523
iteration 400, loss 1.747077226638794
iteration 500, loss 1.7479180097579956
iteration 600, loss 1.6564210653305054
iteration 700, loss 1.721605658531189
iteration 800, loss 1.7034138441085815
iteration 0, loss 1.7185170650482178
iteration 100, loss 1.715636134147644
iteration 200, loss 1.7306054830551147
iteration 300, loss 1.6984771490097046
iteration 400, loss 1.7047171592712402
iteration 500, loss 1.6727060079574585
iteration 600, loss 1.6731998920440674
iteration 700, loss 1.672280192375183
iteration 800, loss 1.620941400527954
iteration 0, loss 1.7002872228622437
iteration 100, loss 1.6646242141723633
iteration 200, loss 1.75729238986969
iteration 300, loss 1.7009164094924927
iteration 400, loss 1.702551245689392
iteration 500, loss 1.7439875602722168
iteration 600, loss 1.698812484741211
iteration 700, loss 1.732074499130249
iteration 800, loss 1.6629101037979126
iteration 0, loss 1.7572005987167358
iteration 100, loss 1.7622385025024414
iteration 200, loss 1.6620055437088013
iteration 300, loss 1.6998262405395508
iteration 400, loss 1.7134709358215332
iteration 500, loss 1.6856560707092285
iteration 600, loss 1.6910749673843384
iteration 700, loss 1.647042155265808
iteration 800, loss 1.6779351234436035
iteration 0, loss 1.7056846618652344
iteration 100, loss 1.6858867406845093
iteration 200, loss 1.667862057685852
iteration 300, loss 1.718238115310669
iteration 400, loss 1.7715450525283813
iteration 500, loss 1.6612744331359863
iteration 600, loss 1.7186061143875122
iteration 700, loss 1.6763204336166382
iteration 800, loss 1.7115037441253662
iteration 0, loss 1.7199032306671143
iteration 100, loss 1.7482972145080566
iteration 200, loss 1.6719492673873901
iteration 300, loss 1.700494408607483
iteration 400, loss 1.7368160486221313
iteration 500, loss 1.6611263751983643
iteration 600, loss 1.684675693511963
iteration 700, loss 1.7114325761795044
iteration 800, loss 1.6555434465408325
iteration 0, loss 1.6746315956115723
iteration 100, loss 1.700183629989624
iteration 200, loss 1.6844669580459595
iteration 300, loss 1.6803492307662964
iteration 400, loss 1.677074909210205
iteration 500, loss 1.6174590587615967
iteration 600, loss 1.7417093515396118
iteration 700, loss 1.7369283437728882
iteration 800, loss 1.6642593145370483
iteration 0, loss 1.6519495248794556
iteration 100, loss 1.6832458972930908
iteration 200, loss 1.6715072393417358
iteration 300, loss 1.6589494943618774
iteration 400, loss 1.7098337411880493
iteration 500, loss 1.7085729837417603
iteration 600, loss 1.6510902643203735
iteration 700, loss 1.7878715991973877
iteration 800, loss 1.6690796613693237
fold 3 accuracy: 0.7233571428571428
iteration 0, loss 1.7453079223632812
iteration 100, loss 1.6342750787734985
iteration 200, loss 1.6935583353042603
iteration 300, loss 1.6693847179412842
iteration 400, loss 1.660739541053772
iteration 500, loss 1.7492542266845703
iteration 600, loss 1.7697895765304565
iteration 700, loss 1.72248375415802
iteration 800, loss 1.7155208587646484
iteration 0, loss 1.680806279182434
iteration 100, loss 1.6823394298553467
iteration 200, loss 1.6756482124328613
iteration 300, loss 1.686284065246582
iteration 400, loss 1.7342907190322876
iteration 500, loss 1.6744576692581177
iteration 600, loss 1.722155213356018
iteration 700, loss 1.6847559213638306
iteration 800, loss 1.7115663290023804
iteration 0, loss 1.700094223022461
iteration 100, loss 1.6574629545211792
iteration 200, loss 1.7210040092468262
iteration 300, loss 1.6061345338821411
iteration 400, loss 1.6984418630599976
iteration 500, loss 1.7158132791519165
iteration 600, loss 1.6951369047164917
iteration 700, loss 1.6659828424453735
iteration 800, loss 1.7080703973770142
iteration 0, loss 1.7403219938278198
iteration 100, loss 1.72530996799469
iteration 200, loss 1.7264009714126587
iteration 300, loss 1.6872717142105103
iteration 400, loss 1.684875249862671
iteration 500, loss 1.62345290184021
iteration 600, loss 1.6112436056137085
iteration 700, loss 1.6860021352767944
iteration 800, loss 1.6964764595031738
iteration 0, loss 1.661781907081604
iteration 100, loss 1.653846263885498
iteration 200, loss 1.6567378044128418
iteration 300, loss 1.6836663484573364
iteration 400, loss 1.7415087223052979
iteration 500, loss 1.656550645828247
iteration 600, loss 1.6758465766906738
iteration 700, loss 1.701008915901184
iteration 800, loss 1.7231159210205078
iteration 0, loss 1.7117464542388916
iteration 100, loss 1.7421391010284424
iteration 200, loss 1.6902722120285034
iteration 300, loss 1.655293345451355
iteration 400, loss 1.6733280420303345
iteration 500, loss 1.6364574432373047
iteration 600, loss 1.6972274780273438
iteration 700, loss 1.7041981220245361
iteration 800, loss 1.599392294883728
iteration 0, loss 1.6956772804260254
iteration 100, loss 1.7402808666229248
iteration 200, loss 1.6525697708129883
iteration 300, loss 1.7220040559768677
iteration 400, loss 1.64676833152771
iteration 500, loss 1.685517430305481
iteration 600, loss 1.6369117498397827
iteration 700, loss 1.7156593799591064
iteration 800, loss 1.649509310722351
iteration 0, loss 1.7908766269683838
iteration 100, loss 1.7149842977523804
iteration 200, loss 1.6296614408493042
iteration 300, loss 1.7154258489608765
iteration 400, loss 1.7005747556686401
iteration 500, loss 1.6870018243789673
iteration 600, loss 1.7292958498001099
iteration 700, loss 1.7296993732452393
iteration 800, loss 1.7210127115249634
iteration 0, loss 1.6856237649917603
iteration 100, loss 1.7575322389602661
iteration 200, loss 1.624670147895813
iteration 300, loss 1.7229796648025513
iteration 400, loss 1.690973162651062
iteration 500, loss 1.709872841835022
iteration 600, loss 1.655288815498352
iteration 700, loss 1.7260100841522217
iteration 800, loss 1.7175285816192627
iteration 0, loss 1.6649866104125977
iteration 100, loss 1.6899292469024658
iteration 200, loss 1.727461338043213
iteration 300, loss 1.6743305921554565
iteration 400, loss 1.6589254140853882
iteration 500, loss 1.7318775653839111
iteration 600, loss 1.726885437965393
iteration 700, loss 1.7228296995162964
iteration 800, loss 1.7087894678115845
iteration 0, loss 1.6868138313293457
iteration 100, loss 1.8073441982269287
iteration 200, loss 1.6838102340698242
iteration 300, loss 1.66579008102417
iteration 400, loss 1.6501896381378174
iteration 500, loss 1.6396574974060059
iteration 600, loss 1.6952265501022339
iteration 700, loss 1.6627317667007446
iteration 800, loss 1.6568130254745483
iteration 0, loss 1.664111614227295
iteration 100, loss 1.7030664682388306
iteration 200, loss 1.7082723379135132
iteration 300, loss 1.6843966245651245
iteration 400, loss 1.7090752124786377
iteration 500, loss 1.694089412689209
iteration 600, loss 1.7252119779586792
iteration 700, loss 1.7783033847808838
iteration 800, loss 1.6312230825424194
iteration 0, loss 1.7113139629364014
iteration 100, loss 1.7113629579544067
iteration 200, loss 1.6272562742233276
iteration 300, loss 1.6833221912384033
iteration 400, loss 1.6663202047348022
iteration 500, loss 1.6986064910888672
iteration 600, loss 1.7442787885665894
iteration 700, loss 1.6536232233047485
iteration 800, loss 1.7431893348693848
iteration 0, loss 1.651095986366272
iteration 100, loss 1.7267112731933594
iteration 200, loss 1.6546905040740967
iteration 300, loss 1.681246280670166
iteration 400, loss 1.6962376832962036
iteration 500, loss 1.6885987520217896
iteration 600, loss 1.7056020498275757
iteration 700, loss 1.6994082927703857
iteration 800, loss 1.6781713962554932
iteration 0, loss 1.693607211112976
iteration 100, loss 1.6255806684494019
iteration 200, loss 1.6936789751052856
iteration 300, loss 1.742684245109558
iteration 400, loss 1.7196706533432007
iteration 500, loss 1.6422936916351318
iteration 600, loss 1.70262610912323
iteration 700, loss 1.7665627002716064
iteration 800, loss 1.7013689279556274
iteration 0, loss 1.71498703956604
iteration 100, loss 1.6603542566299438
iteration 200, loss 1.584723949432373
iteration 300, loss 1.7423471212387085
iteration 400, loss 1.7264282703399658
iteration 500, loss 1.7621792554855347
iteration 600, loss 1.6651813983917236
iteration 700, loss 1.706742763519287
iteration 800, loss 1.6935265064239502
iteration 0, loss 1.705753207206726
iteration 100, loss 1.7026095390319824
iteration 200, loss 1.6487228870391846
iteration 300, loss 1.6717947721481323
iteration 400, loss 1.690501093864441
iteration 500, loss 1.6973148584365845
iteration 600, loss 1.7009862661361694
iteration 700, loss 1.6559172868728638
iteration 800, loss 1.6355822086334229
iteration 0, loss 1.6661266088485718
iteration 100, loss 1.645121455192566
iteration 200, loss 1.655527114868164
iteration 300, loss 1.6811343431472778
iteration 400, loss 1.6678528785705566
iteration 500, loss 1.7097814083099365
iteration 600, loss 1.6960381269454956
iteration 700, loss 1.7339738607406616
iteration 800, loss 1.7315536737442017
iteration 0, loss 1.6836779117584229
iteration 100, loss 1.7463915348052979
iteration 200, loss 1.7602347135543823
iteration 300, loss 1.6678546667099
iteration 400, loss 1.7274866104125977
iteration 500, loss 1.7063438892364502
iteration 600, loss 1.6639453172683716
iteration 700, loss 1.7251965999603271
iteration 800, loss 1.732037901878357
iteration 0, loss 1.746107816696167
iteration 100, loss 1.7094812393188477
iteration 200, loss 1.6636062860488892
iteration 300, loss 1.679527997970581
iteration 400, loss 1.6719602346420288
iteration 500, loss 1.772039771080017
iteration 600, loss 1.6880505084991455
iteration 700, loss 1.6344400644302368
iteration 800, loss 1.7536468505859375
iteration 0, loss 1.7185133695602417
iteration 100, loss 1.6492336988449097
iteration 200, loss 1.6567437648773193
iteration 300, loss 1.7233619689941406
iteration 400, loss 1.7033487558364868
iteration 500, loss 1.7028270959854126
iteration 600, loss 1.7359834909439087
iteration 700, loss 1.6455854177474976
iteration 800, loss 1.7549651861190796
iteration 0, loss 1.6603556871414185
iteration 100, loss 1.697683334350586
iteration 200, loss 1.6420432329177856
iteration 300, loss 1.6522153615951538
iteration 400, loss 1.719788670539856
iteration 500, loss 1.718184232711792
iteration 600, loss 1.645436406135559
iteration 700, loss 1.658290982246399
iteration 800, loss 1.7081137895584106
iteration 0, loss 1.6625982522964478
iteration 100, loss 1.7403590679168701
iteration 200, loss 1.661431074142456
iteration 300, loss 1.706520438194275
iteration 400, loss 1.6897733211517334
iteration 500, loss 1.7181971073150635
iteration 600, loss 1.6485025882720947
iteration 700, loss 1.673555612564087
iteration 800, loss 1.6239787340164185
iteration 0, loss 1.673073172569275
iteration 100, loss 1.7462449073791504
iteration 200, loss 1.6482305526733398
iteration 300, loss 1.596473217010498
iteration 400, loss 1.703757882118225
iteration 500, loss 1.6201462745666504
iteration 600, loss 1.7419427633285522
iteration 700, loss 1.7423341274261475
iteration 800, loss 1.644261360168457
iteration 0, loss 1.687974452972412
iteration 100, loss 1.6714833974838257
iteration 200, loss 1.6008185148239136
iteration 300, loss 1.7710165977478027
iteration 400, loss 1.6918611526489258
iteration 500, loss 1.7731270790100098
iteration 600, loss 1.6062856912612915
iteration 700, loss 1.6331342458724976
iteration 800, loss 1.7191792726516724
iteration 0, loss 1.7205884456634521
iteration 100, loss 1.7721343040466309
iteration 200, loss 1.6700382232666016
iteration 300, loss 1.7058202028274536
iteration 400, loss 1.6876697540283203
iteration 500, loss 1.6533372402191162
iteration 600, loss 1.7618019580841064
iteration 700, loss 1.6493810415267944
iteration 800, loss 1.7386749982833862
iteration 0, loss 1.7282018661499023
iteration 100, loss 1.7438150644302368
iteration 200, loss 1.6475721597671509
iteration 300, loss 1.680147409439087
iteration 400, loss 1.670814871788025
iteration 500, loss 1.7045187950134277
iteration 600, loss 1.707528829574585
iteration 700, loss 1.689319133758545
iteration 800, loss 1.6970268487930298
iteration 0, loss 1.6615142822265625
iteration 100, loss 1.7044869661331177
iteration 200, loss 1.7285206317901611
iteration 300, loss 1.7140522003173828
iteration 400, loss 1.7480148077011108
iteration 500, loss 1.643143892288208
iteration 600, loss 1.7468067407608032
iteration 700, loss 1.659337043762207
iteration 800, loss 1.6895743608474731
iteration 0, loss 1.6492347717285156
iteration 100, loss 1.707131266593933
iteration 200, loss 1.6956332921981812
iteration 300, loss 1.6939951181411743
iteration 400, loss 1.7376434803009033
iteration 500, loss 1.721488118171692
iteration 600, loss 1.7270207405090332
iteration 700, loss 1.69752836227417
iteration 800, loss 1.6691398620605469
iteration 0, loss 1.7182786464691162
iteration 100, loss 1.6722164154052734
iteration 200, loss 1.6656494140625
iteration 300, loss 1.6449888944625854
iteration 400, loss 1.6183393001556396
iteration 500, loss 1.6785829067230225
iteration 600, loss 1.7168933153152466
iteration 700, loss 1.654771089553833
iteration 800, loss 1.6839172840118408
iteration 0, loss 1.688045620918274
iteration 100, loss 1.7300554513931274
iteration 200, loss 1.6801886558532715
iteration 300, loss 1.6539301872253418
iteration 400, loss 1.678480863571167
iteration 500, loss 1.6975401639938354
iteration 600, loss 1.67360520362854
iteration 700, loss 1.6833478212356567
iteration 800, loss 1.737954020500183
iteration 0, loss 1.7025939226150513
iteration 100, loss 1.6923036575317383
iteration 200, loss 1.6160736083984375
iteration 300, loss 1.6727486848831177
iteration 400, loss 1.7014671564102173
iteration 500, loss 1.65879487991333
iteration 600, loss 1.7248334884643555
iteration 700, loss 1.6541849374771118
iteration 800, loss 1.670236349105835
iteration 0, loss 1.6917364597320557
iteration 100, loss 1.6829802989959717
iteration 200, loss 1.7304599285125732
iteration 300, loss 1.7439210414886475
iteration 400, loss 1.6301319599151611
iteration 500, loss 1.7023924589157104
iteration 600, loss 1.6732399463653564
iteration 700, loss 1.7363947629928589
iteration 800, loss 1.682176947593689
iteration 0, loss 1.6670560836791992
iteration 100, loss 1.7207579612731934
iteration 200, loss 1.714778184890747
iteration 300, loss 1.7335270643234253
iteration 400, loss 1.7602068185806274
iteration 500, loss 1.6637446880340576
iteration 600, loss 1.6761362552642822
iteration 700, loss 1.7252771854400635
iteration 800, loss 1.704455852508545
iteration 0, loss 1.726080060005188
iteration 100, loss 1.6422284841537476
iteration 200, loss 1.69478178024292
iteration 300, loss 1.6952687501907349
iteration 400, loss 1.6573636531829834
iteration 500, loss 1.6727710962295532
iteration 600, loss 1.7141332626342773
iteration 700, loss 1.6722476482391357
iteration 800, loss 1.7063344717025757
iteration 0, loss 1.69279146194458
iteration 100, loss 1.7299373149871826
iteration 200, loss 1.7010992765426636
iteration 300, loss 1.6989984512329102
iteration 400, loss 1.6903831958770752
iteration 500, loss 1.7083083391189575
iteration 600, loss 1.6079763174057007
iteration 700, loss 1.6198564767837524
iteration 800, loss 1.6855332851409912
iteration 0, loss 1.770267128944397
iteration 100, loss 1.7194466590881348
iteration 200, loss 1.6530554294586182
iteration 300, loss 1.7388615608215332
iteration 400, loss 1.7198891639709473
iteration 500, loss 1.6369868516921997
iteration 600, loss 1.7618448734283447
iteration 700, loss 1.7026559114456177
iteration 800, loss 1.7009732723236084
iteration 0, loss 1.6443042755126953
iteration 100, loss 1.690773367881775
iteration 200, loss 1.7500991821289062
iteration 300, loss 1.6546103954315186
iteration 400, loss 1.6888281106948853
iteration 500, loss 1.6869704723358154
iteration 600, loss 1.7195736169815063
iteration 700, loss 1.6763015985488892
iteration 800, loss 1.776671290397644
iteration 0, loss 1.6378849744796753
iteration 100, loss 1.6376582384109497
iteration 200, loss 1.706295371055603
iteration 300, loss 1.6334904432296753
iteration 400, loss 1.703231692314148
iteration 500, loss 1.6890716552734375
iteration 600, loss 1.7685413360595703
iteration 700, loss 1.649041771888733
iteration 800, loss 1.7434450387954712
iteration 0, loss 1.659966230392456
iteration 100, loss 1.7196383476257324
iteration 200, loss 1.7142541408538818
iteration 300, loss 1.6445823907852173
iteration 400, loss 1.711201786994934
iteration 500, loss 1.7340662479400635
iteration 600, loss 1.7164400815963745
iteration 700, loss 1.7023859024047852
iteration 800, loss 1.6873908042907715
iteration 0, loss 1.7039763927459717
iteration 100, loss 1.694250464439392
iteration 200, loss 1.688139796257019
iteration 300, loss 1.7318848371505737
iteration 400, loss 1.7056653499603271
iteration 500, loss 1.693800449371338
iteration 600, loss 1.733188271522522
iteration 700, loss 1.6550278663635254
iteration 800, loss 1.6490511894226074
iteration 0, loss 1.671175241470337
iteration 100, loss 1.6709133386611938
iteration 200, loss 1.666978359222412
iteration 300, loss 1.6892436742782593
iteration 400, loss 1.7280148267745972
iteration 500, loss 1.671314001083374
iteration 600, loss 1.7116321325302124
iteration 700, loss 1.6094871759414673
iteration 800, loss 1.6722221374511719
iteration 0, loss 1.7787628173828125
iteration 100, loss 1.6954610347747803
iteration 200, loss 1.6341947317123413
iteration 300, loss 1.7536497116088867
iteration 400, loss 1.7240021228790283
iteration 500, loss 1.6936368942260742
iteration 600, loss 1.6782077550888062
iteration 700, loss 1.673710823059082
iteration 800, loss 1.6768498420715332
iteration 0, loss 1.6893861293792725
iteration 100, loss 1.659724235534668
iteration 200, loss 1.639430284500122
iteration 300, loss 1.7575597763061523
iteration 400, loss 1.6877416372299194
iteration 500, loss 1.6574223041534424
iteration 600, loss 1.6795827150344849
iteration 700, loss 1.6846318244934082
iteration 800, loss 1.7296990156173706
iteration 0, loss 1.7024482488632202
iteration 100, loss 1.7194551229476929
iteration 200, loss 1.6072217226028442
iteration 300, loss 1.7232491970062256
iteration 400, loss 1.7151049375534058
iteration 500, loss 1.6535712480545044
iteration 600, loss 1.667274832725525
iteration 700, loss 1.7061225175857544
iteration 800, loss 1.748944640159607
iteration 0, loss 1.7052229642868042
iteration 100, loss 1.6225570440292358
iteration 200, loss 1.682722568511963
iteration 300, loss 1.697426438331604
iteration 400, loss 1.6769064664840698
iteration 500, loss 1.694251537322998
iteration 600, loss 1.7150142192840576
iteration 700, loss 1.7060037851333618
iteration 800, loss 1.7308189868927002
iteration 0, loss 1.6381120681762695
iteration 100, loss 1.706398367881775
iteration 200, loss 1.7319341897964478
iteration 300, loss 1.7016844749450684
iteration 400, loss 1.6647868156433105
iteration 500, loss 1.6320240497589111
iteration 600, loss 1.686339259147644
iteration 700, loss 1.6925952434539795
iteration 800, loss 1.6857203245162964
iteration 0, loss 1.7172998189926147
iteration 100, loss 1.6980316638946533
iteration 200, loss 1.605953335762024
iteration 300, loss 1.694772481918335
iteration 400, loss 1.6183669567108154
iteration 500, loss 1.684858798980713
iteration 600, loss 1.7369741201400757
iteration 700, loss 1.6921151876449585
iteration 800, loss 1.6948325634002686
iteration 0, loss 1.765552282333374
iteration 100, loss 1.6962916851043701
iteration 200, loss 1.6441892385482788
iteration 300, loss 1.6732416152954102
iteration 400, loss 1.6772022247314453
iteration 500, loss 1.7467761039733887
iteration 600, loss 1.7076010704040527
iteration 700, loss 1.6915440559387207
iteration 800, loss 1.77064049243927
iteration 0, loss 1.7151448726654053
iteration 100, loss 1.7687338590621948
iteration 200, loss 1.7071471214294434
iteration 300, loss 1.739563226699829
iteration 400, loss 1.659205436706543
iteration 500, loss 1.6634023189544678
iteration 600, loss 1.6565850973129272
iteration 700, loss 1.7528635263442993
iteration 800, loss 1.683030128479004
fold 4 accuracy: 0.7208571428571429
[2024-02-29 02:58:33,643] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-29 02:58:33,645] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         654     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       840 MACs
fwd flops per GPU:                                                      1.68 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.68 K  
fwd latency:                                                            350.71 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    4.79 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '654'}
    MACs        - {'KronLayer': '840 MACs'}
    fwd latency - {'KronLayer': '350.71 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  654 = 100% Params, 840 MACs = 100% MACs, 350.71 us = 100% latency, 4.79 MFLOPS
  (kronlinear): KronLinear(654 = 100% Params, 840 MACs = 100% MACs, 258.68 us = 73.76% latency, 6.49 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 26.23 us = 7.48% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-29 02:58:33,647] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.68 K 654
iteration 0, loss 2.34470796585083
iteration 100, loss 2.3036413192749023
iteration 200, loss 2.120922565460205
iteration 300, loss 2.024259090423584
iteration 400, loss 1.9505563974380493
iteration 500, loss 1.846203327178955
iteration 600, loss 1.8780189752578735
iteration 700, loss 1.8095688819885254
iteration 800, loss 1.8219332695007324
iteration 0, loss 1.742075800895691
iteration 100, loss 1.8410979509353638
iteration 200, loss 1.784731388092041
iteration 300, loss 1.7583714723587036
iteration 400, loss 1.7399731874465942
iteration 500, loss 1.7526315450668335
iteration 600, loss 1.7648954391479492
iteration 700, loss 1.710156798362732
iteration 800, loss 1.6879494190216064
iteration 0, loss 1.7310447692871094
iteration 100, loss 1.7941339015960693
iteration 200, loss 1.7736791372299194
iteration 300, loss 1.690459966659546
iteration 400, loss 1.7782546281814575
iteration 500, loss 1.7325962781906128
iteration 600, loss 1.7077566385269165
iteration 700, loss 1.7878036499023438
iteration 800, loss 1.6708015203475952
iteration 0, loss 1.726277470588684
iteration 100, loss 1.77254056930542
iteration 200, loss 1.7513408660888672
iteration 300, loss 1.785812497138977
iteration 400, loss 1.6681766510009766
iteration 500, loss 1.7051734924316406
iteration 600, loss 1.6468603610992432
iteration 700, loss 1.7167993783950806
iteration 800, loss 1.7025810480117798
iteration 0, loss 1.7241790294647217
iteration 100, loss 1.7004835605621338
iteration 200, loss 1.6425443887710571
iteration 300, loss 1.6995384693145752
iteration 400, loss 1.6937146186828613
iteration 500, loss 1.6428056955337524
iteration 600, loss 1.700601577758789
iteration 700, loss 1.6848243474960327
iteration 800, loss 1.6584662199020386
iteration 0, loss 1.6893163919448853
iteration 100, loss 1.6883695125579834
iteration 200, loss 1.7146245241165161
iteration 300, loss 1.6884844303131104
iteration 400, loss 1.6719367504119873
iteration 500, loss 1.6337684392929077
iteration 600, loss 1.6855800151824951
iteration 700, loss 1.663967490196228
iteration 800, loss 1.6614595651626587
iteration 0, loss 1.6588850021362305
iteration 100, loss 1.7200485467910767
iteration 200, loss 1.6636743545532227
iteration 300, loss 1.66634202003479
iteration 400, loss 1.6686041355133057
iteration 500, loss 1.6792176961898804
iteration 600, loss 1.6737916469573975
iteration 700, loss 1.6600215435028076
iteration 800, loss 1.6882414817810059
iteration 0, loss 1.724542260169983
iteration 100, loss 1.6397055387496948
iteration 200, loss 1.6195464134216309
iteration 300, loss 1.6477794647216797
iteration 400, loss 1.7328752279281616
iteration 500, loss 1.6896380186080933
iteration 600, loss 1.6027381420135498
iteration 700, loss 1.6764516830444336
iteration 800, loss 1.7538050413131714
iteration 0, loss 1.663825511932373
iteration 100, loss 1.723765254020691
iteration 200, loss 1.6775729656219482
iteration 300, loss 1.6967475414276123
iteration 400, loss 1.68899405002594
iteration 500, loss 1.6523675918579102
iteration 600, loss 1.6761890649795532
iteration 700, loss 1.7200583219528198
iteration 800, loss 1.6281285285949707
iteration 0, loss 1.6354479789733887
iteration 100, loss 1.6518738269805908
iteration 200, loss 1.7223494052886963
iteration 300, loss 1.7018778324127197
iteration 400, loss 1.629603624343872
iteration 500, loss 1.6978293657302856
iteration 600, loss 1.5912961959838867
iteration 700, loss 1.7429503202438354
iteration 800, loss 1.6490391492843628
iteration 0, loss 1.6856447458267212
iteration 100, loss 1.6614445447921753
iteration 200, loss 1.680800199508667
iteration 300, loss 1.6310901641845703
iteration 400, loss 1.668619990348816
iteration 500, loss 1.6286450624465942
iteration 600, loss 1.7432949542999268
iteration 700, loss 1.6614716053009033
iteration 800, loss 1.6709967851638794
iteration 0, loss 1.6287128925323486
iteration 100, loss 1.6343189477920532
iteration 200, loss 1.641594648361206
iteration 300, loss 1.6383297443389893
iteration 400, loss 1.6364293098449707
iteration 500, loss 1.7418265342712402
iteration 600, loss 1.7485589981079102
iteration 700, loss 1.6959880590438843
iteration 800, loss 1.706539273262024
iteration 0, loss 1.6161311864852905
iteration 100, loss 1.6955766677856445
iteration 200, loss 1.612046241760254
iteration 300, loss 1.6980711221694946
iteration 400, loss 1.6434293985366821
iteration 500, loss 1.6554272174835205
iteration 600, loss 1.6176613569259644
iteration 700, loss 1.6537197828292847
iteration 800, loss 1.5834747552871704
iteration 0, loss 1.6609901189804077
iteration 100, loss 1.6482675075531006
iteration 200, loss 1.6682826280593872
iteration 300, loss 1.6379120349884033
iteration 400, loss 1.6847641468048096
iteration 500, loss 1.651698350906372
iteration 600, loss 1.6670958995819092
iteration 700, loss 1.6146670579910278
iteration 800, loss 1.7710834741592407
iteration 0, loss 1.6088454723358154
iteration 100, loss 1.7179774045944214
iteration 200, loss 1.5852632522583008
iteration 300, loss 1.6347756385803223
iteration 400, loss 1.6410062313079834
iteration 500, loss 1.6612646579742432
iteration 600, loss 1.6574492454528809
iteration 700, loss 1.6931989192962646
iteration 800, loss 1.652795433998108
iteration 0, loss 1.648197889328003
iteration 100, loss 1.6293532848358154
iteration 200, loss 1.6078556776046753
iteration 300, loss 1.6568913459777832
iteration 400, loss 1.6808487176895142
iteration 500, loss 1.7268749475479126
iteration 600, loss 1.6914970874786377
iteration 700, loss 1.6396573781967163
iteration 800, loss 1.6607500314712524
iteration 0, loss 1.6447951793670654
iteration 100, loss 1.6067535877227783
iteration 200, loss 1.61212956905365
iteration 300, loss 1.6198266744613647
iteration 400, loss 1.5792045593261719
iteration 500, loss 1.6172664165496826
iteration 600, loss 1.5858204364776611
iteration 700, loss 1.6308772563934326
iteration 800, loss 1.7037649154663086
iteration 0, loss 1.628833293914795
iteration 100, loss 1.6648951768875122
iteration 200, loss 1.6356909275054932
iteration 300, loss 1.6954960823059082
iteration 400, loss 1.6589235067367554
iteration 500, loss 1.6286907196044922
iteration 600, loss 1.582065463066101
iteration 700, loss 1.671882152557373
iteration 800, loss 1.6611497402191162
iteration 0, loss 1.6041368246078491
iteration 100, loss 1.639132022857666
iteration 200, loss 1.6456871032714844
iteration 300, loss 1.6989384889602661
iteration 400, loss 1.6517261266708374
iteration 500, loss 1.6514161825180054
iteration 600, loss 1.6335887908935547
iteration 700, loss 1.661157250404358
iteration 800, loss 1.6266248226165771
iteration 0, loss 1.6074233055114746
iteration 100, loss 1.6879334449768066
iteration 200, loss 1.6928389072418213
iteration 300, loss 1.5977104902267456
iteration 400, loss 1.6445446014404297
iteration 500, loss 1.6900608539581299
iteration 600, loss 1.624956727027893
iteration 700, loss 1.6455769538879395
iteration 800, loss 1.6130352020263672
iteration 0, loss 1.6880974769592285
iteration 100, loss 1.6723213195800781
iteration 200, loss 1.6053340435028076
iteration 300, loss 1.6204999685287476
iteration 400, loss 1.6548922061920166
iteration 500, loss 1.6296595335006714
iteration 600, loss 1.6512203216552734
iteration 700, loss 1.6026149988174438
iteration 800, loss 1.6804496049880981
iteration 0, loss 1.6409966945648193
iteration 100, loss 1.6409367322921753
iteration 200, loss 1.648855209350586
iteration 300, loss 1.6868427991867065
iteration 400, loss 1.6094416379928589
iteration 500, loss 1.6727956533432007
iteration 600, loss 1.7744555473327637
iteration 700, loss 1.6254196166992188
iteration 800, loss 1.659058690071106
iteration 0, loss 1.6594500541687012
iteration 100, loss 1.6305981874465942
iteration 200, loss 1.6128612756729126
iteration 300, loss 1.6120985746383667
iteration 400, loss 1.62332022190094
iteration 500, loss 1.709259033203125
iteration 600, loss 1.6481025218963623
iteration 700, loss 1.6367383003234863
iteration 800, loss 1.6724892854690552
iteration 0, loss 1.6546680927276611
iteration 100, loss 1.6552823781967163
iteration 200, loss 1.6142314672470093
iteration 300, loss 1.5766021013259888
iteration 400, loss 1.6482893228530884
iteration 500, loss 1.61414635181427
iteration 600, loss 1.5598727464675903
iteration 700, loss 1.6455098390579224
iteration 800, loss 1.662392497062683
iteration 0, loss 1.6237510442733765
iteration 100, loss 1.5866371393203735
iteration 200, loss 1.6986329555511475
iteration 300, loss 1.5975759029388428
iteration 400, loss 1.6351014375686646
iteration 500, loss 1.648310899734497
iteration 600, loss 1.6765732765197754
iteration 700, loss 1.6025803089141846
iteration 800, loss 1.5974971055984497
iteration 0, loss 1.680556058883667
iteration 100, loss 1.7046319246292114
iteration 200, loss 1.6341347694396973
iteration 300, loss 1.6408125162124634
iteration 400, loss 1.6253981590270996
iteration 500, loss 1.6370749473571777
iteration 600, loss 1.6675090789794922
iteration 700, loss 1.6563589572906494
iteration 800, loss 1.7242857217788696
iteration 0, loss 1.5815016031265259
iteration 100, loss 1.6047319173812866
iteration 200, loss 1.6389374732971191
iteration 300, loss 1.6764752864837646
iteration 400, loss 1.643957495689392
iteration 500, loss 1.6710366010665894
iteration 600, loss 1.6491727828979492
iteration 700, loss 1.624562382698059
iteration 800, loss 1.6521817445755005
iteration 0, loss 1.6890095472335815
iteration 100, loss 1.6793371438980103
iteration 200, loss 1.6494970321655273
iteration 300, loss 1.6423237323760986
iteration 400, loss 1.6207646131515503
iteration 500, loss 1.6365221738815308
iteration 600, loss 1.619626760482788
iteration 700, loss 1.6097865104675293
iteration 800, loss 1.6545372009277344
iteration 0, loss 1.5963255167007446
iteration 100, loss 1.6292991638183594
iteration 200, loss 1.6514654159545898
iteration 300, loss 1.6241804361343384
iteration 400, loss 1.600733995437622
iteration 500, loss 1.6613283157348633
iteration 600, loss 1.658133864402771
iteration 700, loss 1.630062222480774
iteration 800, loss 1.6089900732040405
iteration 0, loss 1.644099473953247
iteration 100, loss 1.6694761514663696
iteration 200, loss 1.6951699256896973
iteration 300, loss 1.6131699085235596
iteration 400, loss 1.6681609153747559
iteration 500, loss 1.6491799354553223
iteration 600, loss 1.633958101272583
iteration 700, loss 1.6416188478469849
iteration 800, loss 1.5999654531478882
iteration 0, loss 1.6368398666381836
iteration 100, loss 1.703041434288025
iteration 200, loss 1.716052770614624
iteration 300, loss 1.6064789295196533
iteration 400, loss 1.6859241724014282
iteration 500, loss 1.6563198566436768
iteration 600, loss 1.6444517374038696
iteration 700, loss 1.6653072834014893
iteration 800, loss 1.654803991317749
iteration 0, loss 1.6847097873687744
iteration 100, loss 1.5653074979782104
iteration 200, loss 1.5905171632766724
iteration 300, loss 1.661784052848816
iteration 400, loss 1.610775351524353
iteration 500, loss 1.6881226301193237
iteration 600, loss 1.6100833415985107
iteration 700, loss 1.6294491291046143
iteration 800, loss 1.7254040241241455
iteration 0, loss 1.6718599796295166
iteration 100, loss 1.6358319520950317
iteration 200, loss 1.59824538230896
iteration 300, loss 1.6273643970489502
iteration 400, loss 1.635567307472229
iteration 500, loss 1.6114673614501953
iteration 600, loss 1.591118335723877
iteration 700, loss 1.6535061597824097
iteration 800, loss 1.5835902690887451
iteration 0, loss 1.5962944030761719
iteration 100, loss 1.6239559650421143
iteration 200, loss 1.6503570079803467
iteration 300, loss 1.6114203929901123
iteration 400, loss 1.6030751466751099
iteration 500, loss 1.618592381477356
iteration 600, loss 1.6669425964355469
iteration 700, loss 1.6238040924072266
iteration 800, loss 1.6246498823165894
iteration 0, loss 1.6868630647659302
iteration 100, loss 1.6575833559036255
iteration 200, loss 1.676430106163025
iteration 300, loss 1.6579643487930298
iteration 400, loss 1.6283917427062988
iteration 500, loss 1.6134599447250366
iteration 600, loss 1.6196314096450806
iteration 700, loss 1.6045516729354858
iteration 800, loss 1.6431165933609009
iteration 0, loss 1.6336787939071655
iteration 100, loss 1.6404294967651367
iteration 200, loss 1.6507534980773926
iteration 300, loss 1.6425321102142334
iteration 400, loss 1.6432076692581177
iteration 500, loss 1.6751865148544312
iteration 600, loss 1.7021230459213257
iteration 700, loss 1.5995993614196777
iteration 800, loss 1.6460567712783813
iteration 0, loss 1.675374984741211
iteration 100, loss 1.6284257173538208
iteration 200, loss 1.6292883157730103
iteration 300, loss 1.632482647895813
iteration 400, loss 1.6376515626907349
iteration 500, loss 1.6671937704086304
iteration 600, loss 1.6619343757629395
iteration 700, loss 1.6824886798858643
iteration 800, loss 1.7256733179092407
iteration 0, loss 1.6096117496490479
iteration 100, loss 1.5719448328018188
iteration 200, loss 1.631361961364746
iteration 300, loss 1.6192713975906372
iteration 400, loss 1.6192573308944702
iteration 500, loss 1.5849626064300537
iteration 600, loss 1.6147501468658447
iteration 700, loss 1.657942533493042
iteration 800, loss 1.646998405456543
iteration 0, loss 1.6104812622070312
iteration 100, loss 1.6091046333312988
iteration 200, loss 1.6810201406478882
iteration 300, loss 1.646697998046875
iteration 400, loss 1.6774743795394897
iteration 500, loss 1.6548640727996826
iteration 600, loss 1.6616182327270508
iteration 700, loss 1.6171205043792725
iteration 800, loss 1.6233502626419067
iteration 0, loss 1.689931035041809
iteration 100, loss 1.5886898040771484
iteration 200, loss 1.6235504150390625
iteration 300, loss 1.6420714855194092
iteration 400, loss 1.5997260808944702
iteration 500, loss 1.6289536952972412
iteration 600, loss 1.605747938156128
iteration 700, loss 1.655950665473938
iteration 800, loss 1.5990369319915771
iteration 0, loss 1.6486625671386719
iteration 100, loss 1.622165560722351
iteration 200, loss 1.648202657699585
iteration 300, loss 1.653426170349121
iteration 400, loss 1.6345816850662231
iteration 500, loss 1.630479097366333
iteration 600, loss 1.6282676458358765
iteration 700, loss 1.6868890523910522
iteration 800, loss 1.6547921895980835
iteration 0, loss 1.6286345720291138
iteration 100, loss 1.6519646644592285
iteration 200, loss 1.6301969289779663
iteration 300, loss 1.6399532556533813
iteration 400, loss 1.6489593982696533
iteration 500, loss 1.6707319021224976
iteration 600, loss 1.6723324060440063
iteration 700, loss 1.6365305185317993
iteration 800, loss 1.597760558128357
iteration 0, loss 1.6982909440994263
iteration 100, loss 1.6066752672195435
iteration 200, loss 1.6663190126419067
iteration 300, loss 1.6030569076538086
iteration 400, loss 1.6639970541000366
iteration 500, loss 1.604695439338684
iteration 600, loss 1.6040117740631104
iteration 700, loss 1.6102811098098755
iteration 800, loss 1.5929747819900513
iteration 0, loss 1.6632949113845825
iteration 100, loss 1.6063342094421387
iteration 200, loss 1.6560683250427246
iteration 300, loss 1.6887545585632324
iteration 400, loss 1.6088091135025024
iteration 500, loss 1.610083818435669
iteration 600, loss 1.628481149673462
iteration 700, loss 1.6209664344787598
iteration 800, loss 1.6620045900344849
iteration 0, loss 1.6338850259780884
iteration 100, loss 1.6762652397155762
iteration 200, loss 1.6362851858139038
iteration 300, loss 1.5929651260375977
iteration 400, loss 1.6694104671478271
iteration 500, loss 1.658761739730835
iteration 600, loss 1.6947358846664429
iteration 700, loss 1.5877336263656616
iteration 800, loss 1.6366968154907227
iteration 0, loss 1.6354339122772217
iteration 100, loss 1.5803894996643066
iteration 200, loss 1.7001183032989502
iteration 300, loss 1.6659425497055054
iteration 400, loss 1.6211344003677368
iteration 500, loss 1.6375566720962524
iteration 600, loss 1.6121665239334106
iteration 700, loss 1.663898229598999
iteration 800, loss 1.6399179697036743
iteration 0, loss 1.7115205526351929
iteration 100, loss 1.6490147113800049
iteration 200, loss 1.6261682510375977
iteration 300, loss 1.6247260570526123
iteration 400, loss 1.717729091644287
iteration 500, loss 1.5541313886642456
iteration 600, loss 1.6700241565704346
iteration 700, loss 1.635025978088379
iteration 800, loss 1.6101130247116089
iteration 0, loss 1.6052266359329224
iteration 100, loss 1.5787317752838135
iteration 200, loss 1.6198395490646362
iteration 300, loss 1.6921747922897339
iteration 400, loss 1.6203783750534058
iteration 500, loss 1.6184536218643188
iteration 600, loss 1.6009706258773804
iteration 700, loss 1.6240209341049194
iteration 800, loss 1.6463860273361206
iteration 0, loss 1.650160551071167
iteration 100, loss 1.6311389207839966
iteration 200, loss 1.5951015949249268
iteration 300, loss 1.5991158485412598
iteration 400, loss 1.6246505975723267
iteration 500, loss 1.6328516006469727
iteration 600, loss 1.590081810951233
iteration 700, loss 1.5910381078720093
iteration 800, loss 1.6784958839416504
iteration 0, loss 1.6702567338943481
iteration 100, loss 1.6087726354599
iteration 200, loss 1.6308472156524658
iteration 300, loss 1.6012935638427734
iteration 400, loss 1.6404991149902344
iteration 500, loss 1.6549469232559204
iteration 600, loss 1.63240647315979
iteration 700, loss 1.595115065574646
iteration 800, loss 1.6345574855804443
fold 0 accuracy: 0.7823571428571429
iteration 0, loss 1.6250216960906982
iteration 100, loss 1.6125342845916748
iteration 200, loss 1.5718796253204346
iteration 300, loss 1.6982135772705078
iteration 400, loss 1.5677906274795532
iteration 500, loss 1.6096466779708862
iteration 600, loss 1.6041830778121948
iteration 700, loss 1.6510281562805176
iteration 800, loss 1.6242648363113403
iteration 0, loss 1.6559453010559082
iteration 100, loss 1.573730707168579
iteration 200, loss 1.619719386100769
iteration 300, loss 1.6224192380905151
iteration 400, loss 1.6209665536880493
iteration 500, loss 1.5674983263015747
iteration 600, loss 1.6055117845535278
iteration 700, loss 1.650302529335022
iteration 800, loss 1.6382039785385132
iteration 0, loss 1.6914787292480469
iteration 100, loss 1.697975754737854
iteration 200, loss 1.6576601266860962
iteration 300, loss 1.6291344165802002
iteration 400, loss 1.6344664096832275
iteration 500, loss 1.6146551370620728
iteration 600, loss 1.6229465007781982
iteration 700, loss 1.6382519006729126
iteration 800, loss 1.666271448135376
iteration 0, loss 1.667541265487671
iteration 100, loss 1.704192042350769
iteration 200, loss 1.6210445165634155
iteration 300, loss 1.7131834030151367
iteration 400, loss 1.708150029182434
iteration 500, loss 1.6888490915298462
iteration 600, loss 1.6125179529190063
iteration 700, loss 1.6049023866653442
iteration 800, loss 1.637327790260315
iteration 0, loss 1.6387648582458496
iteration 100, loss 1.6176142692565918
iteration 200, loss 1.6828991174697876
iteration 300, loss 1.6672512292861938
iteration 400, loss 1.5883277654647827
iteration 500, loss 1.625279188156128
iteration 600, loss 1.5809147357940674
iteration 700, loss 1.600908875465393
iteration 800, loss 1.7199662923812866
iteration 0, loss 1.6418997049331665
iteration 100, loss 1.5858014822006226
iteration 200, loss 1.6166695356369019
iteration 300, loss 1.7047911882400513
iteration 400, loss 1.7549164295196533
iteration 500, loss 1.6462846994400024
iteration 600, loss 1.5802891254425049
iteration 700, loss 1.73943030834198
iteration 800, loss 1.5719293355941772
iteration 0, loss 1.6724272966384888
iteration 100, loss 1.6535308361053467
iteration 200, loss 1.6504524946212769
iteration 300, loss 1.6670243740081787
iteration 400, loss 1.6909992694854736
iteration 500, loss 1.6702260971069336
iteration 600, loss 1.6577054262161255
iteration 700, loss 1.6641520261764526
iteration 800, loss 1.622443675994873
iteration 0, loss 1.6706757545471191
iteration 100, loss 1.6252422332763672
iteration 200, loss 1.6719242334365845
iteration 300, loss 1.6475404500961304
iteration 400, loss 1.6631346940994263
iteration 500, loss 1.638739824295044
iteration 600, loss 1.652130126953125
iteration 700, loss 1.6352647542953491
iteration 800, loss 1.5682878494262695
iteration 0, loss 1.613560438156128
iteration 100, loss 1.6514809131622314
iteration 200, loss 1.6347018480300903
iteration 300, loss 1.6064636707305908
iteration 400, loss 1.5805859565734863
iteration 500, loss 1.5769611597061157
iteration 600, loss 1.6409533023834229
iteration 700, loss 1.6008974313735962
iteration 800, loss 1.5896148681640625
iteration 0, loss 1.7159379720687866
iteration 100, loss 1.6545761823654175
iteration 200, loss 1.627651572227478
iteration 300, loss 1.6159554719924927
iteration 400, loss 1.546873688697815
iteration 500, loss 1.6242620944976807
iteration 600, loss 1.5961546897888184
iteration 700, loss 1.64033043384552
iteration 800, loss 1.6918622255325317
iteration 0, loss 1.6196622848510742
iteration 100, loss 1.6604920625686646
iteration 200, loss 1.5979564189910889
iteration 300, loss 1.6106876134872437
iteration 400, loss 1.6276980638504028
iteration 500, loss 1.6614798307418823
iteration 600, loss 1.6318758726119995
iteration 700, loss 1.6404633522033691
iteration 800, loss 1.6499114036560059
iteration 0, loss 1.6180261373519897
iteration 100, loss 1.631042242050171
iteration 200, loss 1.5812597274780273
iteration 300, loss 1.5956380367279053
iteration 400, loss 1.5980279445648193
iteration 500, loss 1.6275861263275146
iteration 600, loss 1.5681818723678589
iteration 700, loss 1.6118800640106201
iteration 800, loss 1.6377995014190674
iteration 0, loss 1.683538794517517
iteration 100, loss 1.6438040733337402
iteration 200, loss 1.650994062423706
iteration 300, loss 1.619320034980774
iteration 400, loss 1.5565940141677856
iteration 500, loss 1.5759533643722534
iteration 600, loss 1.5938141345977783
iteration 700, loss 1.6541248559951782
iteration 800, loss 1.6422182321548462
iteration 0, loss 1.6118409633636475
iteration 100, loss 1.6331483125686646
iteration 200, loss 1.6195523738861084
iteration 300, loss 1.5955686569213867
iteration 400, loss 1.5917935371398926
iteration 500, loss 1.5939372777938843
iteration 600, loss 1.608347773551941
iteration 700, loss 1.6394740343093872
iteration 800, loss 1.6332557201385498
iteration 0, loss 1.632091999053955
iteration 100, loss 1.5888328552246094
iteration 200, loss 1.5689194202423096
iteration 300, loss 1.6074532270431519
iteration 400, loss 1.6057515144348145
iteration 500, loss 1.5807403326034546
iteration 600, loss 1.5903695821762085
iteration 700, loss 1.600172519683838
iteration 800, loss 1.6492934226989746
iteration 0, loss 1.7018564939498901
iteration 100, loss 1.6124178171157837
iteration 200, loss 1.6063835620880127
iteration 300, loss 1.5905224084854126
iteration 400, loss 1.6679397821426392
iteration 500, loss 1.6216650009155273
iteration 600, loss 1.6021435260772705
iteration 700, loss 1.589630126953125
iteration 800, loss 1.6281698942184448
iteration 0, loss 1.6417016983032227
iteration 100, loss 1.5855870246887207
iteration 200, loss 1.6606953144073486
iteration 300, loss 1.618699073791504
iteration 400, loss 1.6472022533416748
iteration 500, loss 1.5905627012252808
iteration 600, loss 1.613617181777954
iteration 700, loss 1.6367803812026978
iteration 800, loss 1.6209505796432495
iteration 0, loss 1.6143125295639038
iteration 100, loss 1.6380559206008911
iteration 200, loss 1.587908387184143
iteration 300, loss 1.6669764518737793
iteration 400, loss 1.5793837308883667
iteration 500, loss 1.6009457111358643
iteration 600, loss 1.6391396522521973
iteration 700, loss 1.6346441507339478
iteration 800, loss 1.6657819747924805
iteration 0, loss 1.6251773834228516
iteration 100, loss 1.6482397317886353
iteration 200, loss 1.6057220697402954
iteration 300, loss 1.6372604370117188
iteration 400, loss 1.6162219047546387
iteration 500, loss 1.6081031560897827
iteration 600, loss 1.6159260272979736
iteration 700, loss 1.595660924911499
iteration 800, loss 1.6302180290222168
iteration 0, loss 1.5856599807739258
iteration 100, loss 1.6427974700927734
iteration 200, loss 1.659834384918213
iteration 300, loss 1.6195181608200073
iteration 400, loss 1.6715586185455322
iteration 500, loss 1.6302070617675781
iteration 600, loss 1.612404704093933
iteration 700, loss 1.6299028396606445
iteration 800, loss 1.6365009546279907
iteration 0, loss 1.621287226676941
iteration 100, loss 1.6290342807769775
iteration 200, loss 1.6549879312515259
iteration 300, loss 1.6664960384368896
iteration 400, loss 1.5702396631240845
iteration 500, loss 1.6177847385406494
iteration 600, loss 1.6645125150680542
iteration 700, loss 1.7074813842773438
iteration 800, loss 1.6053454875946045
iteration 0, loss 1.6589583158493042
iteration 100, loss 1.6189631223678589
iteration 200, loss 1.6035529375076294
iteration 300, loss 1.6856380701065063
iteration 400, loss 1.625622272491455
iteration 500, loss 1.6058636903762817
iteration 600, loss 1.693641185760498
iteration 700, loss 1.6456106901168823
iteration 800, loss 1.6639339923858643
iteration 0, loss 1.649509072303772
iteration 100, loss 1.6610774993896484
iteration 200, loss 1.6274687051773071
iteration 300, loss 1.6291683912277222
iteration 400, loss 1.6471798419952393
iteration 500, loss 1.5959813594818115
iteration 600, loss 1.6099388599395752
iteration 700, loss 1.5999923944473267
iteration 800, loss 1.6684868335723877
iteration 0, loss 1.5773355960845947
iteration 100, loss 1.6226685047149658
iteration 200, loss 1.5766382217407227
iteration 300, loss 1.641007900238037
iteration 400, loss 1.6002660989761353
iteration 500, loss 1.6504285335540771
iteration 600, loss 1.6140910387039185
iteration 700, loss 1.6480873823165894
iteration 800, loss 1.6198484897613525
iteration 0, loss 1.663281798362732
iteration 100, loss 1.6286685466766357
iteration 200, loss 1.6832382678985596
iteration 300, loss 1.6564557552337646
iteration 400, loss 1.594321370124817
iteration 500, loss 1.625327706336975
iteration 600, loss 1.5887510776519775
iteration 700, loss 1.6568248271942139
iteration 800, loss 1.5954350233078003
iteration 0, loss 1.5540533065795898
iteration 100, loss 1.6274375915527344
iteration 200, loss 1.543721318244934
iteration 300, loss 1.577293038368225
iteration 400, loss 1.6139684915542603
iteration 500, loss 1.6118134260177612
iteration 600, loss 1.6380119323730469
iteration 700, loss 1.6346220970153809
iteration 800, loss 1.615964651107788
iteration 0, loss 1.5609866380691528
iteration 100, loss 1.5764228105545044
iteration 200, loss 1.6256269216537476
iteration 300, loss 1.6176799535751343
iteration 400, loss 1.5796432495117188
iteration 500, loss 1.618797779083252
iteration 600, loss 1.5860623121261597
iteration 700, loss 1.594645380973816
iteration 800, loss 1.5700258016586304
iteration 0, loss 1.5912632942199707
iteration 100, loss 1.6406736373901367
iteration 200, loss 1.5874602794647217
iteration 300, loss 1.6016576290130615
iteration 400, loss 1.6027177572250366
iteration 500, loss 1.6724958419799805
iteration 600, loss 1.5987944602966309
iteration 700, loss 1.6837202310562134
iteration 800, loss 1.6273753643035889
iteration 0, loss 1.6481482982635498
iteration 100, loss 1.6776946783065796
iteration 200, loss 1.674513816833496
iteration 300, loss 1.6730972528457642
iteration 400, loss 1.6377278566360474
iteration 500, loss 1.5960946083068848
iteration 600, loss 1.6025241613388062
iteration 700, loss 1.6241505146026611
iteration 800, loss 1.6049171686172485
iteration 0, loss 1.6357237100601196
iteration 100, loss 1.674576997756958
iteration 200, loss 1.6446582078933716
iteration 300, loss 1.6165624856948853
iteration 400, loss 1.6051827669143677
iteration 500, loss 1.6368824243545532
iteration 600, loss 1.5938904285430908
iteration 700, loss 1.6749346256256104
iteration 800, loss 1.6622432470321655
iteration 0, loss 1.647169589996338
iteration 100, loss 1.6172502040863037
iteration 200, loss 1.5817720890045166
iteration 300, loss 1.6387951374053955
iteration 400, loss 1.5964579582214355
iteration 500, loss 1.656866192817688
iteration 600, loss 1.6253961324691772
iteration 700, loss 1.61738920211792
iteration 800, loss 1.6230651140213013
iteration 0, loss 1.6616699695587158
iteration 100, loss 1.6714297533035278
iteration 200, loss 1.6369636058807373
iteration 300, loss 1.5980831384658813
iteration 400, loss 1.6465715169906616
iteration 500, loss 1.682377576828003
iteration 600, loss 1.6392369270324707
iteration 700, loss 1.6097722053527832
iteration 800, loss 1.6454530954360962
iteration 0, loss 1.6107971668243408
iteration 100, loss 1.5783050060272217
iteration 200, loss 1.6673855781555176
iteration 300, loss 1.673190712928772
iteration 400, loss 1.6129828691482544
iteration 500, loss 1.6282439231872559
iteration 600, loss 1.6368769407272339
iteration 700, loss 1.6209121942520142
iteration 800, loss 1.6227399110794067
iteration 0, loss 1.647453784942627
iteration 100, loss 1.6155142784118652
iteration 200, loss 1.62759530544281
iteration 300, loss 1.6378986835479736
iteration 400, loss 1.640142798423767
iteration 500, loss 1.62238347530365
iteration 600, loss 1.6095852851867676
iteration 700, loss 1.615464448928833
iteration 800, loss 1.620823860168457
iteration 0, loss 1.5857197046279907
iteration 100, loss 1.7072248458862305
iteration 200, loss 1.6026746034622192
iteration 300, loss 1.658141851425171
iteration 400, loss 1.6935468912124634
iteration 500, loss 1.692671775817871
iteration 600, loss 1.6375298500061035
iteration 700, loss 1.5645630359649658
iteration 800, loss 1.6459792852401733
iteration 0, loss 1.5929871797561646
iteration 100, loss 1.62260901927948
iteration 200, loss 1.631049394607544
iteration 300, loss 1.5879652500152588
iteration 400, loss 1.6395529508590698
iteration 500, loss 1.6500650644302368
iteration 600, loss 1.6421678066253662
iteration 700, loss 1.6247800588607788
iteration 800, loss 1.6267510652542114
iteration 0, loss 1.6893612146377563
iteration 100, loss 1.6377407312393188
iteration 200, loss 1.6252537965774536
iteration 300, loss 1.5912376642227173
iteration 400, loss 1.6120606660842896
iteration 500, loss 1.611431360244751
iteration 600, loss 1.5813888311386108
iteration 700, loss 1.5824575424194336
iteration 800, loss 1.6172853708267212
iteration 0, loss 1.5828027725219727
iteration 100, loss 1.634073257446289
iteration 200, loss 1.5597003698349
iteration 300, loss 1.6368070840835571
iteration 400, loss 1.5807663202285767
iteration 500, loss 1.6792174577713013
iteration 600, loss 1.6033333539962769
iteration 700, loss 1.607456922531128
iteration 800, loss 1.6467291116714478
iteration 0, loss 1.6535249948501587
iteration 100, loss 1.6640357971191406
iteration 200, loss 1.6491390466690063
iteration 300, loss 1.6014904975891113
iteration 400, loss 1.562358021736145
iteration 500, loss 1.6449127197265625
iteration 600, loss 1.5897753238677979
iteration 700, loss 1.6408824920654297
iteration 800, loss 1.623882532119751
iteration 0, loss 1.6231980323791504
iteration 100, loss 1.6342124938964844
iteration 200, loss 1.6099575757980347
iteration 300, loss 1.664459466934204
iteration 400, loss 1.6528522968292236
iteration 500, loss 1.5729780197143555
iteration 600, loss 1.5866343975067139
iteration 700, loss 1.6284760236740112
iteration 800, loss 1.6278661489486694
iteration 0, loss 1.648618459701538
iteration 100, loss 1.6366066932678223
iteration 200, loss 1.6511677503585815
iteration 300, loss 1.546659231185913
iteration 400, loss 1.616437315940857
iteration 500, loss 1.6664758920669556
iteration 600, loss 1.6547130346298218
iteration 700, loss 1.6383777856826782
iteration 800, loss 1.6486313343048096
iteration 0, loss 1.6151407957077026
iteration 100, loss 1.5840967893600464
iteration 200, loss 1.6160131692886353
iteration 300, loss 1.6624960899353027
iteration 400, loss 1.632084846496582
iteration 500, loss 1.6930296421051025
iteration 600, loss 1.5920010805130005
iteration 700, loss 1.6258732080459595
iteration 800, loss 1.6535356044769287
iteration 0, loss 1.6067049503326416
iteration 100, loss 1.6785715818405151
iteration 200, loss 1.5845506191253662
iteration 300, loss 1.636434555053711
iteration 400, loss 1.673721432685852
iteration 500, loss 1.640914797782898
iteration 600, loss 1.598067045211792
iteration 700, loss 1.6327390670776367
iteration 800, loss 1.6307487487792969
iteration 0, loss 1.6439164876937866
iteration 100, loss 1.6568433046340942
iteration 200, loss 1.6912494897842407
iteration 300, loss 1.5794243812561035
iteration 400, loss 1.6143485307693481
iteration 500, loss 1.7458688020706177
iteration 600, loss 1.6260998249053955
iteration 700, loss 1.690767765045166
iteration 800, loss 1.6386866569519043
iteration 0, loss 1.6439330577850342
iteration 100, loss 1.6190766096115112
iteration 200, loss 1.618665337562561
iteration 300, loss 1.635890245437622
iteration 400, loss 1.652209997177124
iteration 500, loss 1.626266598701477
iteration 600, loss 1.6257561445236206
iteration 700, loss 1.661586880683899
iteration 800, loss 1.594648838043213
iteration 0, loss 1.6346725225448608
iteration 100, loss 1.631579875946045
iteration 200, loss 1.6026827096939087
iteration 300, loss 1.6492637395858765
iteration 400, loss 1.642055630683899
iteration 500, loss 1.6240757703781128
iteration 600, loss 1.6212347745895386
iteration 700, loss 1.6524204015731812
iteration 800, loss 1.5855640172958374
iteration 0, loss 1.664921522140503
iteration 100, loss 1.644903302192688
iteration 200, loss 1.6878752708435059
iteration 300, loss 1.6682751178741455
iteration 400, loss 1.6013075113296509
iteration 500, loss 1.614553451538086
iteration 600, loss 1.6346094608306885
iteration 700, loss 1.6541452407836914
iteration 800, loss 1.6950490474700928
iteration 0, loss 1.607164740562439
iteration 100, loss 1.6436927318572998
iteration 200, loss 1.6579431295394897
iteration 300, loss 1.6105486154556274
iteration 400, loss 1.6254640817642212
iteration 500, loss 1.5796140432357788
iteration 600, loss 1.6955571174621582
iteration 700, loss 1.617834448814392
iteration 800, loss 1.595198631286621
iteration 0, loss 1.5804482698440552
iteration 100, loss 1.6323846578598022
iteration 200, loss 1.7113633155822754
iteration 300, loss 1.5869265794754028
iteration 400, loss 1.6528160572052002
iteration 500, loss 1.6750531196594238
iteration 600, loss 1.6322702169418335
iteration 700, loss 1.592939853668213
iteration 800, loss 1.66743803024292
iteration 0, loss 1.6063032150268555
iteration 100, loss 1.6117651462554932
iteration 200, loss 1.6663495302200317
iteration 300, loss 1.6864855289459229
iteration 400, loss 1.5855063199996948
iteration 500, loss 1.6228142976760864
iteration 600, loss 1.6374061107635498
iteration 700, loss 1.686572551727295
iteration 800, loss 1.6354944705963135
fold 1 accuracy: 0.8034285714285714
iteration 0, loss 1.5950318574905396
iteration 100, loss 1.6200041770935059
iteration 200, loss 1.6236915588378906
iteration 300, loss 1.6866395473480225
iteration 400, loss 1.6134464740753174
iteration 500, loss 1.6569782495498657
iteration 600, loss 1.6048997640609741
iteration 700, loss 1.6136540174484253
iteration 800, loss 1.6214752197265625
iteration 0, loss 1.6340970993041992
iteration 100, loss 1.625356912612915
iteration 200, loss 1.667952537536621
iteration 300, loss 1.6077512502670288
iteration 400, loss 1.6031265258789062
iteration 500, loss 1.630491852760315
iteration 600, loss 1.6766095161437988
iteration 700, loss 1.5698567628860474
iteration 800, loss 1.6375713348388672
iteration 0, loss 1.661847710609436
iteration 100, loss 1.6092277765274048
iteration 200, loss 1.5722249746322632
iteration 300, loss 1.6645253896713257
iteration 400, loss 1.6535420417785645
iteration 500, loss 1.5546343326568604
iteration 600, loss 1.6618046760559082
iteration 700, loss 1.5668147802352905
iteration 800, loss 1.6508315801620483
iteration 0, loss 1.6320880651474
iteration 100, loss 1.6008107662200928
iteration 200, loss 1.601118564605713
iteration 300, loss 1.6525434255599976
iteration 400, loss 1.6042814254760742
iteration 500, loss 1.6828765869140625
iteration 600, loss 1.6463534832000732
iteration 700, loss 1.5605026483535767
iteration 800, loss 1.5992491245269775
iteration 0, loss 1.6079514026641846
iteration 100, loss 1.7012882232666016
iteration 200, loss 1.6542489528656006
iteration 300, loss 1.5671271085739136
iteration 400, loss 1.5958225727081299
iteration 500, loss 1.5869969129562378
iteration 600, loss 1.6310080289840698
iteration 700, loss 1.5958526134490967
iteration 800, loss 1.6145676374435425
iteration 0, loss 1.647318959236145
iteration 100, loss 1.6008250713348389
iteration 200, loss 1.6406017541885376
iteration 300, loss 1.649024486541748
iteration 400, loss 1.6891531944274902
iteration 500, loss 1.5925480127334595
iteration 600, loss 1.6433202028274536
iteration 700, loss 1.6201428174972534
iteration 800, loss 1.687509536743164
iteration 0, loss 1.5947579145431519
iteration 100, loss 1.6831358671188354
iteration 200, loss 1.6361082792282104
iteration 300, loss 1.6053597927093506
iteration 400, loss 1.6437773704528809
iteration 500, loss 1.6374986171722412
iteration 600, loss 1.6861672401428223
iteration 700, loss 1.5604188442230225
iteration 800, loss 1.5904722213745117
iteration 0, loss 1.6166614294052124
iteration 100, loss 1.589263916015625
iteration 200, loss 1.6294934749603271
iteration 300, loss 1.6168307065963745
iteration 400, loss 1.6636583805084229
iteration 500, loss 1.5769996643066406
iteration 600, loss 1.6168298721313477
iteration 700, loss 1.6278729438781738
iteration 800, loss 1.6579269170761108
iteration 0, loss 1.643401861190796
iteration 100, loss 1.6875090599060059
iteration 200, loss 1.6431492567062378
iteration 300, loss 1.6038720607757568
iteration 400, loss 1.5950984954833984
iteration 500, loss 1.6198911666870117
iteration 600, loss 1.6080005168914795
iteration 700, loss 1.6041676998138428
iteration 800, loss 1.6442389488220215
iteration 0, loss 1.695641279220581
iteration 100, loss 1.5895800590515137
iteration 200, loss 1.6373109817504883
iteration 300, loss 1.6411234140396118
iteration 400, loss 1.664548635482788
iteration 500, loss 1.6514157056808472
iteration 600, loss 1.7251118421554565
iteration 700, loss 1.6478440761566162
iteration 800, loss 1.6306641101837158
iteration 0, loss 1.5974539518356323
iteration 100, loss 1.6679974794387817
iteration 200, loss 1.5886383056640625
iteration 300, loss 1.642296552658081
iteration 400, loss 1.7242262363433838
iteration 500, loss 1.634631872177124
iteration 600, loss 1.6667962074279785
iteration 700, loss 1.619144082069397
iteration 800, loss 1.613511562347412
iteration 0, loss 1.6175122261047363
iteration 100, loss 1.633725643157959
iteration 200, loss 1.645237684249878
iteration 300, loss 1.5968698263168335
iteration 400, loss 1.6306970119476318
iteration 500, loss 1.6704814434051514
iteration 600, loss 1.601636528968811
iteration 700, loss 1.601738452911377
iteration 800, loss 1.5960997343063354
iteration 0, loss 1.6015961170196533
iteration 100, loss 1.583735466003418
iteration 200, loss 1.6549863815307617
iteration 300, loss 1.6570701599121094
iteration 400, loss 1.6614996194839478
iteration 500, loss 1.6182730197906494
iteration 600, loss 1.6867597103118896
iteration 700, loss 1.6468557119369507
iteration 800, loss 1.6348764896392822
iteration 0, loss 1.618102788925171
iteration 100, loss 1.6785554885864258
iteration 200, loss 1.5900204181671143
iteration 300, loss 1.6117795705795288
iteration 400, loss 1.6027957201004028
iteration 500, loss 1.6175222396850586
iteration 600, loss 1.705247402191162
iteration 700, loss 1.6481685638427734
iteration 800, loss 1.5988181829452515
iteration 0, loss 1.6703803539276123
iteration 100, loss 1.6317718029022217
iteration 200, loss 1.5919233560562134
iteration 300, loss 1.6218078136444092
iteration 400, loss 1.619412899017334
iteration 500, loss 1.6511268615722656
iteration 600, loss 1.6844416856765747
iteration 700, loss 1.6001248359680176
iteration 800, loss 1.5955556631088257
iteration 0, loss 1.6566877365112305
iteration 100, loss 1.6662967205047607
iteration 200, loss 1.6196690797805786
iteration 300, loss 1.6138588190078735
iteration 400, loss 1.611356496810913
iteration 500, loss 1.6276288032531738
iteration 600, loss 1.6841421127319336
iteration 700, loss 1.5988487005233765
iteration 800, loss 1.646872878074646
iteration 0, loss 1.606593370437622
iteration 100, loss 1.6025538444519043
iteration 200, loss 1.5874619483947754
iteration 300, loss 1.5993306636810303
iteration 400, loss 1.6016905307769775
iteration 500, loss 1.6619716882705688
iteration 600, loss 1.6360198259353638
iteration 700, loss 1.5735394954681396
iteration 800, loss 1.6395857334136963
iteration 0, loss 1.6033426523208618
iteration 100, loss 1.6105260848999023
iteration 200, loss 1.7121354341506958
iteration 300, loss 1.6768698692321777
iteration 400, loss 1.5559333562850952
iteration 500, loss 1.6252796649932861
iteration 600, loss 1.608642578125
iteration 700, loss 1.6424026489257812
iteration 800, loss 1.6225355863571167
iteration 0, loss 1.6973248720169067
iteration 100, loss 1.6184051036834717
iteration 200, loss 1.5994466543197632
iteration 300, loss 1.607300877571106
iteration 400, loss 1.6761914491653442
iteration 500, loss 1.6372342109680176
iteration 600, loss 1.626489520072937
iteration 700, loss 1.6137090921401978
iteration 800, loss 1.549879789352417
iteration 0, loss 1.5849663019180298
iteration 100, loss 1.6149086952209473
iteration 200, loss 1.617826223373413
iteration 300, loss 1.6205500364303589
iteration 400, loss 1.6199802160263062
iteration 500, loss 1.6001478433609009
iteration 600, loss 1.6063510179519653
iteration 700, loss 1.6143535375595093
iteration 800, loss 1.6205805540084839
iteration 0, loss 1.6883857250213623
iteration 100, loss 1.6130318641662598
iteration 200, loss 1.6315996646881104
iteration 300, loss 1.6007360219955444
iteration 400, loss 1.6762595176696777
iteration 500, loss 1.5962108373641968
iteration 600, loss 1.6548560857772827
iteration 700, loss 1.6539058685302734
iteration 800, loss 1.6424479484558105
iteration 0, loss 1.6359941959381104
iteration 100, loss 1.6111037731170654
iteration 200, loss 1.6160314083099365
iteration 300, loss 1.5710428953170776
iteration 400, loss 1.6429708003997803
iteration 500, loss 1.6134586334228516
iteration 600, loss 1.6056413650512695
iteration 700, loss 1.579566478729248
iteration 800, loss 1.5725513696670532
iteration 0, loss 1.6284433603286743
iteration 100, loss 1.6256564855575562
iteration 200, loss 1.6079052686691284
iteration 300, loss 1.6173198223114014
iteration 400, loss 1.6322028636932373
iteration 500, loss 1.6272375583648682
iteration 600, loss 1.6683428287506104
iteration 700, loss 1.6028015613555908
iteration 800, loss 1.6484454870224
iteration 0, loss 1.6442047357559204
iteration 100, loss 1.6315598487854004
iteration 200, loss 1.6019991636276245
iteration 300, loss 1.6623879671096802
iteration 400, loss 1.6370642185211182
iteration 500, loss 1.667648434638977
iteration 600, loss 1.5873218774795532
iteration 700, loss 1.6801179647445679
iteration 800, loss 1.672982096672058
iteration 0, loss 1.6932228803634644
iteration 100, loss 1.6840859651565552
iteration 200, loss 1.585332989692688
iteration 300, loss 1.6191078424453735
iteration 400, loss 1.671865701675415
iteration 500, loss 1.6899292469024658
iteration 600, loss 1.6931360960006714
iteration 700, loss 1.6599527597427368
iteration 800, loss 1.60702383518219
iteration 0, loss 1.6735074520111084
iteration 100, loss 1.63351309299469
iteration 200, loss 1.6583890914916992
iteration 300, loss 1.6421204805374146
iteration 400, loss 1.6217851638793945
iteration 500, loss 1.5344362258911133
iteration 600, loss 1.6175942420959473
iteration 700, loss 1.5918699502944946
iteration 800, loss 1.6353092193603516
iteration 0, loss 1.6630560159683228
iteration 100, loss 1.5770505666732788
iteration 200, loss 1.6252970695495605
iteration 300, loss 1.6448400020599365
iteration 400, loss 1.6355512142181396
iteration 500, loss 1.5760644674301147
iteration 600, loss 1.6570425033569336
iteration 700, loss 1.6151224374771118
iteration 800, loss 1.6260757446289062
iteration 0, loss 1.6900246143341064
iteration 100, loss 1.636034369468689
iteration 200, loss 1.7131297588348389
iteration 300, loss 1.6024271249771118
iteration 400, loss 1.589395523071289
iteration 500, loss 1.606791615486145
iteration 600, loss 1.629589557647705
iteration 700, loss 1.5939737558364868
iteration 800, loss 1.6851643323898315
iteration 0, loss 1.668728232383728
iteration 100, loss 1.6008001565933228
iteration 200, loss 1.654362678527832
iteration 300, loss 1.6448084115982056
iteration 400, loss 1.6063182353973389
iteration 500, loss 1.6444826126098633
iteration 600, loss 1.6536264419555664
iteration 700, loss 1.6685075759887695
iteration 800, loss 1.6469447612762451
iteration 0, loss 1.5953925848007202
iteration 100, loss 1.6516673564910889
iteration 200, loss 1.5969407558441162
iteration 300, loss 1.6468734741210938
iteration 400, loss 1.6529098749160767
iteration 500, loss 1.6540436744689941
iteration 600, loss 1.5619442462921143
iteration 700, loss 1.6035128831863403
iteration 800, loss 1.6600064039230347
iteration 0, loss 1.573487401008606
iteration 100, loss 1.5895378589630127
iteration 200, loss 1.618857741355896
iteration 300, loss 1.6690948009490967
iteration 400, loss 1.6022610664367676
iteration 500, loss 1.5929253101348877
iteration 600, loss 1.6465904712677002
iteration 700, loss 1.6266629695892334
iteration 800, loss 1.6743510961532593
iteration 0, loss 1.6261892318725586
iteration 100, loss 1.595906138420105
iteration 200, loss 1.598463535308838
iteration 300, loss 1.634912133216858
iteration 400, loss 1.6475204229354858
iteration 500, loss 1.639142394065857
iteration 600, loss 1.575311303138733
iteration 700, loss 1.6560372114181519
iteration 800, loss 1.666019320487976
iteration 0, loss 1.634163498878479
iteration 100, loss 1.6335699558258057
iteration 200, loss 1.6142785549163818
iteration 300, loss 1.649436354637146
iteration 400, loss 1.6342676877975464
iteration 500, loss 1.6490061283111572
iteration 600, loss 1.6487947702407837
iteration 700, loss 1.6350595951080322
iteration 800, loss 1.6349667310714722
iteration 0, loss 1.6097259521484375
iteration 100, loss 1.5997873544692993
iteration 200, loss 1.5843884944915771
iteration 300, loss 1.7124420404434204
iteration 400, loss 1.623099446296692
iteration 500, loss 1.6151390075683594
iteration 600, loss 1.6528466939926147
iteration 700, loss 1.6277097463607788
iteration 800, loss 1.6088521480560303
iteration 0, loss 1.616391658782959
iteration 100, loss 1.6229791641235352
iteration 200, loss 1.6265231370925903
iteration 300, loss 1.666039228439331
iteration 400, loss 1.5995415449142456
iteration 500, loss 1.6531766653060913
iteration 600, loss 1.6758363246917725
iteration 700, loss 1.6320720911026
iteration 800, loss 1.5802818536758423
iteration 0, loss 1.608310341835022
iteration 100, loss 1.585860252380371
iteration 200, loss 1.6111111640930176
iteration 300, loss 1.6248074769973755
iteration 400, loss 1.636565089225769
iteration 500, loss 1.62001633644104
iteration 600, loss 1.629031777381897
iteration 700, loss 1.6502506732940674
iteration 800, loss 1.6131713390350342
iteration 0, loss 1.6025923490524292
iteration 100, loss 1.600351333618164
iteration 200, loss 1.5958120822906494
iteration 300, loss 1.600736141204834
iteration 400, loss 1.6345001459121704
iteration 500, loss 1.6145248413085938
iteration 600, loss 1.6029021739959717
iteration 700, loss 1.5892724990844727
iteration 800, loss 1.589327096939087
iteration 0, loss 1.564988374710083
iteration 100, loss 1.7118983268737793
iteration 200, loss 1.6770167350769043
iteration 300, loss 1.6515016555786133
iteration 400, loss 1.6321898698806763
iteration 500, loss 1.6062496900558472
iteration 600, loss 1.6465636491775513
iteration 700, loss 1.6285133361816406
iteration 800, loss 1.6799722909927368
iteration 0, loss 1.6268112659454346
iteration 100, loss 1.5614051818847656
iteration 200, loss 1.609286904335022
iteration 300, loss 1.6205992698669434
iteration 400, loss 1.6329318284988403
iteration 500, loss 1.6243770122528076
iteration 600, loss 1.6726475954055786
iteration 700, loss 1.567906379699707
iteration 800, loss 1.6680463552474976
iteration 0, loss 1.5721298456192017
iteration 100, loss 1.6268951892852783
iteration 200, loss 1.589708685874939
iteration 300, loss 1.5846806764602661
iteration 400, loss 1.6171996593475342
iteration 500, loss 1.6114673614501953
iteration 600, loss 1.5844372510910034
iteration 700, loss 1.6082547903060913
iteration 800, loss 1.5891330242156982
iteration 0, loss 1.6364226341247559
iteration 100, loss 1.5354137420654297
iteration 200, loss 1.5929654836654663
iteration 300, loss 1.691409945487976
iteration 400, loss 1.6757537126541138
iteration 500, loss 1.6299382448196411
iteration 600, loss 1.618667721748352
iteration 700, loss 1.6525429487228394
iteration 800, loss 1.6021658182144165
iteration 0, loss 1.6063945293426514
iteration 100, loss 1.606188416481018
iteration 200, loss 1.648821473121643
iteration 300, loss 1.6250652074813843
iteration 400, loss 1.6102116107940674
iteration 500, loss 1.6370631456375122
iteration 600, loss 1.6175295114517212
iteration 700, loss 1.694641351699829
iteration 800, loss 1.6117501258850098
iteration 0, loss 1.585673213005066
iteration 100, loss 1.5930891036987305
iteration 200, loss 1.6059801578521729
iteration 300, loss 1.683288335800171
iteration 400, loss 1.6487314701080322
iteration 500, loss 1.5761158466339111
iteration 600, loss 1.630552053451538
iteration 700, loss 1.6975077390670776
iteration 800, loss 1.5596294403076172
iteration 0, loss 1.6505805253982544
iteration 100, loss 1.6003379821777344
iteration 200, loss 1.5415639877319336
iteration 300, loss 1.6027839183807373
iteration 400, loss 1.6176339387893677
iteration 500, loss 1.665014624595642
iteration 600, loss 1.665558934211731
iteration 700, loss 1.6500705480575562
iteration 800, loss 1.6394404172897339
iteration 0, loss 1.6191139221191406
iteration 100, loss 1.58244788646698
iteration 200, loss 1.5752592086791992
iteration 300, loss 1.6106348037719727
iteration 400, loss 1.6407114267349243
iteration 500, loss 1.6529829502105713
iteration 600, loss 1.6201976537704468
iteration 700, loss 1.6206451654434204
iteration 800, loss 1.667653203010559
iteration 0, loss 1.655407428741455
iteration 100, loss 1.6440284252166748
iteration 200, loss 1.5941903591156006
iteration 300, loss 1.6694164276123047
iteration 400, loss 1.6030123233795166
iteration 500, loss 1.6366287469863892
iteration 600, loss 1.6799659729003906
iteration 700, loss 1.6187653541564941
iteration 800, loss 1.6442421674728394
iteration 0, loss 1.7074923515319824
iteration 100, loss 1.595144510269165
iteration 200, loss 1.581431269645691
iteration 300, loss 1.6137869358062744
iteration 400, loss 1.5871284008026123
iteration 500, loss 1.6226835250854492
iteration 600, loss 1.6248559951782227
iteration 700, loss 1.5982897281646729
iteration 800, loss 1.6224888563156128
iteration 0, loss 1.6005858182907104
iteration 100, loss 1.6096640825271606
iteration 200, loss 1.6708273887634277
iteration 300, loss 1.6624280214309692
iteration 400, loss 1.584920883178711
iteration 500, loss 1.661006212234497
iteration 600, loss 1.6900067329406738
iteration 700, loss 1.6027753353118896
iteration 800, loss 1.5853065252304077
iteration 0, loss 1.65158212184906
iteration 100, loss 1.5863113403320312
iteration 200, loss 1.5579395294189453
iteration 300, loss 1.6089752912521362
iteration 400, loss 1.6247481107711792
iteration 500, loss 1.643782377243042
iteration 600, loss 1.6475648880004883
iteration 700, loss 1.6080880165100098
iteration 800, loss 1.5913878679275513
iteration 0, loss 1.6175506114959717
iteration 100, loss 1.686231255531311
iteration 200, loss 1.6212785243988037
iteration 300, loss 1.6089650392532349
iteration 400, loss 1.652691125869751
iteration 500, loss 1.6566036939620972
iteration 600, loss 1.6706573963165283
iteration 700, loss 1.708253026008606
iteration 800, loss 1.603708267211914
fold 2 accuracy: 0.8077857142857143
iteration 0, loss 1.653027057647705
iteration 100, loss 1.5991095304489136
iteration 200, loss 1.6021721363067627
iteration 300, loss 1.5662248134613037
iteration 400, loss 1.6167888641357422
iteration 500, loss 1.58760666847229
iteration 600, loss 1.6092710494995117
iteration 700, loss 1.6059316396713257
iteration 800, loss 1.6273574829101562
iteration 0, loss 1.652707815170288
iteration 100, loss 1.6422569751739502
iteration 200, loss 1.6232845783233643
iteration 300, loss 1.6116312742233276
iteration 400, loss 1.552616000175476
iteration 500, loss 1.658505916595459
iteration 600, loss 1.6750329732894897
iteration 700, loss 1.6421928405761719
iteration 800, loss 1.6413657665252686
iteration 0, loss 1.6702896356582642
iteration 100, loss 1.5922157764434814
iteration 200, loss 1.6497677564620972
iteration 300, loss 1.5960955619812012
iteration 400, loss 1.637536883354187
iteration 500, loss 1.6150575876235962
iteration 600, loss 1.6136170625686646
iteration 700, loss 1.6909496784210205
iteration 800, loss 1.653061032295227
iteration 0, loss 1.6419209241867065
iteration 100, loss 1.6645450592041016
iteration 200, loss 1.6219485998153687
iteration 300, loss 1.6589572429656982
iteration 400, loss 1.6172819137573242
iteration 500, loss 1.623437523841858
iteration 600, loss 1.5708985328674316
iteration 700, loss 1.609097957611084
iteration 800, loss 1.6369355916976929
iteration 0, loss 1.6488686800003052
iteration 100, loss 1.6131254434585571
iteration 200, loss 1.5774134397506714
iteration 300, loss 1.5744717121124268
iteration 400, loss 1.6354790925979614
iteration 500, loss 1.6308925151824951
iteration 600, loss 1.6509243249893188
iteration 700, loss 1.5988892316818237
iteration 800, loss 1.5854880809783936
iteration 0, loss 1.5650206804275513
iteration 100, loss 1.6763159036636353
iteration 200, loss 1.6191211938858032
iteration 300, loss 1.6276904344558716
iteration 400, loss 1.5852340459823608
iteration 500, loss 1.6626636981964111
iteration 600, loss 1.5941407680511475
iteration 700, loss 1.6409693956375122
iteration 800, loss 1.599782943725586
iteration 0, loss 1.6269313097000122
iteration 100, loss 1.6348494291305542
iteration 200, loss 1.6840956211090088
iteration 300, loss 1.680052399635315
iteration 400, loss 1.6363146305084229
iteration 500, loss 1.6493170261383057
iteration 600, loss 1.688555359840393
iteration 700, loss 1.6955127716064453
iteration 800, loss 1.5687447786331177
iteration 0, loss 1.6268481016159058
iteration 100, loss 1.5778300762176514
iteration 200, loss 1.6627730131149292
iteration 300, loss 1.6402192115783691
iteration 400, loss 1.6804629564285278
iteration 500, loss 1.6339341402053833
iteration 600, loss 1.6708474159240723
iteration 700, loss 1.5969921350479126
iteration 800, loss 1.6289411783218384
iteration 0, loss 1.629891037940979
iteration 100, loss 1.7067537307739258
iteration 200, loss 1.6773483753204346
iteration 300, loss 1.6146490573883057
iteration 400, loss 1.638051152229309
iteration 500, loss 1.5958197116851807
iteration 600, loss 1.6338320970535278
iteration 700, loss 1.6001933813095093
iteration 800, loss 1.6463934183120728
iteration 0, loss 1.630447268486023
iteration 100, loss 1.6260826587677002
iteration 200, loss 1.6893668174743652
iteration 300, loss 1.6609678268432617
iteration 400, loss 1.5540584325790405
iteration 500, loss 1.6318846940994263
iteration 600, loss 1.580400824546814
iteration 700, loss 1.618664026260376
iteration 800, loss 1.60441255569458
iteration 0, loss 1.5316213369369507
iteration 100, loss 1.647002100944519
iteration 200, loss 1.6721587181091309
iteration 300, loss 1.6293500661849976
iteration 400, loss 1.5888645648956299
iteration 500, loss 1.593579649925232
iteration 600, loss 1.5986406803131104
iteration 700, loss 1.6518946886062622
iteration 800, loss 1.612447738647461
iteration 0, loss 1.6288456916809082
iteration 100, loss 1.629736304283142
iteration 200, loss 1.5897223949432373
iteration 300, loss 1.6408530473709106
iteration 400, loss 1.6222413778305054
iteration 500, loss 1.641347050666809
iteration 600, loss 1.605514645576477
iteration 700, loss 1.600345492362976
iteration 800, loss 1.615082025527954
iteration 0, loss 1.66163170337677
iteration 100, loss 1.6360516548156738
iteration 200, loss 1.6204625368118286
iteration 300, loss 1.593648910522461
iteration 400, loss 1.6458017826080322
iteration 500, loss 1.6347405910491943
iteration 600, loss 1.6068366765975952
iteration 700, loss 1.6182743310928345
iteration 800, loss 1.602677345275879
iteration 0, loss 1.6990201473236084
iteration 100, loss 1.5824167728424072
iteration 200, loss 1.6500900983810425
iteration 300, loss 1.621755838394165
iteration 400, loss 1.6620888710021973
iteration 500, loss 1.5993504524230957
iteration 600, loss 1.594261646270752
iteration 700, loss 1.6178171634674072
iteration 800, loss 1.6615722179412842
iteration 0, loss 1.580718755722046
iteration 100, loss 1.598865032196045
iteration 200, loss 1.6389350891113281
iteration 300, loss 1.648967981338501
iteration 400, loss 1.565246343612671
iteration 500, loss 1.5916856527328491
iteration 600, loss 1.6202901601791382
iteration 700, loss 1.6331251859664917
iteration 800, loss 1.6054794788360596
iteration 0, loss 1.673978328704834
iteration 100, loss 1.6117802858352661
iteration 200, loss 1.6384905576705933
iteration 300, loss 1.584937572479248
iteration 400, loss 1.6083729267120361
iteration 500, loss 1.6549891233444214
iteration 600, loss 1.6108450889587402
iteration 700, loss 1.5858700275421143
iteration 800, loss 1.6358929872512817
iteration 0, loss 1.6437880992889404
iteration 100, loss 1.609647274017334
iteration 200, loss 1.5836806297302246
iteration 300, loss 1.6388664245605469
iteration 400, loss 1.6583995819091797
iteration 500, loss 1.6908705234527588
iteration 600, loss 1.6550226211547852
iteration 700, loss 1.6126893758773804
iteration 800, loss 1.5538959503173828
iteration 0, loss 1.622648000717163
iteration 100, loss 1.6763989925384521
iteration 200, loss 1.6396454572677612
iteration 300, loss 1.6163110733032227
iteration 400, loss 1.5662955045700073
iteration 500, loss 1.612277626991272
iteration 600, loss 1.59392249584198
iteration 700, loss 1.585039496421814
iteration 800, loss 1.5728169679641724
iteration 0, loss 1.6653547286987305
iteration 100, loss 1.696474552154541
iteration 200, loss 1.630587100982666
iteration 300, loss 1.6007609367370605
iteration 400, loss 1.6234333515167236
iteration 500, loss 1.6379486322402954
iteration 600, loss 1.6474575996398926
iteration 700, loss 1.5942292213439941
iteration 800, loss 1.667174220085144
iteration 0, loss 1.6590434312820435
iteration 100, loss 1.6192859411239624
iteration 200, loss 1.6219511032104492
iteration 300, loss 1.6501243114471436
iteration 400, loss 1.6357557773590088
iteration 500, loss 1.5813355445861816
iteration 600, loss 1.5413051843643188
iteration 700, loss 1.654637098312378
iteration 800, loss 1.562209129333496
iteration 0, loss 1.6240122318267822
iteration 100, loss 1.5881810188293457
iteration 200, loss 1.7078912258148193
iteration 300, loss 1.6343300342559814
iteration 400, loss 1.5664385557174683
iteration 500, loss 1.6491692066192627
iteration 600, loss 1.6455578804016113
iteration 700, loss 1.6035492420196533
iteration 800, loss 1.702068567276001
iteration 0, loss 1.6627137660980225
iteration 100, loss 1.5848143100738525
iteration 200, loss 1.6231887340545654
iteration 300, loss 1.608946681022644
iteration 400, loss 1.6119277477264404
iteration 500, loss 1.6330575942993164
iteration 600, loss 1.5926697254180908
iteration 700, loss 1.6781482696533203
iteration 800, loss 1.6888240575790405
iteration 0, loss 1.699391484260559
iteration 100, loss 1.5776355266571045
iteration 200, loss 1.6207339763641357
iteration 300, loss 1.575866937637329
iteration 400, loss 1.5648391246795654
iteration 500, loss 1.636945366859436
iteration 600, loss 1.6423146724700928
iteration 700, loss 1.6135419607162476
iteration 800, loss 1.6484837532043457
iteration 0, loss 1.6321367025375366
iteration 100, loss 1.6190074682235718
iteration 200, loss 1.6153600215911865
iteration 300, loss 1.6451267004013062
iteration 400, loss 1.5985689163208008
iteration 500, loss 1.6119585037231445
iteration 600, loss 1.6361006498336792
iteration 700, loss 1.6397145986557007
iteration 800, loss 1.690289855003357
iteration 0, loss 1.5937039852142334
iteration 100, loss 1.6189557313919067
iteration 200, loss 1.628976821899414
iteration 300, loss 1.6488360166549683
iteration 400, loss 1.6295772790908813
iteration 500, loss 1.671082615852356
iteration 600, loss 1.6443383693695068
iteration 700, loss 1.6096891164779663
iteration 800, loss 1.6563963890075684
iteration 0, loss 1.6499674320220947
iteration 100, loss 1.6273632049560547
iteration 200, loss 1.6227319240570068
iteration 300, loss 1.6009202003479004
iteration 400, loss 1.591515064239502
iteration 500, loss 1.607408881187439
iteration 600, loss 1.6103861331939697
iteration 700, loss 1.6258219480514526
iteration 800, loss 1.6081795692443848
iteration 0, loss 1.6028721332550049
iteration 100, loss 1.619868516921997
iteration 200, loss 1.673965573310852
iteration 300, loss 1.6220256090164185
iteration 400, loss 1.6488555669784546
iteration 500, loss 1.625083327293396
iteration 600, loss 1.5798182487487793
iteration 700, loss 1.661376953125
iteration 800, loss 1.6913021802902222
iteration 0, loss 1.6361298561096191
iteration 100, loss 1.6171787977218628
iteration 200, loss 1.6227571964263916
iteration 300, loss 1.6225193738937378
iteration 400, loss 1.6068334579467773
iteration 500, loss 1.5914417505264282
iteration 600, loss 1.6519240140914917
iteration 700, loss 1.6320488452911377
iteration 800, loss 1.5927826166152954
iteration 0, loss 1.618720531463623
iteration 100, loss 1.6038936376571655
iteration 200, loss 1.6248902082443237
iteration 300, loss 1.6051769256591797
iteration 400, loss 1.638708233833313
iteration 500, loss 1.620300531387329
iteration 600, loss 1.67384934425354
iteration 700, loss 1.5716869831085205
iteration 800, loss 1.619640588760376
iteration 0, loss 1.590437412261963
iteration 100, loss 1.616312026977539
iteration 200, loss 1.6256605386734009
iteration 300, loss 1.7395821809768677
iteration 400, loss 1.653078556060791
iteration 500, loss 1.6077980995178223
iteration 600, loss 1.579932451248169
iteration 700, loss 1.6092625856399536
iteration 800, loss 1.597409963607788
iteration 0, loss 1.6098262071609497
iteration 100, loss 1.6133836507797241
iteration 200, loss 1.6441925764083862
iteration 300, loss 1.682784080505371
iteration 400, loss 1.6460298299789429
iteration 500, loss 1.6673986911773682
iteration 600, loss 1.6231645345687866
iteration 700, loss 1.633923888206482
iteration 800, loss 1.668593406677246
iteration 0, loss 1.5879672765731812
iteration 100, loss 1.5731035470962524
iteration 200, loss 1.6181402206420898
iteration 300, loss 1.5870048999786377
iteration 400, loss 1.637580394744873
iteration 500, loss 1.5769606828689575
iteration 600, loss 1.5892362594604492
iteration 700, loss 1.625549077987671
iteration 800, loss 1.6117374897003174
iteration 0, loss 1.676607370376587
iteration 100, loss 1.6157217025756836
iteration 200, loss 1.5590740442276
iteration 300, loss 1.6207523345947266
iteration 400, loss 1.6142362356185913
iteration 500, loss 1.5905256271362305
iteration 600, loss 1.6767191886901855
iteration 700, loss 1.591475486755371
iteration 800, loss 1.6029943227767944
iteration 0, loss 1.6303458213806152
iteration 100, loss 1.6499934196472168
iteration 200, loss 1.6365697383880615
iteration 300, loss 1.6167830228805542
iteration 400, loss 1.569016456604004
iteration 500, loss 1.6412347555160522
iteration 600, loss 1.623750925064087
iteration 700, loss 1.5955564975738525
iteration 800, loss 1.6415343284606934
iteration 0, loss 1.6342686414718628
iteration 100, loss 1.6040654182434082
iteration 200, loss 1.6387234926223755
iteration 300, loss 1.5943838357925415
iteration 400, loss 1.6200002431869507
iteration 500, loss 1.635280728340149
iteration 600, loss 1.583655595779419
iteration 700, loss 1.614220380783081
iteration 800, loss 1.6446877717971802
iteration 0, loss 1.5688406229019165
iteration 100, loss 1.616764783859253
iteration 200, loss 1.6317059993743896
iteration 300, loss 1.5864150524139404
iteration 400, loss 1.583191990852356
iteration 500, loss 1.6286242008209229
iteration 600, loss 1.5635712146759033
iteration 700, loss 1.6152303218841553
iteration 800, loss 1.6220436096191406
iteration 0, loss 1.6220786571502686
iteration 100, loss 1.6073591709136963
iteration 200, loss 1.6787562370300293
iteration 300, loss 1.6464593410491943
iteration 400, loss 1.6315467357635498
iteration 500, loss 1.6584751605987549
iteration 600, loss 1.6102380752563477
iteration 700, loss 1.6405866146087646
iteration 800, loss 1.6439409255981445
iteration 0, loss 1.5812671184539795
iteration 100, loss 1.6019936800003052
iteration 200, loss 1.5742878913879395
iteration 300, loss 1.6224281787872314
iteration 400, loss 1.6259121894836426
iteration 500, loss 1.6188058853149414
iteration 600, loss 1.5976482629776
iteration 700, loss 1.6322590112686157
iteration 800, loss 1.5877933502197266
iteration 0, loss 1.6552132368087769
iteration 100, loss 1.5659923553466797
iteration 200, loss 1.6472972631454468
iteration 300, loss 1.6754001379013062
iteration 400, loss 1.6547906398773193
iteration 500, loss 1.6383929252624512
iteration 600, loss 1.581088900566101
iteration 700, loss 1.607662320137024
iteration 800, loss 1.6283595561981201
iteration 0, loss 1.672257900238037
iteration 100, loss 1.6866745948791504
iteration 200, loss 1.6423826217651367
iteration 300, loss 1.5943050384521484
iteration 400, loss 1.588662028312683
iteration 500, loss 1.6662886142730713
iteration 600, loss 1.575179100036621
iteration 700, loss 1.5754326581954956
iteration 800, loss 1.5992940664291382
iteration 0, loss 1.6423922777175903
iteration 100, loss 1.602607011795044
iteration 200, loss 1.6766357421875
iteration 300, loss 1.654090166091919
iteration 400, loss 1.6141963005065918
iteration 500, loss 1.657011866569519
iteration 600, loss 1.6318321228027344
iteration 700, loss 1.597621202468872
iteration 800, loss 1.6231707334518433
iteration 0, loss 1.6550828218460083
iteration 100, loss 1.6378533840179443
iteration 200, loss 1.5890253782272339
iteration 300, loss 1.6317354440689087
iteration 400, loss 1.6662112474441528
iteration 500, loss 1.6421352624893188
iteration 600, loss 1.616736650466919
iteration 700, loss 1.6697185039520264
iteration 800, loss 1.6279829740524292
iteration 0, loss 1.6103681325912476
iteration 100, loss 1.6147035360336304
iteration 200, loss 1.6490434408187866
iteration 300, loss 1.7105857133865356
iteration 400, loss 1.565168023109436
iteration 500, loss 1.6530821323394775
iteration 600, loss 1.5723508596420288
iteration 700, loss 1.6448570489883423
iteration 800, loss 1.6243857145309448
iteration 0, loss 1.581125020980835
iteration 100, loss 1.5631297826766968
iteration 200, loss 1.6239850521087646
iteration 300, loss 1.6176468133926392
iteration 400, loss 1.5938661098480225
iteration 500, loss 1.6103794574737549
iteration 600, loss 1.5919042825698853
iteration 700, loss 1.663138747215271
iteration 800, loss 1.6311578750610352
iteration 0, loss 1.6036248207092285
iteration 100, loss 1.610840916633606
iteration 200, loss 1.5701045989990234
iteration 300, loss 1.5895583629608154
iteration 400, loss 1.5687307119369507
iteration 500, loss 1.624405860900879
iteration 600, loss 1.564249873161316
iteration 700, loss 1.5896551609039307
iteration 800, loss 1.654587745666504
iteration 0, loss 1.654551386833191
iteration 100, loss 1.6049959659576416
iteration 200, loss 1.6231412887573242
iteration 300, loss 1.6140661239624023
iteration 400, loss 1.657301664352417
iteration 500, loss 1.595412254333496
iteration 600, loss 1.5986664295196533
iteration 700, loss 1.682344913482666
iteration 800, loss 1.5825250148773193
iteration 0, loss 1.6186202764511108
iteration 100, loss 1.6194117069244385
iteration 200, loss 1.58145010471344
iteration 300, loss 1.6268725395202637
iteration 400, loss 1.713310718536377
iteration 500, loss 1.646746039390564
iteration 600, loss 1.607590675354004
iteration 700, loss 1.5902400016784668
iteration 800, loss 1.6094307899475098
iteration 0, loss 1.5897681713104248
iteration 100, loss 1.6166870594024658
iteration 200, loss 1.6139941215515137
iteration 300, loss 1.5712581872940063
iteration 400, loss 1.5832537412643433
iteration 500, loss 1.5900675058364868
iteration 600, loss 1.609825611114502
iteration 700, loss 1.6929877996444702
iteration 800, loss 1.577938199043274
iteration 0, loss 1.6279858350753784
iteration 100, loss 1.6052595376968384
iteration 200, loss 1.597887635231018
iteration 300, loss 1.6107501983642578
iteration 400, loss 1.608066201210022
iteration 500, loss 1.671209454536438
iteration 600, loss 1.6863330602645874
iteration 700, loss 1.5840370655059814
iteration 800, loss 1.5988829135894775
iteration 0, loss 1.6168156862258911
iteration 100, loss 1.6515846252441406
iteration 200, loss 1.5798077583312988
iteration 300, loss 1.638185739517212
iteration 400, loss 1.6171926259994507
iteration 500, loss 1.6363564729690552
iteration 600, loss 1.636541485786438
iteration 700, loss 1.5817203521728516
iteration 800, loss 1.6081299781799316
fold 3 accuracy: 0.811
iteration 0, loss 1.5759152173995972
iteration 100, loss 1.6388534307479858
iteration 200, loss 1.6270043849945068
iteration 300, loss 1.6064249277114868
iteration 400, loss 1.6135159730911255
iteration 500, loss 1.6384477615356445
iteration 600, loss 1.6232973337173462
iteration 700, loss 1.5789828300476074
iteration 800, loss 1.649469017982483
iteration 0, loss 1.6361360549926758
iteration 100, loss 1.6709662675857544
iteration 200, loss 1.6047263145446777
iteration 300, loss 1.5681122541427612
iteration 400, loss 1.6407010555267334
iteration 500, loss 1.6698448657989502
iteration 600, loss 1.5699373483657837
iteration 700, loss 1.6317764520645142
iteration 800, loss 1.6454752683639526
iteration 0, loss 1.6237999200820923
iteration 100, loss 1.69033944606781
iteration 200, loss 1.6113954782485962
iteration 300, loss 1.598357081413269
iteration 400, loss 1.6177955865859985
iteration 500, loss 1.5816856622695923
iteration 600, loss 1.6109035015106201
iteration 700, loss 1.65606689453125
iteration 800, loss 1.6206402778625488
iteration 0, loss 1.686145305633545
iteration 100, loss 1.6427273750305176
iteration 200, loss 1.5802346467971802
iteration 300, loss 1.678886890411377
iteration 400, loss 1.6192151308059692
iteration 500, loss 1.615934133529663
iteration 600, loss 1.6161918640136719
iteration 700, loss 1.6129870414733887
iteration 800, loss 1.617456316947937
iteration 0, loss 1.5991380214691162
iteration 100, loss 1.586486577987671
iteration 200, loss 1.6510050296783447
iteration 300, loss 1.6525187492370605
iteration 400, loss 1.6589657068252563
iteration 500, loss 1.612391471862793
iteration 600, loss 1.6397138833999634
iteration 700, loss 1.6515247821807861
iteration 800, loss 1.6366748809814453
iteration 0, loss 1.6186246871948242
iteration 100, loss 1.6726410388946533
iteration 200, loss 1.6274127960205078
iteration 300, loss 1.596468210220337
iteration 400, loss 1.5816251039505005
iteration 500, loss 1.5901505947113037
iteration 600, loss 1.6441547870635986
iteration 700, loss 1.6341655254364014
iteration 800, loss 1.6028324365615845
iteration 0, loss 1.6420972347259521
iteration 100, loss 1.6162300109863281
iteration 200, loss 1.6131418943405151
iteration 300, loss 1.6113771200180054
iteration 400, loss 1.5894893407821655
iteration 500, loss 1.618810772895813
iteration 600, loss 1.6046128273010254
iteration 700, loss 1.5939282178878784
iteration 800, loss 1.639207363128662
iteration 0, loss 1.631974458694458
iteration 100, loss 1.6642729043960571
iteration 200, loss 1.70052170753479
iteration 300, loss 1.6223597526550293
iteration 400, loss 1.6012370586395264
iteration 500, loss 1.589669108390808
iteration 600, loss 1.6457546949386597
iteration 700, loss 1.6206321716308594
iteration 800, loss 1.6597009897232056
iteration 0, loss 1.6063116788864136
iteration 100, loss 1.627097249031067
iteration 200, loss 1.5910942554473877
iteration 300, loss 1.6451700925827026
iteration 400, loss 1.5762145519256592
iteration 500, loss 1.6945120096206665
iteration 600, loss 1.6441210508346558
iteration 700, loss 1.657253623008728
iteration 800, loss 1.5941879749298096
iteration 0, loss 1.615930199623108
iteration 100, loss 1.623335838317871
iteration 200, loss 1.6113383769989014
iteration 300, loss 1.6166877746582031
iteration 400, loss 1.6315308809280396
iteration 500, loss 1.5986109972000122
iteration 600, loss 1.6204451322555542
iteration 700, loss 1.6427946090698242
iteration 800, loss 1.5993012189865112
iteration 0, loss 1.6630632877349854
iteration 100, loss 1.6243189573287964
iteration 200, loss 1.6220362186431885
iteration 300, loss 1.5900369882583618
iteration 400, loss 1.5788360834121704
iteration 500, loss 1.6148290634155273
iteration 600, loss 1.5559486150741577
iteration 700, loss 1.6157704591751099
iteration 800, loss 1.6378321647644043
iteration 0, loss 1.6605502367019653
iteration 100, loss 1.631257176399231
iteration 200, loss 1.6677273511886597
iteration 300, loss 1.59852933883667
iteration 400, loss 1.618369460105896
iteration 500, loss 1.5992767810821533
iteration 600, loss 1.6175227165222168
iteration 700, loss 1.61544930934906
iteration 800, loss 1.6117024421691895
iteration 0, loss 1.6207388639450073
iteration 100, loss 1.5909281969070435
iteration 200, loss 1.6780672073364258
iteration 300, loss 1.6697142124176025
iteration 400, loss 1.5904310941696167
iteration 500, loss 1.6826763153076172
iteration 600, loss 1.6131261587142944
iteration 700, loss 1.5666987895965576
iteration 800, loss 1.6439539194107056
iteration 0, loss 1.6865376234054565
iteration 100, loss 1.5660688877105713
iteration 200, loss 1.6385868787765503
iteration 300, loss 1.603097915649414
iteration 400, loss 1.5853034257888794
iteration 500, loss 1.641121745109558
iteration 600, loss 1.6102606058120728
iteration 700, loss 1.6079232692718506
iteration 800, loss 1.608270525932312
iteration 0, loss 1.614429235458374
iteration 100, loss 1.5779465436935425
iteration 200, loss 1.612539529800415
iteration 300, loss 1.6853570938110352
iteration 400, loss 1.61082923412323
iteration 500, loss 1.5628409385681152
iteration 600, loss 1.6392407417297363
iteration 700, loss 1.6884664297103882
iteration 800, loss 1.618422031402588
iteration 0, loss 1.6488300561904907
iteration 100, loss 1.5784794092178345
iteration 200, loss 1.6481709480285645
iteration 300, loss 1.5982733964920044
iteration 400, loss 1.639023780822754
iteration 500, loss 1.564577341079712
iteration 600, loss 1.6932170391082764
iteration 700, loss 1.6376240253448486
iteration 800, loss 1.619136095046997
iteration 0, loss 1.6491405963897705
iteration 100, loss 1.6210334300994873
iteration 200, loss 1.6671273708343506
iteration 300, loss 1.6660619974136353
iteration 400, loss 1.5738645792007446
iteration 500, loss 1.6092820167541504
iteration 600, loss 1.7198067903518677
iteration 700, loss 1.6003249883651733
iteration 800, loss 1.6593809127807617
iteration 0, loss 1.5775526762008667
iteration 100, loss 1.6069281101226807
iteration 200, loss 1.5916526317596436
iteration 300, loss 1.6205825805664062
iteration 400, loss 1.6510149240493774
iteration 500, loss 1.6782233715057373
iteration 600, loss 1.6212340593338013
iteration 700, loss 1.5966731309890747
iteration 800, loss 1.6335008144378662
iteration 0, loss 1.653399109840393
iteration 100, loss 1.5582239627838135
iteration 200, loss 1.6576118469238281
iteration 300, loss 1.5919528007507324
iteration 400, loss 1.6372857093811035
iteration 500, loss 1.6547081470489502
iteration 600, loss 1.602341890335083
iteration 700, loss 1.6688776016235352
iteration 800, loss 1.6030532121658325
iteration 0, loss 1.6201428174972534
iteration 100, loss 1.659348964691162
iteration 200, loss 1.6866106986999512
iteration 300, loss 1.693703532218933
iteration 400, loss 1.5513629913330078
iteration 500, loss 1.6376376152038574
iteration 600, loss 1.6482875347137451
iteration 700, loss 1.5837819576263428
iteration 800, loss 1.6518304347991943
iteration 0, loss 1.64586341381073
iteration 100, loss 1.6059000492095947
iteration 200, loss 1.5583616495132446
iteration 300, loss 1.597402572631836
iteration 400, loss 1.6178159713745117
iteration 500, loss 1.629895806312561
iteration 600, loss 1.6328368186950684
iteration 700, loss 1.6682977676391602
iteration 800, loss 1.6043009757995605
iteration 0, loss 1.6476094722747803
iteration 100, loss 1.5926012992858887
iteration 200, loss 1.6600900888442993
iteration 300, loss 1.590390682220459
iteration 400, loss 1.5842454433441162
iteration 500, loss 1.6061407327651978
iteration 600, loss 1.7305657863616943
iteration 700, loss 1.5824557542800903
iteration 800, loss 1.6357842683792114
iteration 0, loss 1.6228346824645996
iteration 100, loss 1.670470118522644
iteration 200, loss 1.630423665046692
iteration 300, loss 1.5820916891098022
iteration 400, loss 1.6240758895874023
iteration 500, loss 1.6766481399536133
iteration 600, loss 1.623199701309204
iteration 700, loss 1.6289986371994019
iteration 800, loss 1.6218955516815186
iteration 0, loss 1.6178677082061768
iteration 100, loss 1.6241284608840942
iteration 200, loss 1.6197283267974854
iteration 300, loss 1.6533422470092773
iteration 400, loss 1.6175925731658936
iteration 500, loss 1.6269731521606445
iteration 600, loss 1.609879970550537
iteration 700, loss 1.6626144647598267
iteration 800, loss 1.6243464946746826
iteration 0, loss 1.6665117740631104
iteration 100, loss 1.589868426322937
iteration 200, loss 1.6131798028945923
iteration 300, loss 1.6658130884170532
iteration 400, loss 1.625236988067627
iteration 500, loss 1.6122875213623047
iteration 600, loss 1.6234595775604248
iteration 700, loss 1.5828115940093994
iteration 800, loss 1.6652296781539917
iteration 0, loss 1.5802059173583984
iteration 100, loss 1.5822561979293823
iteration 200, loss 1.6417737007141113
iteration 300, loss 1.6410905122756958
iteration 400, loss 1.619847059249878
iteration 500, loss 1.6030563116073608
iteration 600, loss 1.6301745176315308
iteration 700, loss 1.675183892250061
iteration 800, loss 1.629693865776062
iteration 0, loss 1.624022126197815
iteration 100, loss 1.6656494140625
iteration 200, loss 1.6085542440414429
iteration 300, loss 1.601236343383789
iteration 400, loss 1.630812406539917
iteration 500, loss 1.6580053567886353
iteration 600, loss 1.616807460784912
iteration 700, loss 1.6566792726516724
iteration 800, loss 1.6529262065887451
iteration 0, loss 1.6389498710632324
iteration 100, loss 1.6085700988769531
iteration 200, loss 1.6859056949615479
iteration 300, loss 1.542792558670044
iteration 400, loss 1.6205201148986816
iteration 500, loss 1.5966259241104126
iteration 600, loss 1.5938944816589355
iteration 700, loss 1.6390289068222046
iteration 800, loss 1.602097511291504
iteration 0, loss 1.6563290357589722
iteration 100, loss 1.5997005701065063
iteration 200, loss 1.6131985187530518
iteration 300, loss 1.607938289642334
iteration 400, loss 1.6043368577957153
iteration 500, loss 1.6322875022888184
iteration 600, loss 1.607518196105957
iteration 700, loss 1.6146180629730225
iteration 800, loss 1.5934900045394897
iteration 0, loss 1.6264545917510986
iteration 100, loss 1.6138169765472412
iteration 200, loss 1.5753065347671509
iteration 300, loss 1.663570523262024
iteration 400, loss 1.6511800289154053
iteration 500, loss 1.6190299987792969
iteration 600, loss 1.7028067111968994
iteration 700, loss 1.6391938924789429
iteration 800, loss 1.6943806409835815
iteration 0, loss 1.6712907552719116
iteration 100, loss 1.5949403047561646
iteration 200, loss 1.5970004796981812
iteration 300, loss 1.630439043045044
iteration 400, loss 1.5865157842636108
iteration 500, loss 1.6036949157714844
iteration 600, loss 1.6319197416305542
iteration 700, loss 1.6337943077087402
iteration 800, loss 1.6035751104354858
iteration 0, loss 1.5934911966323853
iteration 100, loss 1.6207741498947144
iteration 200, loss 1.6265571117401123
iteration 300, loss 1.6180071830749512
iteration 400, loss 1.5665910243988037
iteration 500, loss 1.662379264831543
iteration 600, loss 1.6271594762802124
iteration 700, loss 1.6465204954147339
iteration 800, loss 1.6537879705429077
iteration 0, loss 1.6289832592010498
iteration 100, loss 1.573403000831604
iteration 200, loss 1.639748454093933
iteration 300, loss 1.653946876525879
iteration 400, loss 1.6062567234039307
iteration 500, loss 1.5899208784103394
iteration 600, loss 1.684407353401184
iteration 700, loss 1.657590627670288
iteration 800, loss 1.6628941297531128
iteration 0, loss 1.58597731590271
iteration 100, loss 1.624361276626587
iteration 200, loss 1.6279139518737793
iteration 300, loss 1.6507936716079712
iteration 400, loss 1.6023988723754883
iteration 500, loss 1.5747785568237305
iteration 600, loss 1.6136250495910645
iteration 700, loss 1.6006829738616943
iteration 800, loss 1.6788997650146484
iteration 0, loss 1.6663283109664917
iteration 100, loss 1.7038605213165283
iteration 200, loss 1.6306205987930298
iteration 300, loss 1.614831566810608
iteration 400, loss 1.5725210905075073
iteration 500, loss 1.6329448223114014
iteration 600, loss 1.6729075908660889
iteration 700, loss 1.5937803983688354
iteration 800, loss 1.6813210248947144
iteration 0, loss 1.6180403232574463
iteration 100, loss 1.643681287765503
iteration 200, loss 1.6387624740600586
iteration 300, loss 1.649986982345581
iteration 400, loss 1.6683719158172607
iteration 500, loss 1.5739110708236694
iteration 600, loss 1.6298781633377075
iteration 700, loss 1.6147412061691284
iteration 800, loss 1.598209261894226
iteration 0, loss 1.6503599882125854
iteration 100, loss 1.597468376159668
iteration 200, loss 1.5959211587905884
iteration 300, loss 1.648297905921936
iteration 400, loss 1.5890402793884277
iteration 500, loss 1.618040680885315
iteration 600, loss 1.5438265800476074
iteration 700, loss 1.6615487337112427
iteration 800, loss 1.6079065799713135
iteration 0, loss 1.594802737236023
iteration 100, loss 1.6212692260742188
iteration 200, loss 1.6858781576156616
iteration 300, loss 1.6335006952285767
iteration 400, loss 1.6234416961669922
iteration 500, loss 1.5868923664093018
iteration 600, loss 1.6067086458206177
iteration 700, loss 1.6258543729782104
iteration 800, loss 1.6326981782913208
iteration 0, loss 1.658309817314148
iteration 100, loss 1.694468379020691
iteration 200, loss 1.621163010597229
iteration 300, loss 1.5966949462890625
iteration 400, loss 1.6274561882019043
iteration 500, loss 1.5791958570480347
iteration 600, loss 1.6391186714172363
iteration 700, loss 1.632595419883728
iteration 800, loss 1.6297870874404907
iteration 0, loss 1.6396684646606445
iteration 100, loss 1.6671180725097656
iteration 200, loss 1.582936406135559
iteration 300, loss 1.6311266422271729
iteration 400, loss 1.6530606746673584
iteration 500, loss 1.5976417064666748
iteration 600, loss 1.6238123178482056
iteration 700, loss 1.6002349853515625
iteration 800, loss 1.6861531734466553
iteration 0, loss 1.6018295288085938
iteration 100, loss 1.6137691736221313
iteration 200, loss 1.5727471113204956
iteration 300, loss 1.730804204940796
iteration 400, loss 1.6430652141571045
iteration 500, loss 1.5891550779342651
iteration 600, loss 1.6322273015975952
iteration 700, loss 1.644810676574707
iteration 800, loss 1.6218376159667969
iteration 0, loss 1.604935646057129
iteration 100, loss 1.5780376195907593
iteration 200, loss 1.626185417175293
iteration 300, loss 1.5996856689453125
iteration 400, loss 1.5969352722167969
iteration 500, loss 1.6357063055038452
iteration 600, loss 1.660828948020935
iteration 700, loss 1.661488652229309
iteration 800, loss 1.6458379030227661
iteration 0, loss 1.6283724308013916
iteration 100, loss 1.5926034450531006
iteration 200, loss 1.6147356033325195
iteration 300, loss 1.579127311706543
iteration 400, loss 1.6481671333312988
iteration 500, loss 1.6260324716567993
iteration 600, loss 1.6274992227554321
iteration 700, loss 1.627483606338501
iteration 800, loss 1.557108759880066
iteration 0, loss 1.647916316986084
iteration 100, loss 1.632175087928772
iteration 200, loss 1.66425359249115
iteration 300, loss 1.6686184406280518
iteration 400, loss 1.6141436100006104
iteration 500, loss 1.5911253690719604
iteration 600, loss 1.6368284225463867
iteration 700, loss 1.577446699142456
iteration 800, loss 1.6175503730773926
iteration 0, loss 1.6168651580810547
iteration 100, loss 1.5857412815093994
iteration 200, loss 1.5783741474151611
iteration 300, loss 1.6330960988998413
iteration 400, loss 1.5807653665542603
iteration 500, loss 1.6817021369934082
iteration 600, loss 1.630070447921753
iteration 700, loss 1.580891489982605
iteration 800, loss 1.6179587841033936
iteration 0, loss 1.6956394910812378
iteration 100, loss 1.6202595233917236
iteration 200, loss 1.6671258211135864
iteration 300, loss 1.6566649675369263
iteration 400, loss 1.6189815998077393
iteration 500, loss 1.6868913173675537
iteration 600, loss 1.6660118103027344
iteration 700, loss 1.6800918579101562
iteration 800, loss 1.5937634706497192
iteration 0, loss 1.6332117319107056
iteration 100, loss 1.5643092393875122
iteration 200, loss 1.631518840789795
iteration 300, loss 1.6442506313323975
iteration 400, loss 1.6146018505096436
iteration 500, loss 1.657127857208252
iteration 600, loss 1.6784394979476929
iteration 700, loss 1.6244298219680786
iteration 800, loss 1.5790596008300781
iteration 0, loss 1.6176953315734863
iteration 100, loss 1.6204893589019775
iteration 200, loss 1.610405445098877
iteration 300, loss 1.5733965635299683
iteration 400, loss 1.644424319267273
iteration 500, loss 1.576511263847351
iteration 600, loss 1.617956280708313
iteration 700, loss 1.6245168447494507
iteration 800, loss 1.6031973361968994
iteration 0, loss 1.6023694276809692
iteration 100, loss 1.6576857566833496
iteration 200, loss 1.6268569231033325
iteration 300, loss 1.635892391204834
iteration 400, loss 1.657843828201294
iteration 500, loss 1.656293272972107
iteration 600, loss 1.6607187986373901
iteration 700, loss 1.5775810480117798
iteration 800, loss 1.5905156135559082
iteration 0, loss 1.6049312353134155
iteration 100, loss 1.6398484706878662
iteration 200, loss 1.6360161304473877
iteration 300, loss 1.6174465417861938
iteration 400, loss 1.585924506187439
iteration 500, loss 1.569070816040039
iteration 600, loss 1.6178252696990967
iteration 700, loss 1.5893802642822266
iteration 800, loss 1.5659863948822021
fold 4 accuracy: 0.8066428571428571
[2024-02-29 03:17:56,510] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-29 03:17:56,512] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         654     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       840 MACs
fwd flops per GPU:                                                      1.68 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.68 K  
fwd latency:                                                            538.35 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.12 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '654'}
    MACs        - {'KronLayer': '840 MACs'}
    fwd latency - {'KronLayer': '538.35 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  654 = 100% Params, 840 MACs = 100% MACs, 538.35 us = 100% latency, 3.12 MFLOPS
  (kronlinear): KronLinear(654 = 100% Params, 840 MACs = 100% MACs, 447.27 us = 83.08% latency, 3.76 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 27.89 us = 5.18% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-29 03:17:56,519] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.68 K 654
iteration 0, loss 2.3037707805633545
iteration 100, loss 2.176651954650879
iteration 200, loss 2.02047061920166
iteration 300, loss 1.9446115493774414
iteration 400, loss 1.8952646255493164
iteration 500, loss 1.8971093893051147
iteration 600, loss 1.8141804933547974
iteration 700, loss 1.8208184242248535
iteration 800, loss 1.8582615852355957
iteration 0, loss 1.7829313278198242
iteration 100, loss 1.8218839168548584
iteration 200, loss 1.7875562906265259
iteration 300, loss 1.772810935974121
iteration 400, loss 1.7403199672698975
iteration 500, loss 1.735310435295105
iteration 600, loss 1.7804734706878662
iteration 700, loss 1.764426827430725
iteration 800, loss 1.742995023727417
iteration 0, loss 1.780617594718933
iteration 100, loss 1.7186102867126465
iteration 200, loss 1.7881330251693726
iteration 300, loss 1.8174455165863037
iteration 400, loss 1.7341909408569336
iteration 500, loss 1.7824894189834595
iteration 600, loss 1.7330515384674072
iteration 700, loss 1.745132565498352
iteration 800, loss 1.7167096138000488
iteration 0, loss 1.7377294301986694
iteration 100, loss 1.7665250301361084
iteration 200, loss 1.7015477418899536
iteration 300, loss 1.721421480178833
iteration 400, loss 1.7132197618484497
iteration 500, loss 1.7400767803192139
iteration 600, loss 1.6869620084762573
iteration 700, loss 1.7263352870941162
iteration 800, loss 1.7090693712234497
iteration 0, loss 1.698006272315979
iteration 100, loss 1.727712869644165
iteration 200, loss 1.7083392143249512
iteration 300, loss 1.737337589263916
iteration 400, loss 1.6957625150680542
iteration 500, loss 1.72988760471344
iteration 600, loss 1.6640571355819702
iteration 700, loss 1.680452823638916
iteration 800, loss 1.7426016330718994
iteration 0, loss 1.7102824449539185
iteration 100, loss 1.7339787483215332
iteration 200, loss 1.671025276184082
iteration 300, loss 1.7313835620880127
iteration 400, loss 1.7207952737808228
iteration 500, loss 1.6398857831954956
iteration 600, loss 1.680015206336975
iteration 700, loss 1.7351515293121338
iteration 800, loss 1.6381592750549316
iteration 0, loss 1.7355091571807861
iteration 100, loss 1.7112438678741455
iteration 200, loss 1.7028148174285889
iteration 300, loss 1.6754162311553955
iteration 400, loss 1.6727749109268188
iteration 500, loss 1.6693657636642456
iteration 600, loss 1.7090213298797607
iteration 700, loss 1.7292560338974
iteration 800, loss 1.7573288679122925
iteration 0, loss 1.7630354166030884
iteration 100, loss 1.6826503276824951
iteration 200, loss 1.7298295497894287
iteration 300, loss 1.6806474924087524
iteration 400, loss 1.648218035697937
iteration 500, loss 1.70363187789917
iteration 600, loss 1.6819169521331787
iteration 700, loss 1.676375389099121
iteration 800, loss 1.7249455451965332
iteration 0, loss 1.7118563652038574
iteration 100, loss 1.6293729543685913
iteration 200, loss 1.7146642208099365
iteration 300, loss 1.606397032737732
iteration 400, loss 1.6816868782043457
iteration 500, loss 1.671074390411377
iteration 600, loss 1.650666356086731
iteration 700, loss 1.647891879081726
iteration 800, loss 1.7173376083374023
iteration 0, loss 1.656517505645752
iteration 100, loss 1.606446623802185
iteration 200, loss 1.7157272100448608
iteration 300, loss 1.6502912044525146
iteration 400, loss 1.6545976400375366
iteration 500, loss 1.6732399463653564
iteration 600, loss 1.6354200839996338
iteration 700, loss 1.6656922101974487
iteration 800, loss 1.6435376405715942
iteration 0, loss 1.6468385457992554
iteration 100, loss 1.6620886325836182
iteration 200, loss 1.6962475776672363
iteration 300, loss 1.6344050168991089
iteration 400, loss 1.68679940700531
iteration 500, loss 1.7462900876998901
iteration 600, loss 1.682086706161499
iteration 700, loss 1.7157869338989258
iteration 800, loss 1.6628741025924683
iteration 0, loss 1.68001389503479
iteration 100, loss 1.6391689777374268
iteration 200, loss 1.6972229480743408
iteration 300, loss 1.6167011260986328
iteration 400, loss 1.7020384073257446
iteration 500, loss 1.7260441780090332
iteration 600, loss 1.6627368927001953
iteration 700, loss 1.6352288722991943
iteration 800, loss 1.6562559604644775
iteration 0, loss 1.732675552368164
iteration 100, loss 1.671904444694519
iteration 200, loss 1.6249650716781616
iteration 300, loss 1.654304027557373
iteration 400, loss 1.6707818508148193
iteration 500, loss 1.6831066608428955
iteration 600, loss 1.7551708221435547
iteration 700, loss 1.7100152969360352
iteration 800, loss 1.6710838079452515
iteration 0, loss 1.6793774366378784
iteration 100, loss 1.7006150484085083
iteration 200, loss 1.7516591548919678
iteration 300, loss 1.7025654315948486
iteration 400, loss 1.6908206939697266
iteration 500, loss 1.6476200819015503
iteration 600, loss 1.6660230159759521
iteration 700, loss 1.6681946516036987
iteration 800, loss 1.6922253370285034
iteration 0, loss 1.6936315298080444
iteration 100, loss 1.608686923980713
iteration 200, loss 1.6273524761199951
iteration 300, loss 1.6567625999450684
iteration 400, loss 1.6356972455978394
iteration 500, loss 1.6820929050445557
iteration 600, loss 1.5960607528686523
iteration 700, loss 1.7188986539840698
iteration 800, loss 1.782204270362854
iteration 0, loss 1.6588317155838013
iteration 100, loss 1.7054669857025146
iteration 200, loss 1.6531906127929688
iteration 300, loss 1.7265490293502808
iteration 400, loss 1.6494808197021484
iteration 500, loss 1.7148551940917969
iteration 600, loss 1.671109676361084
iteration 700, loss 1.614261507987976
iteration 800, loss 1.7208434343338013
iteration 0, loss 1.638660192489624
iteration 100, loss 1.6246228218078613
iteration 200, loss 1.7040148973464966
iteration 300, loss 1.733768105506897
iteration 400, loss 1.6063337326049805
iteration 500, loss 1.7640596628189087
iteration 600, loss 1.683084487915039
iteration 700, loss 1.6996961832046509
iteration 800, loss 1.6808568239212036
iteration 0, loss 1.6508668661117554
iteration 100, loss 1.6696476936340332
iteration 200, loss 1.6883882284164429
iteration 300, loss 1.7037893533706665
iteration 400, loss 1.5949666500091553
iteration 500, loss 1.704805612564087
iteration 600, loss 1.7091120481491089
iteration 700, loss 1.7144805192947388
iteration 800, loss 1.6604727506637573
iteration 0, loss 1.6359882354736328
iteration 100, loss 1.650818109512329
iteration 200, loss 1.6162012815475464
iteration 300, loss 1.654353380203247
iteration 400, loss 1.6027395725250244
iteration 500, loss 1.6056535243988037
iteration 600, loss 1.6840916872024536
iteration 700, loss 1.6623481512069702
iteration 800, loss 1.6062488555908203
iteration 0, loss 1.710366129875183
iteration 100, loss 1.6082996129989624
iteration 200, loss 1.7131388187408447
iteration 300, loss 1.6346750259399414
iteration 400, loss 1.6959729194641113
iteration 500, loss 1.6701626777648926
iteration 600, loss 1.6721787452697754
iteration 700, loss 1.6227599382400513
iteration 800, loss 1.7051697969436646
iteration 0, loss 1.7071787118911743
iteration 100, loss 1.7067956924438477
iteration 200, loss 1.6476950645446777
iteration 300, loss 1.6042693853378296
iteration 400, loss 1.7086586952209473
iteration 500, loss 1.654370903968811
iteration 600, loss 1.6896647214889526
iteration 700, loss 1.6579135656356812
iteration 800, loss 1.6142717599868774
iteration 0, loss 1.6882209777832031
iteration 100, loss 1.6281830072402954
iteration 200, loss 1.7175339460372925
iteration 300, loss 1.6130414009094238
iteration 400, loss 1.6378365755081177
iteration 500, loss 1.6260478496551514
iteration 600, loss 1.613592505455017
iteration 700, loss 1.7092547416687012
iteration 800, loss 1.587085247039795
iteration 0, loss 1.6903095245361328
iteration 100, loss 1.620887279510498
iteration 200, loss 1.6730139255523682
iteration 300, loss 1.6788992881774902
iteration 400, loss 1.7174032926559448
iteration 500, loss 1.6787728071212769
iteration 600, loss 1.6873905658721924
iteration 700, loss 1.673805832862854
iteration 800, loss 1.7028532028198242
iteration 0, loss 1.6116784811019897
iteration 100, loss 1.6802371740341187
iteration 200, loss 1.682076096534729
iteration 300, loss 1.6766093969345093
iteration 400, loss 1.66497004032135
iteration 500, loss 1.6663718223571777
iteration 600, loss 1.6735916137695312
iteration 700, loss 1.7008442878723145
iteration 800, loss 1.6346001625061035
iteration 0, loss 1.6232554912567139
iteration 100, loss 1.693930745124817
iteration 200, loss 1.7008450031280518
iteration 300, loss 1.6672618389129639
iteration 400, loss 1.6652414798736572
iteration 500, loss 1.686545968055725
iteration 600, loss 1.7112553119659424
iteration 700, loss 1.6476613283157349
iteration 800, loss 1.6805728673934937
iteration 0, loss 1.6106213331222534
iteration 100, loss 1.60703706741333
iteration 200, loss 1.6256310939788818
iteration 300, loss 1.6252775192260742
iteration 400, loss 1.7103157043457031
iteration 500, loss 1.6473251581192017
iteration 600, loss 1.6467955112457275
iteration 700, loss 1.6522237062454224
iteration 800, loss 1.6551361083984375
iteration 0, loss 1.7203069925308228
iteration 100, loss 1.6374168395996094
iteration 200, loss 1.6268421411514282
iteration 300, loss 1.6690239906311035
iteration 400, loss 1.6677263975143433
iteration 500, loss 1.6392117738723755
iteration 600, loss 1.657352089881897
iteration 700, loss 1.6622947454452515
iteration 800, loss 1.630185604095459
iteration 0, loss 1.6600539684295654
iteration 100, loss 1.6534737348556519
iteration 200, loss 1.619947910308838
iteration 300, loss 1.6451611518859863
iteration 400, loss 1.671196699142456
iteration 500, loss 1.6190681457519531
iteration 600, loss 1.6598241329193115
iteration 700, loss 1.6220990419387817
iteration 800, loss 1.6532658338546753
iteration 0, loss 1.692939281463623
iteration 100, loss 1.676047682762146
iteration 200, loss 1.7061156034469604
iteration 300, loss 1.6018577814102173
iteration 400, loss 1.6619449853897095
iteration 500, loss 1.5888190269470215
iteration 600, loss 1.6556047201156616
iteration 700, loss 1.740620732307434
iteration 800, loss 1.6618906259536743
iteration 0, loss 1.6837857961654663
iteration 100, loss 1.6404730081558228
iteration 200, loss 1.6372637748718262
iteration 300, loss 1.6714719533920288
iteration 400, loss 1.6382628679275513
iteration 500, loss 1.6322896480560303
iteration 600, loss 1.660390853881836
iteration 700, loss 1.6718894243240356
iteration 800, loss 1.6340949535369873
iteration 0, loss 1.644128680229187
iteration 100, loss 1.653347134590149
iteration 200, loss 1.6737401485443115
iteration 300, loss 1.6791867017745972
iteration 400, loss 1.6899487972259521
iteration 500, loss 1.6080656051635742
iteration 600, loss 1.6413888931274414
iteration 700, loss 1.6256211996078491
iteration 800, loss 1.73206627368927
iteration 0, loss 1.6498007774353027
iteration 100, loss 1.6098779439926147
iteration 200, loss 1.6469722986221313
iteration 300, loss 1.6387794017791748
iteration 400, loss 1.636164903640747
iteration 500, loss 1.6927692890167236
iteration 600, loss 1.6749887466430664
iteration 700, loss 1.6437890529632568
iteration 800, loss 1.6729544401168823
iteration 0, loss 1.635610818862915
iteration 100, loss 1.6381211280822754
iteration 200, loss 1.6554712057113647
iteration 300, loss 1.624017357826233
iteration 400, loss 1.6392250061035156
iteration 500, loss 1.5635772943496704
iteration 600, loss 1.657048225402832
iteration 700, loss 1.647449016571045
iteration 800, loss 1.6105608940124512
iteration 0, loss 1.667402744293213
iteration 100, loss 1.6296610832214355
iteration 200, loss 1.6605693101882935
iteration 300, loss 1.6812348365783691
iteration 400, loss 1.6460412740707397
iteration 500, loss 1.5969254970550537
iteration 600, loss 1.7176727056503296
iteration 700, loss 1.6684997081756592
iteration 800, loss 1.6222416162490845
iteration 0, loss 1.6520109176635742
iteration 100, loss 1.6556153297424316
iteration 200, loss 1.612969160079956
iteration 300, loss 1.6976478099822998
iteration 400, loss 1.6426365375518799
iteration 500, loss 1.606746792793274
iteration 600, loss 1.633722186088562
iteration 700, loss 1.6257671117782593
iteration 800, loss 1.6848750114440918
iteration 0, loss 1.6552072763442993
iteration 100, loss 1.6482144594192505
iteration 200, loss 1.6425448656082153
iteration 300, loss 1.6206436157226562
iteration 400, loss 1.6085375547409058
iteration 500, loss 1.6583565473556519
iteration 600, loss 1.7001031637191772
iteration 700, loss 1.662346601486206
iteration 800, loss 1.70343816280365
iteration 0, loss 1.6285433769226074
iteration 100, loss 1.664648413658142
iteration 200, loss 1.5774471759796143
iteration 300, loss 1.6324520111083984
iteration 400, loss 1.7154020071029663
iteration 500, loss 1.648702621459961
iteration 600, loss 1.6963006258010864
iteration 700, loss 1.636537790298462
iteration 800, loss 1.628897786140442
iteration 0, loss 1.6853435039520264
iteration 100, loss 1.6827800273895264
iteration 200, loss 1.6796659231185913
iteration 300, loss 1.629059910774231
iteration 400, loss 1.6893855333328247
iteration 500, loss 1.6915661096572876
iteration 600, loss 1.6732244491577148
iteration 700, loss 1.6436750888824463
iteration 800, loss 1.6586976051330566
iteration 0, loss 1.6354085206985474
iteration 100, loss 1.6768054962158203
iteration 200, loss 1.6056796312332153
iteration 300, loss 1.6391887664794922
iteration 400, loss 1.672754168510437
iteration 500, loss 1.6429479122161865
iteration 600, loss 1.7058221101760864
iteration 700, loss 1.6385977268218994
iteration 800, loss 1.6703897714614868
iteration 0, loss 1.638884425163269
iteration 100, loss 1.7132377624511719
iteration 200, loss 1.5914838314056396
iteration 300, loss 1.6700531244277954
iteration 400, loss 1.6409225463867188
iteration 500, loss 1.6733149290084839
iteration 600, loss 1.665391206741333
iteration 700, loss 1.6447817087173462
iteration 800, loss 1.6910548210144043
iteration 0, loss 1.7048733234405518
iteration 100, loss 1.5955766439437866
iteration 200, loss 1.678530216217041
iteration 300, loss 1.6052027940750122
iteration 400, loss 1.6047042608261108
iteration 500, loss 1.60502290725708
iteration 600, loss 1.6038142442703247
iteration 700, loss 1.6131922006607056
iteration 800, loss 1.6303181648254395
iteration 0, loss 1.6777628660202026
iteration 100, loss 1.6919986009597778
iteration 200, loss 1.611369252204895
iteration 300, loss 1.6086766719818115
iteration 400, loss 1.5477142333984375
iteration 500, loss 1.658966064453125
iteration 600, loss 1.5845997333526611
iteration 700, loss 1.7006888389587402
iteration 800, loss 1.629502296447754
iteration 0, loss 1.609169363975525
iteration 100, loss 1.6527892351150513
iteration 200, loss 1.6546748876571655
iteration 300, loss 1.622518539428711
iteration 400, loss 1.6228476762771606
iteration 500, loss 1.5980292558670044
iteration 600, loss 1.6656891107559204
iteration 700, loss 1.7134745121002197
iteration 800, loss 1.62305748462677
iteration 0, loss 1.6582227945327759
iteration 100, loss 1.6205496788024902
iteration 200, loss 1.7122241258621216
iteration 300, loss 1.6123592853546143
iteration 400, loss 1.627990484237671
iteration 500, loss 1.6588891744613647
iteration 600, loss 1.6200443506240845
iteration 700, loss 1.6251455545425415
iteration 800, loss 1.6414927244186401
iteration 0, loss 1.6129597425460815
iteration 100, loss 1.6455620527267456
iteration 200, loss 1.6667249202728271
iteration 300, loss 1.6138203144073486
iteration 400, loss 1.591343641281128
iteration 500, loss 1.6040658950805664
iteration 600, loss 1.695095419883728
iteration 700, loss 1.6027737855911255
iteration 800, loss 1.6661680936813354
iteration 0, loss 1.6291323900222778
iteration 100, loss 1.6442396640777588
iteration 200, loss 1.6390093564987183
iteration 300, loss 1.6858210563659668
iteration 400, loss 1.674175500869751
iteration 500, loss 1.6415835618972778
iteration 600, loss 1.669055461883545
iteration 700, loss 1.699775218963623
iteration 800, loss 1.6203917264938354
iteration 0, loss 1.5813032388687134
iteration 100, loss 1.6631734371185303
iteration 200, loss 1.5792661905288696
iteration 300, loss 1.6584070920944214
iteration 400, loss 1.649935245513916
iteration 500, loss 1.5946346521377563
iteration 600, loss 1.6494684219360352
iteration 700, loss 1.6028552055358887
iteration 800, loss 1.6390751600265503
iteration 0, loss 1.6392172574996948
iteration 100, loss 1.7186226844787598
iteration 200, loss 1.6780411005020142
iteration 300, loss 1.6753649711608887
iteration 400, loss 1.5975524187088013
iteration 500, loss 1.6371852159500122
iteration 600, loss 1.5750683546066284
iteration 700, loss 1.6280819177627563
iteration 800, loss 1.6364421844482422
iteration 0, loss 1.643570899963379
iteration 100, loss 1.6579524278640747
iteration 200, loss 1.6188766956329346
iteration 300, loss 1.6384568214416504
iteration 400, loss 1.6769522428512573
iteration 500, loss 1.6209697723388672
iteration 600, loss 1.6300983428955078
iteration 700, loss 1.622875452041626
iteration 800, loss 1.6239392757415771
iteration 0, loss 1.5945123434066772
iteration 100, loss 1.6265689134597778
iteration 200, loss 1.65937340259552
iteration 300, loss 1.6547707319259644
iteration 400, loss 1.7127511501312256
iteration 500, loss 1.6144062280654907
iteration 600, loss 1.615634799003601
iteration 700, loss 1.5447198152542114
iteration 800, loss 1.604462742805481
fold 0 accuracy: 0.815
iteration 0, loss 1.6340835094451904
iteration 100, loss 1.6152243614196777
iteration 200, loss 1.693150281906128
iteration 300, loss 1.6428483724594116
iteration 400, loss 1.6158702373504639
iteration 500, loss 1.6533186435699463
iteration 600, loss 1.636642336845398
iteration 700, loss 1.6571955680847168
iteration 800, loss 1.609100580215454
iteration 0, loss 1.6691913604736328
iteration 100, loss 1.6028918027877808
iteration 200, loss 1.6297756433486938
iteration 300, loss 1.624355673789978
iteration 400, loss 1.6916899681091309
iteration 500, loss 1.591829538345337
iteration 600, loss 1.6485795974731445
iteration 700, loss 1.6308181285858154
iteration 800, loss 1.6172819137573242
iteration 0, loss 1.6734867095947266
iteration 100, loss 1.6354306936264038
iteration 200, loss 1.7050306797027588
iteration 300, loss 1.6121234893798828
iteration 400, loss 1.6745871305465698
iteration 500, loss 1.6341307163238525
iteration 600, loss 1.654776930809021
iteration 700, loss 1.6075702905654907
iteration 800, loss 1.7051575183868408
iteration 0, loss 1.6529899835586548
iteration 100, loss 1.6397513151168823
iteration 200, loss 1.6012768745422363
iteration 300, loss 1.6368035078048706
iteration 400, loss 1.6878597736358643
iteration 500, loss 1.7134106159210205
iteration 600, loss 1.6643913984298706
iteration 700, loss 1.669859766960144
iteration 800, loss 1.6019717454910278
iteration 0, loss 1.6284409761428833
iteration 100, loss 1.668555498123169
iteration 200, loss 1.6456763744354248
iteration 300, loss 1.6458431482315063
iteration 400, loss 1.6642389297485352
iteration 500, loss 1.594043254852295
iteration 600, loss 1.617714762687683
iteration 700, loss 1.636011004447937
iteration 800, loss 1.6649595499038696
iteration 0, loss 1.627795696258545
iteration 100, loss 1.6250699758529663
iteration 200, loss 1.6272810697555542
iteration 300, loss 1.6605325937271118
iteration 400, loss 1.5927155017852783
iteration 500, loss 1.600722074508667
iteration 600, loss 1.6087576150894165
iteration 700, loss 1.6007599830627441
iteration 800, loss 1.6397944688796997
iteration 0, loss 1.6348932981491089
iteration 100, loss 1.6603305339813232
iteration 200, loss 1.6173441410064697
iteration 300, loss 1.5834400653839111
iteration 400, loss 1.584301471710205
iteration 500, loss 1.611194133758545
iteration 600, loss 1.6594313383102417
iteration 700, loss 1.6907447576522827
iteration 800, loss 1.5788973569869995
iteration 0, loss 1.6485081911087036
iteration 100, loss 1.61407470703125
iteration 200, loss 1.5936673879623413
iteration 300, loss 1.6035704612731934
iteration 400, loss 1.6050482988357544
iteration 500, loss 1.5870306491851807
iteration 600, loss 1.6268621683120728
iteration 700, loss 1.6509488821029663
iteration 800, loss 1.64633309841156
iteration 0, loss 1.6832112073898315
iteration 100, loss 1.6466935873031616
iteration 200, loss 1.628912091255188
iteration 300, loss 1.7031179666519165
iteration 400, loss 1.6201099157333374
iteration 500, loss 1.578033447265625
iteration 600, loss 1.6292470693588257
iteration 700, loss 1.6493470668792725
iteration 800, loss 1.62004554271698
iteration 0, loss 1.660385251045227
iteration 100, loss 1.6678909063339233
iteration 200, loss 1.6549365520477295
iteration 300, loss 1.5727237462997437
iteration 400, loss 1.6086825132369995
iteration 500, loss 1.6385786533355713
iteration 600, loss 1.6710572242736816
iteration 700, loss 1.6203497648239136
iteration 800, loss 1.6401593685150146
iteration 0, loss 1.6111881732940674
iteration 100, loss 1.6380738019943237
iteration 200, loss 1.6513768434524536
iteration 300, loss 1.6331112384796143
iteration 400, loss 1.6510639190673828
iteration 500, loss 1.6711530685424805
iteration 600, loss 1.6521482467651367
iteration 700, loss 1.5941851139068604
iteration 800, loss 1.6574082374572754
iteration 0, loss 1.6363497972488403
iteration 100, loss 1.6025675535202026
iteration 200, loss 1.623392105102539
iteration 300, loss 1.5901918411254883
iteration 400, loss 1.6919103860855103
iteration 500, loss 1.6121572256088257
iteration 600, loss 1.647847056388855
iteration 700, loss 1.6132075786590576
iteration 800, loss 1.6372727155685425
iteration 0, loss 1.5905007123947144
iteration 100, loss 1.5981123447418213
iteration 200, loss 1.6246575117111206
iteration 300, loss 1.6267541646957397
iteration 400, loss 1.6278467178344727
iteration 500, loss 1.6000317335128784
iteration 600, loss 1.6162787675857544
iteration 700, loss 1.6120752096176147
iteration 800, loss 1.6678199768066406
iteration 0, loss 1.6090089082717896
iteration 100, loss 1.6127941608428955
iteration 200, loss 1.632487416267395
iteration 300, loss 1.580245852470398
iteration 400, loss 1.6204149723052979
iteration 500, loss 1.6879621744155884
iteration 600, loss 1.660969853401184
iteration 700, loss 1.6264138221740723
iteration 800, loss 1.6207852363586426
iteration 0, loss 1.6185743808746338
iteration 100, loss 1.612073302268982
iteration 200, loss 1.6296770572662354
iteration 300, loss 1.5734992027282715
iteration 400, loss 1.6627117395401
iteration 500, loss 1.6172024011611938
iteration 600, loss 1.6514108180999756
iteration 700, loss 1.6932276487350464
iteration 800, loss 1.63518226146698
iteration 0, loss 1.6276036500930786
iteration 100, loss 1.7194629907608032
iteration 200, loss 1.6835116147994995
iteration 300, loss 1.6433887481689453
iteration 400, loss 1.6366188526153564
iteration 500, loss 1.5607280731201172
iteration 600, loss 1.630534291267395
iteration 700, loss 1.6497923135757446
iteration 800, loss 1.5989757776260376
iteration 0, loss 1.634639024734497
iteration 100, loss 1.6104397773742676
iteration 200, loss 1.623593807220459
iteration 300, loss 1.5953620672225952
iteration 400, loss 1.706161618232727
iteration 500, loss 1.5968393087387085
iteration 600, loss 1.6567260026931763
iteration 700, loss 1.6778184175491333
iteration 800, loss 1.619947075843811
iteration 0, loss 1.572618842124939
iteration 100, loss 1.5740635395050049
iteration 200, loss 1.6596015691757202
iteration 300, loss 1.6149612665176392
iteration 400, loss 1.7005873918533325
iteration 500, loss 1.5816781520843506
iteration 600, loss 1.618828535079956
iteration 700, loss 1.690053105354309
iteration 800, loss 1.680592656135559
iteration 0, loss 1.594361424446106
iteration 100, loss 1.5769542455673218
iteration 200, loss 1.6048742532730103
iteration 300, loss 1.667039394378662
iteration 400, loss 1.663927435874939
iteration 500, loss 1.6126375198364258
iteration 600, loss 1.6129934787750244
iteration 700, loss 1.596468210220337
iteration 800, loss 1.6647807359695435
iteration 0, loss 1.602920651435852
iteration 100, loss 1.609999418258667
iteration 200, loss 1.589762806892395
iteration 300, loss 1.6038397550582886
iteration 400, loss 1.5911626815795898
iteration 500, loss 1.637211561203003
iteration 600, loss 1.6432706117630005
iteration 700, loss 1.7342609167099
iteration 800, loss 1.658307433128357
iteration 0, loss 1.6569122076034546
iteration 100, loss 1.6237826347351074
iteration 200, loss 1.5987459421157837
iteration 300, loss 1.5822348594665527
iteration 400, loss 1.6621923446655273
iteration 500, loss 1.6068583726882935
iteration 600, loss 1.6353437900543213
iteration 700, loss 1.6453250646591187
iteration 800, loss 1.6239252090454102
iteration 0, loss 1.6620501279830933
iteration 100, loss 1.602118730545044
iteration 200, loss 1.5900368690490723
iteration 300, loss 1.6167054176330566
iteration 400, loss 1.598796010017395
iteration 500, loss 1.6080881357192993
iteration 600, loss 1.703295111656189
iteration 700, loss 1.6209886074066162
iteration 800, loss 1.6402753591537476
iteration 0, loss 1.6214417219161987
iteration 100, loss 1.638265609741211
iteration 200, loss 1.634052038192749
iteration 300, loss 1.6142314672470093
iteration 400, loss 1.6249313354492188
iteration 500, loss 1.6764745712280273
iteration 600, loss 1.651871919631958
iteration 700, loss 1.6568255424499512
iteration 800, loss 1.6274734735488892
iteration 0, loss 1.6843379735946655
iteration 100, loss 1.6209503412246704
iteration 200, loss 1.6218442916870117
iteration 300, loss 1.6179453134536743
iteration 400, loss 1.6782230138778687
iteration 500, loss 1.6201412677764893
iteration 600, loss 1.5949134826660156
iteration 700, loss 1.553175449371338
iteration 800, loss 1.6991082429885864
iteration 0, loss 1.6392570734024048
iteration 100, loss 1.7357887029647827
iteration 200, loss 1.587255597114563
iteration 300, loss 1.6201250553131104
iteration 400, loss 1.614884376525879
iteration 500, loss 1.6022073030471802
iteration 600, loss 1.6385859251022339
iteration 700, loss 1.6496719121932983
iteration 800, loss 1.6353133916854858
iteration 0, loss 1.618158221244812
iteration 100, loss 1.6531522274017334
iteration 200, loss 1.698267936706543
iteration 300, loss 1.6338423490524292
iteration 400, loss 1.584970474243164
iteration 500, loss 1.5724000930786133
iteration 600, loss 1.6436773538589478
iteration 700, loss 1.6613726615905762
iteration 800, loss 1.6208112239837646
iteration 0, loss 1.5762953758239746
iteration 100, loss 1.6112476587295532
iteration 200, loss 1.6751755475997925
iteration 300, loss 1.5522098541259766
iteration 400, loss 1.6194151639938354
iteration 500, loss 1.7035434246063232
iteration 600, loss 1.6733100414276123
iteration 700, loss 1.6462523937225342
iteration 800, loss 1.630204677581787
iteration 0, loss 1.6595104932785034
iteration 100, loss 1.7018718719482422
iteration 200, loss 1.7457664012908936
iteration 300, loss 1.5987839698791504
iteration 400, loss 1.6404452323913574
iteration 500, loss 1.6353185176849365
iteration 600, loss 1.6411951780319214
iteration 700, loss 1.6369891166687012
iteration 800, loss 1.622183918952942
iteration 0, loss 1.7243821620941162
iteration 100, loss 1.6842819452285767
iteration 200, loss 1.6062153577804565
iteration 300, loss 1.7100027799606323
iteration 400, loss 1.6055023670196533
iteration 500, loss 1.6325249671936035
iteration 600, loss 1.7020567655563354
iteration 700, loss 1.5962343215942383
iteration 800, loss 1.558979868888855
iteration 0, loss 1.6516526937484741
iteration 100, loss 1.697169303894043
iteration 200, loss 1.6535338163375854
iteration 300, loss 1.65719473361969
iteration 400, loss 1.6436114311218262
iteration 500, loss 1.6430680751800537
iteration 600, loss 1.643739938735962
iteration 700, loss 1.6030548810958862
iteration 800, loss 1.6648790836334229
iteration 0, loss 1.704472303390503
iteration 100, loss 1.643506407737732
iteration 200, loss 1.742895483970642
iteration 300, loss 1.5857775211334229
iteration 400, loss 1.6398203372955322
iteration 500, loss 1.5711023807525635
iteration 600, loss 1.5978556871414185
iteration 700, loss 1.6394519805908203
iteration 800, loss 1.6016671657562256
iteration 0, loss 1.6227450370788574
iteration 100, loss 1.6307384967803955
iteration 200, loss 1.6024763584136963
iteration 300, loss 1.6553071737289429
iteration 400, loss 1.60051429271698
iteration 500, loss 1.635148525238037
iteration 600, loss 1.6052550077438354
iteration 700, loss 1.6262298822402954
iteration 800, loss 1.6370335817337036
iteration 0, loss 1.565651535987854
iteration 100, loss 1.5742857456207275
iteration 200, loss 1.6414541006088257
iteration 300, loss 1.6808253526687622
iteration 400, loss 1.681058406829834
iteration 500, loss 1.6620615720748901
iteration 600, loss 1.648368000984192
iteration 700, loss 1.6388224363327026
iteration 800, loss 1.6424134969711304
iteration 0, loss 1.6291542053222656
iteration 100, loss 1.6589124202728271
iteration 200, loss 1.6016591787338257
iteration 300, loss 1.585314154624939
iteration 400, loss 1.577390432357788
iteration 500, loss 1.6179715394973755
iteration 600, loss 1.628228783607483
iteration 700, loss 1.699426293373108
iteration 800, loss 1.6262319087982178
iteration 0, loss 1.6713154315948486
iteration 100, loss 1.6833100318908691
iteration 200, loss 1.6711139678955078
iteration 300, loss 1.6489280462265015
iteration 400, loss 1.6384495496749878
iteration 500, loss 1.587844967842102
iteration 600, loss 1.6396406888961792
iteration 700, loss 1.6233136653900146
iteration 800, loss 1.5810883045196533
iteration 0, loss 1.679680585861206
iteration 100, loss 1.6604185104370117
iteration 200, loss 1.660447597503662
iteration 300, loss 1.599656581878662
iteration 400, loss 1.5837633609771729
iteration 500, loss 1.6391685009002686
iteration 600, loss 1.6629785299301147
iteration 700, loss 1.6763863563537598
iteration 800, loss 1.6386630535125732
iteration 0, loss 1.6122175455093384
iteration 100, loss 1.6719661951065063
iteration 200, loss 1.5786789655685425
iteration 300, loss 1.598073124885559
iteration 400, loss 1.6244616508483887
iteration 500, loss 1.661818265914917
iteration 600, loss 1.6366726160049438
iteration 700, loss 1.6617509126663208
iteration 800, loss 1.6382791996002197
iteration 0, loss 1.6638661623001099
iteration 100, loss 1.6513350009918213
iteration 200, loss 1.5957341194152832
iteration 300, loss 1.5862245559692383
iteration 400, loss 1.6443227529525757
iteration 500, loss 1.5952986478805542
iteration 600, loss 1.6313488483428955
iteration 700, loss 1.6450270414352417
iteration 800, loss 1.5706727504730225
iteration 0, loss 1.6798710823059082
iteration 100, loss 1.6122958660125732
iteration 200, loss 1.6652233600616455
iteration 300, loss 1.673149585723877
iteration 400, loss 1.6266543865203857
iteration 500, loss 1.6017889976501465
iteration 600, loss 1.63991117477417
iteration 700, loss 1.5822877883911133
iteration 800, loss 1.6101758480072021
iteration 0, loss 1.5726279020309448
iteration 100, loss 1.6372811794281006
iteration 200, loss 1.6201239824295044
iteration 300, loss 1.6688662767410278
iteration 400, loss 1.6558647155761719
iteration 500, loss 1.6138452291488647
iteration 600, loss 1.642313838005066
iteration 700, loss 1.6000620126724243
iteration 800, loss 1.6630898714065552
iteration 0, loss 1.6118296384811401
iteration 100, loss 1.68513023853302
iteration 200, loss 1.6491626501083374
iteration 300, loss 1.6160991191864014
iteration 400, loss 1.6536301374435425
iteration 500, loss 1.6087779998779297
iteration 600, loss 1.6415777206420898
iteration 700, loss 1.6150339841842651
iteration 800, loss 1.5862934589385986
iteration 0, loss 1.6717238426208496
iteration 100, loss 1.6394966840744019
iteration 200, loss 1.6025813817977905
iteration 300, loss 1.6366866827011108
iteration 400, loss 1.667676568031311
iteration 500, loss 1.588840365409851
iteration 600, loss 1.6808679103851318
iteration 700, loss 1.6790356636047363
iteration 800, loss 1.5875285863876343
iteration 0, loss 1.609943151473999
iteration 100, loss 1.6414180994033813
iteration 200, loss 1.6255604028701782
iteration 300, loss 1.6624469757080078
iteration 400, loss 1.6837645769119263
iteration 500, loss 1.6471328735351562
iteration 600, loss 1.5896203517913818
iteration 700, loss 1.6282806396484375
iteration 800, loss 1.6513203382492065
iteration 0, loss 1.637953281402588
iteration 100, loss 1.6712658405303955
iteration 200, loss 1.6145962476730347
iteration 300, loss 1.6374809741973877
iteration 400, loss 1.5457453727722168
iteration 500, loss 1.6355785131454468
iteration 600, loss 1.6160730123519897
iteration 700, loss 1.6187403202056885
iteration 800, loss 1.6543159484863281
iteration 0, loss 1.6379228830337524
iteration 100, loss 1.6339592933654785
iteration 200, loss 1.5983420610427856
iteration 300, loss 1.5540093183517456
iteration 400, loss 1.627135992050171
iteration 500, loss 1.6361030340194702
iteration 600, loss 1.65968918800354
iteration 700, loss 1.5884557962417603
iteration 800, loss 1.6668261289596558
iteration 0, loss 1.5873491764068604
iteration 100, loss 1.6213703155517578
iteration 200, loss 1.6549663543701172
iteration 300, loss 1.6052842140197754
iteration 400, loss 1.6916077136993408
iteration 500, loss 1.644768238067627
iteration 600, loss 1.6531434059143066
iteration 700, loss 1.6029343605041504
iteration 800, loss 1.600144624710083
iteration 0, loss 1.5581434965133667
iteration 100, loss 1.5906364917755127
iteration 200, loss 1.666175365447998
iteration 300, loss 1.6281166076660156
iteration 400, loss 1.596498727798462
iteration 500, loss 1.6004180908203125
iteration 600, loss 1.6560536623001099
iteration 700, loss 1.646293044090271
iteration 800, loss 1.5997437238693237
iteration 0, loss 1.67783522605896
iteration 100, loss 1.6822550296783447
iteration 200, loss 1.604340672492981
iteration 300, loss 1.675865650177002
iteration 400, loss 1.6381086111068726
iteration 500, loss 1.6101247072219849
iteration 600, loss 1.5868014097213745
iteration 700, loss 1.7035068273544312
iteration 800, loss 1.6025378704071045
iteration 0, loss 1.617157220840454
iteration 100, loss 1.6018567085266113
iteration 200, loss 1.5797313451766968
iteration 300, loss 1.5953750610351562
iteration 400, loss 1.569762110710144
iteration 500, loss 1.6053012609481812
iteration 600, loss 1.6158009767532349
iteration 700, loss 1.5736984014511108
iteration 800, loss 1.6710598468780518
iteration 0, loss 1.6354212760925293
iteration 100, loss 1.5728734731674194
iteration 200, loss 1.6170547008514404
iteration 300, loss 1.6126370429992676
iteration 400, loss 1.7140710353851318
iteration 500, loss 1.6040433645248413
iteration 600, loss 1.5783305168151855
iteration 700, loss 1.5884761810302734
iteration 800, loss 1.6128785610198975
fold 1 accuracy: 0.8136428571428571
iteration 0, loss 1.5624992847442627
iteration 100, loss 1.640444278717041
iteration 200, loss 1.5726858377456665
iteration 300, loss 1.59653639793396
iteration 400, loss 1.6203460693359375
iteration 500, loss 1.6331428289413452
iteration 600, loss 1.6258347034454346
iteration 700, loss 1.626617193222046
iteration 800, loss 1.6616475582122803
iteration 0, loss 1.6364455223083496
iteration 100, loss 1.711058497428894
iteration 200, loss 1.7274335622787476
iteration 300, loss 1.5871440172195435
iteration 400, loss 1.6107925176620483
iteration 500, loss 1.687089443206787
iteration 600, loss 1.5631102323532104
iteration 700, loss 1.6504812240600586
iteration 800, loss 1.6762343645095825
iteration 0, loss 1.6880624294281006
iteration 100, loss 1.6335281133651733
iteration 200, loss 1.6389092206954956
iteration 300, loss 1.602649211883545
iteration 400, loss 1.6010534763336182
iteration 500, loss 1.5758044719696045
iteration 600, loss 1.6056511402130127
iteration 700, loss 1.6601006984710693
iteration 800, loss 1.6021132469177246
iteration 0, loss 1.6236106157302856
iteration 100, loss 1.5909380912780762
iteration 200, loss 1.6226800680160522
iteration 300, loss 1.7221076488494873
iteration 400, loss 1.6636608839035034
iteration 500, loss 1.594688057899475
iteration 600, loss 1.612276315689087
iteration 700, loss 1.6233221292495728
iteration 800, loss 1.6178795099258423
iteration 0, loss 1.6483454704284668
iteration 100, loss 1.5967479944229126
iteration 200, loss 1.6027629375457764
iteration 300, loss 1.6182345151901245
iteration 400, loss 1.6637029647827148
iteration 500, loss 1.616830587387085
iteration 600, loss 1.5922166109085083
iteration 700, loss 1.6103029251098633
iteration 800, loss 1.6524782180786133
iteration 0, loss 1.6581002473831177
iteration 100, loss 1.6517701148986816
iteration 200, loss 1.5550552606582642
iteration 300, loss 1.6298907995224
iteration 400, loss 1.5936602354049683
iteration 500, loss 1.643328309059143
iteration 600, loss 1.6890360116958618
iteration 700, loss 1.6367456912994385
iteration 800, loss 1.6052874326705933
iteration 0, loss 1.6157253980636597
iteration 100, loss 1.642142415046692
iteration 200, loss 1.5975990295410156
iteration 300, loss 1.6489644050598145
iteration 400, loss 1.5523141622543335
iteration 500, loss 1.6247427463531494
iteration 600, loss 1.71389901638031
iteration 700, loss 1.6554803848266602
iteration 800, loss 1.6909904479980469
iteration 0, loss 1.6044723987579346
iteration 100, loss 1.610805630683899
iteration 200, loss 1.6855262517929077
iteration 300, loss 1.5714466571807861
iteration 400, loss 1.6122170686721802
iteration 500, loss 1.6772956848144531
iteration 600, loss 1.5606286525726318
iteration 700, loss 1.5853673219680786
iteration 800, loss 1.6699597835540771
iteration 0, loss 1.637039303779602
iteration 100, loss 1.5990731716156006
iteration 200, loss 1.6692304611206055
iteration 300, loss 1.6716407537460327
iteration 400, loss 1.6183832883834839
iteration 500, loss 1.587424874305725
iteration 600, loss 1.5898120403289795
iteration 700, loss 1.5925874710083008
iteration 800, loss 1.6083316802978516
iteration 0, loss 1.626129388809204
iteration 100, loss 1.5782842636108398
iteration 200, loss 1.6334421634674072
iteration 300, loss 1.6330853700637817
iteration 400, loss 1.6180284023284912
iteration 500, loss 1.6294279098510742
iteration 600, loss 1.625332236289978
iteration 700, loss 1.6234662532806396
iteration 800, loss 1.6570335626602173
iteration 0, loss 1.6080231666564941
iteration 100, loss 1.6911178827285767
iteration 200, loss 1.6439363956451416
iteration 300, loss 1.6335901021957397
iteration 400, loss 1.6515458822250366
iteration 500, loss 1.618250846862793
iteration 600, loss 1.6149617433547974
iteration 700, loss 1.6275304555892944
iteration 800, loss 1.6764986515045166
iteration 0, loss 1.639545202255249
iteration 100, loss 1.6545894145965576
iteration 200, loss 1.607618808746338
iteration 300, loss 1.6636360883712769
iteration 400, loss 1.6605368852615356
iteration 500, loss 1.66506826877594
iteration 600, loss 1.5912649631500244
iteration 700, loss 1.6346805095672607
iteration 800, loss 1.6641682386398315
iteration 0, loss 1.5913361310958862
iteration 100, loss 1.6109886169433594
iteration 200, loss 1.5759202241897583
iteration 300, loss 1.6028854846954346
iteration 400, loss 1.5884857177734375
iteration 500, loss 1.6064338684082031
iteration 600, loss 1.6194818019866943
iteration 700, loss 1.6534924507141113
iteration 800, loss 1.5758919715881348
iteration 0, loss 1.6308557987213135
iteration 100, loss 1.6120340824127197
iteration 200, loss 1.5957927703857422
iteration 300, loss 1.674837589263916
iteration 400, loss 1.6334506273269653
iteration 500, loss 1.646490454673767
iteration 600, loss 1.6184636354446411
iteration 700, loss 1.6001102924346924
iteration 800, loss 1.614916205406189
iteration 0, loss 1.6169873476028442
iteration 100, loss 1.635988473892212
iteration 200, loss 1.6525663137435913
iteration 300, loss 1.586329698562622
iteration 400, loss 1.6182758808135986
iteration 500, loss 1.659253478050232
iteration 600, loss 1.5926059484481812
iteration 700, loss 1.6243687868118286
iteration 800, loss 1.5597999095916748
iteration 0, loss 1.6189512014389038
iteration 100, loss 1.670356273651123
iteration 200, loss 1.6455371379852295
iteration 300, loss 1.6817030906677246
iteration 400, loss 1.622963786125183
iteration 500, loss 1.639397382736206
iteration 600, loss 1.6239230632781982
iteration 700, loss 1.6424403190612793
iteration 800, loss 1.559035301208496
iteration 0, loss 1.5989432334899902
iteration 100, loss 1.5446478128433228
iteration 200, loss 1.5686960220336914
iteration 300, loss 1.6821694374084473
iteration 400, loss 1.6295090913772583
iteration 500, loss 1.637510895729065
iteration 600, loss 1.6259212493896484
iteration 700, loss 1.651529312133789
iteration 800, loss 1.6662113666534424
iteration 0, loss 1.68046236038208
iteration 100, loss 1.566572904586792
iteration 200, loss 1.6022121906280518
iteration 300, loss 1.6335844993591309
iteration 400, loss 1.6075440645217896
iteration 500, loss 1.6940422058105469
iteration 600, loss 1.6475794315338135
iteration 700, loss 1.5730092525482178
iteration 800, loss 1.616597294807434
iteration 0, loss 1.626184344291687
iteration 100, loss 1.6129114627838135
iteration 200, loss 1.6760505437850952
iteration 300, loss 1.6177586317062378
iteration 400, loss 1.5735676288604736
iteration 500, loss 1.5921907424926758
iteration 600, loss 1.614504098892212
iteration 700, loss 1.64653742313385
iteration 800, loss 1.5702048540115356
iteration 0, loss 1.5879360437393188
iteration 100, loss 1.642460584640503
iteration 200, loss 1.5850242376327515
iteration 300, loss 1.650212049484253
iteration 400, loss 1.662628173828125
iteration 500, loss 1.650438904762268
iteration 600, loss 1.5916619300842285
iteration 700, loss 1.6679092645645142
iteration 800, loss 1.6119097471237183
iteration 0, loss 1.6167595386505127
iteration 100, loss 1.6386375427246094
iteration 200, loss 1.621414065361023
iteration 300, loss 1.6458709239959717
iteration 400, loss 1.6830570697784424
iteration 500, loss 1.656631588935852
iteration 600, loss 1.5810768604278564
iteration 700, loss 1.6310477256774902
iteration 800, loss 1.6872131824493408
iteration 0, loss 1.5564260482788086
iteration 100, loss 1.635286808013916
iteration 200, loss 1.6115820407867432
iteration 300, loss 1.627271056175232
iteration 400, loss 1.6379704475402832
iteration 500, loss 1.6549837589263916
iteration 600, loss 1.6489460468292236
iteration 700, loss 1.6734466552734375
iteration 800, loss 1.6084976196289062
iteration 0, loss 1.6271262168884277
iteration 100, loss 1.6122726202011108
iteration 200, loss 1.5410511493682861
iteration 300, loss 1.6013582944869995
iteration 400, loss 1.6300203800201416
iteration 500, loss 1.6720069646835327
iteration 600, loss 1.6433134078979492
iteration 700, loss 1.653630018234253
iteration 800, loss 1.6166235208511353
iteration 0, loss 1.6534790992736816
iteration 100, loss 1.662764549255371
iteration 200, loss 1.5671855211257935
iteration 300, loss 1.6285982131958008
iteration 400, loss 1.67435884475708
iteration 500, loss 1.6223701238632202
iteration 600, loss 1.6226552724838257
iteration 700, loss 1.563977837562561
iteration 800, loss 1.610600471496582
iteration 0, loss 1.5910797119140625
iteration 100, loss 1.6308338642120361
iteration 200, loss 1.6316771507263184
iteration 300, loss 1.6061934232711792
iteration 400, loss 1.6485867500305176
iteration 500, loss 1.6227465867996216
iteration 600, loss 1.6622101068496704
iteration 700, loss 1.5985751152038574
iteration 800, loss 1.6680448055267334
iteration 0, loss 1.7119146585464478
iteration 100, loss 1.6675537824630737
iteration 200, loss 1.6527470350265503
iteration 300, loss 1.6639368534088135
iteration 400, loss 1.581298828125
iteration 500, loss 1.645498514175415
iteration 600, loss 1.6739373207092285
iteration 700, loss 1.632140040397644
iteration 800, loss 1.6773138046264648
iteration 0, loss 1.5762568712234497
iteration 100, loss 1.629599690437317
iteration 200, loss 1.5979204177856445
iteration 300, loss 1.6328986883163452
iteration 400, loss 1.6314829587936401
iteration 500, loss 1.6343414783477783
iteration 600, loss 1.6897495985031128
iteration 700, loss 1.6678783893585205
iteration 800, loss 1.653792381286621
iteration 0, loss 1.6751149892807007
iteration 100, loss 1.5720595121383667
iteration 200, loss 1.604041576385498
iteration 300, loss 1.6533116102218628
iteration 400, loss 1.6140190362930298
iteration 500, loss 1.568779706954956
iteration 600, loss 1.6465530395507812
iteration 700, loss 1.6312484741210938
iteration 800, loss 1.640297293663025
iteration 0, loss 1.6352884769439697
iteration 100, loss 1.6026642322540283
iteration 200, loss 1.6057281494140625
iteration 300, loss 1.6749855279922485
iteration 400, loss 1.6473573446273804
iteration 500, loss 1.5681946277618408
iteration 600, loss 1.59018874168396
iteration 700, loss 1.596747636795044
iteration 800, loss 1.6226338148117065
iteration 0, loss 1.6440377235412598
iteration 100, loss 1.682268500328064
iteration 200, loss 1.6589072942733765
iteration 300, loss 1.6574589014053345
iteration 400, loss 1.6611722707748413
iteration 500, loss 1.600399136543274
iteration 600, loss 1.6536509990692139
iteration 700, loss 1.604591965675354
iteration 800, loss 1.7070674896240234
iteration 0, loss 1.594317078590393
iteration 100, loss 1.6411387920379639
iteration 200, loss 1.6343889236450195
iteration 300, loss 1.5851850509643555
iteration 400, loss 1.6551882028579712
iteration 500, loss 1.5681142807006836
iteration 600, loss 1.6587674617767334
iteration 700, loss 1.6644933223724365
iteration 800, loss 1.621805191040039
iteration 0, loss 1.6488118171691895
iteration 100, loss 1.6192229986190796
iteration 200, loss 1.6360360383987427
iteration 300, loss 1.604322075843811
iteration 400, loss 1.6044100522994995
iteration 500, loss 1.5866154432296753
iteration 600, loss 1.6844433546066284
iteration 700, loss 1.634063959121704
iteration 800, loss 1.6033673286437988
iteration 0, loss 1.6592990159988403
iteration 100, loss 1.668823480606079
iteration 200, loss 1.5911556482315063
iteration 300, loss 1.640263557434082
iteration 400, loss 1.6145602464675903
iteration 500, loss 1.7015249729156494
iteration 600, loss 1.623490571975708
iteration 700, loss 1.6742889881134033
iteration 800, loss 1.6092097759246826
iteration 0, loss 1.5885229110717773
iteration 100, loss 1.5930042266845703
iteration 200, loss 1.6542210578918457
iteration 300, loss 1.618243932723999
iteration 400, loss 1.6556357145309448
iteration 500, loss 1.646672248840332
iteration 600, loss 1.5803020000457764
iteration 700, loss 1.6872962713241577
iteration 800, loss 1.5929515361785889
iteration 0, loss 1.5982387065887451
iteration 100, loss 1.612847089767456
iteration 200, loss 1.6761447191238403
iteration 300, loss 1.606764554977417
iteration 400, loss 1.653394341468811
iteration 500, loss 1.6400811672210693
iteration 600, loss 1.6810717582702637
iteration 700, loss 1.6030031442642212
iteration 800, loss 1.5777881145477295
iteration 0, loss 1.6454626321792603
iteration 100, loss 1.6333043575286865
iteration 200, loss 1.6398289203643799
iteration 300, loss 1.628093957901001
iteration 400, loss 1.634287714958191
iteration 500, loss 1.6623389720916748
iteration 600, loss 1.648209810256958
iteration 700, loss 1.6060736179351807
iteration 800, loss 1.6595841646194458
iteration 0, loss 1.5912466049194336
iteration 100, loss 1.6129528284072876
iteration 200, loss 1.7180594205856323
iteration 300, loss 1.6521611213684082
iteration 400, loss 1.609729290008545
iteration 500, loss 1.6519358158111572
iteration 600, loss 1.6024607419967651
iteration 700, loss 1.6454544067382812
iteration 800, loss 1.6100099086761475
iteration 0, loss 1.6405433416366577
iteration 100, loss 1.613010287284851
iteration 200, loss 1.6617753505706787
iteration 300, loss 1.6420904397964478
iteration 400, loss 1.646913766860962
iteration 500, loss 1.630179762840271
iteration 600, loss 1.6337454319000244
iteration 700, loss 1.6771934032440186
iteration 800, loss 1.6035165786743164
iteration 0, loss 1.6397521495819092
iteration 100, loss 1.680165410041809
iteration 200, loss 1.592106580734253
iteration 300, loss 1.6049773693084717
iteration 400, loss 1.6473138332366943
iteration 500, loss 1.634501338005066
iteration 600, loss 1.582672357559204
iteration 700, loss 1.6272153854370117
iteration 800, loss 1.652085781097412
iteration 0, loss 1.6563498973846436
iteration 100, loss 1.6455241441726685
iteration 200, loss 1.686366081237793
iteration 300, loss 1.6328976154327393
iteration 400, loss 1.6019591093063354
iteration 500, loss 1.6651124954223633
iteration 600, loss 1.6293302774429321
iteration 700, loss 1.6228134632110596
iteration 800, loss 1.6545329093933105
iteration 0, loss 1.5957403182983398
iteration 100, loss 1.5769600868225098
iteration 200, loss 1.654055118560791
iteration 300, loss 1.658274531364441
iteration 400, loss 1.5599833726882935
iteration 500, loss 1.621612787246704
iteration 600, loss 1.6217235326766968
iteration 700, loss 1.6307035684585571
iteration 800, loss 1.6493480205535889
iteration 0, loss 1.5639426708221436
iteration 100, loss 1.6392349004745483
iteration 200, loss 1.605273962020874
iteration 300, loss 1.6225513219833374
iteration 400, loss 1.6594880819320679
iteration 500, loss 1.700358271598816
iteration 600, loss 1.5879768133163452
iteration 700, loss 1.611862301826477
iteration 800, loss 1.6024545431137085
iteration 0, loss 1.6736645698547363
iteration 100, loss 1.5839025974273682
iteration 200, loss 1.627492070198059
iteration 300, loss 1.5966997146606445
iteration 400, loss 1.5981563329696655
iteration 500, loss 1.650970697402954
iteration 600, loss 1.6152559518814087
iteration 700, loss 1.6792826652526855
iteration 800, loss 1.6070715188980103
iteration 0, loss 1.6504861116409302
iteration 100, loss 1.5894097089767456
iteration 200, loss 1.6542257070541382
iteration 300, loss 1.6450238227844238
iteration 400, loss 1.6218878030776978
iteration 500, loss 1.569939374923706
iteration 600, loss 1.6904847621917725
iteration 700, loss 1.6368125677108765
iteration 800, loss 1.6405164003372192
iteration 0, loss 1.6505930423736572
iteration 100, loss 1.5896800756454468
iteration 200, loss 1.5859793424606323
iteration 300, loss 1.6404469013214111
iteration 400, loss 1.6475075483322144
iteration 500, loss 1.5864434242248535
iteration 600, loss 1.625367283821106
iteration 700, loss 1.6398247480392456
iteration 800, loss 1.622299313545227
iteration 0, loss 1.5841108560562134
iteration 100, loss 1.6700944900512695
iteration 200, loss 1.5803700685501099
iteration 300, loss 1.6590039730072021
iteration 400, loss 1.6415904760360718
iteration 500, loss 1.6797164678573608
iteration 600, loss 1.6526998281478882
iteration 700, loss 1.6414637565612793
iteration 800, loss 1.6926565170288086
iteration 0, loss 1.6305279731750488
iteration 100, loss 1.5989760160446167
iteration 200, loss 1.7008119821548462
iteration 300, loss 1.6591556072235107
iteration 400, loss 1.7878155708312988
iteration 500, loss 1.6439067125320435
iteration 600, loss 1.6115622520446777
iteration 700, loss 1.5832773447036743
iteration 800, loss 1.649437427520752
iteration 0, loss 1.6500550508499146
iteration 100, loss 1.66645085811615
iteration 200, loss 1.597340703010559
iteration 300, loss 1.611359715461731
iteration 400, loss 1.6249414682388306
iteration 500, loss 1.6144061088562012
iteration 600, loss 1.5466156005859375
iteration 700, loss 1.6800178289413452
iteration 800, loss 1.6277323961257935
iteration 0, loss 1.6240203380584717
iteration 100, loss 1.6353354454040527
iteration 200, loss 1.642103910446167
iteration 300, loss 1.6344095468521118
iteration 400, loss 1.59372079372406
iteration 500, loss 1.5476276874542236
iteration 600, loss 1.6112276315689087
iteration 700, loss 1.6444571018218994
iteration 800, loss 1.64117431640625
iteration 0, loss 1.6125693321228027
iteration 100, loss 1.6332036256790161
iteration 200, loss 1.6273149251937866
iteration 300, loss 1.6789883375167847
iteration 400, loss 1.6289602518081665
iteration 500, loss 1.601262092590332
iteration 600, loss 1.632012963294983
iteration 700, loss 1.5814307928085327
iteration 800, loss 1.6091200113296509
fold 2 accuracy: 0.8162142857142857
iteration 0, loss 1.6146000623703003
iteration 100, loss 1.6011271476745605
iteration 200, loss 1.5979845523834229
iteration 300, loss 1.6657644510269165
iteration 400, loss 1.6567320823669434
iteration 500, loss 1.6755367517471313
iteration 600, loss 1.6013429164886475
iteration 700, loss 1.6435648202896118
iteration 800, loss 1.683725118637085
iteration 0, loss 1.6186470985412598
iteration 100, loss 1.590632438659668
iteration 200, loss 1.6075398921966553
iteration 300, loss 1.603819489479065
iteration 400, loss 1.6107933521270752
iteration 500, loss 1.6264336109161377
iteration 600, loss 1.5782238245010376
iteration 700, loss 1.6136095523834229
iteration 800, loss 1.6026039123535156
iteration 0, loss 1.5608128309249878
iteration 100, loss 1.5880401134490967
iteration 200, loss 1.615451455116272
iteration 300, loss 1.603102684020996
iteration 400, loss 1.592482566833496
iteration 500, loss 1.56349778175354
iteration 600, loss 1.609160304069519
iteration 700, loss 1.5911884307861328
iteration 800, loss 1.6102285385131836
iteration 0, loss 1.6175789833068848
iteration 100, loss 1.6473966836929321
iteration 200, loss 1.5873881578445435
iteration 300, loss 1.630056619644165
iteration 400, loss 1.6874473094940186
iteration 500, loss 1.6159840822219849
iteration 600, loss 1.6370341777801514
iteration 700, loss 1.5910495519638062
iteration 800, loss 1.6043903827667236
iteration 0, loss 1.6704033613204956
iteration 100, loss 1.633101224899292
iteration 200, loss 1.6439028978347778
iteration 300, loss 1.6483688354492188
iteration 400, loss 1.6506315469741821
iteration 500, loss 1.5741204023361206
iteration 600, loss 1.5788615942001343
iteration 700, loss 1.5848561525344849
iteration 800, loss 1.6283578872680664
iteration 0, loss 1.5790050029754639
iteration 100, loss 1.575732946395874
iteration 200, loss 1.7216134071350098
iteration 300, loss 1.632055640220642
iteration 400, loss 1.6385823488235474
iteration 500, loss 1.5931870937347412
iteration 600, loss 1.6187727451324463
iteration 700, loss 1.6569410562515259
iteration 800, loss 1.6711745262145996
iteration 0, loss 1.622223973274231
iteration 100, loss 1.658424973487854
iteration 200, loss 1.656555414199829
iteration 300, loss 1.641571283340454
iteration 400, loss 1.5963019132614136
iteration 500, loss 1.6445764303207397
iteration 600, loss 1.6069490909576416
iteration 700, loss 1.6335054636001587
iteration 800, loss 1.5893607139587402
iteration 0, loss 1.6470298767089844
iteration 100, loss 1.6683955192565918
iteration 200, loss 1.6115049123764038
iteration 300, loss 1.6303151845932007
iteration 400, loss 1.6557294130325317
iteration 500, loss 1.5976049900054932
iteration 600, loss 1.6061363220214844
iteration 700, loss 1.6687421798706055
iteration 800, loss 1.6092017889022827
iteration 0, loss 1.603461742401123
iteration 100, loss 1.591188907623291
iteration 200, loss 1.602382779121399
iteration 300, loss 1.59534752368927
iteration 400, loss 1.6146106719970703
iteration 500, loss 1.5975761413574219
iteration 600, loss 1.6328668594360352
iteration 700, loss 1.6427021026611328
iteration 800, loss 1.5617433786392212
iteration 0, loss 1.633793592453003
iteration 100, loss 1.5867228507995605
iteration 200, loss 1.6200536489486694
iteration 300, loss 1.6010186672210693
iteration 400, loss 1.617806315422058
iteration 500, loss 1.6012378931045532
iteration 600, loss 1.566623568534851
iteration 700, loss 1.573117733001709
iteration 800, loss 1.5921560525894165
iteration 0, loss 1.571734070777893
iteration 100, loss 1.6163266897201538
iteration 200, loss 1.5797799825668335
iteration 300, loss 1.6586596965789795
iteration 400, loss 1.6740117073059082
iteration 500, loss 1.5757124423980713
iteration 600, loss 1.6546051502227783
iteration 700, loss 1.6839003562927246
iteration 800, loss 1.567615270614624
iteration 0, loss 1.5877010822296143
iteration 100, loss 1.5632399320602417
iteration 200, loss 1.5706576108932495
iteration 300, loss 1.6360893249511719
iteration 400, loss 1.6876567602157593
iteration 500, loss 1.594878911972046
iteration 600, loss 1.6341184377670288
iteration 700, loss 1.6218301057815552
iteration 800, loss 1.6622216701507568
iteration 0, loss 1.6076239347457886
iteration 100, loss 1.5799869298934937
iteration 200, loss 1.569187045097351
iteration 300, loss 1.6471364498138428
iteration 400, loss 1.5812028646469116
iteration 500, loss 1.6271882057189941
iteration 600, loss 1.6233727931976318
iteration 700, loss 1.6845778226852417
iteration 800, loss 1.5971852540969849
iteration 0, loss 1.678415298461914
iteration 100, loss 1.5589326620101929
iteration 200, loss 1.5697979927062988
iteration 300, loss 1.618129849433899
iteration 400, loss 1.6257891654968262
iteration 500, loss 1.661576509475708
iteration 600, loss 1.5908137559890747
iteration 700, loss 1.6103864908218384
iteration 800, loss 1.6196831464767456
iteration 0, loss 1.6257086992263794
iteration 100, loss 1.6016546487808228
iteration 200, loss 1.6593106985092163
iteration 300, loss 1.6505762338638306
iteration 400, loss 1.6397420167922974
iteration 500, loss 1.6262091398239136
iteration 600, loss 1.661063551902771
iteration 700, loss 1.598079800605774
iteration 800, loss 1.602466106414795
iteration 0, loss 1.6188498735427856
iteration 100, loss 1.6625823974609375
iteration 200, loss 1.607300043106079
iteration 300, loss 1.6532398462295532
iteration 400, loss 1.6333043575286865
iteration 500, loss 1.6413123607635498
iteration 600, loss 1.6345117092132568
iteration 700, loss 1.6236281394958496
iteration 800, loss 1.6219497919082642
iteration 0, loss 1.6358391046524048
iteration 100, loss 1.6238597631454468
iteration 200, loss 1.605097770690918
iteration 300, loss 1.593675971031189
iteration 400, loss 1.6490572690963745
iteration 500, loss 1.6217689514160156
iteration 600, loss 1.6317121982574463
iteration 700, loss 1.6466522216796875
iteration 800, loss 1.6169843673706055
iteration 0, loss 1.7061210870742798
iteration 100, loss 1.5733425617218018
iteration 200, loss 1.628621220588684
iteration 300, loss 1.6280028820037842
iteration 400, loss 1.6069457530975342
iteration 500, loss 1.6342072486877441
iteration 600, loss 1.6873809099197388
iteration 700, loss 1.6090855598449707
iteration 800, loss 1.6708523035049438
iteration 0, loss 1.5677344799041748
iteration 100, loss 1.6184676885604858
iteration 200, loss 1.608264684677124
iteration 300, loss 1.611879587173462
iteration 400, loss 1.5690757036209106
iteration 500, loss 1.6171209812164307
iteration 600, loss 1.611401081085205
iteration 700, loss 1.6380269527435303
iteration 800, loss 1.6729645729064941
iteration 0, loss 1.6035630702972412
iteration 100, loss 1.6169843673706055
iteration 200, loss 1.579808235168457
iteration 300, loss 1.6291002035140991
iteration 400, loss 1.6143970489501953
iteration 500, loss 1.600467562675476
iteration 600, loss 1.698246955871582
iteration 700, loss 1.6609711647033691
iteration 800, loss 1.620355248451233
iteration 0, loss 1.5793588161468506
iteration 100, loss 1.606727123260498
iteration 200, loss 1.6439179182052612
iteration 300, loss 1.592360496520996
iteration 400, loss 1.6004056930541992
iteration 500, loss 1.5796006917953491
iteration 600, loss 1.6037070751190186
iteration 700, loss 1.6286717653274536
iteration 800, loss 1.6831423044204712
iteration 0, loss 1.5926152467727661
iteration 100, loss 1.6040687561035156
iteration 200, loss 1.6615328788757324
iteration 300, loss 1.558163046836853
iteration 400, loss 1.6234967708587646
iteration 500, loss 1.6446235179901123
iteration 600, loss 1.5974663496017456
iteration 700, loss 1.589847445487976
iteration 800, loss 1.6470882892608643
iteration 0, loss 1.5996897220611572
iteration 100, loss 1.5440033674240112
iteration 200, loss 1.6622776985168457
iteration 300, loss 1.650693655014038
iteration 400, loss 1.596253514289856
iteration 500, loss 1.6360677480697632
iteration 600, loss 1.5711742639541626
iteration 700, loss 1.6171284914016724
iteration 800, loss 1.6362301111221313
iteration 0, loss 1.6152329444885254
iteration 100, loss 1.6315934658050537
iteration 200, loss 1.6152931451797485
iteration 300, loss 1.6317143440246582
iteration 400, loss 1.592151165008545
iteration 500, loss 1.6736485958099365
iteration 600, loss 1.569804072380066
iteration 700, loss 1.5915322303771973
iteration 800, loss 1.614479422569275
iteration 0, loss 1.5792877674102783
iteration 100, loss 1.6297656297683716
iteration 200, loss 1.5954937934875488
iteration 300, loss 1.6829475164413452
iteration 400, loss 1.626206874847412
iteration 500, loss 1.5807101726531982
iteration 600, loss 1.5912673473358154
iteration 700, loss 1.6562020778656006
iteration 800, loss 1.7061593532562256
iteration 0, loss 1.6218786239624023
iteration 100, loss 1.6204949617385864
iteration 200, loss 1.6371620893478394
iteration 300, loss 1.6041635274887085
iteration 400, loss 1.6567753553390503
iteration 500, loss 1.6000622510910034
iteration 600, loss 1.6658204793930054
iteration 700, loss 1.654013991355896
iteration 800, loss 1.6225230693817139
iteration 0, loss 1.6054702997207642
iteration 100, loss 1.6048702001571655
iteration 200, loss 1.6178110837936401
iteration 300, loss 1.5848218202590942
iteration 400, loss 1.6341617107391357
iteration 500, loss 1.5634653568267822
iteration 600, loss 1.6280434131622314
iteration 700, loss 1.6563026905059814
iteration 800, loss 1.6702947616577148
iteration 0, loss 1.6215496063232422
iteration 100, loss 1.6731470823287964
iteration 200, loss 1.6409001350402832
iteration 300, loss 1.6871689558029175
iteration 400, loss 1.610519289970398
iteration 500, loss 1.5863527059555054
iteration 600, loss 1.6417925357818604
iteration 700, loss 1.6156054735183716
iteration 800, loss 1.6093974113464355
iteration 0, loss 1.621888279914856
iteration 100, loss 1.5760502815246582
iteration 200, loss 1.6602280139923096
iteration 300, loss 1.6560028791427612
iteration 400, loss 1.5967546701431274
iteration 500, loss 1.6175472736358643
iteration 600, loss 1.6194835901260376
iteration 700, loss 1.64584481716156
iteration 800, loss 1.6155776977539062
iteration 0, loss 1.635292887687683
iteration 100, loss 1.615465521812439
iteration 200, loss 1.63288414478302
iteration 300, loss 1.6343531608581543
iteration 400, loss 1.585003137588501
iteration 500, loss 1.6867765188217163
iteration 600, loss 1.5866177082061768
iteration 700, loss 1.6119264364242554
iteration 800, loss 1.6551234722137451
iteration 0, loss 1.6842492818832397
iteration 100, loss 1.6096100807189941
iteration 200, loss 1.6286729574203491
iteration 300, loss 1.6340123414993286
iteration 400, loss 1.6194159984588623
iteration 500, loss 1.656425952911377
iteration 600, loss 1.586068034172058
iteration 700, loss 1.5971075296401978
iteration 800, loss 1.6506474018096924
iteration 0, loss 1.641575813293457
iteration 100, loss 1.6410611867904663
iteration 200, loss 1.6414978504180908
iteration 300, loss 1.598335862159729
iteration 400, loss 1.6264063119888306
iteration 500, loss 1.6830757856369019
iteration 600, loss 1.6396375894546509
iteration 700, loss 1.5735297203063965
iteration 800, loss 1.5869320631027222
iteration 0, loss 1.6826558113098145
iteration 100, loss 1.6017085313796997
iteration 200, loss 1.7117640972137451
iteration 300, loss 1.591119647026062
iteration 400, loss 1.6177570819854736
iteration 500, loss 1.5769476890563965
iteration 600, loss 1.6712242364883423
iteration 700, loss 1.5935604572296143
iteration 800, loss 1.624967098236084
iteration 0, loss 1.6058578491210938
iteration 100, loss 1.6619287729263306
iteration 200, loss 1.6112099885940552
iteration 300, loss 1.6637464761734009
iteration 400, loss 1.6275384426116943
iteration 500, loss 1.6229339838027954
iteration 600, loss 1.6278245449066162
iteration 700, loss 1.687316656112671
iteration 800, loss 1.6099107265472412
iteration 0, loss 1.6401351690292358
iteration 100, loss 1.605302333831787
iteration 200, loss 1.6089632511138916
iteration 300, loss 1.6551403999328613
iteration 400, loss 1.6441537141799927
iteration 500, loss 1.5834141969680786
iteration 600, loss 1.63170325756073
iteration 700, loss 1.6005934476852417
iteration 800, loss 1.6119261980056763
iteration 0, loss 1.6880946159362793
iteration 100, loss 1.660086750984192
iteration 200, loss 1.6773934364318848
iteration 300, loss 1.6390749216079712
iteration 400, loss 1.624575138092041
iteration 500, loss 1.5898536443710327
iteration 600, loss 1.6602864265441895
iteration 700, loss 1.6841925382614136
iteration 800, loss 1.6066621541976929
iteration 0, loss 1.606988787651062
iteration 100, loss 1.6161062717437744
iteration 200, loss 1.609663486480713
iteration 300, loss 1.6361987590789795
iteration 400, loss 1.6206014156341553
iteration 500, loss 1.6499065160751343
iteration 600, loss 1.6414015293121338
iteration 700, loss 1.6103389263153076
iteration 800, loss 1.6370179653167725
iteration 0, loss 1.6921395063400269
iteration 100, loss 1.6879197359085083
iteration 200, loss 1.6004503965377808
iteration 300, loss 1.6617363691329956
iteration 400, loss 1.6793923377990723
iteration 500, loss 1.659998893737793
iteration 600, loss 1.5809940099716187
iteration 700, loss 1.6191959381103516
iteration 800, loss 1.6115453243255615
iteration 0, loss 1.631282091140747
iteration 100, loss 1.6403026580810547
iteration 200, loss 1.6053228378295898
iteration 300, loss 1.5963592529296875
iteration 400, loss 1.6100350618362427
iteration 500, loss 1.6141477823257446
iteration 600, loss 1.605717658996582
iteration 700, loss 1.6529940366744995
iteration 800, loss 1.6375927925109863
iteration 0, loss 1.5978273153305054
iteration 100, loss 1.619239091873169
iteration 200, loss 1.7012813091278076
iteration 300, loss 1.6483516693115234
iteration 400, loss 1.5742825269699097
iteration 500, loss 1.6082959175109863
iteration 600, loss 1.6419603824615479
iteration 700, loss 1.5832993984222412
iteration 800, loss 1.6002624034881592
iteration 0, loss 1.579316258430481
iteration 100, loss 1.5774524211883545
iteration 200, loss 1.5770392417907715
iteration 300, loss 1.614545226097107
iteration 400, loss 1.6471004486083984
iteration 500, loss 1.5860182046890259
iteration 600, loss 1.6460191011428833
iteration 700, loss 1.6452744007110596
iteration 800, loss 1.56321120262146
iteration 0, loss 1.6315264701843262
iteration 100, loss 1.5994406938552856
iteration 200, loss 1.5614876747131348
iteration 300, loss 1.6099146604537964
iteration 400, loss 1.59713876247406
iteration 500, loss 1.5945993661880493
iteration 600, loss 1.6457934379577637
iteration 700, loss 1.6729789972305298
iteration 800, loss 1.594232201576233
iteration 0, loss 1.6883970499038696
iteration 100, loss 1.6837849617004395
iteration 200, loss 1.6075962781906128
iteration 300, loss 1.597825527191162
iteration 400, loss 1.5903534889221191
iteration 500, loss 1.633893370628357
iteration 600, loss 1.650395393371582
iteration 700, loss 1.6290961503982544
iteration 800, loss 1.6135966777801514
iteration 0, loss 1.6573147773742676
iteration 100, loss 1.5836050510406494
iteration 200, loss 1.5995503664016724
iteration 300, loss 1.6520237922668457
iteration 400, loss 1.6415554285049438
iteration 500, loss 1.6982886791229248
iteration 600, loss 1.6133087873458862
iteration 700, loss 1.6452337503433228
iteration 800, loss 1.6314560174942017
iteration 0, loss 1.639473795890808
iteration 100, loss 1.6717337369918823
iteration 200, loss 1.6340261697769165
iteration 300, loss 1.6334387063980103
iteration 400, loss 1.630719780921936
iteration 500, loss 1.598923921585083
iteration 600, loss 1.6459416151046753
iteration 700, loss 1.6454243659973145
iteration 800, loss 1.667221188545227
iteration 0, loss 1.629419207572937
iteration 100, loss 1.657976508140564
iteration 200, loss 1.6596218347549438
iteration 300, loss 1.5870554447174072
iteration 400, loss 1.6811000108718872
iteration 500, loss 1.6439913511276245
iteration 600, loss 1.586798071861267
iteration 700, loss 1.6494548320770264
iteration 800, loss 1.689141035079956
iteration 0, loss 1.564314603805542
iteration 100, loss 1.5838009119033813
iteration 200, loss 1.5773721933364868
iteration 300, loss 1.6184061765670776
iteration 400, loss 1.589267611503601
iteration 500, loss 1.6034231185913086
iteration 600, loss 1.6354551315307617
iteration 700, loss 1.6236780881881714
iteration 800, loss 1.6613315343856812
iteration 0, loss 1.6540850400924683
iteration 100, loss 1.620726227760315
iteration 200, loss 1.6146013736724854
iteration 300, loss 1.6648850440979004
iteration 400, loss 1.616094708442688
iteration 500, loss 1.5986329317092896
iteration 600, loss 1.6066970825195312
iteration 700, loss 1.535455584526062
iteration 800, loss 1.6145583391189575
iteration 0, loss 1.6981747150421143
iteration 100, loss 1.587503433227539
iteration 200, loss 1.6471595764160156
iteration 300, loss 1.6149131059646606
iteration 400, loss 1.607351303100586
iteration 500, loss 1.6460083723068237
iteration 600, loss 1.5791003704071045
iteration 700, loss 1.6228744983673096
iteration 800, loss 1.5711742639541626
iteration 0, loss 1.5957952737808228
iteration 100, loss 1.5843315124511719
iteration 200, loss 1.6590243577957153
iteration 300, loss 1.6323363780975342
iteration 400, loss 1.6853710412979126
iteration 500, loss 1.6706417798995972
iteration 600, loss 1.5862888097763062
iteration 700, loss 1.638736605644226
iteration 800, loss 1.583944320678711
fold 3 accuracy: 0.8172857142857143
iteration 0, loss 1.624323844909668
iteration 100, loss 1.5838379859924316
iteration 200, loss 1.6624281406402588
iteration 300, loss 1.681915044784546
iteration 400, loss 1.602744698524475
iteration 500, loss 1.6227021217346191
iteration 600, loss 1.5892976522445679
iteration 700, loss 1.6320618391036987
iteration 800, loss 1.6083368062973022
iteration 0, loss 1.6063247919082642
iteration 100, loss 1.6176780462265015
iteration 200, loss 1.6388192176818848
iteration 300, loss 1.572936773300171
iteration 400, loss 1.6599773168563843
iteration 500, loss 1.6081643104553223
iteration 600, loss 1.6086480617523193
iteration 700, loss 1.6098754405975342
iteration 800, loss 1.6123111248016357
iteration 0, loss 1.6103283166885376
iteration 100, loss 1.6302309036254883
iteration 200, loss 1.6403584480285645
iteration 300, loss 1.6533386707305908
iteration 400, loss 1.6133655309677124
iteration 500, loss 1.639691710472107
iteration 600, loss 1.664192795753479
iteration 700, loss 1.6483196020126343
iteration 800, loss 1.6799875497817993
iteration 0, loss 1.6115282773971558
iteration 100, loss 1.61782705783844
iteration 200, loss 1.6936386823654175
iteration 300, loss 1.5988599061965942
iteration 400, loss 1.6634869575500488
iteration 500, loss 1.6217987537384033
iteration 600, loss 1.6235703229904175
iteration 700, loss 1.6186683177947998
iteration 800, loss 1.570483684539795
iteration 0, loss 1.6611117124557495
iteration 100, loss 1.6413674354553223
iteration 200, loss 1.6470506191253662
iteration 300, loss 1.6741423606872559
iteration 400, loss 1.6692981719970703
iteration 500, loss 1.65713632106781
iteration 600, loss 1.6274813413619995
iteration 700, loss 1.6379777193069458
iteration 800, loss 1.640890121459961
iteration 0, loss 1.5752627849578857
iteration 100, loss 1.5908012390136719
iteration 200, loss 1.6282086372375488
iteration 300, loss 1.5989577770233154
iteration 400, loss 1.6519616842269897
iteration 500, loss 1.634270191192627
iteration 600, loss 1.646366834640503
iteration 700, loss 1.625706672668457
iteration 800, loss 1.5925376415252686
iteration 0, loss 1.6401947736740112
iteration 100, loss 1.5920757055282593
iteration 200, loss 1.6346850395202637
iteration 300, loss 1.6552956104278564
iteration 400, loss 1.6199209690093994
iteration 500, loss 1.6723424196243286
iteration 600, loss 1.603718876838684
iteration 700, loss 1.575471043586731
iteration 800, loss 1.6953606605529785
iteration 0, loss 1.6196157932281494
iteration 100, loss 1.6580787897109985
iteration 200, loss 1.6345953941345215
iteration 300, loss 1.6228634119033813
iteration 400, loss 1.6718229055404663
iteration 500, loss 1.5931346416473389
iteration 600, loss 1.633884072303772
iteration 700, loss 1.6360200643539429
iteration 800, loss 1.6708627939224243
iteration 0, loss 1.6358851194381714
iteration 100, loss 1.6552245616912842
iteration 200, loss 1.6151806116104126
iteration 300, loss 1.6095763444900513
iteration 400, loss 1.6400012969970703
iteration 500, loss 1.599759578704834
iteration 600, loss 1.601369857788086
iteration 700, loss 1.6758637428283691
iteration 800, loss 1.5833358764648438
iteration 0, loss 1.6473819017410278
iteration 100, loss 1.6374187469482422
iteration 200, loss 1.6291102170944214
iteration 300, loss 1.6032923460006714
iteration 400, loss 1.618343472480774
iteration 500, loss 1.597499966621399
iteration 600, loss 1.6159230470657349
iteration 700, loss 1.6262986660003662
iteration 800, loss 1.641970157623291
iteration 0, loss 1.635958194732666
iteration 100, loss 1.6161670684814453
iteration 200, loss 1.5927585363388062
iteration 300, loss 1.5897350311279297
iteration 400, loss 1.6032215356826782
iteration 500, loss 1.5922460556030273
iteration 600, loss 1.613478422164917
iteration 700, loss 1.6382478475570679
iteration 800, loss 1.597936749458313
iteration 0, loss 1.5902186632156372
iteration 100, loss 1.6078654527664185
iteration 200, loss 1.5938371419906616
iteration 300, loss 1.614219069480896
iteration 400, loss 1.588122844696045
iteration 500, loss 1.6478623151779175
iteration 600, loss 1.6500036716461182
iteration 700, loss 1.6402838230133057
iteration 800, loss 1.6390258073806763
iteration 0, loss 1.6775814294815063
iteration 100, loss 1.580350399017334
iteration 200, loss 1.6216094493865967
iteration 300, loss 1.667677640914917
iteration 400, loss 1.6503565311431885
iteration 500, loss 1.6184699535369873
iteration 600, loss 1.6080420017242432
iteration 700, loss 1.6417195796966553
iteration 800, loss 1.6608974933624268
iteration 0, loss 1.6214121580123901
iteration 100, loss 1.6353485584259033
iteration 200, loss 1.6389546394348145
iteration 300, loss 1.6001371145248413
iteration 400, loss 1.5953142642974854
iteration 500, loss 1.5839649438858032
iteration 600, loss 1.6179202795028687
iteration 700, loss 1.5952223539352417
iteration 800, loss 1.5924307107925415
iteration 0, loss 1.6289364099502563
iteration 100, loss 1.6665796041488647
iteration 200, loss 1.6174651384353638
iteration 300, loss 1.5902721881866455
iteration 400, loss 1.609894037246704
iteration 500, loss 1.6105408668518066
iteration 600, loss 1.662438988685608
iteration 700, loss 1.697350025177002
iteration 800, loss 1.6104973554611206
iteration 0, loss 1.629133701324463
iteration 100, loss 1.5832501649856567
iteration 200, loss 1.638138771057129
iteration 300, loss 1.6401619911193848
iteration 400, loss 1.574253797531128
iteration 500, loss 1.5987870693206787
iteration 600, loss 1.6286143064498901
iteration 700, loss 1.6205132007598877
iteration 800, loss 1.620223879814148
iteration 0, loss 1.6527302265167236
iteration 100, loss 1.6221137046813965
iteration 200, loss 1.6177419424057007
iteration 300, loss 1.6179838180541992
iteration 400, loss 1.621185064315796
iteration 500, loss 1.5912824869155884
iteration 600, loss 1.6612762212753296
iteration 700, loss 1.5772037506103516
iteration 800, loss 1.6046382188796997
iteration 0, loss 1.6752630472183228
iteration 100, loss 1.644869327545166
iteration 200, loss 1.6206626892089844
iteration 300, loss 1.5810655355453491
iteration 400, loss 1.6616607904434204
iteration 500, loss 1.661325216293335
iteration 600, loss 1.707390308380127
iteration 700, loss 1.6081383228302002
iteration 800, loss 1.609537124633789
iteration 0, loss 1.6212517023086548
iteration 100, loss 1.6861896514892578
iteration 200, loss 1.5973894596099854
iteration 300, loss 1.6263737678527832
iteration 400, loss 1.6138863563537598
iteration 500, loss 1.6848138570785522
iteration 600, loss 1.6784677505493164
iteration 700, loss 1.624572515487671
iteration 800, loss 1.6972090005874634
iteration 0, loss 1.5427577495574951
iteration 100, loss 1.5948337316513062
iteration 200, loss 1.6276195049285889
iteration 300, loss 1.5801016092300415
iteration 400, loss 1.580750823020935
iteration 500, loss 1.6518948078155518
iteration 600, loss 1.6445178985595703
iteration 700, loss 1.6167166233062744
iteration 800, loss 1.6362465620040894
iteration 0, loss 1.620719075202942
iteration 100, loss 1.6743922233581543
iteration 200, loss 1.5794546604156494
iteration 300, loss 1.6446579694747925
iteration 400, loss 1.7065682411193848
iteration 500, loss 1.5977954864501953
iteration 600, loss 1.6521729230880737
iteration 700, loss 1.6464011669158936
iteration 800, loss 1.6226122379302979
iteration 0, loss 1.5916061401367188
iteration 100, loss 1.6307655572891235
iteration 200, loss 1.629266381263733
iteration 300, loss 1.7187331914901733
iteration 400, loss 1.6242066621780396
iteration 500, loss 1.6363589763641357
iteration 600, loss 1.5492085218429565
iteration 700, loss 1.620434284210205
iteration 800, loss 1.6474426984786987
iteration 0, loss 1.6188315153121948
iteration 100, loss 1.6135063171386719
iteration 200, loss 1.638052225112915
iteration 300, loss 1.6098880767822266
iteration 400, loss 1.6388888359069824
iteration 500, loss 1.626434087753296
iteration 600, loss 1.6762038469314575
iteration 700, loss 1.602489709854126
iteration 800, loss 1.612828254699707
iteration 0, loss 1.6488221883773804
iteration 100, loss 1.6716653108596802
iteration 200, loss 1.6257390975952148
iteration 300, loss 1.6219325065612793
iteration 400, loss 1.6145623922348022
iteration 500, loss 1.6044501066207886
iteration 600, loss 1.6206547021865845
iteration 700, loss 1.633909821510315
iteration 800, loss 1.5980002880096436
iteration 0, loss 1.5945249795913696
iteration 100, loss 1.6239943504333496
iteration 200, loss 1.5802462100982666
iteration 300, loss 1.6760908365249634
iteration 400, loss 1.611015796661377
iteration 500, loss 1.619962453842163
iteration 600, loss 1.6427967548370361
iteration 700, loss 1.6533584594726562
iteration 800, loss 1.6231378316879272
iteration 0, loss 1.598171353340149
iteration 100, loss 1.638915777206421
iteration 200, loss 1.5855531692504883
iteration 300, loss 1.612541913986206
iteration 400, loss 1.6545779705047607
iteration 500, loss 1.5805690288543701
iteration 600, loss 1.6317808628082275
iteration 700, loss 1.6194614171981812
iteration 800, loss 1.6307038068771362
iteration 0, loss 1.6723226308822632
iteration 100, loss 1.6110140085220337
iteration 200, loss 1.599684476852417
iteration 300, loss 1.588911771774292
iteration 400, loss 1.6860604286193848
iteration 500, loss 1.6408153772354126
iteration 600, loss 1.611251711845398
iteration 700, loss 1.5912652015686035
iteration 800, loss 1.6419092416763306
iteration 0, loss 1.6226325035095215
iteration 100, loss 1.6183462142944336
iteration 200, loss 1.6263476610183716
iteration 300, loss 1.6223396062850952
iteration 400, loss 1.6833246946334839
iteration 500, loss 1.6183418035507202
iteration 600, loss 1.6041510105133057
iteration 700, loss 1.5982998609542847
iteration 800, loss 1.6038099527359009
iteration 0, loss 1.6530640125274658
iteration 100, loss 1.6779521703720093
iteration 200, loss 1.6688700914382935
iteration 300, loss 1.6634485721588135
iteration 400, loss 1.7021564245224
iteration 500, loss 1.5759193897247314
iteration 600, loss 1.6393358707427979
iteration 700, loss 1.6198256015777588
iteration 800, loss 1.617545247077942
iteration 0, loss 1.6367106437683105
iteration 100, loss 1.5802175998687744
iteration 200, loss 1.593805193901062
iteration 300, loss 1.5550748109817505
iteration 400, loss 1.6224932670593262
iteration 500, loss 1.5558242797851562
iteration 600, loss 1.619590163230896
iteration 700, loss 1.6090483665466309
iteration 800, loss 1.5602214336395264
iteration 0, loss 1.6505752801895142
iteration 100, loss 1.6192810535430908
iteration 200, loss 1.6426318883895874
iteration 300, loss 1.6562082767486572
iteration 400, loss 1.6365582942962646
iteration 500, loss 1.5787463188171387
iteration 600, loss 1.6445342302322388
iteration 700, loss 1.6281267404556274
iteration 800, loss 1.6689974069595337
iteration 0, loss 1.5460773706436157
iteration 100, loss 1.5795543193817139
iteration 200, loss 1.6040318012237549
iteration 300, loss 1.5674687623977661
iteration 400, loss 1.6099135875701904
iteration 500, loss 1.690924048423767
iteration 600, loss 1.5964009761810303
iteration 700, loss 1.5769717693328857
iteration 800, loss 1.6318089962005615
iteration 0, loss 1.5780324935913086
iteration 100, loss 1.6322072744369507
iteration 200, loss 1.6397302150726318
iteration 300, loss 1.6463212966918945
iteration 400, loss 1.624977469444275
iteration 500, loss 1.6382309198379517
iteration 600, loss 1.5897068977355957
iteration 700, loss 1.6306085586547852
iteration 800, loss 1.6571506261825562
iteration 0, loss 1.5959925651550293
iteration 100, loss 1.585386872291565
iteration 200, loss 1.6227056980133057
iteration 300, loss 1.638407588005066
iteration 400, loss 1.6192137002944946
iteration 500, loss 1.6046993732452393
iteration 600, loss 1.6489144563674927
iteration 700, loss 1.6422783136367798
iteration 800, loss 1.6243295669555664
iteration 0, loss 1.6315374374389648
iteration 100, loss 1.6391500234603882
iteration 200, loss 1.6632333993911743
iteration 300, loss 1.6329330205917358
iteration 400, loss 1.5695300102233887
iteration 500, loss 1.600567102432251
iteration 600, loss 1.600279450416565
iteration 700, loss 1.665841817855835
iteration 800, loss 1.6368544101715088
iteration 0, loss 1.595868706703186
iteration 100, loss 1.6756658554077148
iteration 200, loss 1.5898220539093018
iteration 300, loss 1.5997307300567627
iteration 400, loss 1.6062740087509155
iteration 500, loss 1.6313258409500122
iteration 600, loss 1.5965428352355957
iteration 700, loss 1.6179167032241821
iteration 800, loss 1.6834110021591187
iteration 0, loss 1.638329029083252
iteration 100, loss 1.6502777338027954
iteration 200, loss 1.640840768814087
iteration 300, loss 1.6152727603912354
iteration 400, loss 1.5687601566314697
iteration 500, loss 1.6254764795303345
iteration 600, loss 1.555073618888855
iteration 700, loss 1.6207807064056396
iteration 800, loss 1.6318860054016113
iteration 0, loss 1.6442592144012451
iteration 100, loss 1.6251500844955444
iteration 200, loss 1.6389639377593994
iteration 300, loss 1.596893310546875
iteration 400, loss 1.5968537330627441
iteration 500, loss 1.5965416431427002
iteration 600, loss 1.6778515577316284
iteration 700, loss 1.6212899684906006
iteration 800, loss 1.6340185403823853
iteration 0, loss 1.6001468896865845
iteration 100, loss 1.6432045698165894
iteration 200, loss 1.5776898860931396
iteration 300, loss 1.6056269407272339
iteration 400, loss 1.6151306629180908
iteration 500, loss 1.5951898097991943
iteration 600, loss 1.5727516412734985
iteration 700, loss 1.633435845375061
iteration 800, loss 1.6823158264160156
iteration 0, loss 1.628680944442749
iteration 100, loss 1.6683745384216309
iteration 200, loss 1.5541760921478271
iteration 300, loss 1.6544301509857178
iteration 400, loss 1.6203267574310303
iteration 500, loss 1.5678329467773438
iteration 600, loss 1.5652209520339966
iteration 700, loss 1.632509469985962
iteration 800, loss 1.6017563343048096
iteration 0, loss 1.6052738428115845
iteration 100, loss 1.6294561624526978
iteration 200, loss 1.6499028205871582
iteration 300, loss 1.6370880603790283
iteration 400, loss 1.6336368322372437
iteration 500, loss 1.6312355995178223
iteration 600, loss 1.6257057189941406
iteration 700, loss 1.676361322402954
iteration 800, loss 1.6774778366088867
iteration 0, loss 1.7004461288452148
iteration 100, loss 1.5910401344299316
iteration 200, loss 1.637829065322876
iteration 300, loss 1.5614947080612183
iteration 400, loss 1.6033872365951538
iteration 500, loss 1.6280490159988403
iteration 600, loss 1.6449220180511475
iteration 700, loss 1.5768245458602905
iteration 800, loss 1.6360410451889038
iteration 0, loss 1.613677740097046
iteration 100, loss 1.6074532270431519
iteration 200, loss 1.640897512435913
iteration 300, loss 1.647502064704895
iteration 400, loss 1.6411337852478027
iteration 500, loss 1.6009998321533203
iteration 600, loss 1.5612677335739136
iteration 700, loss 1.6094887256622314
iteration 800, loss 1.6736477613449097
iteration 0, loss 1.6171374320983887
iteration 100, loss 1.630725383758545
iteration 200, loss 1.6333035230636597
iteration 300, loss 1.5649614334106445
iteration 400, loss 1.6163203716278076
iteration 500, loss 1.6418613195419312
iteration 600, loss 1.5994471311569214
iteration 700, loss 1.6547294855117798
iteration 800, loss 1.6211119890213013
iteration 0, loss 1.6404385566711426
iteration 100, loss 1.6203562021255493
iteration 200, loss 1.6057583093643188
iteration 300, loss 1.6212232112884521
iteration 400, loss 1.6258670091629028
iteration 500, loss 1.6460076570510864
iteration 600, loss 1.6181325912475586
iteration 700, loss 1.609039545059204
iteration 800, loss 1.6461526155471802
iteration 0, loss 1.6181856393814087
iteration 100, loss 1.6474878787994385
iteration 200, loss 1.6601061820983887
iteration 300, loss 1.6738507747650146
iteration 400, loss 1.5905269384384155
iteration 500, loss 1.6070891618728638
iteration 600, loss 1.6316770315170288
iteration 700, loss 1.5591824054718018
iteration 800, loss 1.6290165185928345
iteration 0, loss 1.6263952255249023
iteration 100, loss 1.6832640171051025
iteration 200, loss 1.6276086568832397
iteration 300, loss 1.6092844009399414
iteration 400, loss 1.6601744890213013
iteration 500, loss 1.6558899879455566
iteration 600, loss 1.6754106283187866
iteration 700, loss 1.5846449136734009
iteration 800, loss 1.6030045747756958
iteration 0, loss 1.5887542963027954
iteration 100, loss 1.5753347873687744
iteration 200, loss 1.5852410793304443
iteration 300, loss 1.5879051685333252
iteration 400, loss 1.6213107109069824
iteration 500, loss 1.6166636943817139
iteration 600, loss 1.5534584522247314
iteration 700, loss 1.6670197248458862
iteration 800, loss 1.591597080230713
iteration 0, loss 1.655390977859497
iteration 100, loss 1.6068971157073975
iteration 200, loss 1.6665257215499878
iteration 300, loss 1.6672430038452148
iteration 400, loss 1.5930629968643188
iteration 500, loss 1.5996805429458618
iteration 600, loss 1.5831081867218018
iteration 700, loss 1.6494702100753784
iteration 800, loss 1.6037288904190063
iteration 0, loss 1.6392241716384888
iteration 100, loss 1.6261076927185059
iteration 200, loss 1.6330275535583496
iteration 300, loss 1.5958794355392456
iteration 400, loss 1.6519807577133179
iteration 500, loss 1.6636899709701538
iteration 600, loss 1.6437081098556519
iteration 700, loss 1.6908292770385742
iteration 800, loss 1.61880362033844
fold 4 accuracy: 0.8196428571428571
[2024-02-29 03:37:59,414] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-29 03:37:59,415] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         654     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       840 MACs
fwd flops per GPU:                                                      1.68 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.68 K  
fwd latency:                                                            310.42 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    5.41 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '654'}
    MACs        - {'KronLayer': '840 MACs'}
    fwd latency - {'KronLayer': '310.42 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  654 = 100% Params, 840 MACs = 100% MACs, 310.42 us = 100% latency, 5.41 MFLOPS
  (kronlinear): KronLinear(654 = 100% Params, 840 MACs = 100% MACs, 223.16 us = 71.89% latency, 7.53 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 7.83% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-29 03:37:59,418] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.68 K 654
iteration 0, loss 2.2976109981536865
iteration 100, loss 2.265052556991577
iteration 200, loss 2.0818564891815186
iteration 300, loss 1.9637038707733154
iteration 400, loss 1.9343042373657227
iteration 500, loss 1.8905680179595947
iteration 600, loss 1.8419159650802612
iteration 700, loss 1.7813236713409424
iteration 800, loss 1.8405022621154785
iteration 0, loss 1.7863354682922363
iteration 100, loss 1.7935501337051392
iteration 200, loss 1.8354530334472656
iteration 300, loss 1.7955949306488037
iteration 400, loss 1.7530707120895386
iteration 500, loss 1.798105239868164
iteration 600, loss 1.8039543628692627
iteration 700, loss 1.7854959964752197
iteration 800, loss 1.7021456956863403
iteration 0, loss 1.799129843711853
iteration 100, loss 1.7536754608154297
iteration 200, loss 1.7043817043304443
iteration 300, loss 1.8298267126083374
iteration 400, loss 1.7801393270492554
iteration 500, loss 1.749836802482605
iteration 600, loss 1.701485514640808
iteration 700, loss 1.72670316696167
iteration 800, loss 1.78215491771698
iteration 0, loss 1.7253050804138184
iteration 100, loss 1.7429128885269165
iteration 200, loss 1.7959051132202148
iteration 300, loss 1.6626944541931152
iteration 400, loss 1.6356834173202515
iteration 500, loss 1.6918278932571411
iteration 600, loss 1.7010493278503418
iteration 700, loss 1.737870693206787
iteration 800, loss 1.6931452751159668
iteration 0, loss 1.721112608909607
iteration 100, loss 1.6411993503570557
iteration 200, loss 1.790101408958435
iteration 300, loss 1.735202431678772
iteration 400, loss 1.7295660972595215
iteration 500, loss 1.6766654253005981
iteration 600, loss 1.691568374633789
iteration 700, loss 1.743080496788025
iteration 800, loss 1.6806691884994507
iteration 0, loss 1.6861393451690674
iteration 100, loss 1.6139092445373535
iteration 200, loss 1.7262765169143677
iteration 300, loss 1.7433257102966309
iteration 400, loss 1.708798885345459
iteration 500, loss 1.6846892833709717
iteration 600, loss 1.7524640560150146
iteration 700, loss 1.7024924755096436
iteration 800, loss 1.6868984699249268
iteration 0, loss 1.7815871238708496
iteration 100, loss 1.6419787406921387
iteration 200, loss 1.6514567136764526
iteration 300, loss 1.6867434978485107
iteration 400, loss 1.672120451927185
iteration 500, loss 1.6397380828857422
iteration 600, loss 1.7524129152297974
iteration 700, loss 1.7091138362884521
iteration 800, loss 1.691178321838379
iteration 0, loss 1.6575473546981812
iteration 100, loss 1.6853338479995728
iteration 200, loss 1.621171236038208
iteration 300, loss 1.6540154218673706
iteration 400, loss 1.7212722301483154
iteration 500, loss 1.6498987674713135
iteration 600, loss 1.6986877918243408
iteration 700, loss 1.6070501804351807
iteration 800, loss 1.6531758308410645
iteration 0, loss 1.6646373271942139
iteration 100, loss 1.6663519144058228
iteration 200, loss 1.6638487577438354
iteration 300, loss 1.6953198909759521
iteration 400, loss 1.7304153442382812
iteration 500, loss 1.678053617477417
iteration 600, loss 1.64887535572052
iteration 700, loss 1.6619948148727417
iteration 800, loss 1.6805075407028198
iteration 0, loss 1.6733908653259277
iteration 100, loss 1.625230073928833
iteration 200, loss 1.7210332155227661
iteration 300, loss 1.6384330987930298
iteration 400, loss 1.648318886756897
iteration 500, loss 1.6173255443572998
iteration 600, loss 1.632751226425171
iteration 700, loss 1.6539497375488281
iteration 800, loss 1.7052462100982666
iteration 0, loss 1.692094087600708
iteration 100, loss 1.6879454851150513
iteration 200, loss 1.6945183277130127
iteration 300, loss 1.6732186079025269
iteration 400, loss 1.6750801801681519
iteration 500, loss 1.6198779344558716
iteration 600, loss 1.6471202373504639
iteration 700, loss 1.6380202770233154
iteration 800, loss 1.712763786315918
iteration 0, loss 1.6700870990753174
iteration 100, loss 1.6117602586746216
iteration 200, loss 1.7071940898895264
iteration 300, loss 1.5779123306274414
iteration 400, loss 1.6621769666671753
iteration 500, loss 1.6277624368667603
iteration 600, loss 1.6771883964538574
iteration 700, loss 1.662136197090149
iteration 800, loss 1.666494369506836
iteration 0, loss 1.6600189208984375
iteration 100, loss 1.6293789148330688
iteration 200, loss 1.714324712753296
iteration 300, loss 1.6399555206298828
iteration 400, loss 1.669877052307129
iteration 500, loss 1.6168913841247559
iteration 600, loss 1.6440845727920532
iteration 700, loss 1.644269585609436
iteration 800, loss 1.6757248640060425
iteration 0, loss 1.6676952838897705
iteration 100, loss 1.6628376245498657
iteration 200, loss 1.5733941793441772
iteration 300, loss 1.6646097898483276
iteration 400, loss 1.606903076171875
iteration 500, loss 1.6779567003250122
iteration 600, loss 1.5915164947509766
iteration 700, loss 1.6664440631866455
iteration 800, loss 1.6838350296020508
iteration 0, loss 1.6146001815795898
iteration 100, loss 1.61980402469635
iteration 200, loss 1.6537866592407227
iteration 300, loss 1.6024376153945923
iteration 400, loss 1.6681835651397705
iteration 500, loss 1.5868675708770752
iteration 600, loss 1.6886296272277832
iteration 700, loss 1.6473385095596313
iteration 800, loss 1.6851890087127686
iteration 0, loss 1.7109452486038208
iteration 100, loss 1.6807705163955688
iteration 200, loss 1.699385643005371
iteration 300, loss 1.5973314046859741
iteration 400, loss 1.6791846752166748
iteration 500, loss 1.6937791109085083
iteration 600, loss 1.6165246963500977
iteration 700, loss 1.6832786798477173
iteration 800, loss 1.6843323707580566
iteration 0, loss 1.6216822862625122
iteration 100, loss 1.6277570724487305
iteration 200, loss 1.6603055000305176
iteration 300, loss 1.6483956575393677
iteration 400, loss 1.6597602367401123
iteration 500, loss 1.5962562561035156
iteration 600, loss 1.6037482023239136
iteration 700, loss 1.689069390296936
iteration 800, loss 1.6999225616455078
iteration 0, loss 1.6545491218566895
iteration 100, loss 1.708999514579773
iteration 200, loss 1.657943606376648
iteration 300, loss 1.725272297859192
iteration 400, loss 1.6611502170562744
iteration 500, loss 1.661751389503479
iteration 600, loss 1.6428284645080566
iteration 700, loss 1.6433969736099243
iteration 800, loss 1.6386332511901855
iteration 0, loss 1.5970284938812256
iteration 100, loss 1.6479439735412598
iteration 200, loss 1.636148452758789
iteration 300, loss 1.7204174995422363
iteration 400, loss 1.654239535331726
iteration 500, loss 1.6184673309326172
iteration 600, loss 1.6602275371551514
iteration 700, loss 1.6464735269546509
iteration 800, loss 1.6207923889160156
iteration 0, loss 1.6845710277557373
iteration 100, loss 1.6521813869476318
iteration 200, loss 1.6520839929580688
iteration 300, loss 1.645371913909912
iteration 400, loss 1.644357442855835
iteration 500, loss 1.6647857427597046
iteration 600, loss 1.6566637754440308
iteration 700, loss 1.6000149250030518
iteration 800, loss 1.662832498550415
iteration 0, loss 1.628650188446045
iteration 100, loss 1.6509064435958862
iteration 200, loss 1.6221342086791992
iteration 300, loss 1.676376223564148
iteration 400, loss 1.6682943105697632
iteration 500, loss 1.6136438846588135
iteration 600, loss 1.6482412815093994
iteration 700, loss 1.643048882484436
iteration 800, loss 1.6334264278411865
iteration 0, loss 1.5916866064071655
iteration 100, loss 1.6183323860168457
iteration 200, loss 1.6700332164764404
iteration 300, loss 1.6069481372833252
iteration 400, loss 1.7140525579452515
iteration 500, loss 1.634627342224121
iteration 600, loss 1.6412785053253174
iteration 700, loss 1.7100976705551147
iteration 800, loss 1.6661944389343262
iteration 0, loss 1.635345458984375
iteration 100, loss 1.655737042427063
iteration 200, loss 1.6559149026870728
iteration 300, loss 1.6179664134979248
iteration 400, loss 1.6639517545700073
iteration 500, loss 1.662176489830017
iteration 600, loss 1.6323317289352417
iteration 700, loss 1.631949543952942
iteration 800, loss 1.676180124282837
iteration 0, loss 1.6951004266738892
iteration 100, loss 1.658431887626648
iteration 200, loss 1.6506500244140625
iteration 300, loss 1.6680430173873901
iteration 400, loss 1.5898839235305786
iteration 500, loss 1.631232500076294
iteration 600, loss 1.6232579946517944
iteration 700, loss 1.681018590927124
iteration 800, loss 1.6730453968048096
iteration 0, loss 1.6703521013259888
iteration 100, loss 1.6668833494186401
iteration 200, loss 1.6397619247436523
iteration 300, loss 1.6410324573516846
iteration 400, loss 1.7185218334197998
iteration 500, loss 1.6218031644821167
iteration 600, loss 1.6863502264022827
iteration 700, loss 1.6805810928344727
iteration 800, loss 1.6739557981491089
iteration 0, loss 1.6587302684783936
iteration 100, loss 1.6209715604782104
iteration 200, loss 1.6606931686401367
iteration 300, loss 1.680202603340149
iteration 400, loss 1.6654541492462158
iteration 500, loss 1.6828784942626953
iteration 600, loss 1.6701571941375732
iteration 700, loss 1.7046397924423218
iteration 800, loss 1.6870366334915161
iteration 0, loss 1.5763963460922241
iteration 100, loss 1.684241533279419
iteration 200, loss 1.6755017042160034
iteration 300, loss 1.5945796966552734
iteration 400, loss 1.6510769128799438
iteration 500, loss 1.6338447332382202
iteration 600, loss 1.7010133266448975
iteration 700, loss 1.6227656602859497
iteration 800, loss 1.6049737930297852
iteration 0, loss 1.6755855083465576
iteration 100, loss 1.6456316709518433
iteration 200, loss 1.5802199840545654
iteration 300, loss 1.657099723815918
iteration 400, loss 1.6383932828903198
iteration 500, loss 1.6687474250793457
iteration 600, loss 1.627962350845337
iteration 700, loss 1.6006622314453125
iteration 800, loss 1.6719615459442139
iteration 0, loss 1.6671348810195923
iteration 100, loss 1.6043403148651123
iteration 200, loss 1.600038766860962
iteration 300, loss 1.5712532997131348
iteration 400, loss 1.575373649597168
iteration 500, loss 1.629638433456421
iteration 600, loss 1.6819273233413696
iteration 700, loss 1.6507028341293335
iteration 800, loss 1.5955541133880615
iteration 0, loss 1.6802124977111816
iteration 100, loss 1.6252996921539307
iteration 200, loss 1.7000093460083008
iteration 300, loss 1.7123394012451172
iteration 400, loss 1.6734564304351807
iteration 500, loss 1.6571003198623657
iteration 600, loss 1.6653562784194946
iteration 700, loss 1.6248527765274048
iteration 800, loss 1.6502749919891357
iteration 0, loss 1.6383295059204102
iteration 100, loss 1.6528524160385132
iteration 200, loss 1.683410406112671
iteration 300, loss 1.6851739883422852
iteration 400, loss 1.6125177145004272
iteration 500, loss 1.6488317251205444
iteration 600, loss 1.6550376415252686
iteration 700, loss 1.6138361692428589
iteration 800, loss 1.6846665143966675
iteration 0, loss 1.5995330810546875
iteration 100, loss 1.6570801734924316
iteration 200, loss 1.647302508354187
iteration 300, loss 1.6718924045562744
iteration 400, loss 1.7095900774002075
iteration 500, loss 1.6002449989318848
iteration 600, loss 1.6148055791854858
iteration 700, loss 1.6532039642333984
iteration 800, loss 1.6450005769729614
iteration 0, loss 1.6300194263458252
iteration 100, loss 1.6342166662216187
iteration 200, loss 1.6209850311279297
iteration 300, loss 1.6436184644699097
iteration 400, loss 1.665911078453064
iteration 500, loss 1.724190592765808
iteration 600, loss 1.6470357179641724
iteration 700, loss 1.6169421672821045
iteration 800, loss 1.6376091241836548
iteration 0, loss 1.6353806257247925
iteration 100, loss 1.5893185138702393
iteration 200, loss 1.6426035165786743
iteration 300, loss 1.6694257259368896
iteration 400, loss 1.6580535173416138
iteration 500, loss 1.65166175365448
iteration 600, loss 1.6338894367218018
iteration 700, loss 1.6238702535629272
iteration 800, loss 1.6634786128997803
iteration 0, loss 1.6810033321380615
iteration 100, loss 1.6295838356018066
iteration 200, loss 1.7059919834136963
iteration 300, loss 1.6020652055740356
iteration 400, loss 1.7276824712753296
iteration 500, loss 1.6063621044158936
iteration 600, loss 1.6394715309143066
iteration 700, loss 1.629206657409668
iteration 800, loss 1.6755015850067139
iteration 0, loss 1.6418272256851196
iteration 100, loss 1.5825659036636353
iteration 200, loss 1.5877467393875122
iteration 300, loss 1.641930341720581
iteration 400, loss 1.6342765092849731
iteration 500, loss 1.6512408256530762
iteration 600, loss 1.6170209646224976
iteration 700, loss 1.644658088684082
iteration 800, loss 1.702925682067871
iteration 0, loss 1.5890603065490723
iteration 100, loss 1.613022804260254
iteration 200, loss 1.6439776420593262
iteration 300, loss 1.6954835653305054
iteration 400, loss 1.6927459239959717
iteration 500, loss 1.5740275382995605
iteration 600, loss 1.6675018072128296
iteration 700, loss 1.6274834871292114
iteration 800, loss 1.621968388557434
iteration 0, loss 1.600281000137329
iteration 100, loss 1.7032822370529175
iteration 200, loss 1.6137044429779053
iteration 300, loss 1.6560977697372437
iteration 400, loss 1.6955668926239014
iteration 500, loss 1.6402310132980347
iteration 600, loss 1.6738221645355225
iteration 700, loss 1.6725974082946777
iteration 800, loss 1.6303179264068604
iteration 0, loss 1.5930196046829224
iteration 100, loss 1.6514577865600586
iteration 200, loss 1.5923024415969849
iteration 300, loss 1.650826334953308
iteration 400, loss 1.6943861246109009
iteration 500, loss 1.6520280838012695
iteration 600, loss 1.6864526271820068
iteration 700, loss 1.6219253540039062
iteration 800, loss 1.678837776184082
iteration 0, loss 1.661754846572876
iteration 100, loss 1.6589908599853516
iteration 200, loss 1.6447443962097168
iteration 300, loss 1.6302471160888672
iteration 400, loss 1.6306803226470947
iteration 500, loss 1.6587055921554565
iteration 600, loss 1.6121104955673218
iteration 700, loss 1.605021595954895
iteration 800, loss 1.6044317483901978
iteration 0, loss 1.6926110982894897
iteration 100, loss 1.6236743927001953
iteration 200, loss 1.583087682723999
iteration 300, loss 1.6482689380645752
iteration 400, loss 1.6823278665542603
iteration 500, loss 1.6032649278640747
iteration 600, loss 1.6560131311416626
iteration 700, loss 1.6231331825256348
iteration 800, loss 1.6585357189178467
iteration 0, loss 1.6136536598205566
iteration 100, loss 1.5873064994812012
iteration 200, loss 1.6064683198928833
iteration 300, loss 1.6593812704086304
iteration 400, loss 1.6665384769439697
iteration 500, loss 1.7264938354492188
iteration 600, loss 1.5737160444259644
iteration 700, loss 1.6571017503738403
iteration 800, loss 1.6330978870391846
iteration 0, loss 1.6075150966644287
iteration 100, loss 1.6512768268585205
iteration 200, loss 1.667665719985962
iteration 300, loss 1.6840626001358032
iteration 400, loss 1.6513649225234985
iteration 500, loss 1.7541968822479248
iteration 600, loss 1.6565626859664917
iteration 700, loss 1.6501518487930298
iteration 800, loss 1.613476276397705
iteration 0, loss 1.6130707263946533
iteration 100, loss 1.6286066770553589
iteration 200, loss 1.6226723194122314
iteration 300, loss 1.5913755893707275
iteration 400, loss 1.6565203666687012
iteration 500, loss 1.5988538265228271
iteration 600, loss 1.6017968654632568
iteration 700, loss 1.6648857593536377
iteration 800, loss 1.6419596672058105
iteration 0, loss 1.578595519065857
iteration 100, loss 1.6338647603988647
iteration 200, loss 1.62336003780365
iteration 300, loss 1.7014241218566895
iteration 400, loss 1.6913094520568848
iteration 500, loss 1.6437636613845825
iteration 600, loss 1.6676385402679443
iteration 700, loss 1.5784868001937866
iteration 800, loss 1.6537619829177856
iteration 0, loss 1.6965361833572388
iteration 100, loss 1.614005208015442
iteration 200, loss 1.6798200607299805
iteration 300, loss 1.681565284729004
iteration 400, loss 1.6486177444458008
iteration 500, loss 1.6083730459213257
iteration 600, loss 1.6179367303848267
iteration 700, loss 1.6409331560134888
iteration 800, loss 1.6648433208465576
iteration 0, loss 1.613350749015808
iteration 100, loss 1.576589584350586
iteration 200, loss 1.5782707929611206
iteration 300, loss 1.6334002017974854
iteration 400, loss 1.5935145616531372
iteration 500, loss 1.6584488153457642
iteration 600, loss 1.616776466369629
iteration 700, loss 1.600806713104248
iteration 800, loss 1.6850589513778687
iteration 0, loss 1.6174392700195312
iteration 100, loss 1.6544870138168335
iteration 200, loss 1.6019326448440552
iteration 300, loss 1.6553003787994385
iteration 400, loss 1.6013799905776978
iteration 500, loss 1.6107603311538696
iteration 600, loss 1.6218196153640747
iteration 700, loss 1.6335780620574951
iteration 800, loss 1.6584358215332031
iteration 0, loss 1.619325876235962
iteration 100, loss 1.6454633474349976
iteration 200, loss 1.6358468532562256
iteration 300, loss 1.6193755865097046
iteration 400, loss 1.5802785158157349
iteration 500, loss 1.6504859924316406
iteration 600, loss 1.659664511680603
iteration 700, loss 1.6504366397857666
iteration 800, loss 1.6059983968734741
iteration 0, loss 1.6143543720245361
iteration 100, loss 1.6159937381744385
iteration 200, loss 1.585539698600769
iteration 300, loss 1.6222537755966187
iteration 400, loss 1.6778212785720825
iteration 500, loss 1.6509402990341187
iteration 600, loss 1.6573121547698975
iteration 700, loss 1.6421887874603271
iteration 800, loss 1.671276330947876
fold 0 accuracy: 0.8037142857142857
iteration 0, loss 1.6197727918624878
iteration 100, loss 1.6803278923034668
iteration 200, loss 1.5955172777175903
iteration 300, loss 1.6329658031463623
iteration 400, loss 1.6379201412200928
iteration 500, loss 1.6169042587280273
iteration 600, loss 1.6083002090454102
iteration 700, loss 1.590964436531067
iteration 800, loss 1.6046700477600098
iteration 0, loss 1.6035076379776
iteration 100, loss 1.600441575050354
iteration 200, loss 1.6379518508911133
iteration 300, loss 1.6005362272262573
iteration 400, loss 1.652554988861084
iteration 500, loss 1.6387232542037964
iteration 600, loss 1.618201494216919
iteration 700, loss 1.640529990196228
iteration 800, loss 1.623525857925415
iteration 0, loss 1.6075650453567505
iteration 100, loss 1.6360241174697876
iteration 200, loss 1.6275782585144043
iteration 300, loss 1.5838404893875122
iteration 400, loss 1.648182988166809
iteration 500, loss 1.5665638446807861
iteration 600, loss 1.627087116241455
iteration 700, loss 1.661516547203064
iteration 800, loss 1.6226356029510498
iteration 0, loss 1.6117273569107056
iteration 100, loss 1.6205908060073853
iteration 200, loss 1.5853289365768433
iteration 300, loss 1.618719220161438
iteration 400, loss 1.629995346069336
iteration 500, loss 1.6135613918304443
iteration 600, loss 1.6428861618041992
iteration 700, loss 1.5780702829360962
iteration 800, loss 1.6226775646209717
iteration 0, loss 1.6091911792755127
iteration 100, loss 1.6014342308044434
iteration 200, loss 1.6547859907150269
iteration 300, loss 1.619771957397461
iteration 400, loss 1.6324174404144287
iteration 500, loss 1.620581030845642
iteration 600, loss 1.6116783618927002
iteration 700, loss 1.6277426481246948
iteration 800, loss 1.6306241750717163
iteration 0, loss 1.620453953742981
iteration 100, loss 1.6414120197296143
iteration 200, loss 1.652928352355957
iteration 300, loss 1.6240628957748413
iteration 400, loss 1.655769944190979
iteration 500, loss 1.6374571323394775
iteration 600, loss 1.6185096502304077
iteration 700, loss 1.6384164094924927
iteration 800, loss 1.602595329284668
iteration 0, loss 1.6119145154953003
iteration 100, loss 1.6270527839660645
iteration 200, loss 1.681423544883728
iteration 300, loss 1.5860894918441772
iteration 400, loss 1.652925968170166
iteration 500, loss 1.6424998044967651
iteration 600, loss 1.6244057416915894
iteration 700, loss 1.6751679182052612
iteration 800, loss 1.6498085260391235
iteration 0, loss 1.5892488956451416
iteration 100, loss 1.59712553024292
iteration 200, loss 1.6159714460372925
iteration 300, loss 1.697043538093567
iteration 400, loss 1.6000704765319824
iteration 500, loss 1.5671430826187134
iteration 600, loss 1.6510339975357056
iteration 700, loss 1.700652003288269
iteration 800, loss 1.5977824926376343
iteration 0, loss 1.553529143333435
iteration 100, loss 1.5873318910598755
iteration 200, loss 1.6210408210754395
iteration 300, loss 1.620059609413147
iteration 400, loss 1.6400177478790283
iteration 500, loss 1.590789794921875
iteration 600, loss 1.7074639797210693
iteration 700, loss 1.571819543838501
iteration 800, loss 1.6345702409744263
iteration 0, loss 1.5878658294677734
iteration 100, loss 1.654930830001831
iteration 200, loss 1.615807294845581
iteration 300, loss 1.632218837738037
iteration 400, loss 1.6786226034164429
iteration 500, loss 1.661522626876831
iteration 600, loss 1.5957247018814087
iteration 700, loss 1.689859390258789
iteration 800, loss 1.580741047859192
iteration 0, loss 1.6194552183151245
iteration 100, loss 1.6538441181182861
iteration 200, loss 1.602386713027954
iteration 300, loss 1.6226953268051147
iteration 400, loss 1.6440470218658447
iteration 500, loss 1.6674997806549072
iteration 600, loss 1.683998942375183
iteration 700, loss 1.616268277168274
iteration 800, loss 1.5897349119186401
iteration 0, loss 1.7006794214248657
iteration 100, loss 1.601106882095337
iteration 200, loss 1.6748511791229248
iteration 300, loss 1.6428334712982178
iteration 400, loss 1.6724050045013428
iteration 500, loss 1.5888502597808838
iteration 600, loss 1.6255336999893188
iteration 700, loss 1.6416740417480469
iteration 800, loss 1.5790048837661743
iteration 0, loss 1.6238417625427246
iteration 100, loss 1.6432819366455078
iteration 200, loss 1.588518738746643
iteration 300, loss 1.6105461120605469
iteration 400, loss 1.6208701133728027
iteration 500, loss 1.7305320501327515
iteration 600, loss 1.6052900552749634
iteration 700, loss 1.6252708435058594
iteration 800, loss 1.6087759733200073
iteration 0, loss 1.6048927307128906
iteration 100, loss 1.6382033824920654
iteration 200, loss 1.663055658340454
iteration 300, loss 1.6803525686264038
iteration 400, loss 1.6914163827896118
iteration 500, loss 1.5809412002563477
iteration 600, loss 1.6655457019805908
iteration 700, loss 1.684127926826477
iteration 800, loss 1.5443627834320068
iteration 0, loss 1.5862796306610107
iteration 100, loss 1.633658766746521
iteration 200, loss 1.5874993801116943
iteration 300, loss 1.6337610483169556
iteration 400, loss 1.6376595497131348
iteration 500, loss 1.6309665441513062
iteration 600, loss 1.6121469736099243
iteration 700, loss 1.6649249792099
iteration 800, loss 1.6629676818847656
iteration 0, loss 1.7061454057693481
iteration 100, loss 1.6193612813949585
iteration 200, loss 1.644308090209961
iteration 300, loss 1.5840413570404053
iteration 400, loss 1.6317704916000366
iteration 500, loss 1.6554149389266968
iteration 600, loss 1.6858526468276978
iteration 700, loss 1.6073617935180664
iteration 800, loss 1.6707525253295898
iteration 0, loss 1.6007088422775269
iteration 100, loss 1.6742310523986816
iteration 200, loss 1.6698890924453735
iteration 300, loss 1.6825456619262695
iteration 400, loss 1.6422265768051147
iteration 500, loss 1.5842660665512085
iteration 600, loss 1.668354868888855
iteration 700, loss 1.675057053565979
iteration 800, loss 1.6766259670257568
iteration 0, loss 1.6619267463684082
iteration 100, loss 1.618894100189209
iteration 200, loss 1.6656055450439453
iteration 300, loss 1.6103016138076782
iteration 400, loss 1.56986665725708
iteration 500, loss 1.6304049491882324
iteration 600, loss 1.6461217403411865
iteration 700, loss 1.614208459854126
iteration 800, loss 1.6027886867523193
iteration 0, loss 1.6110296249389648
iteration 100, loss 1.5843486785888672
iteration 200, loss 1.634805679321289
iteration 300, loss 1.6566959619522095
iteration 400, loss 1.6311100721359253
iteration 500, loss 1.6328097581863403
iteration 600, loss 1.5970511436462402
iteration 700, loss 1.7228957414627075
iteration 800, loss 1.7073761224746704
iteration 0, loss 1.6432150602340698
iteration 100, loss 1.6712404489517212
iteration 200, loss 1.5972787141799927
iteration 300, loss 1.5945345163345337
iteration 400, loss 1.683851957321167
iteration 500, loss 1.6261893510818481
iteration 600, loss 1.6302632093429565
iteration 700, loss 1.649538278579712
iteration 800, loss 1.660079002380371
iteration 0, loss 1.5809657573699951
iteration 100, loss 1.6101024150848389
iteration 200, loss 1.6617259979248047
iteration 300, loss 1.6237373352050781
iteration 400, loss 1.5601787567138672
iteration 500, loss 1.6649699211120605
iteration 600, loss 1.6630358695983887
iteration 700, loss 1.5734989643096924
iteration 800, loss 1.5933271646499634
iteration 0, loss 1.6660125255584717
iteration 100, loss 1.6174339056015015
iteration 200, loss 1.6297632455825806
iteration 300, loss 1.6682376861572266
iteration 400, loss 1.6260050535202026
iteration 500, loss 1.616953730583191
iteration 600, loss 1.7075663805007935
iteration 700, loss 1.6323422193527222
iteration 800, loss 1.606481909751892
iteration 0, loss 1.6200487613677979
iteration 100, loss 1.6292593479156494
iteration 200, loss 1.7216973304748535
iteration 300, loss 1.6237388849258423
iteration 400, loss 1.6581125259399414
iteration 500, loss 1.6076242923736572
iteration 600, loss 1.616824984550476
iteration 700, loss 1.5666018724441528
iteration 800, loss 1.6815353631973267
iteration 0, loss 1.61431086063385
iteration 100, loss 1.6678284406661987
iteration 200, loss 1.5559617280960083
iteration 300, loss 1.6153355836868286
iteration 400, loss 1.6093387603759766
iteration 500, loss 1.621703863143921
iteration 600, loss 1.6823548078536987
iteration 700, loss 1.6511794328689575
iteration 800, loss 1.597541332244873
iteration 0, loss 1.616598129272461
iteration 100, loss 1.6317596435546875
iteration 200, loss 1.6649830341339111
iteration 300, loss 1.628103494644165
iteration 400, loss 1.6536314487457275
iteration 500, loss 1.5501024723052979
iteration 600, loss 1.615299940109253
iteration 700, loss 1.6697893142700195
iteration 800, loss 1.6419458389282227
iteration 0, loss 1.6153464317321777
iteration 100, loss 1.5548731088638306
iteration 200, loss 1.6147414445877075
iteration 300, loss 1.5611177682876587
iteration 400, loss 1.6439954042434692
iteration 500, loss 1.601054310798645
iteration 600, loss 1.719679832458496
iteration 700, loss 1.6628109216690063
iteration 800, loss 1.5957155227661133
iteration 0, loss 1.6028237342834473
iteration 100, loss 1.6233134269714355
iteration 200, loss 1.6415992975234985
iteration 300, loss 1.6317840814590454
iteration 400, loss 1.6084680557250977
iteration 500, loss 1.631811499595642
iteration 600, loss 1.598354458808899
iteration 700, loss 1.5922496318817139
iteration 800, loss 1.6263407468795776
iteration 0, loss 1.677769660949707
iteration 100, loss 1.6599866151809692
iteration 200, loss 1.6363171339035034
iteration 300, loss 1.6474921703338623
iteration 400, loss 1.6217625141143799
iteration 500, loss 1.6531343460083008
iteration 600, loss 1.6256636381149292
iteration 700, loss 1.6222385168075562
iteration 800, loss 1.6636989116668701
iteration 0, loss 1.6273363828659058
iteration 100, loss 1.5603325366973877
iteration 200, loss 1.6558853387832642
iteration 300, loss 1.6075026988983154
iteration 400, loss 1.6476536989212036
iteration 500, loss 1.646594762802124
iteration 600, loss 1.62454092502594
iteration 700, loss 1.6261138916015625
iteration 800, loss 1.6383540630340576
iteration 0, loss 1.6082665920257568
iteration 100, loss 1.5631259679794312
iteration 200, loss 1.7142276763916016
iteration 300, loss 1.6222798824310303
iteration 400, loss 1.6653543710708618
iteration 500, loss 1.6279488801956177
iteration 600, loss 1.6410001516342163
iteration 700, loss 1.7060130834579468
iteration 800, loss 1.6080552339553833
iteration 0, loss 1.6126874685287476
iteration 100, loss 1.6296294927597046
iteration 200, loss 1.607519507408142
iteration 300, loss 1.5941646099090576
iteration 400, loss 1.58904230594635
iteration 500, loss 1.6078548431396484
iteration 600, loss 1.6258597373962402
iteration 700, loss 1.6372418403625488
iteration 800, loss 1.6695352792739868
iteration 0, loss 1.6450883150100708
iteration 100, loss 1.624601125717163
iteration 200, loss 1.6082520484924316
iteration 300, loss 1.6429545879364014
iteration 400, loss 1.6685465574264526
iteration 500, loss 1.6226997375488281
iteration 600, loss 1.6205320358276367
iteration 700, loss 1.6523860692977905
iteration 800, loss 1.651290774345398
iteration 0, loss 1.6451302766799927
iteration 100, loss 1.6289154291152954
iteration 200, loss 1.6549432277679443
iteration 300, loss 1.640160083770752
iteration 400, loss 1.615150809288025
iteration 500, loss 1.623713493347168
iteration 600, loss 1.623303771018982
iteration 700, loss 1.607315182685852
iteration 800, loss 1.6316463947296143
iteration 0, loss 1.6652286052703857
iteration 100, loss 1.6664777994155884
iteration 200, loss 1.6170521974563599
iteration 300, loss 1.614607572555542
iteration 400, loss 1.655669093132019
iteration 500, loss 1.6695761680603027
iteration 600, loss 1.6363569498062134
iteration 700, loss 1.7313638925552368
iteration 800, loss 1.6408385038375854
iteration 0, loss 1.683266520500183
iteration 100, loss 1.7073408365249634
iteration 200, loss 1.614080548286438
iteration 300, loss 1.618438482284546
iteration 400, loss 1.6601110696792603
iteration 500, loss 1.630635380744934
iteration 600, loss 1.6522530317306519
iteration 700, loss 1.6727286577224731
iteration 800, loss 1.6219357252120972
iteration 0, loss 1.5943619012832642
iteration 100, loss 1.6518187522888184
iteration 200, loss 1.6825820207595825
iteration 300, loss 1.7234660387039185
iteration 400, loss 1.6773395538330078
iteration 500, loss 1.6194361448287964
iteration 600, loss 1.6851955652236938
iteration 700, loss 1.633194088935852
iteration 800, loss 1.6052252054214478
iteration 0, loss 1.6574922800064087
iteration 100, loss 1.6871153116226196
iteration 200, loss 1.6340227127075195
iteration 300, loss 1.6035468578338623
iteration 400, loss 1.705336332321167
iteration 500, loss 1.5850138664245605
iteration 600, loss 1.6340638399124146
iteration 700, loss 1.6222937107086182
iteration 800, loss 1.5427578687667847
iteration 0, loss 1.6052095890045166
iteration 100, loss 1.5981348752975464
iteration 200, loss 1.6271007061004639
iteration 300, loss 1.6563069820404053
iteration 400, loss 1.6424895524978638
iteration 500, loss 1.6562999486923218
iteration 600, loss 1.611972451210022
iteration 700, loss 1.6107767820358276
iteration 800, loss 1.667720079421997
iteration 0, loss 1.7080694437026978
iteration 100, loss 1.5942203998565674
iteration 200, loss 1.6583565473556519
iteration 300, loss 1.6705492734909058
iteration 400, loss 1.628186583518982
iteration 500, loss 1.590747356414795
iteration 600, loss 1.5946054458618164
iteration 700, loss 1.6085820198059082
iteration 800, loss 1.656076431274414
iteration 0, loss 1.660404920578003
iteration 100, loss 1.5909712314605713
iteration 200, loss 1.652815580368042
iteration 300, loss 1.6287592649459839
iteration 400, loss 1.6520800590515137
iteration 500, loss 1.6065503358840942
iteration 600, loss 1.6006768941879272
iteration 700, loss 1.5962700843811035
iteration 800, loss 1.6794008016586304
iteration 0, loss 1.615253210067749
iteration 100, loss 1.581411361694336
iteration 200, loss 1.6476049423217773
iteration 300, loss 1.6406636238098145
iteration 400, loss 1.58229398727417
iteration 500, loss 1.642410397529602
iteration 600, loss 1.624889850616455
iteration 700, loss 1.644127368927002
iteration 800, loss 1.651233196258545
iteration 0, loss 1.6236869096755981
iteration 100, loss 1.6387993097305298
iteration 200, loss 1.6389647722244263
iteration 300, loss 1.6037606000900269
iteration 400, loss 1.6625558137893677
iteration 500, loss 1.6289312839508057
iteration 600, loss 1.6392260789871216
iteration 700, loss 1.6395792961120605
iteration 800, loss 1.6296958923339844
iteration 0, loss 1.664942741394043
iteration 100, loss 1.6103464365005493
iteration 200, loss 1.6495691537857056
iteration 300, loss 1.6254678964614868
iteration 400, loss 1.6408225297927856
iteration 500, loss 1.6483815908432007
iteration 600, loss 1.5901058912277222
iteration 700, loss 1.6306589841842651
iteration 800, loss 1.7049860954284668
iteration 0, loss 1.666293740272522
iteration 100, loss 1.670220136642456
iteration 200, loss 1.6572825908660889
iteration 300, loss 1.6499325037002563
iteration 400, loss 1.6542850732803345
iteration 500, loss 1.6950033903121948
iteration 600, loss 1.6320708990097046
iteration 700, loss 1.636201024055481
iteration 800, loss 1.5849850177764893
iteration 0, loss 1.571624755859375
iteration 100, loss 1.6374891996383667
iteration 200, loss 1.6277457475662231
iteration 300, loss 1.6162081956863403
iteration 400, loss 1.6229736804962158
iteration 500, loss 1.6133009195327759
iteration 600, loss 1.5758219957351685
iteration 700, loss 1.6070866584777832
iteration 800, loss 1.6810534000396729
iteration 0, loss 1.649796962738037
iteration 100, loss 1.6275843381881714
iteration 200, loss 1.6630035638809204
iteration 300, loss 1.620522379875183
iteration 400, loss 1.640730381011963
iteration 500, loss 1.613654375076294
iteration 600, loss 1.629151463508606
iteration 700, loss 1.6290357112884521
iteration 800, loss 1.599259853363037
iteration 0, loss 1.668274998664856
iteration 100, loss 1.6422120332717896
iteration 200, loss 1.6485072374343872
iteration 300, loss 1.6290937662124634
iteration 400, loss 1.6117839813232422
iteration 500, loss 1.6080514192581177
iteration 600, loss 1.5860934257507324
iteration 700, loss 1.5774364471435547
iteration 800, loss 1.635542869567871
iteration 0, loss 1.5983402729034424
iteration 100, loss 1.6955407857894897
iteration 200, loss 1.5631952285766602
iteration 300, loss 1.6105599403381348
iteration 400, loss 1.6059845685958862
iteration 500, loss 1.6004000902175903
iteration 600, loss 1.631463646888733
iteration 700, loss 1.6318936347961426
iteration 800, loss 1.705367922782898
iteration 0, loss 1.653904914855957
iteration 100, loss 1.5946862697601318
iteration 200, loss 1.6204380989074707
iteration 300, loss 1.6500550508499146
iteration 400, loss 1.6215628385543823
iteration 500, loss 1.600447416305542
iteration 600, loss 1.5463008880615234
iteration 700, loss 1.5994051694869995
iteration 800, loss 1.649659514427185
iteration 0, loss 1.6257264614105225
iteration 100, loss 1.6333602666854858
iteration 200, loss 1.6969935894012451
iteration 300, loss 1.664582371711731
iteration 400, loss 1.6234714984893799
iteration 500, loss 1.6672766208648682
iteration 600, loss 1.584466814994812
iteration 700, loss 1.6053720712661743
iteration 800, loss 1.6682778596878052
fold 1 accuracy: 0.8110714285714286
iteration 0, loss 1.6335129737854004
iteration 100, loss 1.6931651830673218
iteration 200, loss 1.6366002559661865
iteration 300, loss 1.6222364902496338
iteration 400, loss 1.6469653844833374
iteration 500, loss 1.601789951324463
iteration 600, loss 1.6244083642959595
iteration 700, loss 1.578383207321167
iteration 800, loss 1.6201719045639038
iteration 0, loss 1.6509639024734497
iteration 100, loss 1.6053260564804077
iteration 200, loss 1.6098185777664185
iteration 300, loss 1.5663870573043823
iteration 400, loss 1.7099553346633911
iteration 500, loss 1.655866265296936
iteration 600, loss 1.612484335899353
iteration 700, loss 1.6021678447723389
iteration 800, loss 1.6169337034225464
iteration 0, loss 1.6769475936889648
iteration 100, loss 1.630521535873413
iteration 200, loss 1.6677851676940918
iteration 300, loss 1.5687997341156006
iteration 400, loss 1.6055960655212402
iteration 500, loss 1.6561691761016846
iteration 600, loss 1.6528886556625366
iteration 700, loss 1.692810297012329
iteration 800, loss 1.628654956817627
iteration 0, loss 1.5871602296829224
iteration 100, loss 1.6094555854797363
iteration 200, loss 1.613008975982666
iteration 300, loss 1.6464439630508423
iteration 400, loss 1.5956405401229858
iteration 500, loss 1.6219549179077148
iteration 600, loss 1.6026028394699097
iteration 700, loss 1.604750633239746
iteration 800, loss 1.6419923305511475
iteration 0, loss 1.6444429159164429
iteration 100, loss 1.647060751914978
iteration 200, loss 1.6215671300888062
iteration 300, loss 1.6135591268539429
iteration 400, loss 1.5941903591156006
iteration 500, loss 1.5833145380020142
iteration 600, loss 1.638695478439331
iteration 700, loss 1.6824837923049927
iteration 800, loss 1.62543785572052
iteration 0, loss 1.653135061264038
iteration 100, loss 1.5988504886627197
iteration 200, loss 1.643568515777588
iteration 300, loss 1.6179360151290894
iteration 400, loss 1.6758198738098145
iteration 500, loss 1.669623851776123
iteration 600, loss 1.6510181427001953
iteration 700, loss 1.5842292308807373
iteration 800, loss 1.6216167211532593
iteration 0, loss 1.5764561891555786
iteration 100, loss 1.6182141304016113
iteration 200, loss 1.6725902557373047
iteration 300, loss 1.6126635074615479
iteration 400, loss 1.627690076828003
iteration 500, loss 1.633872628211975
iteration 600, loss 1.6192423105239868
iteration 700, loss 1.5944979190826416
iteration 800, loss 1.6716649532318115
iteration 0, loss 1.6077402830123901
iteration 100, loss 1.654421091079712
iteration 200, loss 1.6574411392211914
iteration 300, loss 1.6697978973388672
iteration 400, loss 1.6343532800674438
iteration 500, loss 1.7025058269500732
iteration 600, loss 1.6082597970962524
iteration 700, loss 1.6994839906692505
iteration 800, loss 1.6303372383117676
iteration 0, loss 1.6138875484466553
iteration 100, loss 1.6797914505004883
iteration 200, loss 1.6170862913131714
iteration 300, loss 1.6842410564422607
iteration 400, loss 1.591866135597229
iteration 500, loss 1.6601201295852661
iteration 600, loss 1.649834394454956
iteration 700, loss 1.6028261184692383
iteration 800, loss 1.635136604309082
iteration 0, loss 1.6359440088272095
iteration 100, loss 1.5719549655914307
iteration 200, loss 1.6383179426193237
iteration 300, loss 1.5888495445251465
iteration 400, loss 1.6016744375228882
iteration 500, loss 1.607116937637329
iteration 600, loss 1.5798144340515137
iteration 700, loss 1.5716643333435059
iteration 800, loss 1.6060116291046143
iteration 0, loss 1.6062098741531372
iteration 100, loss 1.653960943222046
iteration 200, loss 1.7020384073257446
iteration 300, loss 1.7012083530426025
iteration 400, loss 1.6641488075256348
iteration 500, loss 1.6049377918243408
iteration 600, loss 1.5711863040924072
iteration 700, loss 1.6226179599761963
iteration 800, loss 1.6300907135009766
iteration 0, loss 1.597251057624817
iteration 100, loss 1.5848883390426636
iteration 200, loss 1.7115012407302856
iteration 300, loss 1.6452430486679077
iteration 400, loss 1.633791446685791
iteration 500, loss 1.6723378896713257
iteration 600, loss 1.6251201629638672
iteration 700, loss 1.5994014739990234
iteration 800, loss 1.6126247644424438
iteration 0, loss 1.601711392402649
iteration 100, loss 1.6300406455993652
iteration 200, loss 1.5781599283218384
iteration 300, loss 1.6656028032302856
iteration 400, loss 1.6120024919509888
iteration 500, loss 1.556517243385315
iteration 600, loss 1.6205153465270996
iteration 700, loss 1.651027798652649
iteration 800, loss 1.6372863054275513
iteration 0, loss 1.6605263948440552
iteration 100, loss 1.6309911012649536
iteration 200, loss 1.6179511547088623
iteration 300, loss 1.6418730020523071
iteration 400, loss 1.6708054542541504
iteration 500, loss 1.5810234546661377
iteration 600, loss 1.6123466491699219
iteration 700, loss 1.5632438659667969
iteration 800, loss 1.746769666671753
iteration 0, loss 1.6283864974975586
iteration 100, loss 1.593024730682373
iteration 200, loss 1.6217154264450073
iteration 300, loss 1.660227656364441
iteration 400, loss 1.6262211799621582
iteration 500, loss 1.6430597305297852
iteration 600, loss 1.6540101766586304
iteration 700, loss 1.6448146104812622
iteration 800, loss 1.6181083917617798
iteration 0, loss 1.6167490482330322
iteration 100, loss 1.6747498512268066
iteration 200, loss 1.6414618492126465
iteration 300, loss 1.6733369827270508
iteration 400, loss 1.7094634771347046
iteration 500, loss 1.6124120950698853
iteration 600, loss 1.605083703994751
iteration 700, loss 1.680585265159607
iteration 800, loss 1.6658555269241333
iteration 0, loss 1.6683080196380615
iteration 100, loss 1.6066153049468994
iteration 200, loss 1.6886074542999268
iteration 300, loss 1.658907175064087
iteration 400, loss 1.6864951848983765
iteration 500, loss 1.7160887718200684
iteration 600, loss 1.5885590314865112
iteration 700, loss 1.6331547498703003
iteration 800, loss 1.6223427057266235
iteration 0, loss 1.6287323236465454
iteration 100, loss 1.6426182985305786
iteration 200, loss 1.574265956878662
iteration 300, loss 1.6379696130752563
iteration 400, loss 1.6373049020767212
iteration 500, loss 1.6203067302703857
iteration 600, loss 1.6543443202972412
iteration 700, loss 1.684827208518982
iteration 800, loss 1.6370749473571777
iteration 0, loss 1.6345757246017456
iteration 100, loss 1.6466143131256104
iteration 200, loss 1.6279360055923462
iteration 300, loss 1.6813682317733765
iteration 400, loss 1.6524280309677124
iteration 500, loss 1.5840550661087036
iteration 600, loss 1.6472487449645996
iteration 700, loss 1.609228491783142
iteration 800, loss 1.6184042692184448
iteration 0, loss 1.6456925868988037
iteration 100, loss 1.661383867263794
iteration 200, loss 1.612225890159607
iteration 300, loss 1.6132841110229492
iteration 400, loss 1.6492881774902344
iteration 500, loss 1.6166582107543945
iteration 600, loss 1.598726749420166
iteration 700, loss 1.6251169443130493
iteration 800, loss 1.6586252450942993
iteration 0, loss 1.6251187324523926
iteration 100, loss 1.6328601837158203
iteration 200, loss 1.5402023792266846
iteration 300, loss 1.6152048110961914
iteration 400, loss 1.614825963973999
iteration 500, loss 1.6191763877868652
iteration 600, loss 1.6083849668502808
iteration 700, loss 1.598873496055603
iteration 800, loss 1.5969429016113281
iteration 0, loss 1.6216208934783936
iteration 100, loss 1.579465627670288
iteration 200, loss 1.580227255821228
iteration 300, loss 1.618146300315857
iteration 400, loss 1.5976989269256592
iteration 500, loss 1.6062712669372559
iteration 600, loss 1.68246591091156
iteration 700, loss 1.649411678314209
iteration 800, loss 1.6153937578201294
iteration 0, loss 1.636008381843567
iteration 100, loss 1.7044601440429688
iteration 200, loss 1.605205774307251
iteration 300, loss 1.6749918460845947
iteration 400, loss 1.6333688497543335
iteration 500, loss 1.5902173519134521
iteration 600, loss 1.5964338779449463
iteration 700, loss 1.6123169660568237
iteration 800, loss 1.6924858093261719
iteration 0, loss 1.6808710098266602
iteration 100, loss 1.5748730897903442
iteration 200, loss 1.6085740327835083
iteration 300, loss 1.6256775856018066
iteration 400, loss 1.6064094305038452
iteration 500, loss 1.6291381120681763
iteration 600, loss 1.5719408988952637
iteration 700, loss 1.7099969387054443
iteration 800, loss 1.6667070388793945
iteration 0, loss 1.595015525817871
iteration 100, loss 1.5936602354049683
iteration 200, loss 1.6073447465896606
iteration 300, loss 1.6508017778396606
iteration 400, loss 1.6400768756866455
iteration 500, loss 1.6346619129180908
iteration 600, loss 1.6574243307113647
iteration 700, loss 1.6200857162475586
iteration 800, loss 1.646246314048767
iteration 0, loss 1.6212615966796875
iteration 100, loss 1.63680100440979
iteration 200, loss 1.6513285636901855
iteration 300, loss 1.6539082527160645
iteration 400, loss 1.664025068283081
iteration 500, loss 1.6558407545089722
iteration 600, loss 1.6302862167358398
iteration 700, loss 1.5819875001907349
iteration 800, loss 1.6439298391342163
iteration 0, loss 1.622750997543335
iteration 100, loss 1.6146647930145264
iteration 200, loss 1.6525378227233887
iteration 300, loss 1.5833512544631958
iteration 400, loss 1.6287283897399902
iteration 500, loss 1.6401337385177612
iteration 600, loss 1.5882036685943604
iteration 700, loss 1.6592907905578613
iteration 800, loss 1.6433441638946533
iteration 0, loss 1.6262990236282349
iteration 100, loss 1.608393669128418
iteration 200, loss 1.6198172569274902
iteration 300, loss 1.5819506645202637
iteration 400, loss 1.6527297496795654
iteration 500, loss 1.6463974714279175
iteration 600, loss 1.616891622543335
iteration 700, loss 1.595271110534668
iteration 800, loss 1.6227580308914185
iteration 0, loss 1.6104336977005005
iteration 100, loss 1.6259688138961792
iteration 200, loss 1.753191590309143
iteration 300, loss 1.6570169925689697
iteration 400, loss 1.6198176145553589
iteration 500, loss 1.6648095846176147
iteration 600, loss 1.614480972290039
iteration 700, loss 1.6169971227645874
iteration 800, loss 1.6710072755813599
iteration 0, loss 1.6045657396316528
iteration 100, loss 1.6119292974472046
iteration 200, loss 1.615277886390686
iteration 300, loss 1.622758388519287
iteration 400, loss 1.6487441062927246
iteration 500, loss 1.6740206480026245
iteration 600, loss 1.6161105632781982
iteration 700, loss 1.6306289434432983
iteration 800, loss 1.602257490158081
iteration 0, loss 1.5725150108337402
iteration 100, loss 1.6107227802276611
iteration 200, loss 1.6239089965820312
iteration 300, loss 1.6417982578277588
iteration 400, loss 1.5679093599319458
iteration 500, loss 1.5920777320861816
iteration 600, loss 1.5944852828979492
iteration 700, loss 1.6368520259857178
iteration 800, loss 1.6313085556030273
iteration 0, loss 1.6339970827102661
iteration 100, loss 1.6323491334915161
iteration 200, loss 1.6523833274841309
iteration 300, loss 1.6572598218917847
iteration 400, loss 1.6202417612075806
iteration 500, loss 1.5653315782546997
iteration 600, loss 1.5869557857513428
iteration 700, loss 1.5641603469848633
iteration 800, loss 1.6371625661849976
iteration 0, loss 1.706822395324707
iteration 100, loss 1.6893671751022339
iteration 200, loss 1.6550798416137695
iteration 300, loss 1.5678699016571045
iteration 400, loss 1.6822196245193481
iteration 500, loss 1.647157907485962
iteration 600, loss 1.664732813835144
iteration 700, loss 1.7233355045318604
iteration 800, loss 1.6281609535217285
iteration 0, loss 1.5880625247955322
iteration 100, loss 1.6380575895309448
iteration 200, loss 1.6345306634902954
iteration 300, loss 1.6284027099609375
iteration 400, loss 1.6822054386138916
iteration 500, loss 1.5946933031082153
iteration 600, loss 1.6873748302459717
iteration 700, loss 1.6421908140182495
iteration 800, loss 1.6077275276184082
iteration 0, loss 1.6940302848815918
iteration 100, loss 1.6508979797363281
iteration 200, loss 1.5907416343688965
iteration 300, loss 1.6673030853271484
iteration 400, loss 1.6387470960617065
iteration 500, loss 1.570007562637329
iteration 600, loss 1.6561819314956665
iteration 700, loss 1.609859585762024
iteration 800, loss 1.5923221111297607
iteration 0, loss 1.664106845855713
iteration 100, loss 1.594855546951294
iteration 200, loss 1.5824475288391113
iteration 300, loss 1.6116917133331299
iteration 400, loss 1.6296794414520264
iteration 500, loss 1.6059870719909668
iteration 600, loss 1.5989681482315063
iteration 700, loss 1.631256341934204
iteration 800, loss 1.5868772268295288
iteration 0, loss 1.6375142335891724
iteration 100, loss 1.672719955444336
iteration 200, loss 1.7046788930892944
iteration 300, loss 1.5922894477844238
iteration 400, loss 1.6284360885620117
iteration 500, loss 1.6182550191879272
iteration 600, loss 1.6424347162246704
iteration 700, loss 1.5696005821228027
iteration 800, loss 1.687235951423645
iteration 0, loss 1.6149353981018066
iteration 100, loss 1.664764165878296
iteration 200, loss 1.6117992401123047
iteration 300, loss 1.6816799640655518
iteration 400, loss 1.6260924339294434
iteration 500, loss 1.6028034687042236
iteration 600, loss 1.644921064376831
iteration 700, loss 1.6751890182495117
iteration 800, loss 1.5842390060424805
iteration 0, loss 1.5749925374984741
iteration 100, loss 1.6022186279296875
iteration 200, loss 1.5938624143600464
iteration 300, loss 1.6172528266906738
iteration 400, loss 1.6019090414047241
iteration 500, loss 1.6428784132003784
iteration 600, loss 1.6491014957427979
iteration 700, loss 1.586621880531311
iteration 800, loss 1.6164803504943848
iteration 0, loss 1.6650354862213135
iteration 100, loss 1.6002154350280762
iteration 200, loss 1.605347752571106
iteration 300, loss 1.6791032552719116
iteration 400, loss 1.6122996807098389
iteration 500, loss 1.588034749031067
iteration 600, loss 1.6513776779174805
iteration 700, loss 1.6189380884170532
iteration 800, loss 1.6490845680236816
iteration 0, loss 1.6143618822097778
iteration 100, loss 1.5760258436203003
iteration 200, loss 1.6375919580459595
iteration 300, loss 1.6199702024459839
iteration 400, loss 1.6179498434066772
iteration 500, loss 1.6178257465362549
iteration 600, loss 1.6920537948608398
iteration 700, loss 1.6148749589920044
iteration 800, loss 1.6027723550796509
iteration 0, loss 1.6094615459442139
iteration 100, loss 1.6486331224441528
iteration 200, loss 1.6492763757705688
iteration 300, loss 1.6403347253799438
iteration 400, loss 1.6146905422210693
iteration 500, loss 1.6442508697509766
iteration 600, loss 1.6478599309921265
iteration 700, loss 1.5858991146087646
iteration 800, loss 1.6229714155197144
iteration 0, loss 1.6868385076522827
iteration 100, loss 1.5829575061798096
iteration 200, loss 1.6123545169830322
iteration 300, loss 1.618896245956421
iteration 400, loss 1.6275575160980225
iteration 500, loss 1.6496869325637817
iteration 600, loss 1.6676461696624756
iteration 700, loss 1.6156001091003418
iteration 800, loss 1.6074271202087402
iteration 0, loss 1.5941723585128784
iteration 100, loss 1.6565518379211426
iteration 200, loss 1.6188137531280518
iteration 300, loss 1.5753765106201172
iteration 400, loss 1.6795037984848022
iteration 500, loss 1.5832726955413818
iteration 600, loss 1.667229413986206
iteration 700, loss 1.6446402072906494
iteration 800, loss 1.623476266860962
iteration 0, loss 1.6130014657974243
iteration 100, loss 1.6304471492767334
iteration 200, loss 1.5950320959091187
iteration 300, loss 1.6140124797821045
iteration 400, loss 1.671521782875061
iteration 500, loss 1.617343783378601
iteration 600, loss 1.6608233451843262
iteration 700, loss 1.6213233470916748
iteration 800, loss 1.6359907388687134
iteration 0, loss 1.609576940536499
iteration 100, loss 1.5554250478744507
iteration 200, loss 1.6546461582183838
iteration 300, loss 1.7149598598480225
iteration 400, loss 1.6224110126495361
iteration 500, loss 1.6197786331176758
iteration 600, loss 1.629233956336975
iteration 700, loss 1.5768685340881348
iteration 800, loss 1.5910218954086304
iteration 0, loss 1.604543924331665
iteration 100, loss 1.5996297597885132
iteration 200, loss 1.609166145324707
iteration 300, loss 1.6159693002700806
iteration 400, loss 1.633453369140625
iteration 500, loss 1.5920190811157227
iteration 600, loss 1.6535207033157349
iteration 700, loss 1.590036153793335
iteration 800, loss 1.6082649230957031
iteration 0, loss 1.591021180152893
iteration 100, loss 1.6363457441329956
iteration 200, loss 1.601232647895813
iteration 300, loss 1.5910403728485107
iteration 400, loss 1.6869291067123413
iteration 500, loss 1.6245427131652832
iteration 600, loss 1.6434015035629272
iteration 700, loss 1.7015197277069092
iteration 800, loss 1.7037334442138672
iteration 0, loss 1.6480138301849365
iteration 100, loss 1.6145268678665161
iteration 200, loss 1.5874348878860474
iteration 300, loss 1.6166846752166748
iteration 400, loss 1.650037169456482
iteration 500, loss 1.5858454704284668
iteration 600, loss 1.6461747884750366
iteration 700, loss 1.6340028047561646
iteration 800, loss 1.5752389430999756
iteration 0, loss 1.651268720626831
iteration 100, loss 1.6028705835342407
iteration 200, loss 1.562742829322815
iteration 300, loss 1.6094080209732056
iteration 400, loss 1.5837968587875366
iteration 500, loss 1.6514540910720825
iteration 600, loss 1.6090682744979858
iteration 700, loss 1.609244465827942
iteration 800, loss 1.5463435649871826
fold 2 accuracy: 0.8105714285714286
iteration 0, loss 1.622600793838501
iteration 100, loss 1.6480952501296997
iteration 200, loss 1.642331600189209
iteration 300, loss 1.6212495565414429
iteration 400, loss 1.6522002220153809
iteration 500, loss 1.6340525150299072
iteration 600, loss 1.580794334411621
iteration 700, loss 1.6839019060134888
iteration 800, loss 1.6306860446929932
iteration 0, loss 1.6037983894348145
iteration 100, loss 1.646988868713379
iteration 200, loss 1.634163737297058
iteration 300, loss 1.611878514289856
iteration 400, loss 1.6188997030258179
iteration 500, loss 1.586503505706787
iteration 600, loss 1.6138836145401
iteration 700, loss 1.6043695211410522
iteration 800, loss 1.612005591392517
iteration 0, loss 1.5803472995758057
iteration 100, loss 1.6297764778137207
iteration 200, loss 1.600577712059021
iteration 300, loss 1.706743836402893
iteration 400, loss 1.6098148822784424
iteration 500, loss 1.6768741607666016
iteration 600, loss 1.624470829963684
iteration 700, loss 1.7107453346252441
iteration 800, loss 1.5976133346557617
iteration 0, loss 1.7142254114151
iteration 100, loss 1.6139390468597412
iteration 200, loss 1.6504404544830322
iteration 300, loss 1.642556071281433
iteration 400, loss 1.6387519836425781
iteration 500, loss 1.6244102716445923
iteration 600, loss 1.630877137184143
iteration 700, loss 1.5890694856643677
iteration 800, loss 1.6227911710739136
iteration 0, loss 1.6098700761795044
iteration 100, loss 1.565388560295105
iteration 200, loss 1.6000837087631226
iteration 300, loss 1.6699705123901367
iteration 400, loss 1.6489429473876953
iteration 500, loss 1.6692678928375244
iteration 600, loss 1.6330162286758423
iteration 700, loss 1.620927095413208
iteration 800, loss 1.6191484928131104
iteration 0, loss 1.6409200429916382
iteration 100, loss 1.6456245183944702
iteration 200, loss 1.602549433708191
iteration 300, loss 1.737737774848938
iteration 400, loss 1.6523125171661377
iteration 500, loss 1.6218258142471313
iteration 600, loss 1.5848406553268433
iteration 700, loss 1.6363526582717896
iteration 800, loss 1.7084758281707764
iteration 0, loss 1.6445382833480835
iteration 100, loss 1.5757416486740112
iteration 200, loss 1.5767474174499512
iteration 300, loss 1.6930232048034668
iteration 400, loss 1.625175952911377
iteration 500, loss 1.5816830396652222
iteration 600, loss 1.6112838983535767
iteration 700, loss 1.650187373161316
iteration 800, loss 1.6137858629226685
iteration 0, loss 1.6274913549423218
iteration 100, loss 1.5749893188476562
iteration 200, loss 1.6383179426193237
iteration 300, loss 1.653075098991394
iteration 400, loss 1.603296160697937
iteration 500, loss 1.6073511838912964
iteration 600, loss 1.6857198476791382
iteration 700, loss 1.652816653251648
iteration 800, loss 1.6655845642089844
iteration 0, loss 1.6439096927642822
iteration 100, loss 1.6515365839004517
iteration 200, loss 1.6364470720291138
iteration 300, loss 1.6686077117919922
iteration 400, loss 1.6223875284194946
iteration 500, loss 1.5921719074249268
iteration 600, loss 1.64226233959198
iteration 700, loss 1.7035881280899048
iteration 800, loss 1.6595221757888794
iteration 0, loss 1.6254961490631104
iteration 100, loss 1.63726007938385
iteration 200, loss 1.6100683212280273
iteration 300, loss 1.577139139175415
iteration 400, loss 1.5781667232513428
iteration 500, loss 1.5399473905563354
iteration 600, loss 1.5799241065979004
iteration 700, loss 1.6918219327926636
iteration 800, loss 1.644669532775879
iteration 0, loss 1.564683437347412
iteration 100, loss 1.6562812328338623
iteration 200, loss 1.645891547203064
iteration 300, loss 1.6177312135696411
iteration 400, loss 1.602502465248108
iteration 500, loss 1.611601710319519
iteration 600, loss 1.6820569038391113
iteration 700, loss 1.6120117902755737
iteration 800, loss 1.6053245067596436
iteration 0, loss 1.6106951236724854
iteration 100, loss 1.618794322013855
iteration 200, loss 1.6496026515960693
iteration 300, loss 1.6438568830490112
iteration 400, loss 1.590274453163147
iteration 500, loss 1.56498122215271
iteration 600, loss 1.632047176361084
iteration 700, loss 1.6535159349441528
iteration 800, loss 1.663508415222168
iteration 0, loss 1.6411627531051636
iteration 100, loss 1.625793695449829
iteration 200, loss 1.5691367387771606
iteration 300, loss 1.6950736045837402
iteration 400, loss 1.6006286144256592
iteration 500, loss 1.6044902801513672
iteration 600, loss 1.5908867120742798
iteration 700, loss 1.6232593059539795
iteration 800, loss 1.6789125204086304
iteration 0, loss 1.574187994003296
iteration 100, loss 1.5955686569213867
iteration 200, loss 1.6491750478744507
iteration 300, loss 1.6877107620239258
iteration 400, loss 1.6724402904510498
iteration 500, loss 1.6238923072814941
iteration 600, loss 1.6332277059555054
iteration 700, loss 1.6203161478042603
iteration 800, loss 1.6424752473831177
iteration 0, loss 1.6390748023986816
iteration 100, loss 1.5992757081985474
iteration 200, loss 1.5853164196014404
iteration 300, loss 1.6430308818817139
iteration 400, loss 1.6613917350769043
iteration 500, loss 1.6793270111083984
iteration 600, loss 1.6422199010849
iteration 700, loss 1.649547815322876
iteration 800, loss 1.6025655269622803
iteration 0, loss 1.6381136178970337
iteration 100, loss 1.6928043365478516
iteration 200, loss 1.6091299057006836
iteration 300, loss 1.7210606336593628
iteration 400, loss 1.7260795831680298
iteration 500, loss 1.6792234182357788
iteration 600, loss 1.6363258361816406
iteration 700, loss 1.6072497367858887
iteration 800, loss 1.6094856262207031
iteration 0, loss 1.5878520011901855
iteration 100, loss 1.5542750358581543
iteration 200, loss 1.6535848379135132
iteration 300, loss 1.6767362356185913
iteration 400, loss 1.6299166679382324
iteration 500, loss 1.5674247741699219
iteration 600, loss 1.6538317203521729
iteration 700, loss 1.621037244796753
iteration 800, loss 1.5749778747558594
iteration 0, loss 1.6053428649902344
iteration 100, loss 1.5888705253601074
iteration 200, loss 1.622432827949524
iteration 300, loss 1.5875524282455444
iteration 400, loss 1.5952764749526978
iteration 500, loss 1.6089000701904297
iteration 600, loss 1.6809366941452026
iteration 700, loss 1.6008645296096802
iteration 800, loss 1.644975185394287
iteration 0, loss 1.6297956705093384
iteration 100, loss 1.6668329238891602
iteration 200, loss 1.5885586738586426
iteration 300, loss 1.562100887298584
iteration 400, loss 1.5894830226898193
iteration 500, loss 1.677070140838623
iteration 600, loss 1.5732591152191162
iteration 700, loss 1.6186567544937134
iteration 800, loss 1.6033716201782227
iteration 0, loss 1.674446940422058
iteration 100, loss 1.5679351091384888
iteration 200, loss 1.6433236598968506
iteration 300, loss 1.634945034980774
iteration 400, loss 1.6087783575057983
iteration 500, loss 1.5989910364151
iteration 600, loss 1.5700496435165405
iteration 700, loss 1.5674033164978027
iteration 800, loss 1.611454963684082
iteration 0, loss 1.628242015838623
iteration 100, loss 1.589133620262146
iteration 200, loss 1.607848048210144
iteration 300, loss 1.6490696668624878
iteration 400, loss 1.6181000471115112
iteration 500, loss 1.679309368133545
iteration 600, loss 1.6773853302001953
iteration 700, loss 1.7004319429397583
iteration 800, loss 1.6478663682937622
iteration 0, loss 1.5487523078918457
iteration 100, loss 1.6376851797103882
iteration 200, loss 1.5869582891464233
iteration 300, loss 1.6484143733978271
iteration 400, loss 1.6425683498382568
iteration 500, loss 1.6086875200271606
iteration 600, loss 1.6194956302642822
iteration 700, loss 1.6639716625213623
iteration 800, loss 1.6556416749954224
iteration 0, loss 1.6471798419952393
iteration 100, loss 1.627577781677246
iteration 200, loss 1.62542724609375
iteration 300, loss 1.629462718963623
iteration 400, loss 1.6459102630615234
iteration 500, loss 1.6402478218078613
iteration 600, loss 1.665406346321106
iteration 700, loss 1.5791243314743042
iteration 800, loss 1.6566007137298584
iteration 0, loss 1.643774151802063
iteration 100, loss 1.608725666999817
iteration 200, loss 1.609045147895813
iteration 300, loss 1.609833002090454
iteration 400, loss 1.618760108947754
iteration 500, loss 1.609800100326538
iteration 600, loss 1.6941654682159424
iteration 700, loss 1.6243621110916138
iteration 800, loss 1.7199726104736328
iteration 0, loss 1.5887418985366821
iteration 100, loss 1.5764219760894775
iteration 200, loss 1.6963086128234863
iteration 300, loss 1.6422395706176758
iteration 400, loss 1.5960172414779663
iteration 500, loss 1.5860991477966309
iteration 600, loss 1.6163294315338135
iteration 700, loss 1.6542779207229614
iteration 800, loss 1.5932608842849731
iteration 0, loss 1.6417144536972046
iteration 100, loss 1.6424932479858398
iteration 200, loss 1.635270118713379
iteration 300, loss 1.7394580841064453
iteration 400, loss 1.6091235876083374
iteration 500, loss 1.655941128730774
iteration 600, loss 1.652798056602478
iteration 700, loss 1.642082929611206
iteration 800, loss 1.5837067365646362
iteration 0, loss 1.6419365406036377
iteration 100, loss 1.6009609699249268
iteration 200, loss 1.6646983623504639
iteration 300, loss 1.6709767580032349
iteration 400, loss 1.67144775390625
iteration 500, loss 1.6515289545059204
iteration 600, loss 1.6572366952896118
iteration 700, loss 1.569727897644043
iteration 800, loss 1.6159451007843018
iteration 0, loss 1.6099251508712769
iteration 100, loss 1.6304155588150024
iteration 200, loss 1.6588438749313354
iteration 300, loss 1.5798060894012451
iteration 400, loss 1.6273908615112305
iteration 500, loss 1.6333363056182861
iteration 600, loss 1.5956089496612549
iteration 700, loss 1.6757346391677856
iteration 800, loss 1.6519293785095215
iteration 0, loss 1.6188828945159912
iteration 100, loss 1.606279730796814
iteration 200, loss 1.6273789405822754
iteration 300, loss 1.6359376907348633
iteration 400, loss 1.5916550159454346
iteration 500, loss 1.6190050840377808
iteration 600, loss 1.5952057838439941
iteration 700, loss 1.6107732057571411
iteration 800, loss 1.6193138360977173
iteration 0, loss 1.5801994800567627
iteration 100, loss 1.5762935876846313
iteration 200, loss 1.6504957675933838
iteration 300, loss 1.60068941116333
iteration 400, loss 1.6047327518463135
iteration 500, loss 1.6297996044158936
iteration 600, loss 1.6442780494689941
iteration 700, loss 1.6158969402313232
iteration 800, loss 1.64946711063385
iteration 0, loss 1.6602873802185059
iteration 100, loss 1.6050360202789307
iteration 200, loss 1.6586331129074097
iteration 300, loss 1.6153749227523804
iteration 400, loss 1.6456170082092285
iteration 500, loss 1.5574650764465332
iteration 600, loss 1.589227557182312
iteration 700, loss 1.615555763244629
iteration 800, loss 1.5708436965942383
iteration 0, loss 1.701723337173462
iteration 100, loss 1.7254939079284668
iteration 200, loss 1.6159470081329346
iteration 300, loss 1.6184946298599243
iteration 400, loss 1.5852786302566528
iteration 500, loss 1.5655689239501953
iteration 600, loss 1.6192281246185303
iteration 700, loss 1.558882236480713
iteration 800, loss 1.6218886375427246
iteration 0, loss 1.597520112991333
iteration 100, loss 1.6272748708724976
iteration 200, loss 1.63825523853302
iteration 300, loss 1.6089890003204346
iteration 400, loss 1.618732213973999
iteration 500, loss 1.625057578086853
iteration 600, loss 1.6592921018600464
iteration 700, loss 1.6150239706039429
iteration 800, loss 1.6161068677902222
iteration 0, loss 1.6217234134674072
iteration 100, loss 1.620629072189331
iteration 200, loss 1.6146377325057983
iteration 300, loss 1.6918323040008545
iteration 400, loss 1.6742390394210815
iteration 500, loss 1.6770133972167969
iteration 600, loss 1.6159768104553223
iteration 700, loss 1.6067171096801758
iteration 800, loss 1.6384280920028687
iteration 0, loss 1.616075038909912
iteration 100, loss 1.5941390991210938
iteration 200, loss 1.6855517625808716
iteration 300, loss 1.6540956497192383
iteration 400, loss 1.6588054895401
iteration 500, loss 1.6504889726638794
iteration 600, loss 1.6045870780944824
iteration 700, loss 1.5862677097320557
iteration 800, loss 1.6290907859802246
iteration 0, loss 1.6464468240737915
iteration 100, loss 1.6182979345321655
iteration 200, loss 1.628912329673767
iteration 300, loss 1.5825883150100708
iteration 400, loss 1.6031494140625
iteration 500, loss 1.648061990737915
iteration 600, loss 1.5836151838302612
iteration 700, loss 1.5547595024108887
iteration 800, loss 1.6414414644241333
iteration 0, loss 1.5893149375915527
iteration 100, loss 1.5946154594421387
iteration 200, loss 1.6237382888793945
iteration 300, loss 1.561747670173645
iteration 400, loss 1.690332055091858
iteration 500, loss 1.584875464439392
iteration 600, loss 1.6198537349700928
iteration 700, loss 1.6487680673599243
iteration 800, loss 1.6301722526550293
iteration 0, loss 1.5784363746643066
iteration 100, loss 1.663930892944336
iteration 200, loss 1.6108481884002686
iteration 300, loss 1.6187827587127686
iteration 400, loss 1.6048088073730469
iteration 500, loss 1.6253670454025269
iteration 600, loss 1.613821029663086
iteration 700, loss 1.6395384073257446
iteration 800, loss 1.610236644744873
iteration 0, loss 1.6187018156051636
iteration 100, loss 1.6408942937850952
iteration 200, loss 1.703249454498291
iteration 300, loss 1.6225295066833496
iteration 400, loss 1.6064846515655518
iteration 500, loss 1.6174672842025757
iteration 600, loss 1.6550451517105103
iteration 700, loss 1.6317158937454224
iteration 800, loss 1.6437526941299438
iteration 0, loss 1.7036824226379395
iteration 100, loss 1.6076501607894897
iteration 200, loss 1.6758320331573486
iteration 300, loss 1.602570652961731
iteration 400, loss 1.5667941570281982
iteration 500, loss 1.6260709762573242
iteration 600, loss 1.6015278100967407
iteration 700, loss 1.6368881464004517
iteration 800, loss 1.598577618598938
iteration 0, loss 1.5921505689620972
iteration 100, loss 1.5721968412399292
iteration 200, loss 1.6428016424179077
iteration 300, loss 1.5922465324401855
iteration 400, loss 1.5541696548461914
iteration 500, loss 1.6318902969360352
iteration 600, loss 1.6644532680511475
iteration 700, loss 1.6313822269439697
iteration 800, loss 1.6359257698059082
iteration 0, loss 1.5989853143692017
iteration 100, loss 1.6014564037322998
iteration 200, loss 1.6006178855895996
iteration 300, loss 1.5690072774887085
iteration 400, loss 1.6498255729675293
iteration 500, loss 1.6400994062423706
iteration 600, loss 1.5996589660644531
iteration 700, loss 1.581668734550476
iteration 800, loss 1.5722466707229614
iteration 0, loss 1.614803433418274
iteration 100, loss 1.6209311485290527
iteration 200, loss 1.6098004579544067
iteration 300, loss 1.6659839153289795
iteration 400, loss 1.6500338315963745
iteration 500, loss 1.6498578786849976
iteration 600, loss 1.581764817237854
iteration 700, loss 1.5787841081619263
iteration 800, loss 1.6185719966888428
iteration 0, loss 1.6126258373260498
iteration 100, loss 1.672913670539856
iteration 200, loss 1.727739691734314
iteration 300, loss 1.6147468090057373
iteration 400, loss 1.6750861406326294
iteration 500, loss 1.6670247316360474
iteration 600, loss 1.7295702695846558
iteration 700, loss 1.6312556266784668
iteration 800, loss 1.5777039527893066
iteration 0, loss 1.6343638896942139
iteration 100, loss 1.624753713607788
iteration 200, loss 1.5962791442871094
iteration 300, loss 1.6077802181243896
iteration 400, loss 1.6541078090667725
iteration 500, loss 1.5920004844665527
iteration 600, loss 1.6273859739303589
iteration 700, loss 1.5765433311462402
iteration 800, loss 1.6246874332427979
iteration 0, loss 1.6159186363220215
iteration 100, loss 1.6184477806091309
iteration 200, loss 1.6592750549316406
iteration 300, loss 1.6031744480133057
iteration 400, loss 1.6238038539886475
iteration 500, loss 1.638342261314392
iteration 600, loss 1.6438666582107544
iteration 700, loss 1.6807459592819214
iteration 800, loss 1.5783380270004272
iteration 0, loss 1.622430682182312
iteration 100, loss 1.6297893524169922
iteration 200, loss 1.6544958353042603
iteration 300, loss 1.5692765712738037
iteration 400, loss 1.6301088333129883
iteration 500, loss 1.551743984222412
iteration 600, loss 1.55849027633667
iteration 700, loss 1.6181919574737549
iteration 800, loss 1.6596260070800781
iteration 0, loss 1.6359144449234009
iteration 100, loss 1.626540184020996
iteration 200, loss 1.6409482955932617
iteration 300, loss 1.5761682987213135
iteration 400, loss 1.605183482170105
iteration 500, loss 1.6198525428771973
iteration 600, loss 1.6488540172576904
iteration 700, loss 1.6056158542633057
iteration 800, loss 1.6640535593032837
iteration 0, loss 1.6659728288650513
iteration 100, loss 1.6539255380630493
iteration 200, loss 1.6867321729660034
iteration 300, loss 1.5986688137054443
iteration 400, loss 1.6456830501556396
iteration 500, loss 1.5963304042816162
iteration 600, loss 1.634641408920288
iteration 700, loss 1.635169506072998
iteration 800, loss 1.6640141010284424
iteration 0, loss 1.6078728437423706
iteration 100, loss 1.609655499458313
iteration 200, loss 1.6700332164764404
iteration 300, loss 1.605296015739441
iteration 400, loss 1.6041959524154663
iteration 500, loss 1.5932117700576782
iteration 600, loss 1.6058167219161987
iteration 700, loss 1.6313300132751465
iteration 800, loss 1.6178436279296875
fold 3 accuracy: 0.8156428571428571
iteration 0, loss 1.6522610187530518
iteration 100, loss 1.5762913227081299
iteration 200, loss 1.6318974494934082
iteration 300, loss 1.6581048965454102
iteration 400, loss 1.6689754724502563
iteration 500, loss 1.651126742362976
iteration 600, loss 1.598263144493103
iteration 700, loss 1.6109446287155151
iteration 800, loss 1.6123909950256348
iteration 0, loss 1.601941704750061
iteration 100, loss 1.6155277490615845
iteration 200, loss 1.582550287246704
iteration 300, loss 1.5670171976089478
iteration 400, loss 1.5956802368164062
iteration 500, loss 1.6512740850448608
iteration 600, loss 1.646328091621399
iteration 700, loss 1.661823034286499
iteration 800, loss 1.6606380939483643
iteration 0, loss 1.5912014245986938
iteration 100, loss 1.617306113243103
iteration 200, loss 1.6112806797027588
iteration 300, loss 1.5796304941177368
iteration 400, loss 1.6241605281829834
iteration 500, loss 1.6540580987930298
iteration 600, loss 1.6495611667633057
iteration 700, loss 1.6257885694503784
iteration 800, loss 1.652819037437439
iteration 0, loss 1.5767483711242676
iteration 100, loss 1.6511313915252686
iteration 200, loss 1.6816306114196777
iteration 300, loss 1.663673996925354
iteration 400, loss 1.6380772590637207
iteration 500, loss 1.6000211238861084
iteration 600, loss 1.5955570936203003
iteration 700, loss 1.602236270904541
iteration 800, loss 1.5744744539260864
iteration 0, loss 1.6133078336715698
iteration 100, loss 1.591577410697937
iteration 200, loss 1.6502095460891724
iteration 300, loss 1.6007280349731445
iteration 400, loss 1.6702944040298462
iteration 500, loss 1.6170390844345093
iteration 600, loss 1.6687694787979126
iteration 700, loss 1.6860538721084595
iteration 800, loss 1.603739619255066
iteration 0, loss 1.5635954141616821
iteration 100, loss 1.681261658668518
iteration 200, loss 1.725554347038269
iteration 300, loss 1.6101596355438232
iteration 400, loss 1.530282974243164
iteration 500, loss 1.6358680725097656
iteration 600, loss 1.5995930433273315
iteration 700, loss 1.6234338283538818
iteration 800, loss 1.5912526845932007
iteration 0, loss 1.6151678562164307
iteration 100, loss 1.6492000818252563
iteration 200, loss 1.685543179512024
iteration 300, loss 1.6328006982803345
iteration 400, loss 1.6176639795303345
iteration 500, loss 1.6833381652832031
iteration 600, loss 1.621177315711975
iteration 700, loss 1.5717229843139648
iteration 800, loss 1.6219292879104614
iteration 0, loss 1.6513965129852295
iteration 100, loss 1.6199936866760254
iteration 200, loss 1.6543940305709839
iteration 300, loss 1.6179472208023071
iteration 400, loss 1.6791929006576538
iteration 500, loss 1.655331015586853
iteration 600, loss 1.6503305435180664
iteration 700, loss 1.6203198432922363
iteration 800, loss 1.6196907758712769
iteration 0, loss 1.5820136070251465
iteration 100, loss 1.6429975032806396
iteration 200, loss 1.6104665994644165
iteration 300, loss 1.6407300233840942
iteration 400, loss 1.678096055984497
iteration 500, loss 1.6331658363342285
iteration 600, loss 1.5968154668807983
iteration 700, loss 1.7245334386825562
iteration 800, loss 1.58039128780365
iteration 0, loss 1.583160400390625
iteration 100, loss 1.6203268766403198
iteration 200, loss 1.5664221048355103
iteration 300, loss 1.6917150020599365
iteration 400, loss 1.6000288724899292
iteration 500, loss 1.6313365697860718
iteration 600, loss 1.6368279457092285
iteration 700, loss 1.637006402015686
iteration 800, loss 1.591561198234558
iteration 0, loss 1.597787618637085
iteration 100, loss 1.5936719179153442
iteration 200, loss 1.6104989051818848
iteration 300, loss 1.623290777206421
iteration 400, loss 1.6796226501464844
iteration 500, loss 1.6181797981262207
iteration 600, loss 1.5984539985656738
iteration 700, loss 1.6600747108459473
iteration 800, loss 1.5870308876037598
iteration 0, loss 1.574103832244873
iteration 100, loss 1.707167148590088
iteration 200, loss 1.6396820545196533
iteration 300, loss 1.6793601512908936
iteration 400, loss 1.5632400512695312
iteration 500, loss 1.6292493343353271
iteration 600, loss 1.6825016736984253
iteration 700, loss 1.5488871335983276
iteration 800, loss 1.5876840353012085
iteration 0, loss 1.603266954421997
iteration 100, loss 1.6457886695861816
iteration 200, loss 1.5967841148376465
iteration 300, loss 1.6130824089050293
iteration 400, loss 1.6189823150634766
iteration 500, loss 1.632615327835083
iteration 600, loss 1.6029348373413086
iteration 700, loss 1.5971423387527466
iteration 800, loss 1.6085482835769653
iteration 0, loss 1.6528939008712769
iteration 100, loss 1.6114487648010254
iteration 200, loss 1.5823352336883545
iteration 300, loss 1.6198381185531616
iteration 400, loss 1.691519021987915
iteration 500, loss 1.6532212495803833
iteration 600, loss 1.6489773988723755
iteration 700, loss 1.639207363128662
iteration 800, loss 1.5975946187973022
iteration 0, loss 1.5650115013122559
iteration 100, loss 1.6474436521530151
iteration 200, loss 1.6581532955169678
iteration 300, loss 1.6075913906097412
iteration 400, loss 1.630444884300232
iteration 500, loss 1.6637799739837646
iteration 600, loss 1.6721041202545166
iteration 700, loss 1.5743266344070435
iteration 800, loss 1.6006141901016235
iteration 0, loss 1.6631981134414673
iteration 100, loss 1.6152881383895874
iteration 200, loss 1.618889570236206
iteration 300, loss 1.6149909496307373
iteration 400, loss 1.6217548847198486
iteration 500, loss 1.566149115562439
iteration 600, loss 1.6604963541030884
iteration 700, loss 1.6470789909362793
iteration 800, loss 1.6395636796951294
iteration 0, loss 1.6258476972579956
iteration 100, loss 1.6864469051361084
iteration 200, loss 1.624916672706604
iteration 300, loss 1.5930225849151611
iteration 400, loss 1.566917061805725
iteration 500, loss 1.681602120399475
iteration 600, loss 1.6225957870483398
iteration 700, loss 1.6266292333602905
iteration 800, loss 1.6192572116851807
iteration 0, loss 1.6200002431869507
iteration 100, loss 1.535548210144043
iteration 200, loss 1.6291124820709229
iteration 300, loss 1.5962693691253662
iteration 400, loss 1.589248538017273
iteration 500, loss 1.6273561716079712
iteration 600, loss 1.6983791589736938
iteration 700, loss 1.659131646156311
iteration 800, loss 1.573177695274353
iteration 0, loss 1.6037606000900269
iteration 100, loss 1.6390713453292847
iteration 200, loss 1.6276271343231201
iteration 300, loss 1.5695092678070068
iteration 400, loss 1.5892107486724854
iteration 500, loss 1.6535356044769287
iteration 600, loss 1.6060912609100342
iteration 700, loss 1.671867847442627
iteration 800, loss 1.5787140130996704
iteration 0, loss 1.6198545694351196
iteration 100, loss 1.596769094467163
iteration 200, loss 1.5905988216400146
iteration 300, loss 1.6372320652008057
iteration 400, loss 1.632333517074585
iteration 500, loss 1.6687531471252441
iteration 600, loss 1.6224408149719238
iteration 700, loss 1.6281371116638184
iteration 800, loss 1.6544477939605713
iteration 0, loss 1.6050878763198853
iteration 100, loss 1.6255944967269897
iteration 200, loss 1.6216826438903809
iteration 300, loss 1.621091365814209
iteration 400, loss 1.628712773323059
iteration 500, loss 1.5991997718811035
iteration 600, loss 1.5627927780151367
iteration 700, loss 1.630627155303955
iteration 800, loss 1.6472656726837158
iteration 0, loss 1.6014431715011597
iteration 100, loss 1.5970971584320068
iteration 200, loss 1.5923513174057007
iteration 300, loss 1.5807702541351318
iteration 400, loss 1.6337714195251465
iteration 500, loss 1.6321022510528564
iteration 600, loss 1.640892744064331
iteration 700, loss 1.6868561506271362
iteration 800, loss 1.6546235084533691
iteration 0, loss 1.6375283002853394
iteration 100, loss 1.6252185106277466
iteration 200, loss 1.6105797290802002
iteration 300, loss 1.6149721145629883
iteration 400, loss 1.6870161294937134
iteration 500, loss 1.6068769693374634
iteration 600, loss 1.6251381635665894
iteration 700, loss 1.6196835041046143
iteration 800, loss 1.6298396587371826
iteration 0, loss 1.6396822929382324
iteration 100, loss 1.6109910011291504
iteration 200, loss 1.5804964303970337
iteration 300, loss 1.6099835634231567
iteration 400, loss 1.6159054040908813
iteration 500, loss 1.6373648643493652
iteration 600, loss 1.6341991424560547
iteration 700, loss 1.6877834796905518
iteration 800, loss 1.630506157875061
iteration 0, loss 1.635064959526062
iteration 100, loss 1.5686899423599243
iteration 200, loss 1.6039186716079712
iteration 300, loss 1.6592216491699219
iteration 400, loss 1.5565823316574097
iteration 500, loss 1.6308420896530151
iteration 600, loss 1.6117390394210815
iteration 700, loss 1.678851842880249
iteration 800, loss 1.6408354043960571
iteration 0, loss 1.6276344060897827
iteration 100, loss 1.587136149406433
iteration 200, loss 1.5929652452468872
iteration 300, loss 1.5851293802261353
iteration 400, loss 1.5766057968139648
iteration 500, loss 1.6195869445800781
iteration 600, loss 1.5664960145950317
iteration 700, loss 1.6170575618743896
iteration 800, loss 1.5987982749938965
iteration 0, loss 1.6142865419387817
iteration 100, loss 1.5740551948547363
iteration 200, loss 1.599050521850586
iteration 300, loss 1.5660300254821777
iteration 400, loss 1.7003649473190308
iteration 500, loss 1.6019922494888306
iteration 600, loss 1.6458847522735596
iteration 700, loss 1.6133015155792236
iteration 800, loss 1.6051746606826782
iteration 0, loss 1.5694658756256104
iteration 100, loss 1.6130567789077759
iteration 200, loss 1.6211051940917969
iteration 300, loss 1.6025476455688477
iteration 400, loss 1.566146731376648
iteration 500, loss 1.6224173307418823
iteration 600, loss 1.6001737117767334
iteration 700, loss 1.6373685598373413
iteration 800, loss 1.6705231666564941
iteration 0, loss 1.6062039136886597
iteration 100, loss 1.5515516996383667
iteration 200, loss 1.6407817602157593
iteration 300, loss 1.5930171012878418
iteration 400, loss 1.610552191734314
iteration 500, loss 1.6180226802825928
iteration 600, loss 1.6758304834365845
iteration 700, loss 1.6151976585388184
iteration 800, loss 1.5743879079818726
iteration 0, loss 1.620525598526001
iteration 100, loss 1.6395381689071655
iteration 200, loss 1.6648110151290894
iteration 300, loss 1.6053688526153564
iteration 400, loss 1.6000422239303589
iteration 500, loss 1.6821811199188232
iteration 600, loss 1.6816151142120361
iteration 700, loss 1.659201979637146
iteration 800, loss 1.6714173555374146
iteration 0, loss 1.6688742637634277
iteration 100, loss 1.684932827949524
iteration 200, loss 1.6028075218200684
iteration 300, loss 1.6271406412124634
iteration 400, loss 1.6102643013000488
iteration 500, loss 1.6193240880966187
iteration 600, loss 1.6345901489257812
iteration 700, loss 1.6127283573150635
iteration 800, loss 1.6167356967926025
iteration 0, loss 1.6673840284347534
iteration 100, loss 1.7181569337844849
iteration 200, loss 1.5678400993347168
iteration 300, loss 1.6747851371765137
iteration 400, loss 1.5849257707595825
iteration 500, loss 1.621862769126892
iteration 600, loss 1.6132055521011353
iteration 700, loss 1.5889359712600708
iteration 800, loss 1.6192362308502197
iteration 0, loss 1.647275447845459
iteration 100, loss 1.557782769203186
iteration 200, loss 1.644083857536316
iteration 300, loss 1.6077536344528198
iteration 400, loss 1.546805500984192
iteration 500, loss 1.5940067768096924
iteration 600, loss 1.6871801614761353
iteration 700, loss 1.57170569896698
iteration 800, loss 1.661816120147705
iteration 0, loss 1.6493767499923706
iteration 100, loss 1.6731750965118408
iteration 200, loss 1.6147593259811401
iteration 300, loss 1.641099452972412
iteration 400, loss 1.6363714933395386
iteration 500, loss 1.6146539449691772
iteration 600, loss 1.6722967624664307
iteration 700, loss 1.5866894721984863
iteration 800, loss 1.639984130859375
iteration 0, loss 1.571281909942627
iteration 100, loss 1.6323362588882446
iteration 200, loss 1.6277846097946167
iteration 300, loss 1.678208589553833
iteration 400, loss 1.6507623195648193
iteration 500, loss 1.645745038986206
iteration 600, loss 1.6235895156860352
iteration 700, loss 1.6382681131362915
iteration 800, loss 1.631559133529663
iteration 0, loss 1.6391793489456177
iteration 100, loss 1.6183719635009766
iteration 200, loss 1.6126738786697388
iteration 300, loss 1.602085828781128
iteration 400, loss 1.6848145723342896
iteration 500, loss 1.592458963394165
iteration 600, loss 1.6075172424316406
iteration 700, loss 1.619178056716919
iteration 800, loss 1.5610382556915283
iteration 0, loss 1.6641044616699219
iteration 100, loss 1.6117485761642456
iteration 200, loss 1.5491374731063843
iteration 300, loss 1.6015750169754028
iteration 400, loss 1.649299144744873
iteration 500, loss 1.6455127000808716
iteration 600, loss 1.622401237487793
iteration 700, loss 1.6360564231872559
iteration 800, loss 1.6169297695159912
iteration 0, loss 1.6374971866607666
iteration 100, loss 1.5964789390563965
iteration 200, loss 1.5649945735931396
iteration 300, loss 1.6282587051391602
iteration 400, loss 1.6038087606430054
iteration 500, loss 1.6320714950561523
iteration 600, loss 1.5835195779800415
iteration 700, loss 1.6682336330413818
iteration 800, loss 1.6164721250534058
iteration 0, loss 1.6694176197052002
iteration 100, loss 1.5627177953720093
iteration 200, loss 1.6714352369308472
iteration 300, loss 1.6499934196472168
iteration 400, loss 1.6476423740386963
iteration 500, loss 1.6246418952941895
iteration 600, loss 1.6231282949447632
iteration 700, loss 1.6117842197418213
iteration 800, loss 1.597703456878662
iteration 0, loss 1.6258970499038696
iteration 100, loss 1.5968912839889526
iteration 200, loss 1.6133804321289062
iteration 300, loss 1.6516664028167725
iteration 400, loss 1.5966225862503052
iteration 500, loss 1.6032652854919434
iteration 600, loss 1.6094775199890137
iteration 700, loss 1.6349376440048218
iteration 800, loss 1.6541037559509277
iteration 0, loss 1.6485923528671265
iteration 100, loss 1.6316068172454834
iteration 200, loss 1.6600362062454224
iteration 300, loss 1.6339633464813232
iteration 400, loss 1.7300621271133423
iteration 500, loss 1.6040507555007935
iteration 600, loss 1.6608750820159912
iteration 700, loss 1.65819251537323
iteration 800, loss 1.6509073972702026
iteration 0, loss 1.5977145433425903
iteration 100, loss 1.649407982826233
iteration 200, loss 1.679931640625
iteration 300, loss 1.678483247756958
iteration 400, loss 1.6407190561294556
iteration 500, loss 1.6084985733032227
iteration 600, loss 1.6079590320587158
iteration 700, loss 1.6644867658615112
iteration 800, loss 1.6680418252944946
iteration 0, loss 1.5665736198425293
iteration 100, loss 1.6258749961853027
iteration 200, loss 1.6272144317626953
iteration 300, loss 1.599130630493164
iteration 400, loss 1.6149837970733643
iteration 500, loss 1.5960944890975952
iteration 600, loss 1.5945132970809937
iteration 700, loss 1.6626977920532227
iteration 800, loss 1.67801833152771
iteration 0, loss 1.5862687826156616
iteration 100, loss 1.60360848903656
iteration 200, loss 1.6433292627334595
iteration 300, loss 1.6053744554519653
iteration 400, loss 1.5884238481521606
iteration 500, loss 1.5733354091644287
iteration 600, loss 1.5959233045578003
iteration 700, loss 1.5954864025115967
iteration 800, loss 1.6369305849075317
iteration 0, loss 1.5987614393234253
iteration 100, loss 1.6172395944595337
iteration 200, loss 1.6375706195831299
iteration 300, loss 1.5874321460723877
iteration 400, loss 1.6162612438201904
iteration 500, loss 1.6829081773757935
iteration 600, loss 1.6055799722671509
iteration 700, loss 1.6351444721221924
iteration 800, loss 1.621682047843933
iteration 0, loss 1.5993191003799438
iteration 100, loss 1.6443966627120972
iteration 200, loss 1.6133155822753906
iteration 300, loss 1.5793566703796387
iteration 400, loss 1.5732251405715942
iteration 500, loss 1.625576376914978
iteration 600, loss 1.6726518869400024
iteration 700, loss 1.5859262943267822
iteration 800, loss 1.643026351928711
iteration 0, loss 1.646046757698059
iteration 100, loss 1.6327123641967773
iteration 200, loss 1.58497953414917
iteration 300, loss 1.6538258790969849
iteration 400, loss 1.6307649612426758
iteration 500, loss 1.7315666675567627
iteration 600, loss 1.5977227687835693
iteration 700, loss 1.6139808893203735
iteration 800, loss 1.6618348360061646
iteration 0, loss 1.5728498697280884
iteration 100, loss 1.688333511352539
iteration 200, loss 1.6091885566711426
iteration 300, loss 1.5739946365356445
iteration 400, loss 1.6790984869003296
iteration 500, loss 1.6774389743804932
iteration 600, loss 1.6857796907424927
iteration 700, loss 1.5795384645462036
iteration 800, loss 1.6033583879470825
iteration 0, loss 1.5854121446609497
iteration 100, loss 1.6241369247436523
iteration 200, loss 1.6303893327713013
iteration 300, loss 1.5690999031066895
iteration 400, loss 1.6156234741210938
iteration 500, loss 1.6366581916809082
iteration 600, loss 1.665533185005188
iteration 700, loss 1.574034333229065
iteration 800, loss 1.5875611305236816
iteration 0, loss 1.6494475603103638
iteration 100, loss 1.6370036602020264
iteration 200, loss 1.6282156705856323
iteration 300, loss 1.5729296207427979
iteration 400, loss 1.704618215560913
iteration 500, loss 1.6433866024017334
iteration 600, loss 1.6627147197723389
iteration 700, loss 1.6076912879943848
iteration 800, loss 1.6020270586013794
fold 4 accuracy: 0.8163571428571429
[2024-02-29 03:56:56,304] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-29 03:56:56,305] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         654     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       840 MACs
fwd flops per GPU:                                                      1.68 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.68 K  
fwd latency:                                                            339.51 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    4.95 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '654'}
    MACs        - {'KronLayer': '840 MACs'}
    fwd latency - {'KronLayer': '339.51 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  654 = 100% Params, 840 MACs = 100% MACs, 339.51 us = 100% latency, 4.95 MFLOPS
  (kronlinear): KronLinear(654 = 100% Params, 840 MACs = 100% MACs, 249.86 us = 73.6% latency, 6.72 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 26.46 us = 7.79% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-29 03:56:56,308] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.68 K 654
iteration 0, loss 2.3138389587402344
iteration 100, loss 2.2425734996795654
iteration 200, loss 2.0607454776763916
iteration 300, loss 1.9276986122131348
iteration 400, loss 1.9261164665222168
iteration 500, loss 1.8907980918884277
iteration 600, loss 1.8156392574310303
iteration 700, loss 1.8062270879745483
iteration 800, loss 1.763382077217102
iteration 0, loss 1.7717583179473877
iteration 100, loss 1.8338509798049927
iteration 200, loss 1.7556381225585938
iteration 300, loss 1.764493703842163
iteration 400, loss 1.728161334991455
iteration 500, loss 1.7713415622711182
iteration 600, loss 1.723914384841919
iteration 700, loss 1.7500247955322266
iteration 800, loss 1.7076183557510376
iteration 0, loss 1.7304080724716187
iteration 100, loss 1.7349379062652588
iteration 200, loss 1.6624432802200317
iteration 300, loss 1.6423195600509644
iteration 400, loss 1.732303500175476
iteration 500, loss 1.67436683177948
iteration 600, loss 1.72431480884552
iteration 700, loss 1.7602977752685547
iteration 800, loss 1.6847156286239624
iteration 0, loss 1.7036073207855225
iteration 100, loss 1.7158746719360352
iteration 200, loss 1.6739606857299805
iteration 300, loss 1.6988712549209595
iteration 400, loss 1.6540507078170776
iteration 500, loss 1.6721913814544678
iteration 600, loss 1.7013520002365112
iteration 700, loss 1.7270467281341553
iteration 800, loss 1.701756238937378
iteration 0, loss 1.7103301286697388
iteration 100, loss 1.7051695585250854
iteration 200, loss 1.7226790189743042
iteration 300, loss 1.6964744329452515
iteration 400, loss 1.7006468772888184
iteration 500, loss 1.7298181056976318
iteration 600, loss 1.6725329160690308
iteration 700, loss 1.6494048833847046
iteration 800, loss 1.6640068292617798
iteration 0, loss 1.7108250856399536
iteration 100, loss 1.643604040145874
iteration 200, loss 1.7068986892700195
iteration 300, loss 1.6918355226516724
iteration 400, loss 1.7088464498519897
iteration 500, loss 1.6902738809585571
iteration 600, loss 1.6248633861541748
iteration 700, loss 1.6522316932678223
iteration 800, loss 1.6297894716262817
iteration 0, loss 1.7280778884887695
iteration 100, loss 1.6748427152633667
iteration 200, loss 1.6820204257965088
iteration 300, loss 1.6618448495864868
iteration 400, loss 1.6407419443130493
iteration 500, loss 1.6809403896331787
iteration 600, loss 1.7218430042266846
iteration 700, loss 1.7153301239013672
iteration 800, loss 1.7535121440887451
iteration 0, loss 1.6199841499328613
iteration 100, loss 1.7114405632019043
iteration 200, loss 1.6557385921478271
iteration 300, loss 1.6824077367782593
iteration 400, loss 1.667669653892517
iteration 500, loss 1.6599469184875488
iteration 600, loss 1.5788017511367798
iteration 700, loss 1.6941980123519897
iteration 800, loss 1.6115089654922485
iteration 0, loss 1.6101213693618774
iteration 100, loss 1.6583927869796753
iteration 200, loss 1.5979551076889038
iteration 300, loss 1.626146912574768
iteration 400, loss 1.7342283725738525
iteration 500, loss 1.6552830934524536
iteration 600, loss 1.7400764226913452
iteration 700, loss 1.6652543544769287
iteration 800, loss 1.7031805515289307
iteration 0, loss 1.665032148361206
iteration 100, loss 1.6177524328231812
iteration 200, loss 1.6811925172805786
iteration 300, loss 1.6628576517105103
iteration 400, loss 1.7074270248413086
iteration 500, loss 1.7148380279541016
iteration 600, loss 1.6715677976608276
iteration 700, loss 1.6622222661972046
iteration 800, loss 1.672749400138855
iteration 0, loss 1.6408900022506714
iteration 100, loss 1.6640684604644775
iteration 200, loss 1.6814390420913696
iteration 300, loss 1.6356916427612305
iteration 400, loss 1.7017157077789307
iteration 500, loss 1.7146270275115967
iteration 600, loss 1.66908597946167
iteration 700, loss 1.705894112586975
iteration 800, loss 1.6039950847625732
iteration 0, loss 1.6897718906402588
iteration 100, loss 1.6353269815444946
iteration 200, loss 1.6209769248962402
iteration 300, loss 1.6741199493408203
iteration 400, loss 1.604858160018921
iteration 500, loss 1.7063747644424438
iteration 600, loss 1.6424325704574585
iteration 700, loss 1.6338542699813843
iteration 800, loss 1.6657944917678833
iteration 0, loss 1.634329915046692
iteration 100, loss 1.6550190448760986
iteration 200, loss 1.6635748147964478
iteration 300, loss 1.702096939086914
iteration 400, loss 1.5946602821350098
iteration 500, loss 1.6678566932678223
iteration 600, loss 1.6419830322265625
iteration 700, loss 1.6187094449996948
iteration 800, loss 1.6295005083084106
iteration 0, loss 1.6350245475769043
iteration 100, loss 1.7064961194992065
iteration 200, loss 1.6134394407272339
iteration 300, loss 1.6959866285324097
iteration 400, loss 1.7049524784088135
iteration 500, loss 1.6396464109420776
iteration 600, loss 1.6594810485839844
iteration 700, loss 1.7161352634429932
iteration 800, loss 1.6722886562347412
iteration 0, loss 1.6522270441055298
iteration 100, loss 1.7368732690811157
iteration 200, loss 1.6219923496246338
iteration 300, loss 1.6997233629226685
iteration 400, loss 1.6697392463684082
iteration 500, loss 1.6714346408843994
iteration 600, loss 1.6488202810287476
iteration 700, loss 1.66749906539917
iteration 800, loss 1.6536411046981812
iteration 0, loss 1.67447829246521
iteration 100, loss 1.6206547021865845
iteration 200, loss 1.6279652118682861
iteration 300, loss 1.616753101348877
iteration 400, loss 1.6347514390945435
iteration 500, loss 1.5871676206588745
iteration 600, loss 1.6729099750518799
iteration 700, loss 1.6799733638763428
iteration 800, loss 1.6079436540603638
iteration 0, loss 1.580203652381897
iteration 100, loss 1.6332238912582397
iteration 200, loss 1.6346168518066406
iteration 300, loss 1.6848061084747314
iteration 400, loss 1.6382815837860107
iteration 500, loss 1.6258772611618042
iteration 600, loss 1.6918308734893799
iteration 700, loss 1.630576491355896
iteration 800, loss 1.6014217138290405
iteration 0, loss 1.6481928825378418
iteration 100, loss 1.6113170385360718
iteration 200, loss 1.6131198406219482
iteration 300, loss 1.6593605279922485
iteration 400, loss 1.6403411626815796
iteration 500, loss 1.6607187986373901
iteration 600, loss 1.6376327276229858
iteration 700, loss 1.5931531190872192
iteration 800, loss 1.6030105352401733
iteration 0, loss 1.7020981311798096
iteration 100, loss 1.6539803743362427
iteration 200, loss 1.6308774948120117
iteration 300, loss 1.6854337453842163
iteration 400, loss 1.6340036392211914
iteration 500, loss 1.6314587593078613
iteration 600, loss 1.6590749025344849
iteration 700, loss 1.6233257055282593
iteration 800, loss 1.6325632333755493
iteration 0, loss 1.6947550773620605
iteration 100, loss 1.651221513748169
iteration 200, loss 1.6633213758468628
iteration 300, loss 1.6678109169006348
iteration 400, loss 1.6441245079040527
iteration 500, loss 1.6164904832839966
iteration 600, loss 1.6162185668945312
iteration 700, loss 1.6618627309799194
iteration 800, loss 1.6124920845031738
iteration 0, loss 1.6640150547027588
iteration 100, loss 1.616064190864563
iteration 200, loss 1.6395812034606934
iteration 300, loss 1.6495553255081177
iteration 400, loss 1.6326543092727661
iteration 500, loss 1.58809232711792
iteration 600, loss 1.6492525339126587
iteration 700, loss 1.6876188516616821
iteration 800, loss 1.631423830986023
iteration 0, loss 1.6153078079223633
iteration 100, loss 1.6431211233139038
iteration 200, loss 1.5995503664016724
iteration 300, loss 1.6514478921890259
iteration 400, loss 1.6996147632598877
iteration 500, loss 1.6437122821807861
iteration 600, loss 1.6715748310089111
iteration 700, loss 1.5999972820281982
iteration 800, loss 1.6285678148269653
iteration 0, loss 1.6424219608306885
iteration 100, loss 1.5973231792449951
iteration 200, loss 1.6300592422485352
iteration 300, loss 1.6734719276428223
iteration 400, loss 1.5861753225326538
iteration 500, loss 1.6411235332489014
iteration 600, loss 1.5534082651138306
iteration 700, loss 1.6375280618667603
iteration 800, loss 1.6530582904815674
iteration 0, loss 1.6020293235778809
iteration 100, loss 1.6343883275985718
iteration 200, loss 1.7349134683609009
iteration 300, loss 1.6342653036117554
iteration 400, loss 1.6198328733444214
iteration 500, loss 1.6162302494049072
iteration 600, loss 1.6344231367111206
iteration 700, loss 1.601123332977295
iteration 800, loss 1.5892585515975952
iteration 0, loss 1.615738034248352
iteration 100, loss 1.566452980041504
iteration 200, loss 1.6688920259475708
iteration 300, loss 1.6207239627838135
iteration 400, loss 1.6375248432159424
iteration 500, loss 1.6211426258087158
iteration 600, loss 1.6298093795776367
iteration 700, loss 1.6238996982574463
iteration 800, loss 1.641635537147522
iteration 0, loss 1.6179240942001343
iteration 100, loss 1.6022822856903076
iteration 200, loss 1.7193948030471802
iteration 300, loss 1.6981638669967651
iteration 400, loss 1.6746569871902466
iteration 500, loss 1.6733129024505615
iteration 600, loss 1.6615102291107178
iteration 700, loss 1.7371363639831543
iteration 800, loss 1.622600793838501
iteration 0, loss 1.6306939125061035
iteration 100, loss 1.6104360818862915
iteration 200, loss 1.6616512537002563
iteration 300, loss 1.6527494192123413
iteration 400, loss 1.6437290906906128
iteration 500, loss 1.5951948165893555
iteration 600, loss 1.6071922779083252
iteration 700, loss 1.6679636240005493
iteration 800, loss 1.7380568981170654
iteration 0, loss 1.622728943824768
iteration 100, loss 1.5979700088500977
iteration 200, loss 1.6248314380645752
iteration 300, loss 1.594104528427124
iteration 400, loss 1.6277663707733154
iteration 500, loss 1.6079490184783936
iteration 600, loss 1.5711627006530762
iteration 700, loss 1.6079421043395996
iteration 800, loss 1.612294316291809
iteration 0, loss 1.645145058631897
iteration 100, loss 1.6581730842590332
iteration 200, loss 1.62148916721344
iteration 300, loss 1.6309138536453247
iteration 400, loss 1.656017780303955
iteration 500, loss 1.670953392982483
iteration 600, loss 1.6071768999099731
iteration 700, loss 1.6489579677581787
iteration 800, loss 1.6532752513885498
iteration 0, loss 1.6786391735076904
iteration 100, loss 1.6956957578659058
iteration 200, loss 1.6165882349014282
iteration 300, loss 1.6534672975540161
iteration 400, loss 1.6200320720672607
iteration 500, loss 1.6711740493774414
iteration 600, loss 1.6381843090057373
iteration 700, loss 1.6524118185043335
iteration 800, loss 1.6431231498718262
iteration 0, loss 1.6401662826538086
iteration 100, loss 1.6386938095092773
iteration 200, loss 1.623228669166565
iteration 300, loss 1.6479685306549072
iteration 400, loss 1.6712474822998047
iteration 500, loss 1.684424638748169
iteration 600, loss 1.6494929790496826
iteration 700, loss 1.5942305326461792
iteration 800, loss 1.6338878870010376
iteration 0, loss 1.608833909034729
iteration 100, loss 1.6247968673706055
iteration 200, loss 1.663841962814331
iteration 300, loss 1.6427003145217896
iteration 400, loss 1.6454689502716064
iteration 500, loss 1.6787482500076294
iteration 600, loss 1.6273850202560425
iteration 700, loss 1.653829574584961
iteration 800, loss 1.6687482595443726
iteration 0, loss 1.6205501556396484
iteration 100, loss 1.6006230115890503
iteration 200, loss 1.6351879835128784
iteration 300, loss 1.657880425453186
iteration 400, loss 1.5970600843429565
iteration 500, loss 1.691667079925537
iteration 600, loss 1.6726487874984741
iteration 700, loss 1.6152186393737793
iteration 800, loss 1.6700605154037476
iteration 0, loss 1.6049915552139282
iteration 100, loss 1.6972582340240479
iteration 200, loss 1.6168869733810425
iteration 300, loss 1.6399880647659302
iteration 400, loss 1.6068538427352905
iteration 500, loss 1.6691954135894775
iteration 600, loss 1.5882307291030884
iteration 700, loss 1.6877886056900024
iteration 800, loss 1.6394976377487183
iteration 0, loss 1.6294002532958984
iteration 100, loss 1.5961307287216187
iteration 200, loss 1.6552537679672241
iteration 300, loss 1.6388654708862305
iteration 400, loss 1.6071245670318604
iteration 500, loss 1.592017412185669
iteration 600, loss 1.6026090383529663
iteration 700, loss 1.6203936338424683
iteration 800, loss 1.630945086479187
iteration 0, loss 1.636404275894165
iteration 100, loss 1.6285372972488403
iteration 200, loss 1.6169027090072632
iteration 300, loss 1.668601155281067
iteration 400, loss 1.6155060529708862
iteration 500, loss 1.6449307203292847
iteration 600, loss 1.6185582876205444
iteration 700, loss 1.6452659368515015
iteration 800, loss 1.6447635889053345
iteration 0, loss 1.6333072185516357
iteration 100, loss 1.6403162479400635
iteration 200, loss 1.6591298580169678
iteration 300, loss 1.5663048028945923
iteration 400, loss 1.654455304145813
iteration 500, loss 1.6458710432052612
iteration 600, loss 1.6569448709487915
iteration 700, loss 1.5741565227508545
iteration 800, loss 1.6319986581802368
iteration 0, loss 1.6867704391479492
iteration 100, loss 1.598810076713562
iteration 200, loss 1.6732062101364136
iteration 300, loss 1.600076675415039
iteration 400, loss 1.5734832286834717
iteration 500, loss 1.6412731409072876
iteration 600, loss 1.5649782419204712
iteration 700, loss 1.6775784492492676
iteration 800, loss 1.676256537437439
iteration 0, loss 1.6729068756103516
iteration 100, loss 1.6222424507141113
iteration 200, loss 1.6704987287521362
iteration 300, loss 1.668327808380127
iteration 400, loss 1.6163452863693237
iteration 500, loss 1.6035935878753662
iteration 600, loss 1.6247702836990356
iteration 700, loss 1.6402162313461304
iteration 800, loss 1.6812688112258911
iteration 0, loss 1.6416312456130981
iteration 100, loss 1.6079635620117188
iteration 200, loss 1.670314908027649
iteration 300, loss 1.6281017065048218
iteration 400, loss 1.6534595489501953
iteration 500, loss 1.5703840255737305
iteration 600, loss 1.6650691032409668
iteration 700, loss 1.6528456211090088
iteration 800, loss 1.6069982051849365
iteration 0, loss 1.6823346614837646
iteration 100, loss 1.6332451105117798
iteration 200, loss 1.6238999366760254
iteration 300, loss 1.5796563625335693
iteration 400, loss 1.6507583856582642
iteration 500, loss 1.6237847805023193
iteration 600, loss 1.6363557577133179
iteration 700, loss 1.5898663997650146
iteration 800, loss 1.5954560041427612
iteration 0, loss 1.6324551105499268
iteration 100, loss 1.6493114233016968
iteration 200, loss 1.6019294261932373
iteration 300, loss 1.6577962636947632
iteration 400, loss 1.7073808908462524
iteration 500, loss 1.6306458711624146
iteration 600, loss 1.6434992551803589
iteration 700, loss 1.6243587732315063
iteration 800, loss 1.5776280164718628
iteration 0, loss 1.5854833126068115
iteration 100, loss 1.6633014678955078
iteration 200, loss 1.6382434368133545
iteration 300, loss 1.649713397026062
iteration 400, loss 1.6283164024353027
iteration 500, loss 1.644384741783142
iteration 600, loss 1.65921151638031
iteration 700, loss 1.6017570495605469
iteration 800, loss 1.6112557649612427
iteration 0, loss 1.6863868236541748
iteration 100, loss 1.57966947555542
iteration 200, loss 1.5951285362243652
iteration 300, loss 1.6141327619552612
iteration 400, loss 1.6493613719940186
iteration 500, loss 1.625257134437561
iteration 600, loss 1.6217893362045288
iteration 700, loss 1.6925406455993652
iteration 800, loss 1.6043182611465454
iteration 0, loss 1.6555397510528564
iteration 100, loss 1.6022701263427734
iteration 200, loss 1.6589988470077515
iteration 300, loss 1.5945135354995728
iteration 400, loss 1.66635262966156
iteration 500, loss 1.6314539909362793
iteration 600, loss 1.7017567157745361
iteration 700, loss 1.624743103981018
iteration 800, loss 1.6045441627502441
iteration 0, loss 1.5911505222320557
iteration 100, loss 1.6313812732696533
iteration 200, loss 1.588428020477295
iteration 300, loss 1.6077206134796143
iteration 400, loss 1.6340065002441406
iteration 500, loss 1.6844122409820557
iteration 600, loss 1.6810342073440552
iteration 700, loss 1.5863978862762451
iteration 800, loss 1.6470088958740234
iteration 0, loss 1.6238508224487305
iteration 100, loss 1.6217761039733887
iteration 200, loss 1.6443867683410645
iteration 300, loss 1.6183335781097412
iteration 400, loss 1.6647472381591797
iteration 500, loss 1.626993179321289
iteration 600, loss 1.6398259401321411
iteration 700, loss 1.6101330518722534
iteration 800, loss 1.6241703033447266
iteration 0, loss 1.6121183633804321
iteration 100, loss 1.6307451725006104
iteration 200, loss 1.661431074142456
iteration 300, loss 1.6347424983978271
iteration 400, loss 1.599871277809143
iteration 500, loss 1.5861903429031372
iteration 600, loss 1.5977861881256104
iteration 700, loss 1.6950929164886475
iteration 800, loss 1.5903334617614746
iteration 0, loss 1.60830557346344
iteration 100, loss 1.6091212034225464
iteration 200, loss 1.6679232120513916
iteration 300, loss 1.602518916130066
iteration 400, loss 1.6146161556243896
iteration 500, loss 1.698690414428711
iteration 600, loss 1.6662747859954834
iteration 700, loss 1.6152935028076172
iteration 800, loss 1.6244735717773438
iteration 0, loss 1.6833618879318237
iteration 100, loss 1.5852224826812744
iteration 200, loss 1.6053671836853027
iteration 300, loss 1.6482399702072144
iteration 400, loss 1.6464040279388428
iteration 500, loss 1.622088074684143
iteration 600, loss 1.6603115797042847
iteration 700, loss 1.5995596647262573
iteration 800, loss 1.608533263206482
fold 0 accuracy: 0.806
iteration 0, loss 1.6626697778701782
iteration 100, loss 1.6138757467269897
iteration 200, loss 1.6501743793487549
iteration 300, loss 1.6546411514282227
iteration 400, loss 1.6273770332336426
iteration 500, loss 1.637242078781128
iteration 600, loss 1.698265552520752
iteration 700, loss 1.6428965330123901
iteration 800, loss 1.6943143606185913
iteration 0, loss 1.583366870880127
iteration 100, loss 1.5900870561599731
iteration 200, loss 1.6806418895721436
iteration 300, loss 1.6166627407073975
iteration 400, loss 1.6788123846054077
iteration 500, loss 1.6679751873016357
iteration 600, loss 1.7055052518844604
iteration 700, loss 1.6493924856185913
iteration 800, loss 1.624730110168457
iteration 0, loss 1.6402535438537598
iteration 100, loss 1.7035377025604248
iteration 200, loss 1.6179221868515015
iteration 300, loss 1.6140121221542358
iteration 400, loss 1.6306748390197754
iteration 500, loss 1.5708898305892944
iteration 600, loss 1.5492972135543823
iteration 700, loss 1.6449201107025146
iteration 800, loss 1.6431002616882324
iteration 0, loss 1.595771312713623
iteration 100, loss 1.629744291305542
iteration 200, loss 1.6105891466140747
iteration 300, loss 1.627897024154663
iteration 400, loss 1.649881362915039
iteration 500, loss 1.5854977369308472
iteration 600, loss 1.5946950912475586
iteration 700, loss 1.6073408126831055
iteration 800, loss 1.6427977085113525
iteration 0, loss 1.6873297691345215
iteration 100, loss 1.6266629695892334
iteration 200, loss 1.6644846200942993
iteration 300, loss 1.6483252048492432
iteration 400, loss 1.617573618888855
iteration 500, loss 1.6156437397003174
iteration 600, loss 1.6534174680709839
iteration 700, loss 1.5994877815246582
iteration 800, loss 1.6677547693252563
iteration 0, loss 1.6024776697158813
iteration 100, loss 1.6284375190734863
iteration 200, loss 1.6626056432724
iteration 300, loss 1.6433528661727905
iteration 400, loss 1.5907105207443237
iteration 500, loss 1.6467758417129517
iteration 600, loss 1.6182292699813843
iteration 700, loss 1.5805951356887817
iteration 800, loss 1.6197670698165894
iteration 0, loss 1.6334850788116455
iteration 100, loss 1.6461899280548096
iteration 200, loss 1.654016375541687
iteration 300, loss 1.6264228820800781
iteration 400, loss 1.6394524574279785
iteration 500, loss 1.6099143028259277
iteration 600, loss 1.7059506177902222
iteration 700, loss 1.6231443881988525
iteration 800, loss 1.6771225929260254
iteration 0, loss 1.6802312135696411
iteration 100, loss 1.6786326169967651
iteration 200, loss 1.6096140146255493
iteration 300, loss 1.6278880834579468
iteration 400, loss 1.6825586557388306
iteration 500, loss 1.620652198791504
iteration 600, loss 1.6308830976486206
iteration 700, loss 1.6467612981796265
iteration 800, loss 1.6384841203689575
iteration 0, loss 1.6072205305099487
iteration 100, loss 1.6284430027008057
iteration 200, loss 1.615390419960022
iteration 300, loss 1.6442266702651978
iteration 400, loss 1.606119990348816
iteration 500, loss 1.665910243988037
iteration 600, loss 1.6756553649902344
iteration 700, loss 1.6220247745513916
iteration 800, loss 1.588287591934204
iteration 0, loss 1.6116375923156738
iteration 100, loss 1.6261671781539917
iteration 200, loss 1.6217758655548096
iteration 300, loss 1.6724908351898193
iteration 400, loss 1.6192725896835327
iteration 500, loss 1.6290169954299927
iteration 600, loss 1.5650029182434082
iteration 700, loss 1.585027813911438
iteration 800, loss 1.643009901046753
iteration 0, loss 1.6019729375839233
iteration 100, loss 1.6462715864181519
iteration 200, loss 1.6081531047821045
iteration 300, loss 1.6457996368408203
iteration 400, loss 1.6096434593200684
iteration 500, loss 1.708195447921753
iteration 600, loss 1.6506991386413574
iteration 700, loss 1.5901771783828735
iteration 800, loss 1.6601425409317017
iteration 0, loss 1.6057430505752563
iteration 100, loss 1.638615608215332
iteration 200, loss 1.6633013486862183
iteration 300, loss 1.6733030080795288
iteration 400, loss 1.5883262157440186
iteration 500, loss 1.6680200099945068
iteration 600, loss 1.6564041376113892
iteration 700, loss 1.6586724519729614
iteration 800, loss 1.6453633308410645
iteration 0, loss 1.660150408744812
iteration 100, loss 1.6721904277801514
iteration 200, loss 1.6169822216033936
iteration 300, loss 1.6207069158554077
iteration 400, loss 1.6121670007705688
iteration 500, loss 1.7105462551116943
iteration 600, loss 1.591160535812378
iteration 700, loss 1.6270334720611572
iteration 800, loss 1.6503065824508667
iteration 0, loss 1.6465163230895996
iteration 100, loss 1.5488778352737427
iteration 200, loss 1.6250249147415161
iteration 300, loss 1.6155531406402588
iteration 400, loss 1.597033143043518
iteration 500, loss 1.6369407176971436
iteration 600, loss 1.6273571252822876
iteration 700, loss 1.6440140008926392
iteration 800, loss 1.619140625
iteration 0, loss 1.609310269355774
iteration 100, loss 1.7196937799453735
iteration 200, loss 1.6260857582092285
iteration 300, loss 1.6516995429992676
iteration 400, loss 1.6249921321868896
iteration 500, loss 1.6297377347946167
iteration 600, loss 1.6900328397750854
iteration 700, loss 1.5634608268737793
iteration 800, loss 1.652409553527832
iteration 0, loss 1.6421064138412476
iteration 100, loss 1.5811145305633545
iteration 200, loss 1.5830812454223633
iteration 300, loss 1.6322678327560425
iteration 400, loss 1.595429539680481
iteration 500, loss 1.5900685787200928
iteration 600, loss 1.5745691061019897
iteration 700, loss 1.6222326755523682
iteration 800, loss 1.7007414102554321
iteration 0, loss 1.6134469509124756
iteration 100, loss 1.632914662361145
iteration 200, loss 1.6528619527816772
iteration 300, loss 1.5900616645812988
iteration 400, loss 1.5891964435577393
iteration 500, loss 1.623104214668274
iteration 600, loss 1.585795283317566
iteration 700, loss 1.6033705472946167
iteration 800, loss 1.6812200546264648
iteration 0, loss 1.5910993814468384
iteration 100, loss 1.611730933189392
iteration 200, loss 1.6403905153274536
iteration 300, loss 1.6350131034851074
iteration 400, loss 1.606935977935791
iteration 500, loss 1.6245157718658447
iteration 600, loss 1.6597000360488892
iteration 700, loss 1.6511784791946411
iteration 800, loss 1.6130635738372803
iteration 0, loss 1.6630514860153198
iteration 100, loss 1.6570684909820557
iteration 200, loss 1.6148653030395508
iteration 300, loss 1.6166785955429077
iteration 400, loss 1.5912595987319946
iteration 500, loss 1.6181212663650513
iteration 600, loss 1.6761314868927002
iteration 700, loss 1.6309168338775635
iteration 800, loss 1.6522027254104614
iteration 0, loss 1.6370790004730225
iteration 100, loss 1.6042660474777222
iteration 200, loss 1.6867784261703491
iteration 300, loss 1.6526145935058594
iteration 400, loss 1.6126587390899658
iteration 500, loss 1.595918893814087
iteration 600, loss 1.608803629875183
iteration 700, loss 1.6161091327667236
iteration 800, loss 1.6513599157333374
iteration 0, loss 1.642540454864502
iteration 100, loss 1.6063175201416016
iteration 200, loss 1.601568341255188
iteration 300, loss 1.6691737174987793
iteration 400, loss 1.5914653539657593
iteration 500, loss 1.7577019929885864
iteration 600, loss 1.6312353610992432
iteration 700, loss 1.6210482120513916
iteration 800, loss 1.6161702871322632
iteration 0, loss 1.6511683464050293
iteration 100, loss 1.6215420961380005
iteration 200, loss 1.6195216178894043
iteration 300, loss 1.6718132495880127
iteration 400, loss 1.5908513069152832
iteration 500, loss 1.6239163875579834
iteration 600, loss 1.6065415143966675
iteration 700, loss 1.6644246578216553
iteration 800, loss 1.6230114698410034
iteration 0, loss 1.5976078510284424
iteration 100, loss 1.6773607730865479
iteration 200, loss 1.632489800453186
iteration 300, loss 1.6077909469604492
iteration 400, loss 1.6600614786148071
iteration 500, loss 1.6042729616165161
iteration 600, loss 1.586235761642456
iteration 700, loss 1.6189080476760864
iteration 800, loss 1.6328853368759155
iteration 0, loss 1.5989913940429688
iteration 100, loss 1.6265501976013184
iteration 200, loss 1.6061376333236694
iteration 300, loss 1.6372698545455933
iteration 400, loss 1.6380304098129272
iteration 500, loss 1.5627155303955078
iteration 600, loss 1.6227567195892334
iteration 700, loss 1.6192686557769775
iteration 800, loss 1.673993706703186
iteration 0, loss 1.599581003189087
iteration 100, loss 1.6605987548828125
iteration 200, loss 1.606774091720581
iteration 300, loss 1.6131476163864136
iteration 400, loss 1.592617154121399
iteration 500, loss 1.6591639518737793
iteration 600, loss 1.6224329471588135
iteration 700, loss 1.634218454360962
iteration 800, loss 1.6514378786087036
iteration 0, loss 1.7166469097137451
iteration 100, loss 1.6486310958862305
iteration 200, loss 1.6459919214248657
iteration 300, loss 1.6231547594070435
iteration 400, loss 1.605078935623169
iteration 500, loss 1.6303269863128662
iteration 600, loss 1.621171236038208
iteration 700, loss 1.5885157585144043
iteration 800, loss 1.6703271865844727
iteration 0, loss 1.5869868993759155
iteration 100, loss 1.5667203664779663
iteration 200, loss 1.5904916524887085
iteration 300, loss 1.6175060272216797
iteration 400, loss 1.6371525526046753
iteration 500, loss 1.6035311222076416
iteration 600, loss 1.6021227836608887
iteration 700, loss 1.6414624452590942
iteration 800, loss 1.607380747795105
iteration 0, loss 1.6664857864379883
iteration 100, loss 1.6204311847686768
iteration 200, loss 1.6465669870376587
iteration 300, loss 1.6313329935073853
iteration 400, loss 1.584668517112732
iteration 500, loss 1.638216257095337
iteration 600, loss 1.6779005527496338
iteration 700, loss 1.5874582529067993
iteration 800, loss 1.586705207824707
iteration 0, loss 1.593825101852417
iteration 100, loss 1.6671935319900513
iteration 200, loss 1.591460108757019
iteration 300, loss 1.6419044733047485
iteration 400, loss 1.5705589056015015
iteration 500, loss 1.6066974401474
iteration 600, loss 1.6265455484390259
iteration 700, loss 1.6745150089263916
iteration 800, loss 1.6633459329605103
iteration 0, loss 1.6590548753738403
iteration 100, loss 1.65036141872406
iteration 200, loss 1.6515920162200928
iteration 300, loss 1.6319886445999146
iteration 400, loss 1.6605585813522339
iteration 500, loss 1.6409330368041992
iteration 600, loss 1.6668338775634766
iteration 700, loss 1.6090885400772095
iteration 800, loss 1.6237821578979492
iteration 0, loss 1.6098166704177856
iteration 100, loss 1.6158115863800049
iteration 200, loss 1.6574499607086182
iteration 300, loss 1.680503010749817
iteration 400, loss 1.5874028205871582
iteration 500, loss 1.647792100906372
iteration 600, loss 1.623311161994934
iteration 700, loss 1.6833208799362183
iteration 800, loss 1.622223973274231
iteration 0, loss 1.6526405811309814
iteration 100, loss 1.619644284248352
iteration 200, loss 1.746099829673767
iteration 300, loss 1.6669094562530518
iteration 400, loss 1.6585993766784668
iteration 500, loss 1.580451250076294
iteration 600, loss 1.6552627086639404
iteration 700, loss 1.6071361303329468
iteration 800, loss 1.6897282600402832
iteration 0, loss 1.6391053199768066
iteration 100, loss 1.6157655715942383
iteration 200, loss 1.6025787591934204
iteration 300, loss 1.6179944276809692
iteration 400, loss 1.6397212743759155
iteration 500, loss 1.6098047494888306
iteration 600, loss 1.6314778327941895
iteration 700, loss 1.6157312393188477
iteration 800, loss 1.6119786500930786
iteration 0, loss 1.6661417484283447
iteration 100, loss 1.6248687505722046
iteration 200, loss 1.5821211338043213
iteration 300, loss 1.655326008796692
iteration 400, loss 1.5779154300689697
iteration 500, loss 1.624375820159912
iteration 600, loss 1.6517776250839233
iteration 700, loss 1.6492825746536255
iteration 800, loss 1.6414211988449097
iteration 0, loss 1.634919285774231
iteration 100, loss 1.5970799922943115
iteration 200, loss 1.6662077903747559
iteration 300, loss 1.6355403661727905
iteration 400, loss 1.6072458028793335
iteration 500, loss 1.5887165069580078
iteration 600, loss 1.5625330209732056
iteration 700, loss 1.6404095888137817
iteration 800, loss 1.6574604511260986
iteration 0, loss 1.58698570728302
iteration 100, loss 1.5935595035552979
iteration 200, loss 1.6695489883422852
iteration 300, loss 1.5945405960083008
iteration 400, loss 1.6323353052139282
iteration 500, loss 1.5930322408676147
iteration 600, loss 1.5926905870437622
iteration 700, loss 1.6687214374542236
iteration 800, loss 1.651495099067688
iteration 0, loss 1.6369580030441284
iteration 100, loss 1.6668592691421509
iteration 200, loss 1.6521707773208618
iteration 300, loss 1.6502176523208618
iteration 400, loss 1.618385672569275
iteration 500, loss 1.662527084350586
iteration 600, loss 1.6347757577896118
iteration 700, loss 1.6475201845169067
iteration 800, loss 1.631617546081543
iteration 0, loss 1.5906022787094116
iteration 100, loss 1.5687485933303833
iteration 200, loss 1.6267712116241455
iteration 300, loss 1.6420234441757202
iteration 400, loss 1.6054116487503052
iteration 500, loss 1.6712086200714111
iteration 600, loss 1.674713373184204
iteration 700, loss 1.6003143787384033
iteration 800, loss 1.6521756649017334
iteration 0, loss 1.5976753234863281
iteration 100, loss 1.5879228115081787
iteration 200, loss 1.6192760467529297
iteration 300, loss 1.637490153312683
iteration 400, loss 1.572932481765747
iteration 500, loss 1.663905143737793
iteration 600, loss 1.662001132965088
iteration 700, loss 1.6860064268112183
iteration 800, loss 1.6693196296691895
iteration 0, loss 1.633410930633545
iteration 100, loss 1.610490083694458
iteration 200, loss 1.6761730909347534
iteration 300, loss 1.6646281480789185
iteration 400, loss 1.6293060779571533
iteration 500, loss 1.576063632965088
iteration 600, loss 1.656540870666504
iteration 700, loss 1.612770676612854
iteration 800, loss 1.6220508813858032
iteration 0, loss 1.5624583959579468
iteration 100, loss 1.6597450971603394
iteration 200, loss 1.7340407371520996
iteration 300, loss 1.5926072597503662
iteration 400, loss 1.6240589618682861
iteration 500, loss 1.6604689359664917
iteration 600, loss 1.6251190900802612
iteration 700, loss 1.559722900390625
iteration 800, loss 1.602870225906372
iteration 0, loss 1.6627528667449951
iteration 100, loss 1.6433790922164917
iteration 200, loss 1.5770447254180908
iteration 300, loss 1.5763471126556396
iteration 400, loss 1.5977708101272583
iteration 500, loss 1.60952889919281
iteration 600, loss 1.668757677078247
iteration 700, loss 1.6403123140335083
iteration 800, loss 1.611664891242981
iteration 0, loss 1.6364006996154785
iteration 100, loss 1.6575942039489746
iteration 200, loss 1.7126905918121338
iteration 300, loss 1.6691490411758423
iteration 400, loss 1.6470582485198975
iteration 500, loss 1.6974984407424927
iteration 600, loss 1.6187893152236938
iteration 700, loss 1.6067485809326172
iteration 800, loss 1.6354156732559204
iteration 0, loss 1.5713552236557007
iteration 100, loss 1.608510136604309
iteration 200, loss 1.621751070022583
iteration 300, loss 1.596016764640808
iteration 400, loss 1.594323992729187
iteration 500, loss 1.6179709434509277
iteration 600, loss 1.6407281160354614
iteration 700, loss 1.6250038146972656
iteration 800, loss 1.6106600761413574
iteration 0, loss 1.6449106931686401
iteration 100, loss 1.6394413709640503
iteration 200, loss 1.6532741785049438
iteration 300, loss 1.608678936958313
iteration 400, loss 1.6741151809692383
iteration 500, loss 1.589078664779663
iteration 600, loss 1.6383637189865112
iteration 700, loss 1.7106741666793823
iteration 800, loss 1.6121009588241577
iteration 0, loss 1.6459665298461914
iteration 100, loss 1.6809242963790894
iteration 200, loss 1.6054381132125854
iteration 300, loss 1.580079197883606
iteration 400, loss 1.687242031097412
iteration 500, loss 1.6100940704345703
iteration 600, loss 1.6439656019210815
iteration 700, loss 1.5876712799072266
iteration 800, loss 1.6522481441497803
iteration 0, loss 1.6993529796600342
iteration 100, loss 1.584627628326416
iteration 200, loss 1.6236838102340698
iteration 300, loss 1.6743658781051636
iteration 400, loss 1.57034432888031
iteration 500, loss 1.63289475440979
iteration 600, loss 1.5999559164047241
iteration 700, loss 1.6497524976730347
iteration 800, loss 1.6280276775360107
iteration 0, loss 1.603878378868103
iteration 100, loss 1.702387809753418
iteration 200, loss 1.5888315439224243
iteration 300, loss 1.6058449745178223
iteration 400, loss 1.6374903917312622
iteration 500, loss 1.6864817142486572
iteration 600, loss 1.60724675655365
iteration 700, loss 1.6690804958343506
iteration 800, loss 1.7019604444503784
iteration 0, loss 1.6932778358459473
iteration 100, loss 1.5539054870605469
iteration 200, loss 1.7135570049285889
iteration 300, loss 1.672100305557251
iteration 400, loss 1.6015490293502808
iteration 500, loss 1.5744649171829224
iteration 600, loss 1.5841253995895386
iteration 700, loss 1.6547473669052124
iteration 800, loss 1.5873130559921265
iteration 0, loss 1.626394271850586
iteration 100, loss 1.6166096925735474
iteration 200, loss 1.651165246963501
iteration 300, loss 1.605918526649475
iteration 400, loss 1.6691765785217285
iteration 500, loss 1.602641224861145
iteration 600, loss 1.6249451637268066
iteration 700, loss 1.6414748430252075
iteration 800, loss 1.6248587369918823
fold 1 accuracy: 0.8148571428571428
iteration 0, loss 1.7364283800125122
iteration 100, loss 1.6924599409103394
iteration 200, loss 1.668801188468933
iteration 300, loss 1.5597668886184692
iteration 400, loss 1.668715238571167
iteration 500, loss 1.6430435180664062
iteration 600, loss 1.6326019763946533
iteration 700, loss 1.623365879058838
iteration 800, loss 1.5592528581619263
iteration 0, loss 1.609434962272644
iteration 100, loss 1.5985888242721558
iteration 200, loss 1.6429697275161743
iteration 300, loss 1.6143968105316162
iteration 400, loss 1.6114650964736938
iteration 500, loss 1.6190295219421387
iteration 600, loss 1.6219041347503662
iteration 700, loss 1.6035586595535278
iteration 800, loss 1.6377836465835571
iteration 0, loss 1.64493989944458
iteration 100, loss 1.6293455362319946
iteration 200, loss 1.6075599193572998
iteration 300, loss 1.6359977722167969
iteration 400, loss 1.6245315074920654
iteration 500, loss 1.7076764106750488
iteration 600, loss 1.5910084247589111
iteration 700, loss 1.6324183940887451
iteration 800, loss 1.6340683698654175
iteration 0, loss 1.6666983366012573
iteration 100, loss 1.6493620872497559
iteration 200, loss 1.6495100259780884
iteration 300, loss 1.591449499130249
iteration 400, loss 1.5758365392684937
iteration 500, loss 1.6063871383666992
iteration 600, loss 1.6342172622680664
iteration 700, loss 1.5822088718414307
iteration 800, loss 1.6187002658843994
iteration 0, loss 1.6816445589065552
iteration 100, loss 1.6444369554519653
iteration 200, loss 1.635437250137329
iteration 300, loss 1.6094095706939697
iteration 400, loss 1.627699613571167
iteration 500, loss 1.605385661125183
iteration 600, loss 1.6634690761566162
iteration 700, loss 1.663299798965454
iteration 800, loss 1.6018316745758057
iteration 0, loss 1.6386889219284058
iteration 100, loss 1.5666786432266235
iteration 200, loss 1.5978952646255493
iteration 300, loss 1.5961953401565552
iteration 400, loss 1.5927236080169678
iteration 500, loss 1.6135684251785278
iteration 600, loss 1.6056700944900513
iteration 700, loss 1.5586202144622803
iteration 800, loss 1.6245448589324951
iteration 0, loss 1.5757968425750732
iteration 100, loss 1.5957951545715332
iteration 200, loss 1.6383883953094482
iteration 300, loss 1.6906163692474365
iteration 400, loss 1.6589370965957642
iteration 500, loss 1.6322729587554932
iteration 600, loss 1.530874252319336
iteration 700, loss 1.6926219463348389
iteration 800, loss 1.6685580015182495
iteration 0, loss 1.6763663291931152
iteration 100, loss 1.6325550079345703
iteration 200, loss 1.6397817134857178
iteration 300, loss 1.6465436220169067
iteration 400, loss 1.627658724784851
iteration 500, loss 1.598960518836975
iteration 600, loss 1.5957880020141602
iteration 700, loss 1.651154637336731
iteration 800, loss 1.6325064897537231
iteration 0, loss 1.5851231813430786
iteration 100, loss 1.606704592704773
iteration 200, loss 1.6179522275924683
iteration 300, loss 1.644993543624878
iteration 400, loss 1.6425994634628296
iteration 500, loss 1.6143016815185547
iteration 600, loss 1.6012670993804932
iteration 700, loss 1.5983566045761108
iteration 800, loss 1.641422986984253
iteration 0, loss 1.6463302373886108
iteration 100, loss 1.5930588245391846
iteration 200, loss 1.6356620788574219
iteration 300, loss 1.60478937625885
iteration 400, loss 1.629624605178833
iteration 500, loss 1.6875392198562622
iteration 600, loss 1.618290662765503
iteration 700, loss 1.653777003288269
iteration 800, loss 1.6113821268081665
iteration 0, loss 1.643782615661621
iteration 100, loss 1.5854835510253906
iteration 200, loss 1.625417947769165
iteration 300, loss 1.6163324117660522
iteration 400, loss 1.545491337776184
iteration 500, loss 1.6570606231689453
iteration 600, loss 1.691135048866272
iteration 700, loss 1.6590774059295654
iteration 800, loss 1.6096209287643433
iteration 0, loss 1.5975152254104614
iteration 100, loss 1.5783905982971191
iteration 200, loss 1.7042194604873657
iteration 300, loss 1.5907282829284668
iteration 400, loss 1.635382890701294
iteration 500, loss 1.6098610162734985
iteration 600, loss 1.6099339723587036
iteration 700, loss 1.6165437698364258
iteration 800, loss 1.6576367616653442
iteration 0, loss 1.5788708925247192
iteration 100, loss 1.6884591579437256
iteration 200, loss 1.5970377922058105
iteration 300, loss 1.5950647592544556
iteration 400, loss 1.621539831161499
iteration 500, loss 1.600895643234253
iteration 600, loss 1.6249147653579712
iteration 700, loss 1.6521724462509155
iteration 800, loss 1.6606521606445312
iteration 0, loss 1.6126092672348022
iteration 100, loss 1.611997365951538
iteration 200, loss 1.6772128343582153
iteration 300, loss 1.5898905992507935
iteration 400, loss 1.6787548065185547
iteration 500, loss 1.5711910724639893
iteration 600, loss 1.618858814239502
iteration 700, loss 1.7503663301467896
iteration 800, loss 1.601108193397522
iteration 0, loss 1.611655831336975
iteration 100, loss 1.5966105461120605
iteration 200, loss 1.6671754121780396
iteration 300, loss 1.578351378440857
iteration 400, loss 1.6115458011627197
iteration 500, loss 1.615568995475769
iteration 600, loss 1.6386350393295288
iteration 700, loss 1.6119186878204346
iteration 800, loss 1.6431808471679688
iteration 0, loss 1.6304179430007935
iteration 100, loss 1.605651617050171
iteration 200, loss 1.608109951019287
iteration 300, loss 1.6291543245315552
iteration 400, loss 1.6329257488250732
iteration 500, loss 1.6020041704177856
iteration 600, loss 1.594287633895874
iteration 700, loss 1.5703414678573608
iteration 800, loss 1.639483094215393
iteration 0, loss 1.640256404876709
iteration 100, loss 1.6253939867019653
iteration 200, loss 1.6384388208389282
iteration 300, loss 1.5867360830307007
iteration 400, loss 1.6513961553573608
iteration 500, loss 1.620041847229004
iteration 600, loss 1.639757752418518
iteration 700, loss 1.5838475227355957
iteration 800, loss 1.6194432973861694
iteration 0, loss 1.6023660898208618
iteration 100, loss 1.641505479812622
iteration 200, loss 1.589209794998169
iteration 300, loss 1.6286307573318481
iteration 400, loss 1.5650041103363037
iteration 500, loss 1.6552599668502808
iteration 600, loss 1.6337838172912598
iteration 700, loss 1.6462138891220093
iteration 800, loss 1.632988452911377
iteration 0, loss 1.6090896129608154
iteration 100, loss 1.6149455308914185
iteration 200, loss 1.6311229467391968
iteration 300, loss 1.6632949113845825
iteration 400, loss 1.5787155628204346
iteration 500, loss 1.6396838426589966
iteration 600, loss 1.6216460466384888
iteration 700, loss 1.5951625108718872
iteration 800, loss 1.637784719467163
iteration 0, loss 1.688517451286316
iteration 100, loss 1.6345772743225098
iteration 200, loss 1.594344973564148
iteration 300, loss 1.6708203554153442
iteration 400, loss 1.6135454177856445
iteration 500, loss 1.654653549194336
iteration 600, loss 1.6524658203125
iteration 700, loss 1.6239924430847168
iteration 800, loss 1.630444884300232
iteration 0, loss 1.6139131784439087
iteration 100, loss 1.6270381212234497
iteration 200, loss 1.6158615350723267
iteration 300, loss 1.6146445274353027
iteration 400, loss 1.557620644569397
iteration 500, loss 1.6392982006072998
iteration 600, loss 1.6896052360534668
iteration 700, loss 1.6521399021148682
iteration 800, loss 1.6085293292999268
iteration 0, loss 1.562813639640808
iteration 100, loss 1.6508103609085083
iteration 200, loss 1.6184511184692383
iteration 300, loss 1.5845636129379272
iteration 400, loss 1.6069400310516357
iteration 500, loss 1.5616164207458496
iteration 600, loss 1.6446161270141602
iteration 700, loss 1.6035155057907104
iteration 800, loss 1.6383901834487915
iteration 0, loss 1.6684672832489014
iteration 100, loss 1.6257972717285156
iteration 200, loss 1.6785582304000854
iteration 300, loss 1.611443042755127
iteration 400, loss 1.5896400213241577
iteration 500, loss 1.6505775451660156
iteration 600, loss 1.6307787895202637
iteration 700, loss 1.6799890995025635
iteration 800, loss 1.6176565885543823
iteration 0, loss 1.6045116186141968
iteration 100, loss 1.6857032775878906
iteration 200, loss 1.6042536497116089
iteration 300, loss 1.581727385520935
iteration 400, loss 1.644454002380371
iteration 500, loss 1.6558051109313965
iteration 600, loss 1.623093605041504
iteration 700, loss 1.5971741676330566
iteration 800, loss 1.580919623374939
iteration 0, loss 1.6679545640945435
iteration 100, loss 1.6677685976028442
iteration 200, loss 1.5951145887374878
iteration 300, loss 1.5999834537506104
iteration 400, loss 1.617447018623352
iteration 500, loss 1.6448451280593872
iteration 600, loss 1.6280200481414795
iteration 700, loss 1.6415129899978638
iteration 800, loss 1.5729358196258545
iteration 0, loss 1.668935775756836
iteration 100, loss 1.6694526672363281
iteration 200, loss 1.6323097944259644
iteration 300, loss 1.657213568687439
iteration 400, loss 1.6073265075683594
iteration 500, loss 1.7016396522521973
iteration 600, loss 1.6316248178482056
iteration 700, loss 1.6031558513641357
iteration 800, loss 1.73542058467865
iteration 0, loss 1.5922892093658447
iteration 100, loss 1.5491267442703247
iteration 200, loss 1.607452392578125
iteration 300, loss 1.5924468040466309
iteration 400, loss 1.577504277229309
iteration 500, loss 1.624437928199768
iteration 600, loss 1.6156933307647705
iteration 700, loss 1.628868818283081
iteration 800, loss 1.6792888641357422
iteration 0, loss 1.617424726486206
iteration 100, loss 1.640761375427246
iteration 200, loss 1.6636605262756348
iteration 300, loss 1.641478419303894
iteration 400, loss 1.6518783569335938
iteration 500, loss 1.552490472793579
iteration 600, loss 1.6320478916168213
iteration 700, loss 1.6633416414260864
iteration 800, loss 1.6286824941635132
iteration 0, loss 1.6284633874893188
iteration 100, loss 1.5973938703536987
iteration 200, loss 1.630428671836853
iteration 300, loss 1.5436652898788452
iteration 400, loss 1.5931600332260132
iteration 500, loss 1.663442611694336
iteration 600, loss 1.6198058128356934
iteration 700, loss 1.6488118171691895
iteration 800, loss 1.642751932144165
iteration 0, loss 1.5815051794052124
iteration 100, loss 1.6812697649002075
iteration 200, loss 1.6218090057373047
iteration 300, loss 1.592374563217163
iteration 400, loss 1.6544158458709717
iteration 500, loss 1.6162734031677246
iteration 600, loss 1.6406019926071167
iteration 700, loss 1.5916471481323242
iteration 800, loss 1.6272542476654053
iteration 0, loss 1.6119872331619263
iteration 100, loss 1.6768513917922974
iteration 200, loss 1.6125833988189697
iteration 300, loss 1.597991943359375
iteration 400, loss 1.6637239456176758
iteration 500, loss 1.5889089107513428
iteration 600, loss 1.5995713472366333
iteration 700, loss 1.5821473598480225
iteration 800, loss 1.6348634958267212
iteration 0, loss 1.5826529264450073
iteration 100, loss 1.607324481010437
iteration 200, loss 1.6196898221969604
iteration 300, loss 1.6509454250335693
iteration 400, loss 1.6567988395690918
iteration 500, loss 1.5912809371948242
iteration 600, loss 1.5980409383773804
iteration 700, loss 1.6824588775634766
iteration 800, loss 1.593772530555725
iteration 0, loss 1.6806719303131104
iteration 100, loss 1.5692222118377686
iteration 200, loss 1.6665617227554321
iteration 300, loss 1.626067042350769
iteration 400, loss 1.6269229650497437
iteration 500, loss 1.5976134538650513
iteration 600, loss 1.6303631067276
iteration 700, loss 1.609816074371338
iteration 800, loss 1.5815638303756714
iteration 0, loss 1.649287223815918
iteration 100, loss 1.6213550567626953
iteration 200, loss 1.719590425491333
iteration 300, loss 1.6123793125152588
iteration 400, loss 1.6083446741104126
iteration 500, loss 1.6720552444458008
iteration 600, loss 1.6570860147476196
iteration 700, loss 1.649071455001831
iteration 800, loss 1.6368238925933838
iteration 0, loss 1.626624345779419
iteration 100, loss 1.5798571109771729
iteration 200, loss 1.6381877660751343
iteration 300, loss 1.6101278066635132
iteration 400, loss 1.6245694160461426
iteration 500, loss 1.6549251079559326
iteration 600, loss 1.6055999994277954
iteration 700, loss 1.6243135929107666
iteration 800, loss 1.648172378540039
iteration 0, loss 1.6618270874023438
iteration 100, loss 1.5954340696334839
iteration 200, loss 1.6788742542266846
iteration 300, loss 1.670773983001709
iteration 400, loss 1.6263008117675781
iteration 500, loss 1.6225658655166626
iteration 600, loss 1.6268044710159302
iteration 700, loss 1.584937572479248
iteration 800, loss 1.6033520698547363
iteration 0, loss 1.5910749435424805
iteration 100, loss 1.6010769605636597
iteration 200, loss 1.6419686079025269
iteration 300, loss 1.602968454360962
iteration 400, loss 1.6197280883789062
iteration 500, loss 1.6358835697174072
iteration 600, loss 1.6338202953338623
iteration 700, loss 1.6072965860366821
iteration 800, loss 1.550215482711792
iteration 0, loss 1.672896385192871
iteration 100, loss 1.624398112297058
iteration 200, loss 1.630005121231079
iteration 300, loss 1.6639809608459473
iteration 400, loss 1.5828689336776733
iteration 500, loss 1.6139869689941406
iteration 600, loss 1.6326321363449097
iteration 700, loss 1.6459981203079224
iteration 800, loss 1.6485168933868408
iteration 0, loss 1.6159300804138184
iteration 100, loss 1.6611781120300293
iteration 200, loss 1.641086459159851
iteration 300, loss 1.6232032775878906
iteration 400, loss 1.6247905492782593
iteration 500, loss 1.6045501232147217
iteration 600, loss 1.6388218402862549
iteration 700, loss 1.5973215103149414
iteration 800, loss 1.57909095287323
iteration 0, loss 1.7244536876678467
iteration 100, loss 1.5822324752807617
iteration 200, loss 1.6859928369522095
iteration 300, loss 1.6802970170974731
iteration 400, loss 1.6261591911315918
iteration 500, loss 1.6720738410949707
iteration 600, loss 1.5687012672424316
iteration 700, loss 1.6403558254241943
iteration 800, loss 1.6400879621505737
iteration 0, loss 1.615372896194458
iteration 100, loss 1.6258177757263184
iteration 200, loss 1.5840615034103394
iteration 300, loss 1.6751396656036377
iteration 400, loss 1.6530826091766357
iteration 500, loss 1.6416722536087036
iteration 600, loss 1.636502742767334
iteration 700, loss 1.6345081329345703
iteration 800, loss 1.6084948778152466
iteration 0, loss 1.5851178169250488
iteration 100, loss 1.6532758474349976
iteration 200, loss 1.6691796779632568
iteration 300, loss 1.5974221229553223
iteration 400, loss 1.6698447465896606
iteration 500, loss 1.606370449066162
iteration 600, loss 1.7144571542739868
iteration 700, loss 1.611060380935669
iteration 800, loss 1.5908911228179932
iteration 0, loss 1.684576153755188
iteration 100, loss 1.6012259721755981
iteration 200, loss 1.6443525552749634
iteration 300, loss 1.590095043182373
iteration 400, loss 1.5795443058013916
iteration 500, loss 1.6199949979782104
iteration 600, loss 1.6404175758361816
iteration 700, loss 1.6310912370681763
iteration 800, loss 1.6129945516586304
iteration 0, loss 1.575443148612976
iteration 100, loss 1.6188491582870483
iteration 200, loss 1.579825758934021
iteration 300, loss 1.5807251930236816
iteration 400, loss 1.5902700424194336
iteration 500, loss 1.6676805019378662
iteration 600, loss 1.599255084991455
iteration 700, loss 1.6175589561462402
iteration 800, loss 1.6646345853805542
iteration 0, loss 1.601921558380127
iteration 100, loss 1.5914400815963745
iteration 200, loss 1.6860893964767456
iteration 300, loss 1.6532793045043945
iteration 400, loss 1.6397286653518677
iteration 500, loss 1.5913628339767456
iteration 600, loss 1.6527221202850342
iteration 700, loss 1.6888848543167114
iteration 800, loss 1.6449140310287476
iteration 0, loss 1.6151376962661743
iteration 100, loss 1.610357403755188
iteration 200, loss 1.5717672109603882
iteration 300, loss 1.6614285707473755
iteration 400, loss 1.6196212768554688
iteration 500, loss 1.6395385265350342
iteration 600, loss 1.6173813343048096
iteration 700, loss 1.6490566730499268
iteration 800, loss 1.6779569387435913
iteration 0, loss 1.6112083196640015
iteration 100, loss 1.6647653579711914
iteration 200, loss 1.634995698928833
iteration 300, loss 1.5729893445968628
iteration 400, loss 1.5985416173934937
iteration 500, loss 1.6618696451187134
iteration 600, loss 1.60292649269104
iteration 700, loss 1.609502911567688
iteration 800, loss 1.6201062202453613
iteration 0, loss 1.6174319982528687
iteration 100, loss 1.6109107732772827
iteration 200, loss 1.594626784324646
iteration 300, loss 1.5657581090927124
iteration 400, loss 1.6237633228302002
iteration 500, loss 1.615301489830017
iteration 600, loss 1.5670126676559448
iteration 700, loss 1.6494325399398804
iteration 800, loss 1.6106293201446533
iteration 0, loss 1.6972390413284302
iteration 100, loss 1.621142864227295
iteration 200, loss 1.6254515647888184
iteration 300, loss 1.5782177448272705
iteration 400, loss 1.6279301643371582
iteration 500, loss 1.66242516040802
iteration 600, loss 1.5865250825881958
iteration 700, loss 1.5994579792022705
iteration 800, loss 1.6179441213607788
iteration 0, loss 1.6241183280944824
iteration 100, loss 1.6324383020401
iteration 200, loss 1.609320044517517
iteration 300, loss 1.615901231765747
iteration 400, loss 1.5601096153259277
iteration 500, loss 1.593819499015808
iteration 600, loss 1.5871176719665527
iteration 700, loss 1.6176954507827759
iteration 800, loss 1.5883649587631226
fold 2 accuracy: 0.8112142857142857
iteration 0, loss 1.610152244567871
iteration 100, loss 1.590043306350708
iteration 200, loss 1.5941921472549438
iteration 300, loss 1.666582465171814
iteration 400, loss 1.6143213510513306
iteration 500, loss 1.7136499881744385
iteration 600, loss 1.5997660160064697
iteration 700, loss 1.6454566717147827
iteration 800, loss 1.7054578065872192
iteration 0, loss 1.6345582008361816
iteration 100, loss 1.7266099452972412
iteration 200, loss 1.6314791440963745
iteration 300, loss 1.68593168258667
iteration 400, loss 1.67893648147583
iteration 500, loss 1.6111453771591187
iteration 600, loss 1.669729232788086
iteration 700, loss 1.6253482103347778
iteration 800, loss 1.6287494897842407
iteration 0, loss 1.6136058568954468
iteration 100, loss 1.6210793256759644
iteration 200, loss 1.7081174850463867
iteration 300, loss 1.6307401657104492
iteration 400, loss 1.6042124032974243
iteration 500, loss 1.6556051969528198
iteration 600, loss 1.574460744857788
iteration 700, loss 1.5809807777404785
iteration 800, loss 1.586961269378662
iteration 0, loss 1.6295318603515625
iteration 100, loss 1.6551148891448975
iteration 200, loss 1.6224861145019531
iteration 300, loss 1.645534634590149
iteration 400, loss 1.6638792753219604
iteration 500, loss 1.59015691280365
iteration 600, loss 1.6372977495193481
iteration 700, loss 1.6492128372192383
iteration 800, loss 1.6008363962173462
iteration 0, loss 1.5898367166519165
iteration 100, loss 1.6641662120819092
iteration 200, loss 1.6336621046066284
iteration 300, loss 1.6950725317001343
iteration 400, loss 1.6455734968185425
iteration 500, loss 1.6670514345169067
iteration 600, loss 1.5925841331481934
iteration 700, loss 1.6596986055374146
iteration 800, loss 1.6245403289794922
iteration 0, loss 1.6310292482376099
iteration 100, loss 1.606318473815918
iteration 200, loss 1.6087151765823364
iteration 300, loss 1.7057820558547974
iteration 400, loss 1.6086806058883667
iteration 500, loss 1.5496609210968018
iteration 600, loss 1.6671539545059204
iteration 700, loss 1.5810683965682983
iteration 800, loss 1.6491780281066895
iteration 0, loss 1.5511348247528076
iteration 100, loss 1.5526140928268433
iteration 200, loss 1.6068878173828125
iteration 300, loss 1.643873691558838
iteration 400, loss 1.6450998783111572
iteration 500, loss 1.6571379899978638
iteration 600, loss 1.6017464399337769
iteration 700, loss 1.6421781778335571
iteration 800, loss 1.606278896331787
iteration 0, loss 1.6277259588241577
iteration 100, loss 1.6203372478485107
iteration 200, loss 1.6874974966049194
iteration 300, loss 1.5878278017044067
iteration 400, loss 1.6300475597381592
iteration 500, loss 1.5769193172454834
iteration 600, loss 1.6581732034683228
iteration 700, loss 1.5764219760894775
iteration 800, loss 1.6946855783462524
iteration 0, loss 1.6515676975250244
iteration 100, loss 1.6228468418121338
iteration 200, loss 1.6349170207977295
iteration 300, loss 1.6022610664367676
iteration 400, loss 1.6364021301269531
iteration 500, loss 1.6784183979034424
iteration 600, loss 1.6195625066757202
iteration 700, loss 1.5869293212890625
iteration 800, loss 1.719109058380127
iteration 0, loss 1.6810061931610107
iteration 100, loss 1.6338039636611938
iteration 200, loss 1.6286466121673584
iteration 300, loss 1.6914523839950562
iteration 400, loss 1.5956600904464722
iteration 500, loss 1.604442834854126
iteration 600, loss 1.644870638847351
iteration 700, loss 1.5983026027679443
iteration 800, loss 1.662004828453064
iteration 0, loss 1.6770997047424316
iteration 100, loss 1.643449306488037
iteration 200, loss 1.6534829139709473
iteration 300, loss 1.5870708227157593
iteration 400, loss 1.6845513582229614
iteration 500, loss 1.6503207683563232
iteration 600, loss 1.6506781578063965
iteration 700, loss 1.5925607681274414
iteration 800, loss 1.6205660104751587
iteration 0, loss 1.6256903409957886
iteration 100, loss 1.5876860618591309
iteration 200, loss 1.6027745008468628
iteration 300, loss 1.6232995986938477
iteration 400, loss 1.6662466526031494
iteration 500, loss 1.6992284059524536
iteration 600, loss 1.613564372062683
iteration 700, loss 1.6329420804977417
iteration 800, loss 1.665994644165039
iteration 0, loss 1.5844237804412842
iteration 100, loss 1.636739730834961
iteration 200, loss 1.6201491355895996
iteration 300, loss 1.566985011100769
iteration 400, loss 1.5738102197647095
iteration 500, loss 1.6374778747558594
iteration 600, loss 1.6000795364379883
iteration 700, loss 1.5599279403686523
iteration 800, loss 1.6149828433990479
iteration 0, loss 1.6283279657363892
iteration 100, loss 1.7053158283233643
iteration 200, loss 1.6043179035186768
iteration 300, loss 1.6876205205917358
iteration 400, loss 1.6068603992462158
iteration 500, loss 1.6111557483673096
iteration 600, loss 1.6592234373092651
iteration 700, loss 1.6896365880966187
iteration 800, loss 1.6309866905212402
iteration 0, loss 1.5919032096862793
iteration 100, loss 1.6084486246109009
iteration 200, loss 1.6512137651443481
iteration 300, loss 1.6417871713638306
iteration 400, loss 1.6126328706741333
iteration 500, loss 1.6137423515319824
iteration 600, loss 1.5922147035598755
iteration 700, loss 1.6385458707809448
iteration 800, loss 1.6585373878479004
iteration 0, loss 1.6460775136947632
iteration 100, loss 1.6337722539901733
iteration 200, loss 1.6586545705795288
iteration 300, loss 1.5511153936386108
iteration 400, loss 1.6830825805664062
iteration 500, loss 1.6185070276260376
iteration 600, loss 1.5775697231292725
iteration 700, loss 1.632179617881775
iteration 800, loss 1.5814955234527588
iteration 0, loss 1.6313714981079102
iteration 100, loss 1.5759921073913574
iteration 200, loss 1.586917757987976
iteration 300, loss 1.6511796712875366
iteration 400, loss 1.6172142028808594
iteration 500, loss 1.5998873710632324
iteration 600, loss 1.6167784929275513
iteration 700, loss 1.6268751621246338
iteration 800, loss 1.6229276657104492
iteration 0, loss 1.6064610481262207
iteration 100, loss 1.6467183828353882
iteration 200, loss 1.6597071886062622
iteration 300, loss 1.6727874279022217
iteration 400, loss 1.6094099283218384
iteration 500, loss 1.5928361415863037
iteration 600, loss 1.6653043031692505
iteration 700, loss 1.709832787513733
iteration 800, loss 1.6642370223999023
iteration 0, loss 1.6457291841506958
iteration 100, loss 1.604488730430603
iteration 200, loss 1.6499403715133667
iteration 300, loss 1.5758708715438843
iteration 400, loss 1.6120517253875732
iteration 500, loss 1.6832506656646729
iteration 600, loss 1.644188404083252
iteration 700, loss 1.6436806917190552
iteration 800, loss 1.6170002222061157
iteration 0, loss 1.5677820444107056
iteration 100, loss 1.5953000783920288
iteration 200, loss 1.5828901529312134
iteration 300, loss 1.636946439743042
iteration 400, loss 1.5783190727233887
iteration 500, loss 1.6204675436019897
iteration 600, loss 1.6174993515014648
iteration 700, loss 1.6258835792541504
iteration 800, loss 1.5825384855270386
iteration 0, loss 1.5524539947509766
iteration 100, loss 1.6266086101531982
iteration 200, loss 1.677880048751831
iteration 300, loss 1.6047672033309937
iteration 400, loss 1.6789135932922363
iteration 500, loss 1.5598052740097046
iteration 600, loss 1.5977323055267334
iteration 700, loss 1.6147373914718628
iteration 800, loss 1.5678932666778564
iteration 0, loss 1.6312358379364014
iteration 100, loss 1.6780015230178833
iteration 200, loss 1.5749871730804443
iteration 300, loss 1.6070430278778076
iteration 400, loss 1.6481605768203735
iteration 500, loss 1.5938962697982788
iteration 600, loss 1.5871113538742065
iteration 700, loss 1.6495075225830078
iteration 800, loss 1.597757339477539
iteration 0, loss 1.6127750873565674
iteration 100, loss 1.6214606761932373
iteration 200, loss 1.590456247329712
iteration 300, loss 1.6109803915023804
iteration 400, loss 1.668012022972107
iteration 500, loss 1.6522237062454224
iteration 600, loss 1.646347999572754
iteration 700, loss 1.6456151008605957
iteration 800, loss 1.645432472229004
iteration 0, loss 1.5930824279785156
iteration 100, loss 1.612680435180664
iteration 200, loss 1.5490498542785645
iteration 300, loss 1.6146281957626343
iteration 400, loss 1.6600341796875
iteration 500, loss 1.632460117340088
iteration 600, loss 1.6245523691177368
iteration 700, loss 1.5902963876724243
iteration 800, loss 1.6972752809524536
iteration 0, loss 1.617933988571167
iteration 100, loss 1.5785200595855713
iteration 200, loss 1.5900743007659912
iteration 300, loss 1.632365107536316
iteration 400, loss 1.6248297691345215
iteration 500, loss 1.6538397073745728
iteration 600, loss 1.6005674600601196
iteration 700, loss 1.6162760257720947
iteration 800, loss 1.6533547639846802
iteration 0, loss 1.6467112302780151
iteration 100, loss 1.5811042785644531
iteration 200, loss 1.6548324823379517
iteration 300, loss 1.6501317024230957
iteration 400, loss 1.6616036891937256
iteration 500, loss 1.5915266275405884
iteration 600, loss 1.6550772190093994
iteration 700, loss 1.6518664360046387
iteration 800, loss 1.6372606754302979
iteration 0, loss 1.5771812200546265
iteration 100, loss 1.6001859903335571
iteration 200, loss 1.5980201959609985
iteration 300, loss 1.6751095056533813
iteration 400, loss 1.6532496213912964
iteration 500, loss 1.6031423807144165
iteration 600, loss 1.641059160232544
iteration 700, loss 1.6092041730880737
iteration 800, loss 1.635635495185852
iteration 0, loss 1.5934431552886963
iteration 100, loss 1.5791527032852173
iteration 200, loss 1.6950510740280151
iteration 300, loss 1.6013216972351074
iteration 400, loss 1.6184989213943481
iteration 500, loss 1.6012086868286133
iteration 600, loss 1.6190178394317627
iteration 700, loss 1.6510096788406372
iteration 800, loss 1.648302674293518
iteration 0, loss 1.631022334098816
iteration 100, loss 1.6086443662643433
iteration 200, loss 1.5770864486694336
iteration 300, loss 1.6405080556869507
iteration 400, loss 1.6067732572555542
iteration 500, loss 1.57430100440979
iteration 600, loss 1.6122760772705078
iteration 700, loss 1.6378270387649536
iteration 800, loss 1.6297612190246582
iteration 0, loss 1.631279706954956
iteration 100, loss 1.6261565685272217
iteration 200, loss 1.5736796855926514
iteration 300, loss 1.6286871433258057
iteration 400, loss 1.628166675567627
iteration 500, loss 1.5784114599227905
iteration 600, loss 1.5767287015914917
iteration 700, loss 1.5757228136062622
iteration 800, loss 1.6468337774276733
iteration 0, loss 1.609721302986145
iteration 100, loss 1.5973609685897827
iteration 200, loss 1.6130003929138184
iteration 300, loss 1.6160250902175903
iteration 400, loss 1.6397647857666016
iteration 500, loss 1.6519854068756104
iteration 600, loss 1.6250708103179932
iteration 700, loss 1.6297450065612793
iteration 800, loss 1.6652278900146484
iteration 0, loss 1.6288224458694458
iteration 100, loss 1.6253836154937744
iteration 200, loss 1.5908852815628052
iteration 300, loss 1.5933681726455688
iteration 400, loss 1.578537940979004
iteration 500, loss 1.5801281929016113
iteration 600, loss 1.6819441318511963
iteration 700, loss 1.6074873208999634
iteration 800, loss 1.6361738443374634
iteration 0, loss 1.609068512916565
iteration 100, loss 1.6984270811080933
iteration 200, loss 1.6090731620788574
iteration 300, loss 1.5867513418197632
iteration 400, loss 1.6172658205032349
iteration 500, loss 1.5636948347091675
iteration 600, loss 1.5997231006622314
iteration 700, loss 1.6404383182525635
iteration 800, loss 1.6111998558044434
iteration 0, loss 1.5526962280273438
iteration 100, loss 1.633509635925293
iteration 200, loss 1.6054770946502686
iteration 300, loss 1.6206836700439453
iteration 400, loss 1.617472767829895
iteration 500, loss 1.6351999044418335
iteration 600, loss 1.6397868394851685
iteration 700, loss 1.590908408164978
iteration 800, loss 1.6047499179840088
iteration 0, loss 1.6107233762741089
iteration 100, loss 1.6161558628082275
iteration 200, loss 1.6043702363967896
iteration 300, loss 1.6005960702896118
iteration 400, loss 1.6052563190460205
iteration 500, loss 1.607590913772583
iteration 600, loss 1.6277817487716675
iteration 700, loss 1.6788345575332642
iteration 800, loss 1.650067925453186
iteration 0, loss 1.5967950820922852
iteration 100, loss 1.5842866897583008
iteration 200, loss 1.60565984249115
iteration 300, loss 1.6281323432922363
iteration 400, loss 1.5699818134307861
iteration 500, loss 1.6178038120269775
iteration 600, loss 1.5607490539550781
iteration 700, loss 1.6030693054199219
iteration 800, loss 1.5915182828903198
iteration 0, loss 1.6658421754837036
iteration 100, loss 1.6185991764068604
iteration 200, loss 1.5922974348068237
iteration 300, loss 1.5963773727416992
iteration 400, loss 1.6326193809509277
iteration 500, loss 1.5593485832214355
iteration 600, loss 1.5824121236801147
iteration 700, loss 1.644638180732727
iteration 800, loss 1.6948323249816895
iteration 0, loss 1.6187629699707031
iteration 100, loss 1.6393176317214966
iteration 200, loss 1.645247220993042
iteration 300, loss 1.6273554563522339
iteration 400, loss 1.6178404092788696
iteration 500, loss 1.6412149667739868
iteration 600, loss 1.5935096740722656
iteration 700, loss 1.571399211883545
iteration 800, loss 1.6225149631500244
iteration 0, loss 1.5962814092636108
iteration 100, loss 1.5971121788024902
iteration 200, loss 1.5893737077713013
iteration 300, loss 1.5934796333312988
iteration 400, loss 1.5877161026000977
iteration 500, loss 1.5981009006500244
iteration 600, loss 1.6243393421173096
iteration 700, loss 1.632597804069519
iteration 800, loss 1.6669584512710571
iteration 0, loss 1.6101105213165283
iteration 100, loss 1.6323984861373901
iteration 200, loss 1.6202998161315918
iteration 300, loss 1.6522797346115112
iteration 400, loss 1.6531109809875488
iteration 500, loss 1.6064162254333496
iteration 600, loss 1.639282464981079
iteration 700, loss 1.600762963294983
iteration 800, loss 1.6408419609069824
iteration 0, loss 1.6792528629302979
iteration 100, loss 1.65609610080719
iteration 200, loss 1.6152697801589966
iteration 300, loss 1.6128566265106201
iteration 400, loss 1.6461920738220215
iteration 500, loss 1.666536808013916
iteration 600, loss 1.6514443159103394
iteration 700, loss 1.6361888647079468
iteration 800, loss 1.5856280326843262
iteration 0, loss 1.6001131534576416
iteration 100, loss 1.6258678436279297
iteration 200, loss 1.596277117729187
iteration 300, loss 1.6048284769058228
iteration 400, loss 1.6229091882705688
iteration 500, loss 1.6547141075134277
iteration 600, loss 1.6692125797271729
iteration 700, loss 1.6319698095321655
iteration 800, loss 1.6570124626159668
iteration 0, loss 1.582732915878296
iteration 100, loss 1.627335786819458
iteration 200, loss 1.6201728582382202
iteration 300, loss 1.6306358575820923
iteration 400, loss 1.606884479522705
iteration 500, loss 1.667067050933838
iteration 600, loss 1.6118656396865845
iteration 700, loss 1.630258321762085
iteration 800, loss 1.6693203449249268
iteration 0, loss 1.5981709957122803
iteration 100, loss 1.6213458776474
iteration 200, loss 1.5891382694244385
iteration 300, loss 1.639789342880249
iteration 400, loss 1.641318440437317
iteration 500, loss 1.6422418355941772
iteration 600, loss 1.6055548191070557
iteration 700, loss 1.6491596698760986
iteration 800, loss 1.6082228422164917
iteration 0, loss 1.6183700561523438
iteration 100, loss 1.5938222408294678
iteration 200, loss 1.6477680206298828
iteration 300, loss 1.594573974609375
iteration 400, loss 1.603864312171936
iteration 500, loss 1.613276720046997
iteration 600, loss 1.5725350379943848
iteration 700, loss 1.6281603574752808
iteration 800, loss 1.756746768951416
iteration 0, loss 1.6092041730880737
iteration 100, loss 1.6004971265792847
iteration 200, loss 1.607112169265747
iteration 300, loss 1.5937608480453491
iteration 400, loss 1.6256721019744873
iteration 500, loss 1.6268023252487183
iteration 600, loss 1.6321141719818115
iteration 700, loss 1.637370228767395
iteration 800, loss 1.6693826913833618
iteration 0, loss 1.5864211320877075
iteration 100, loss 1.6387708187103271
iteration 200, loss 1.6234018802642822
iteration 300, loss 1.5957847833633423
iteration 400, loss 1.6359487771987915
iteration 500, loss 1.6314194202423096
iteration 600, loss 1.67072331905365
iteration 700, loss 1.6260826587677002
iteration 800, loss 1.6202161312103271
iteration 0, loss 1.591413140296936
iteration 100, loss 1.6225472688674927
iteration 200, loss 1.6053930521011353
iteration 300, loss 1.671404242515564
iteration 400, loss 1.617095947265625
iteration 500, loss 1.6332498788833618
iteration 600, loss 1.6678236722946167
iteration 700, loss 1.6055012941360474
iteration 800, loss 1.6029430627822876
iteration 0, loss 1.642171025276184
iteration 100, loss 1.6334606409072876
iteration 200, loss 1.6332179307937622
iteration 300, loss 1.640215516090393
iteration 400, loss 1.6339243650436401
iteration 500, loss 1.644492268562317
iteration 600, loss 1.5709927082061768
iteration 700, loss 1.6061694622039795
iteration 800, loss 1.5865620374679565
iteration 0, loss 1.6323127746582031
iteration 100, loss 1.6957436800003052
iteration 200, loss 1.6851205825805664
iteration 300, loss 1.6431399583816528
iteration 400, loss 1.5833935737609863
iteration 500, loss 1.6558233499526978
iteration 600, loss 1.592203974723816
iteration 700, loss 1.6070647239685059
iteration 800, loss 1.6481667757034302
fold 3 accuracy: 0.8191428571428572
iteration 0, loss 1.5705515146255493
iteration 100, loss 1.6674489974975586
iteration 200, loss 1.7243674993515015
iteration 300, loss 1.624863862991333
iteration 400, loss 1.6024789810180664
iteration 500, loss 1.5713611841201782
iteration 600, loss 1.660942792892456
iteration 700, loss 1.640493631362915
iteration 800, loss 1.6548116207122803
iteration 0, loss 1.6160274744033813
iteration 100, loss 1.6131561994552612
iteration 200, loss 1.616915225982666
iteration 300, loss 1.6279791593551636
iteration 400, loss 1.5931649208068848
iteration 500, loss 1.6027740240097046
iteration 600, loss 1.6145962476730347
iteration 700, loss 1.630864143371582
iteration 800, loss 1.6639842987060547
iteration 0, loss 1.657160758972168
iteration 100, loss 1.6123603582382202
iteration 200, loss 1.6015846729278564
iteration 300, loss 1.641913890838623
iteration 400, loss 1.6441246271133423
iteration 500, loss 1.6589785814285278
iteration 600, loss 1.6414562463760376
iteration 700, loss 1.6768182516098022
iteration 800, loss 1.587374210357666
iteration 0, loss 1.6607613563537598
iteration 100, loss 1.6197543144226074
iteration 200, loss 1.665855884552002
iteration 300, loss 1.6792101860046387
iteration 400, loss 1.5999778509140015
iteration 500, loss 1.596914529800415
iteration 600, loss 1.6043968200683594
iteration 700, loss 1.6727110147476196
iteration 800, loss 1.6807502508163452
iteration 0, loss 1.6459261178970337
iteration 100, loss 1.6455224752426147
iteration 200, loss 1.6457552909851074
iteration 300, loss 1.5646851062774658
iteration 400, loss 1.6751078367233276
iteration 500, loss 1.5666130781173706
iteration 600, loss 1.617701768875122
iteration 700, loss 1.58604097366333
iteration 800, loss 1.5733146667480469
iteration 0, loss 1.607230305671692
iteration 100, loss 1.564630389213562
iteration 200, loss 1.6094375848770142
iteration 300, loss 1.6457539796829224
iteration 400, loss 1.6004785299301147
iteration 500, loss 1.6569231748580933
iteration 600, loss 1.608826994895935
iteration 700, loss 1.608100175857544
iteration 800, loss 1.60871160030365
iteration 0, loss 1.6501342058181763
iteration 100, loss 1.5664218664169312
iteration 200, loss 1.612783670425415
iteration 300, loss 1.5679841041564941
iteration 400, loss 1.6301276683807373
iteration 500, loss 1.6199597120285034
iteration 600, loss 1.6145694255828857
iteration 700, loss 1.636500597000122
iteration 800, loss 1.6444270610809326
iteration 0, loss 1.6307542324066162
iteration 100, loss 1.598512053489685
iteration 200, loss 1.6174176931381226
iteration 300, loss 1.6378037929534912
iteration 400, loss 1.6412676572799683
iteration 500, loss 1.6357145309448242
iteration 600, loss 1.5794744491577148
iteration 700, loss 1.5586113929748535
iteration 800, loss 1.550843358039856
iteration 0, loss 1.5607110261917114
iteration 100, loss 1.5797773599624634
iteration 200, loss 1.6241101026535034
iteration 300, loss 1.6185979843139648
iteration 400, loss 1.6783177852630615
iteration 500, loss 1.6439133882522583
iteration 600, loss 1.610679030418396
iteration 700, loss 1.683841586112976
iteration 800, loss 1.6080732345581055
iteration 0, loss 1.5625938177108765
iteration 100, loss 1.640775442123413
iteration 200, loss 1.5839444398880005
iteration 300, loss 1.6269550323486328
iteration 400, loss 1.608587622642517
iteration 500, loss 1.603310465812683
iteration 600, loss 1.6450703144073486
iteration 700, loss 1.6097369194030762
iteration 800, loss 1.6248252391815186
iteration 0, loss 1.6153161525726318
iteration 100, loss 1.6115038394927979
iteration 200, loss 1.6253912448883057
iteration 300, loss 1.6391270160675049
iteration 400, loss 1.6411921977996826
iteration 500, loss 1.6289178133010864
iteration 600, loss 1.6521602869033813
iteration 700, loss 1.6986626386642456
iteration 800, loss 1.6348761320114136
iteration 0, loss 1.6107313632965088
iteration 100, loss 1.6327022314071655
iteration 200, loss 1.6421186923980713
iteration 300, loss 1.6255184412002563
iteration 400, loss 1.6118886470794678
iteration 500, loss 1.6154024600982666
iteration 600, loss 1.593451738357544
iteration 700, loss 1.6687661409378052
iteration 800, loss 1.5625079870224
iteration 0, loss 1.6219401359558105
iteration 100, loss 1.6127266883850098
iteration 200, loss 1.6614892482757568
iteration 300, loss 1.5881365537643433
iteration 400, loss 1.6507185697555542
iteration 500, loss 1.6287212371826172
iteration 600, loss 1.5724871158599854
iteration 700, loss 1.702760934829712
iteration 800, loss 1.6284337043762207
iteration 0, loss 1.599129557609558
iteration 100, loss 1.5983142852783203
iteration 200, loss 1.5967665910720825
iteration 300, loss 1.581589937210083
iteration 400, loss 1.6788175106048584
iteration 500, loss 1.6100300550460815
iteration 600, loss 1.65122389793396
iteration 700, loss 1.6342170238494873
iteration 800, loss 1.6298847198486328
iteration 0, loss 1.677303433418274
iteration 100, loss 1.5971523523330688
iteration 200, loss 1.6155030727386475
iteration 300, loss 1.6218221187591553
iteration 400, loss 1.6840074062347412
iteration 500, loss 1.6240763664245605
iteration 600, loss 1.6575902700424194
iteration 700, loss 1.6066464185714722
iteration 800, loss 1.6660844087600708
iteration 0, loss 1.674566388130188
iteration 100, loss 1.6026923656463623
iteration 200, loss 1.6388987302780151
iteration 300, loss 1.6484414339065552
iteration 400, loss 1.6020684242248535
iteration 500, loss 1.629679560661316
iteration 600, loss 1.5686737298965454
iteration 700, loss 1.54669189453125
iteration 800, loss 1.5922622680664062
iteration 0, loss 1.6505082845687866
iteration 100, loss 1.6189292669296265
iteration 200, loss 1.6505051851272583
iteration 300, loss 1.577054738998413
iteration 400, loss 1.6296296119689941
iteration 500, loss 1.605965256690979
iteration 600, loss 1.6584140062332153
iteration 700, loss 1.6348631381988525
iteration 800, loss 1.6343421936035156
iteration 0, loss 1.6420624256134033
iteration 100, loss 1.6186301708221436
iteration 200, loss 1.6143783330917358
iteration 300, loss 1.6433440446853638
iteration 400, loss 1.6696784496307373
iteration 500, loss 1.6664316654205322
iteration 600, loss 1.6527962684631348
iteration 700, loss 1.6201646327972412
iteration 800, loss 1.6036192178726196
iteration 0, loss 1.636157751083374
iteration 100, loss 1.6192971467971802
iteration 200, loss 1.5485578775405884
iteration 300, loss 1.605248212814331
iteration 400, loss 1.6538490056991577
iteration 500, loss 1.5899593830108643
iteration 600, loss 1.6790533065795898
iteration 700, loss 1.6020127534866333
iteration 800, loss 1.6080715656280518
iteration 0, loss 1.588456392288208
iteration 100, loss 1.6013129949569702
iteration 200, loss 1.5845061540603638
iteration 300, loss 1.6366270780563354
iteration 400, loss 1.5686371326446533
iteration 500, loss 1.6025121212005615
iteration 600, loss 1.6018180847167969
iteration 700, loss 1.63123619556427
iteration 800, loss 1.698257565498352
iteration 0, loss 1.5956668853759766
iteration 100, loss 1.5460261106491089
iteration 200, loss 1.651220440864563
iteration 300, loss 1.5932550430297852
iteration 400, loss 1.601132869720459
iteration 500, loss 1.6087572574615479
iteration 600, loss 1.6078931093215942
iteration 700, loss 1.612142562866211
iteration 800, loss 1.6468493938446045
iteration 0, loss 1.5962668657302856
iteration 100, loss 1.6280508041381836
iteration 200, loss 1.6498398780822754
iteration 300, loss 1.615885615348816
iteration 400, loss 1.5481822490692139
iteration 500, loss 1.6561510562896729
iteration 600, loss 1.7012524604797363
iteration 700, loss 1.5836479663848877
iteration 800, loss 1.5879615545272827
iteration 0, loss 1.573961615562439
iteration 100, loss 1.582318902015686
iteration 200, loss 1.61030113697052
iteration 300, loss 1.5995625257492065
iteration 400, loss 1.589096188545227
iteration 500, loss 1.6228227615356445
iteration 600, loss 1.5798392295837402
iteration 700, loss 1.6074817180633545
iteration 800, loss 1.756914496421814
iteration 0, loss 1.6323790550231934
iteration 100, loss 1.6637303829193115
iteration 200, loss 1.60023033618927
iteration 300, loss 1.6895856857299805
iteration 400, loss 1.6517542600631714
iteration 500, loss 1.6631791591644287
iteration 600, loss 1.6353003978729248
iteration 700, loss 1.6547855138778687
iteration 800, loss 1.6083872318267822
iteration 0, loss 1.6473549604415894
iteration 100, loss 1.6104055643081665
iteration 200, loss 1.6210500001907349
iteration 300, loss 1.6519451141357422
iteration 400, loss 1.565271019935608
iteration 500, loss 1.6144310235977173
iteration 600, loss 1.7061028480529785
iteration 700, loss 1.6228691339492798
iteration 800, loss 1.6675243377685547
iteration 0, loss 1.5895731449127197
iteration 100, loss 1.5993324518203735
iteration 200, loss 1.608558177947998
iteration 300, loss 1.63994562625885
iteration 400, loss 1.6438374519348145
iteration 500, loss 1.5631985664367676
iteration 600, loss 1.5957447290420532
iteration 700, loss 1.6383824348449707
iteration 800, loss 1.707517147064209
iteration 0, loss 1.6123137474060059
iteration 100, loss 1.603956699371338
iteration 200, loss 1.6403732299804688
iteration 300, loss 1.660950779914856
iteration 400, loss 1.616585612297058
iteration 500, loss 1.6159923076629639
iteration 600, loss 1.6466065645217896
iteration 700, loss 1.619372844696045
iteration 800, loss 1.693265676498413
iteration 0, loss 1.6192742586135864
iteration 100, loss 1.6356563568115234
iteration 200, loss 1.6183602809906006
iteration 300, loss 1.631555199623108
iteration 400, loss 1.6254122257232666
iteration 500, loss 1.597670078277588
iteration 600, loss 1.6356837749481201
iteration 700, loss 1.6053240299224854
iteration 800, loss 1.6248884201049805
iteration 0, loss 1.621906042098999
iteration 100, loss 1.6179817914962769
iteration 200, loss 1.6106340885162354
iteration 300, loss 1.59041428565979
iteration 400, loss 1.6261146068572998
iteration 500, loss 1.6568007469177246
iteration 600, loss 1.6162906885147095
iteration 700, loss 1.6392481327056885
iteration 800, loss 1.602704405784607
iteration 0, loss 1.6300877332687378
iteration 100, loss 1.5970587730407715
iteration 200, loss 1.6476497650146484
iteration 300, loss 1.6125688552856445
iteration 400, loss 1.567115068435669
iteration 500, loss 1.5764957666397095
iteration 600, loss 1.720726728439331
iteration 700, loss 1.641423225402832
iteration 800, loss 1.695030927658081
iteration 0, loss 1.6184159517288208
iteration 100, loss 1.59031343460083
iteration 200, loss 1.5792280435562134
iteration 300, loss 1.670052409172058
iteration 400, loss 1.5763992071151733
iteration 500, loss 1.6188679933547974
iteration 600, loss 1.665207028388977
iteration 700, loss 1.657481074333191
iteration 800, loss 1.6263303756713867
iteration 0, loss 1.6434975862503052
iteration 100, loss 1.6543726921081543
iteration 200, loss 1.6331989765167236
iteration 300, loss 1.6629890203475952
iteration 400, loss 1.6674381494522095
iteration 500, loss 1.6684160232543945
iteration 600, loss 1.658112645149231
iteration 700, loss 1.6812630891799927
iteration 800, loss 1.6811996698379517
iteration 0, loss 1.6538617610931396
iteration 100, loss 1.6276752948760986
iteration 200, loss 1.659924864768982
iteration 300, loss 1.5905680656433105
iteration 400, loss 1.590217113494873
iteration 500, loss 1.6155314445495605
iteration 600, loss 1.6017539501190186
iteration 700, loss 1.5779842138290405
iteration 800, loss 1.632970929145813
iteration 0, loss 1.6073466539382935
iteration 100, loss 1.5903304815292358
iteration 200, loss 1.544875144958496
iteration 300, loss 1.56525456905365
iteration 400, loss 1.6031521558761597
iteration 500, loss 1.5819402933120728
iteration 600, loss 1.6037036180496216
iteration 700, loss 1.675634503364563
iteration 800, loss 1.6553541421890259
iteration 0, loss 1.5658711194992065
iteration 100, loss 1.5966646671295166
iteration 200, loss 1.6107748746871948
iteration 300, loss 1.6356451511383057
iteration 400, loss 1.6058964729309082
iteration 500, loss 1.6725475788116455
iteration 600, loss 1.6136765480041504
iteration 700, loss 1.6330764293670654
iteration 800, loss 1.5561944246292114
iteration 0, loss 1.6734658479690552
iteration 100, loss 1.6450530290603638
iteration 200, loss 1.6666133403778076
iteration 300, loss 1.6407121419906616
iteration 400, loss 1.6682542562484741
iteration 500, loss 1.5960105657577515
iteration 600, loss 1.6243999004364014
iteration 700, loss 1.6410681009292603
iteration 800, loss 1.629014492034912
iteration 0, loss 1.5715899467468262
iteration 100, loss 1.5601361989974976
iteration 200, loss 1.6197092533111572
iteration 300, loss 1.609023928642273
iteration 400, loss 1.611615538597107
iteration 500, loss 1.6118305921554565
iteration 600, loss 1.598270058631897
iteration 700, loss 1.6297417879104614
iteration 800, loss 1.6019915342330933
iteration 0, loss 1.6127055883407593
iteration 100, loss 1.5711379051208496
iteration 200, loss 1.6326547861099243
iteration 300, loss 1.5817272663116455
iteration 400, loss 1.580147624015808
iteration 500, loss 1.6604053974151611
iteration 600, loss 1.5846116542816162
iteration 700, loss 1.676615595817566
iteration 800, loss 1.648613452911377
iteration 0, loss 1.6029847860336304
iteration 100, loss 1.6341149806976318
iteration 200, loss 1.6630821228027344
iteration 300, loss 1.7151830196380615
iteration 400, loss 1.6486542224884033
iteration 500, loss 1.7070081233978271
iteration 600, loss 1.5694056749343872
iteration 700, loss 1.6576085090637207
iteration 800, loss 1.5913137197494507
iteration 0, loss 1.5612181425094604
iteration 100, loss 1.6082358360290527
iteration 200, loss 1.5898332595825195
iteration 300, loss 1.7014011144638062
iteration 400, loss 1.6164485216140747
iteration 500, loss 1.6530780792236328
iteration 600, loss 1.5768039226531982
iteration 700, loss 1.7032451629638672
iteration 800, loss 1.6116186380386353
iteration 0, loss 1.6128311157226562
iteration 100, loss 1.6235301494598389
iteration 200, loss 1.634526252746582
iteration 300, loss 1.5791374444961548
iteration 400, loss 1.626215934753418
iteration 500, loss 1.652957558631897
iteration 600, loss 1.6055126190185547
iteration 700, loss 1.5941909551620483
iteration 800, loss 1.6076856851577759
iteration 0, loss 1.656874418258667
iteration 100, loss 1.6792596578598022
iteration 200, loss 1.6226056814193726
iteration 300, loss 1.5984593629837036
iteration 400, loss 1.6442843675613403
iteration 500, loss 1.6176178455352783
iteration 600, loss 1.6159580945968628
iteration 700, loss 1.609395146369934
iteration 800, loss 1.5933973789215088
iteration 0, loss 1.5930434465408325
iteration 100, loss 1.6307837963104248
iteration 200, loss 1.580694317817688
iteration 300, loss 1.682185173034668
iteration 400, loss 1.666887879371643
iteration 500, loss 1.6255301237106323
iteration 600, loss 1.6729950904846191
iteration 700, loss 1.6120493412017822
iteration 800, loss 1.6404342651367188
iteration 0, loss 1.6127992868423462
iteration 100, loss 1.6138972043991089
iteration 200, loss 1.5948152542114258
iteration 300, loss 1.6094378232955933
iteration 400, loss 1.5666970014572144
iteration 500, loss 1.6483409404754639
iteration 600, loss 1.6133164167404175
iteration 700, loss 1.6133759021759033
iteration 800, loss 1.6112488508224487
iteration 0, loss 1.6215708255767822
iteration 100, loss 1.6109732389450073
iteration 200, loss 1.6309994459152222
iteration 300, loss 1.697619915008545
iteration 400, loss 1.6445462703704834
iteration 500, loss 1.6137696504592896
iteration 600, loss 1.6459572315216064
iteration 700, loss 1.6180191040039062
iteration 800, loss 1.6740283966064453
iteration 0, loss 1.582908034324646
iteration 100, loss 1.5770741701126099
iteration 200, loss 1.616849660873413
iteration 300, loss 1.6308656930923462
iteration 400, loss 1.599984884262085
iteration 500, loss 1.7007088661193848
iteration 600, loss 1.6538277864456177
iteration 700, loss 1.60793936252594
iteration 800, loss 1.6759743690490723
iteration 0, loss 1.5817029476165771
iteration 100, loss 1.6936277151107788
iteration 200, loss 1.62617826461792
iteration 300, loss 1.6569509506225586
iteration 400, loss 1.5935972929000854
iteration 500, loss 1.5948741436004639
iteration 600, loss 1.6471667289733887
iteration 700, loss 1.6131575107574463
iteration 800, loss 1.6887919902801514
iteration 0, loss 1.6452139616012573
iteration 100, loss 1.654453158378601
iteration 200, loss 1.593143343925476
iteration 300, loss 1.6239148378372192
iteration 400, loss 1.605360984802246
iteration 500, loss 1.594907522201538
iteration 600, loss 1.6174025535583496
iteration 700, loss 1.666731834411621
iteration 800, loss 1.632218599319458
iteration 0, loss 1.6734424829483032
iteration 100, loss 1.6071734428405762
iteration 200, loss 1.579014539718628
iteration 300, loss 1.6707243919372559
iteration 400, loss 1.5922373533248901
iteration 500, loss 1.6427037715911865
iteration 600, loss 1.6086723804473877
iteration 700, loss 1.5655696392059326
iteration 800, loss 1.608580470085144
iteration 0, loss 1.6418074369430542
iteration 100, loss 1.6375033855438232
iteration 200, loss 1.6154721975326538
iteration 300, loss 1.6147387027740479
iteration 400, loss 1.6271531581878662
iteration 500, loss 1.6072211265563965
iteration 600, loss 1.5882524251937866
iteration 700, loss 1.6262398958206177
iteration 800, loss 1.5764472484588623
fold 4 accuracy: 0.8140714285714286
[2024-02-29 04:15:56,719] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...
[2024-02-29 04:15:56,720] [INFO] [profiler.py:80:start_profile] Flops profiler started

-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 1:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

params per GPU:                                                         654     
params of model = params per GPU * mp_size:                             0       
fwd MACs per GPU:                                                       840 MACs
fwd flops per GPU:                                                      1.68 K  
fwd flops of model = fwd flops per GPU * mp_size:                       1.68 K  
fwd latency:                                                            349.52 us
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    4.81 MFLOPS

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'KronLayer': '654'}
    MACs        - {'KronLayer': '840 MACs'}
    fwd latency - {'KronLayer': '349.52 us'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

KronLayer(
  654 = 100% Params, 840 MACs = 100% MACs, 349.52 us = 100% latency, 4.81 MFLOPS
  (kronlinear): KronLinear(654 = 100% Params, 840 MACs = 100% MACs, 266.08 us = 76.13% latency, 6.31 MFLOPS)
  (activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 7.03% latency, 0 FLOPS)
)
------------------------------------------------------------------------------
[2024-02-29 04:15:56,721] [INFO] [profiler.py:226:end_profile] Flops profiler finished
1.68 K 654
iteration 0, loss 2.3018717765808105
iteration 100, loss 2.2086217403411865
iteration 200, loss 2.0433883666992188
iteration 300, loss 1.8968194723129272
iteration 400, loss 1.8636599779129028
iteration 500, loss 1.843099594116211
iteration 600, loss 1.7851965427398682
iteration 700, loss 1.772849678993225
iteration 800, loss 1.7892875671386719
iteration 0, loss 1.7557686567306519
iteration 100, loss 1.695753812789917
iteration 200, loss 1.7243883609771729
iteration 300, loss 1.793851613998413
iteration 400, loss 1.732222080230713
iteration 500, loss 1.741665005683899
iteration 600, loss 1.6511976718902588
iteration 700, loss 1.7382363080978394
iteration 800, loss 1.7079933881759644
iteration 0, loss 1.6885807514190674
iteration 100, loss 1.7086719274520874
iteration 200, loss 1.688563346862793
iteration 300, loss 1.7481663227081299
iteration 400, loss 1.6632052659988403
iteration 500, loss 1.6488093137741089
iteration 600, loss 1.7039337158203125
iteration 700, loss 1.6565847396850586
iteration 800, loss 1.6768498420715332
iteration 0, loss 1.712268352508545
iteration 100, loss 1.7499326467514038
iteration 200, loss 1.675389051437378
iteration 300, loss 1.6854097843170166
iteration 400, loss 1.668763279914856
iteration 500, loss 1.6572802066802979
iteration 600, loss 1.679075837135315
iteration 700, loss 1.7714186906814575
iteration 800, loss 1.6619664430618286
iteration 0, loss 1.7054404020309448
iteration 100, loss 1.6607980728149414
iteration 200, loss 1.669682264328003
iteration 300, loss 1.7107505798339844
iteration 400, loss 1.641575574874878
iteration 500, loss 1.6893583536148071
iteration 600, loss 1.700400948524475
iteration 700, loss 1.7030010223388672
iteration 800, loss 1.638364315032959
iteration 0, loss 1.6751517057418823
iteration 100, loss 1.6713553667068481
iteration 200, loss 1.6464405059814453
iteration 300, loss 1.676692008972168
iteration 400, loss 1.6519590616226196
iteration 500, loss 1.6744892597198486
iteration 600, loss 1.6180944442749023
iteration 700, loss 1.6566835641860962
iteration 800, loss 1.6562031507492065
iteration 0, loss 1.684790015220642
iteration 100, loss 1.6635797023773193
iteration 200, loss 1.6994200944900513
iteration 300, loss 1.677330732345581
iteration 400, loss 1.6545010805130005
iteration 500, loss 1.7003929615020752
iteration 600, loss 1.6577812433242798
iteration 700, loss 1.6198848485946655
iteration 800, loss 1.7090226411819458
iteration 0, loss 1.6769170761108398
iteration 100, loss 1.6428701877593994
iteration 200, loss 1.6635241508483887
iteration 300, loss 1.6593377590179443
iteration 400, loss 1.7825490236282349
iteration 500, loss 1.6414963006973267
iteration 600, loss 1.694253921508789
iteration 700, loss 1.6263755559921265
iteration 800, loss 1.5949995517730713
iteration 0, loss 1.6045972108840942
iteration 100, loss 1.6567509174346924
iteration 200, loss 1.5922396183013916
iteration 300, loss 1.6357181072235107
iteration 400, loss 1.6255371570587158
iteration 500, loss 1.645882248878479
iteration 600, loss 1.6048041582107544
iteration 700, loss 1.6669811010360718
iteration 800, loss 1.68534517288208
iteration 0, loss 1.6423500776290894
iteration 100, loss 1.688741683959961
iteration 200, loss 1.6490254402160645
iteration 300, loss 1.6404668092727661
iteration 400, loss 1.624963402748108
iteration 500, loss 1.6478415727615356
iteration 600, loss 1.706079125404358
iteration 700, loss 1.641311764717102
iteration 800, loss 1.651731252670288
iteration 0, loss 1.661240577697754
iteration 100, loss 1.6424264907836914
iteration 200, loss 1.6903550624847412
iteration 300, loss 1.610036015510559
iteration 400, loss 1.5981212854385376
iteration 500, loss 1.6555217504501343
iteration 600, loss 1.6610199213027954
iteration 700, loss 1.6443063020706177
iteration 800, loss 1.659206509590149
iteration 0, loss 1.61665678024292
iteration 100, loss 1.6837968826293945
iteration 200, loss 1.6409605741500854
iteration 300, loss 1.6601366996765137
iteration 400, loss 1.6661527156829834
iteration 500, loss 1.6505706310272217
iteration 600, loss 1.6867026090621948
iteration 700, loss 1.664060115814209
iteration 800, loss 1.6530975103378296
iteration 0, loss 1.6986067295074463
iteration 100, loss 1.6278324127197266
iteration 200, loss 1.6668647527694702
iteration 300, loss 1.6719954013824463
iteration 400, loss 1.6885285377502441
iteration 500, loss 1.6780552864074707
iteration 600, loss 1.666546106338501
iteration 700, loss 1.6282752752304077
iteration 800, loss 1.6568834781646729
iteration 0, loss 1.6875170469284058
iteration 100, loss 1.688481092453003
iteration 200, loss 1.632774829864502
iteration 300, loss 1.6910991668701172
iteration 400, loss 1.6644673347473145
iteration 500, loss 1.650473713874817
iteration 600, loss 1.6516070365905762
iteration 700, loss 1.6980979442596436
iteration 800, loss 1.6445790529251099
iteration 0, loss 1.7370041608810425
iteration 100, loss 1.589117169380188
iteration 200, loss 1.6306250095367432
iteration 300, loss 1.6229921579360962
iteration 400, loss 1.691077470779419
iteration 500, loss 1.711937427520752
iteration 600, loss 1.6978785991668701
iteration 700, loss 1.746840000152588
iteration 800, loss 1.6446410417556763
iteration 0, loss 1.7246259450912476
iteration 100, loss 1.6303329467773438
iteration 200, loss 1.6562747955322266
iteration 300, loss 1.6385133266448975
iteration 400, loss 1.6953202486038208
iteration 500, loss 1.667786717414856
iteration 600, loss 1.6077286005020142
iteration 700, loss 1.6623479127883911
iteration 800, loss 1.6298139095306396
iteration 0, loss 1.624268651008606
iteration 100, loss 1.6663992404937744
iteration 200, loss 1.6306854486465454
iteration 300, loss 1.6594021320343018
iteration 400, loss 1.6151087284088135
iteration 500, loss 1.6518369913101196
iteration 600, loss 1.6488230228424072
iteration 700, loss 1.6289775371551514
iteration 800, loss 1.7111191749572754
iteration 0, loss 1.6073213815689087
iteration 100, loss 1.672873854637146
iteration 200, loss 1.703477144241333
iteration 300, loss 1.673451542854309
iteration 400, loss 1.6020429134368896
iteration 500, loss 1.7030748128890991
iteration 600, loss 1.6344594955444336
iteration 700, loss 1.6392827033996582
iteration 800, loss 1.6826624870300293
iteration 0, loss 1.6797327995300293
iteration 100, loss 1.6288853883743286
iteration 200, loss 1.616040825843811
iteration 300, loss 1.639667272567749
iteration 400, loss 1.645860195159912
iteration 500, loss 1.6534056663513184
iteration 600, loss 1.608724594116211
iteration 700, loss 1.6461496353149414
iteration 800, loss 1.737628698348999
iteration 0, loss 1.6933834552764893
iteration 100, loss 1.6379320621490479
iteration 200, loss 1.6512411832809448
iteration 300, loss 1.6741280555725098
iteration 400, loss 1.6731555461883545
iteration 500, loss 1.6324201822280884
iteration 600, loss 1.6501200199127197
iteration 700, loss 1.6565951108932495
iteration 800, loss 1.7111588716506958
iteration 0, loss 1.601058006286621
iteration 100, loss 1.5757759809494019
iteration 200, loss 1.6360341310501099
iteration 300, loss 1.726752758026123
iteration 400, loss 1.6194125413894653
iteration 500, loss 1.7005466222763062
iteration 600, loss 1.6022279262542725
iteration 700, loss 1.7086155414581299
iteration 800, loss 1.5733599662780762
iteration 0, loss 1.5937923192977905
iteration 100, loss 1.5915911197662354
iteration 200, loss 1.589389681816101
iteration 300, loss 1.6631200313568115
iteration 400, loss 1.608555555343628
iteration 500, loss 1.6715227365493774
iteration 600, loss 1.6170337200164795
iteration 700, loss 1.6442245244979858
iteration 800, loss 1.624551773071289
iteration 0, loss 1.6551148891448975
iteration 100, loss 1.6610305309295654
iteration 200, loss 1.6284009218215942
iteration 300, loss 1.728248119354248
iteration 400, loss 1.6884781122207642
iteration 500, loss 1.6146494150161743
iteration 600, loss 1.6391938924789429
iteration 700, loss 1.6353883743286133
iteration 800, loss 1.6223526000976562
iteration 0, loss 1.620876431465149
iteration 100, loss 1.604284405708313
iteration 200, loss 1.615157961845398
iteration 300, loss 1.620574712753296
iteration 400, loss 1.6384036540985107
iteration 500, loss 1.653555989265442
iteration 600, loss 1.693627953529358
iteration 700, loss 1.6804152727127075
iteration 800, loss 1.5996067523956299
iteration 0, loss 1.665934681892395
iteration 100, loss 1.6440974473953247
iteration 200, loss 1.6647900342941284
iteration 300, loss 1.6099873781204224
iteration 400, loss 1.6685020923614502
iteration 500, loss 1.6976009607315063
iteration 600, loss 1.6414648294448853
iteration 700, loss 1.6517224311828613
iteration 800, loss 1.646995186805725
iteration 0, loss 1.6600916385650635
iteration 100, loss 1.686318039894104
iteration 200, loss 1.6417044401168823
iteration 300, loss 1.6219851970672607
iteration 400, loss 1.6031930446624756
iteration 500, loss 1.6448549032211304
iteration 600, loss 1.6650218963623047
iteration 700, loss 1.6826130151748657
iteration 800, loss 1.598534107208252
iteration 0, loss 1.6657010316848755
iteration 100, loss 1.6249346733093262
iteration 200, loss 1.6379897594451904
iteration 300, loss 1.6218701601028442
iteration 400, loss 1.6312148571014404
iteration 500, loss 1.572446584701538
iteration 600, loss 1.6164201498031616
iteration 700, loss 1.6436477899551392
iteration 800, loss 1.6302998065948486
iteration 0, loss 1.6422678232192993
iteration 100, loss 1.6366949081420898
iteration 200, loss 1.6706403493881226
iteration 300, loss 1.6176426410675049
iteration 400, loss 1.6328158378601074
iteration 500, loss 1.6066639423370361
iteration 600, loss 1.6408611536026
iteration 700, loss 1.6798820495605469
iteration 800, loss 1.6369332075119019
iteration 0, loss 1.7014703750610352
iteration 100, loss 1.6314955949783325
iteration 200, loss 1.643502950668335
iteration 300, loss 1.6017515659332275
iteration 400, loss 1.6174232959747314
iteration 500, loss 1.6541110277175903
iteration 600, loss 1.6529117822647095
iteration 700, loss 1.6611982583999634
iteration 800, loss 1.6086680889129639
iteration 0, loss 1.63169527053833
iteration 100, loss 1.6472463607788086
iteration 200, loss 1.6624736785888672
iteration 300, loss 1.6615276336669922
iteration 400, loss 1.6120798587799072
iteration 500, loss 1.6718276739120483
iteration 600, loss 1.707511067390442
iteration 700, loss 1.6156307458877563
iteration 800, loss 1.6828569173812866
iteration 0, loss 1.6016209125518799
iteration 100, loss 1.6739333868026733
iteration 200, loss 1.6175388097763062
iteration 300, loss 1.5992883443832397
iteration 400, loss 1.6829420328140259
iteration 500, loss 1.6237213611602783
iteration 600, loss 1.6808112859725952
iteration 700, loss 1.6921685934066772
iteration 800, loss 1.62742018699646
iteration 0, loss 1.6448460817337036
iteration 100, loss 1.7239433526992798
iteration 200, loss 1.6388463973999023
iteration 300, loss 1.6387910842895508
iteration 400, loss 1.6512889862060547
iteration 500, loss 1.639024019241333
iteration 600, loss 1.653148889541626
iteration 700, loss 1.6448675394058228
iteration 800, loss 1.609529972076416
iteration 0, loss 1.6111729145050049
iteration 100, loss 1.6067869663238525
iteration 200, loss 1.6060495376586914
iteration 300, loss 1.6129021644592285
iteration 400, loss 1.6131302118301392
iteration 500, loss 1.6306507587432861
iteration 600, loss 1.58098566532135
iteration 700, loss 1.6605569124221802
iteration 800, loss 1.6300040483474731
iteration 0, loss 1.6097116470336914
iteration 100, loss 1.6387702226638794
iteration 200, loss 1.623147964477539
iteration 300, loss 1.704795002937317
iteration 400, loss 1.6469544172286987
iteration 500, loss 1.6248058080673218
iteration 600, loss 1.5949710607528687
iteration 700, loss 1.6875520944595337
iteration 800, loss 1.635358214378357
iteration 0, loss 1.6854199171066284
iteration 100, loss 1.717803716659546
iteration 200, loss 1.637616753578186
iteration 300, loss 1.585618495941162
iteration 400, loss 1.6307014226913452
iteration 500, loss 1.6901257038116455
iteration 600, loss 1.6956143379211426
iteration 700, loss 1.7056739330291748
iteration 800, loss 1.6185868978500366
iteration 0, loss 1.6140470504760742
iteration 100, loss 1.617777943611145
iteration 200, loss 1.5890620946884155
iteration 300, loss 1.635023832321167
iteration 400, loss 1.6419720649719238
iteration 500, loss 1.6317100524902344
iteration 600, loss 1.6103578805923462
iteration 700, loss 1.6061680316925049
iteration 800, loss 1.621960997581482
iteration 0, loss 1.5850504636764526
iteration 100, loss 1.6160192489624023
iteration 200, loss 1.593299150466919
iteration 300, loss 1.673007845878601
iteration 400, loss 1.668260931968689
iteration 500, loss 1.6711937189102173
iteration 600, loss 1.6221482753753662
iteration 700, loss 1.5906447172164917
iteration 800, loss 1.6165735721588135
iteration 0, loss 1.6294864416122437
iteration 100, loss 1.6859368085861206
iteration 200, loss 1.6663916110992432
iteration 300, loss 1.6616899967193604
iteration 400, loss 1.6625162363052368
iteration 500, loss 1.5940626859664917
iteration 600, loss 1.5895503759384155
iteration 700, loss 1.6319941282272339
iteration 800, loss 1.6180537939071655
iteration 0, loss 1.6924214363098145
iteration 100, loss 1.6473301649093628
iteration 200, loss 1.6221861839294434
iteration 300, loss 1.6649725437164307
iteration 400, loss 1.7216497659683228
iteration 500, loss 1.610470175743103
iteration 600, loss 1.6065919399261475
iteration 700, loss 1.647742748260498
iteration 800, loss 1.6265928745269775
iteration 0, loss 1.6667410135269165
iteration 100, loss 1.7034883499145508
iteration 200, loss 1.6690863370895386
iteration 300, loss 1.6235271692276
iteration 400, loss 1.651524543762207
iteration 500, loss 1.6390167474746704
iteration 600, loss 1.5937443971633911
iteration 700, loss 1.622532606124878
iteration 800, loss 1.6255284547805786
iteration 0, loss 1.5744602680206299
iteration 100, loss 1.6387684345245361
iteration 200, loss 1.691659927368164
iteration 300, loss 1.6387085914611816
iteration 400, loss 1.620527982711792
iteration 500, loss 1.6809982061386108
iteration 600, loss 1.6559301614761353
iteration 700, loss 1.6160930395126343
iteration 800, loss 1.6908929347991943
iteration 0, loss 1.6181119680404663
iteration 100, loss 1.654312252998352
iteration 200, loss 1.6480199098587036
iteration 300, loss 1.6593170166015625
iteration 400, loss 1.6263456344604492
iteration 500, loss 1.580909252166748
iteration 600, loss 1.6353853940963745
iteration 700, loss 1.7038522958755493
iteration 800, loss 1.6338534355163574
iteration 0, loss 1.6334197521209717
iteration 100, loss 1.6168409585952759
iteration 200, loss 1.6596884727478027
iteration 300, loss 1.5723789930343628
iteration 400, loss 1.6243330240249634
iteration 500, loss 1.6553887128829956
iteration 600, loss 1.6043168306350708
iteration 700, loss 1.6252037286758423
iteration 800, loss 1.5615549087524414
iteration 0, loss 1.6006916761398315
iteration 100, loss 1.578957438468933
iteration 200, loss 1.6180346012115479
iteration 300, loss 1.708490252494812
iteration 400, loss 1.685545563697815
iteration 500, loss 1.6376607418060303
iteration 600, loss 1.5924075841903687
iteration 700, loss 1.6367846727371216
iteration 800, loss 1.6762179136276245
iteration 0, loss 1.6053506135940552
iteration 100, loss 1.6579697132110596
iteration 200, loss 1.594666600227356
iteration 300, loss 1.687237024307251
iteration 400, loss 1.6320997476577759
iteration 500, loss 1.6680257320404053
iteration 600, loss 1.612214207649231
iteration 700, loss 1.6478333473205566
iteration 800, loss 1.672022819519043
iteration 0, loss 1.660387635231018
iteration 100, loss 1.6345317363739014
iteration 200, loss 1.6360433101654053
iteration 300, loss 1.6078462600708008
iteration 400, loss 1.681640863418579
iteration 500, loss 1.7013604640960693
iteration 600, loss 1.6298757791519165
iteration 700, loss 1.6086927652359009
iteration 800, loss 1.6363446712493896
iteration 0, loss 1.6577244997024536
iteration 100, loss 1.604156494140625
iteration 200, loss 1.6355100870132446
iteration 300, loss 1.604020118713379
iteration 400, loss 1.6626166105270386
iteration 500, loss 1.655224084854126
iteration 600, loss 1.7592004537582397
iteration 700, loss 1.6072412729263306
iteration 800, loss 1.6129231452941895
iteration 0, loss 1.6260669231414795
iteration 100, loss 1.665065884590149
iteration 200, loss 1.7109458446502686
iteration 300, loss 1.5934697389602661
iteration 400, loss 1.631909728050232
iteration 500, loss 1.660754680633545
iteration 600, loss 1.6393942832946777
iteration 700, loss 1.6478956937789917
iteration 800, loss 1.6093058586120605
iteration 0, loss 1.6238765716552734
iteration 100, loss 1.614912509918213
iteration 200, loss 1.6855335235595703
iteration 300, loss 1.6496858596801758
iteration 400, loss 1.676025152206421
iteration 500, loss 1.6519386768341064
iteration 600, loss 1.649814486503601
iteration 700, loss 1.6201919317245483
iteration 800, loss 1.6300740242004395
iteration 0, loss 1.6084188222885132
iteration 100, loss 1.6662566661834717
iteration 200, loss 1.597704291343689
iteration 300, loss 1.6159327030181885
iteration 400, loss 1.616980791091919
iteration 500, loss 1.6349393129348755
iteration 600, loss 1.6388945579528809
iteration 700, loss 1.6447032690048218
iteration 800, loss 1.597182273864746
fold 0 accuracy: 0.7874285714285715
iteration 0, loss 1.5927950143814087
iteration 100, loss 1.6042687892913818
iteration 200, loss 1.6341155767440796
iteration 300, loss 1.6559714078903198
iteration 400, loss 1.6161330938339233
iteration 500, loss 1.6022368669509888
iteration 600, loss 1.6413559913635254
iteration 700, loss 1.6449812650680542
iteration 800, loss 1.609425663948059
iteration 0, loss 1.647943139076233
iteration 100, loss 1.6694456338882446
iteration 200, loss 1.5789262056350708
iteration 300, loss 1.6365059614181519
iteration 400, loss 1.6640032529830933
iteration 500, loss 1.6493436098098755
iteration 600, loss 1.615882396697998
iteration 700, loss 1.5858051776885986
iteration 800, loss 1.628674864768982
iteration 0, loss 1.604942798614502
iteration 100, loss 1.6186800003051758
iteration 200, loss 1.6793192625045776
iteration 300, loss 1.6670539379119873
iteration 400, loss 1.6211918592453003
iteration 500, loss 1.651745319366455
iteration 600, loss 1.6299864053726196
iteration 700, loss 1.6815063953399658
iteration 800, loss 1.6661524772644043
iteration 0, loss 1.6169745922088623
iteration 100, loss 1.6025608777999878
iteration 200, loss 1.6430872678756714
iteration 300, loss 1.6517075300216675
iteration 400, loss 1.6415326595306396
iteration 500, loss 1.6080827713012695
iteration 600, loss 1.6538708209991455
iteration 700, loss 1.6315187215805054
iteration 800, loss 1.6583898067474365
iteration 0, loss 1.598515272140503
iteration 100, loss 1.636474609375
iteration 200, loss 1.647228479385376
iteration 300, loss 1.5960564613342285
iteration 400, loss 1.561288833618164
iteration 500, loss 1.6647024154663086
iteration 600, loss 1.5864747762680054
iteration 700, loss 1.5933586359024048
iteration 800, loss 1.6341323852539062
iteration 0, loss 1.6582962274551392
iteration 100, loss 1.6624441146850586
iteration 200, loss 1.6417217254638672
iteration 300, loss 1.6664483547210693
iteration 400, loss 1.6625322103500366
iteration 500, loss 1.6467103958129883
iteration 600, loss 1.664672613143921
iteration 700, loss 1.647562026977539
iteration 800, loss 1.621217131614685
iteration 0, loss 1.6144368648529053
iteration 100, loss 1.6315267086029053
iteration 200, loss 1.6493116617202759
iteration 300, loss 1.622098684310913
iteration 400, loss 1.6730244159698486
iteration 500, loss 1.6763442754745483
iteration 600, loss 1.6047972440719604
iteration 700, loss 1.6202245950698853
iteration 800, loss 1.609745740890503
iteration 0, loss 1.5689243078231812
iteration 100, loss 1.5872567892074585
iteration 200, loss 1.6611789464950562
iteration 300, loss 1.6651289463043213
iteration 400, loss 1.5849995613098145
iteration 500, loss 1.589719295501709
iteration 600, loss 1.6004621982574463
iteration 700, loss 1.6072723865509033
iteration 800, loss 1.5901901721954346
iteration 0, loss 1.603278398513794
iteration 100, loss 1.641919493675232
iteration 200, loss 1.6862666606903076
iteration 300, loss 1.6616177558898926
iteration 400, loss 1.598900318145752
iteration 500, loss 1.6631197929382324
iteration 600, loss 1.6075304746627808
iteration 700, loss 1.6462101936340332
iteration 800, loss 1.6468360424041748
iteration 0, loss 1.6010812520980835
iteration 100, loss 1.6157915592193604
iteration 200, loss 1.5985163450241089
iteration 300, loss 1.6694061756134033
iteration 400, loss 1.5989084243774414
iteration 500, loss 1.6531484127044678
iteration 600, loss 1.641195297241211
iteration 700, loss 1.5975327491760254
iteration 800, loss 1.6445039510726929
iteration 0, loss 1.6296461820602417
iteration 100, loss 1.563399314880371
iteration 200, loss 1.6145033836364746
iteration 300, loss 1.6673091650009155
iteration 400, loss 1.6132903099060059
iteration 500, loss 1.661730408668518
iteration 600, loss 1.6440292596817017
iteration 700, loss 1.6483871936798096
iteration 800, loss 1.6251471042633057
iteration 0, loss 1.6216331720352173
iteration 100, loss 1.5928659439086914
iteration 200, loss 1.6691625118255615
iteration 300, loss 1.605810284614563
iteration 400, loss 1.6114628314971924
iteration 500, loss 1.5939098596572876
iteration 600, loss 1.6222578287124634
iteration 700, loss 1.620308756828308
iteration 800, loss 1.718841552734375
iteration 0, loss 1.6385347843170166
iteration 100, loss 1.601273536682129
iteration 200, loss 1.590436339378357
iteration 300, loss 1.681222915649414
iteration 400, loss 1.61677885055542
iteration 500, loss 1.622599482536316
iteration 600, loss 1.649051308631897
iteration 700, loss 1.5690470933914185
iteration 800, loss 1.6455645561218262
iteration 0, loss 1.6851929426193237
iteration 100, loss 1.5634946823120117
iteration 200, loss 1.6117442846298218
iteration 300, loss 1.61970853805542
iteration 400, loss 1.5984129905700684
iteration 500, loss 1.619842290878296
iteration 600, loss 1.5723843574523926
iteration 700, loss 1.6318156719207764
iteration 800, loss 1.6662081480026245
iteration 0, loss 1.6683564186096191
iteration 100, loss 1.6775752305984497
iteration 200, loss 1.6684324741363525
iteration 300, loss 1.6558527946472168
iteration 400, loss 1.653696060180664
iteration 500, loss 1.6290321350097656
iteration 600, loss 1.6160237789154053
iteration 700, loss 1.6251232624053955
iteration 800, loss 1.6339086294174194
iteration 0, loss 1.6817158460617065
iteration 100, loss 1.6995235681533813
iteration 200, loss 1.6895606517791748
iteration 300, loss 1.579993724822998
iteration 400, loss 1.6253489255905151
iteration 500, loss 1.573551058769226
iteration 600, loss 1.612095594406128
iteration 700, loss 1.5948045253753662
iteration 800, loss 1.6220728158950806
iteration 0, loss 1.6344345808029175
iteration 100, loss 1.645644187927246
iteration 200, loss 1.6935583353042603
iteration 300, loss 1.6642740964889526
iteration 400, loss 1.6332200765609741
iteration 500, loss 1.6110656261444092
iteration 600, loss 1.614600658416748
iteration 700, loss 1.6114747524261475
iteration 800, loss 1.6769564151763916
iteration 0, loss 1.647169589996338
iteration 100, loss 1.6643121242523193
iteration 200, loss 1.605202555656433
iteration 300, loss 1.6627964973449707
iteration 400, loss 1.678966760635376
iteration 500, loss 1.6289923191070557
iteration 600, loss 1.5806241035461426
iteration 700, loss 1.6511906385421753
iteration 800, loss 1.6192604303359985
iteration 0, loss 1.5624085664749146
iteration 100, loss 1.6271634101867676
iteration 200, loss 1.6412609815597534
iteration 300, loss 1.6067473888397217
iteration 400, loss 1.6735239028930664
iteration 500, loss 1.6407458782196045
iteration 600, loss 1.6180882453918457
iteration 700, loss 1.6922484636306763
iteration 800, loss 1.613487720489502
iteration 0, loss 1.5839552879333496
iteration 100, loss 1.6335138082504272
iteration 200, loss 1.6473474502563477
iteration 300, loss 1.6112972497940063
iteration 400, loss 1.6673468351364136
iteration 500, loss 1.6249040365219116
iteration 600, loss 1.6420807838439941
iteration 700, loss 1.6152933835983276
iteration 800, loss 1.6057554483413696
iteration 0, loss 1.603393793106079
iteration 100, loss 1.6560226678848267
iteration 200, loss 1.6485825777053833
iteration 300, loss 1.590997576713562
iteration 400, loss 1.643363356590271
iteration 500, loss 1.6381113529205322
iteration 600, loss 1.6633785963058472
iteration 700, loss 1.6208984851837158
iteration 800, loss 1.6605812311172485
iteration 0, loss 1.6198906898498535
iteration 100, loss 1.6585025787353516
iteration 200, loss 1.595780611038208
iteration 300, loss 1.6463485956192017
iteration 400, loss 1.6001557111740112
iteration 500, loss 1.6351253986358643
iteration 600, loss 1.6454415321350098
iteration 700, loss 1.664579153060913
iteration 800, loss 1.5881164073944092
iteration 0, loss 1.6382802724838257
iteration 100, loss 1.6279761791229248
iteration 200, loss 1.6316577196121216
iteration 300, loss 1.6324151754379272
iteration 400, loss 1.5827537775039673
iteration 500, loss 1.6392273902893066
iteration 600, loss 1.5622445344924927
iteration 700, loss 1.6477229595184326
iteration 800, loss 1.597285509109497
iteration 0, loss 1.6482130289077759
iteration 100, loss 1.5939891338348389
iteration 200, loss 1.67020845413208
iteration 300, loss 1.648617148399353
iteration 400, loss 1.662803053855896
iteration 500, loss 1.6107882261276245
iteration 600, loss 1.609296202659607
iteration 700, loss 1.6291038990020752
iteration 800, loss 1.6218113899230957
iteration 0, loss 1.6550204753875732
iteration 100, loss 1.611569881439209
iteration 200, loss 1.6699198484420776
iteration 300, loss 1.613246202468872
iteration 400, loss 1.6084246635437012
iteration 500, loss 1.596719741821289
iteration 600, loss 1.6205662488937378
iteration 700, loss 1.5802481174468994
iteration 800, loss 1.6378703117370605
iteration 0, loss 1.6249022483825684
iteration 100, loss 1.6265827417373657
iteration 200, loss 1.614088535308838
iteration 300, loss 1.6079927682876587
iteration 400, loss 1.592507243156433
iteration 500, loss 1.5887575149536133
iteration 600, loss 1.6406760215759277
iteration 700, loss 1.6629903316497803
iteration 800, loss 1.5959628820419312
iteration 0, loss 1.6427711248397827
iteration 100, loss 1.6405980587005615
iteration 200, loss 1.647935152053833
iteration 300, loss 1.5953104496002197
iteration 400, loss 1.6291311979293823
iteration 500, loss 1.6384291648864746
iteration 600, loss 1.6180200576782227
iteration 700, loss 1.5692508220672607
iteration 800, loss 1.6112266778945923
iteration 0, loss 1.6391819715499878
iteration 100, loss 1.5942966938018799
iteration 200, loss 1.6421096324920654
iteration 300, loss 1.6323423385620117
iteration 400, loss 1.5939456224441528
iteration 500, loss 1.5675601959228516
iteration 600, loss 1.6093872785568237
iteration 700, loss 1.6302920579910278
iteration 800, loss 1.6256530284881592
iteration 0, loss 1.6371821165084839
iteration 100, loss 1.7179292440414429
iteration 200, loss 1.5735979080200195
iteration 300, loss 1.6241472959518433
iteration 400, loss 1.5745763778686523
iteration 500, loss 1.6212809085845947
iteration 600, loss 1.5915132761001587
iteration 700, loss 1.6139898300170898
iteration 800, loss 1.6289222240447998
iteration 0, loss 1.6509292125701904
iteration 100, loss 1.588692545890808
iteration 200, loss 1.5960302352905273
iteration 300, loss 1.6328049898147583
iteration 400, loss 1.573589563369751
iteration 500, loss 1.5836213827133179
iteration 600, loss 1.685675859451294
iteration 700, loss 1.6503701210021973
iteration 800, loss 1.6545414924621582
iteration 0, loss 1.6337780952453613
iteration 100, loss 1.6028072834014893
iteration 200, loss 1.615211009979248
iteration 300, loss 1.6230669021606445
iteration 400, loss 1.7089293003082275
iteration 500, loss 1.5364327430725098
iteration 600, loss 1.6199954748153687
iteration 700, loss 1.5996280908584595
iteration 800, loss 1.6446573734283447
iteration 0, loss 1.5826207399368286
iteration 100, loss 1.6271780729293823
iteration 200, loss 1.6433309316635132
iteration 300, loss 1.6435545682907104
iteration 400, loss 1.562359094619751
iteration 500, loss 1.7095730304718018
iteration 600, loss 1.6281687021255493
iteration 700, loss 1.6395913362503052
iteration 800, loss 1.6090844869613647
iteration 0, loss 1.627339482307434
iteration 100, loss 1.6032088994979858
iteration 200, loss 1.6371594667434692
iteration 300, loss 1.6913881301879883
iteration 400, loss 1.6006383895874023
iteration 500, loss 1.5699762105941772
iteration 600, loss 1.5919665098190308
iteration 700, loss 1.6155648231506348
iteration 800, loss 1.6189488172531128
iteration 0, loss 1.606992483139038
iteration 100, loss 1.63661789894104
iteration 200, loss 1.5990796089172363
iteration 300, loss 1.6248724460601807
iteration 400, loss 1.5880589485168457
iteration 500, loss 1.6405380964279175
iteration 600, loss 1.6518605947494507
iteration 700, loss 1.606892704963684
iteration 800, loss 1.637687087059021
iteration 0, loss 1.6634469032287598
iteration 100, loss 1.6448519229888916
iteration 200, loss 1.6117722988128662
iteration 300, loss 1.7316902875900269
iteration 400, loss 1.6096491813659668
iteration 500, loss 1.6373592615127563
iteration 600, loss 1.5958688259124756
iteration 700, loss 1.6532032489776611
iteration 800, loss 1.6000066995620728
iteration 0, loss 1.6099189519882202
iteration 100, loss 1.644518256187439
iteration 200, loss 1.6184136867523193
iteration 300, loss 1.6165176630020142
iteration 400, loss 1.6155364513397217
iteration 500, loss 1.6377378702163696
iteration 600, loss 1.5859363079071045
iteration 700, loss 1.725538730621338
iteration 800, loss 1.6195080280303955
iteration 0, loss 1.621498703956604
iteration 100, loss 1.6300667524337769
iteration 200, loss 1.672663927078247
iteration 300, loss 1.5963987112045288
iteration 400, loss 1.6043236255645752
iteration 500, loss 1.665270447731018
iteration 600, loss 1.6381971836090088
iteration 700, loss 1.657247543334961
iteration 800, loss 1.5832202434539795
iteration 0, loss 1.6413413286209106
iteration 100, loss 1.6232551336288452
iteration 200, loss 1.6476786136627197
iteration 300, loss 1.6451698541641235
iteration 400, loss 1.6176996231079102
iteration 500, loss 1.5880275964736938
iteration 600, loss 1.6440725326538086
iteration 700, loss 1.698838472366333
iteration 800, loss 1.6395177841186523
iteration 0, loss 1.575706124305725
iteration 100, loss 1.6418908834457397
iteration 200, loss 1.6935566663742065
iteration 300, loss 1.5782644748687744
iteration 400, loss 1.6530667543411255
iteration 500, loss 1.6349718570709229
iteration 600, loss 1.6139204502105713
iteration 700, loss 1.5891752243041992
iteration 800, loss 1.6488430500030518
iteration 0, loss 1.5923829078674316
iteration 100, loss 1.6839650869369507
iteration 200, loss 1.6761153936386108
iteration 300, loss 1.639154314994812
iteration 400, loss 1.6514440774917603
iteration 500, loss 1.6699641942977905
iteration 600, loss 1.625152826309204
iteration 700, loss 1.6511633396148682
iteration 800, loss 1.5754196643829346
iteration 0, loss 1.641810417175293
iteration 100, loss 1.635671615600586
iteration 200, loss 1.598389744758606
iteration 300, loss 1.6129003763198853
iteration 400, loss 1.626933217048645
iteration 500, loss 1.6457798480987549
iteration 600, loss 1.726540207862854
iteration 700, loss 1.5896705389022827
iteration 800, loss 1.5815887451171875
iteration 0, loss 1.6433329582214355
iteration 100, loss 1.5984622240066528
iteration 200, loss 1.6066848039627075
iteration 300, loss 1.6438360214233398
iteration 400, loss 1.6025358438491821
iteration 500, loss 1.6388013362884521
iteration 600, loss 1.623069405555725
iteration 700, loss 1.620560884475708
iteration 800, loss 1.5877363681793213
iteration 0, loss 1.6610373258590698
iteration 100, loss 1.6467410326004028
iteration 200, loss 1.7362396717071533
iteration 300, loss 1.6353293657302856
iteration 400, loss 1.6170779466629028
iteration 500, loss 1.6031261682510376
iteration 600, loss 1.6138569116592407
iteration 700, loss 1.6383233070373535
iteration 800, loss 1.6188242435455322
iteration 0, loss 1.6533013582229614
iteration 100, loss 1.6090755462646484
iteration 200, loss 1.5791592597961426
iteration 300, loss 1.6602622270584106
iteration 400, loss 1.60711669921875
iteration 500, loss 1.6475672721862793
iteration 600, loss 1.6638901233673096
iteration 700, loss 1.6667348146438599
iteration 800, loss 1.6354542970657349
iteration 0, loss 1.6468942165374756
iteration 100, loss 1.6247087717056274
iteration 200, loss 1.7037599086761475
iteration 300, loss 1.6492708921432495
iteration 400, loss 1.6411665678024292
iteration 500, loss 1.6727862358093262
iteration 600, loss 1.582234501838684
iteration 700, loss 1.6235235929489136
iteration 800, loss 1.5766676664352417
iteration 0, loss 1.5872572660446167
iteration 100, loss 1.644604206085205
iteration 200, loss 1.5769973993301392
iteration 300, loss 1.6420948505401611
iteration 400, loss 1.6614910364151
iteration 500, loss 1.6498973369598389
iteration 600, loss 1.5963430404663086
iteration 700, loss 1.5612986087799072
iteration 800, loss 1.5843851566314697
iteration 0, loss 1.6429182291030884
iteration 100, loss 1.609129548072815
iteration 200, loss 1.621484398841858
iteration 300, loss 1.6651966571807861
iteration 400, loss 1.617368221282959
iteration 500, loss 1.5646188259124756
iteration 600, loss 1.6679083108901978
iteration 700, loss 1.6453540325164795
iteration 800, loss 1.63166081905365
iteration 0, loss 1.6659425497055054
iteration 100, loss 1.5829449892044067
iteration 200, loss 1.5783551931381226
iteration 300, loss 1.6426595449447632
iteration 400, loss 1.6213105916976929
iteration 500, loss 1.580850601196289
iteration 600, loss 1.5883101224899292
iteration 700, loss 1.6344470977783203
iteration 800, loss 1.5626165866851807
iteration 0, loss 1.6009669303894043
iteration 100, loss 1.579035997390747
iteration 200, loss 1.5853054523468018
iteration 300, loss 1.6187018156051636
iteration 400, loss 1.6072927713394165
iteration 500, loss 1.5508363246917725
iteration 600, loss 1.6535595655441284
iteration 700, loss 1.6344627141952515
iteration 800, loss 1.6242907047271729
iteration 0, loss 1.6365272998809814
iteration 100, loss 1.6791775226593018
iteration 200, loss 1.6651513576507568
iteration 300, loss 1.6058778762817383
iteration 400, loss 1.633634090423584
iteration 500, loss 1.645544409751892
iteration 600, loss 1.640903115272522
iteration 700, loss 1.6148890256881714
iteration 800, loss 1.5729442834854126
fold 1 accuracy: 0.8137142857142857
iteration 0, loss 1.5925472974777222
iteration 100, loss 1.6303350925445557
iteration 200, loss 1.6361334323883057
iteration 300, loss 1.5633807182312012
iteration 400, loss 1.6516876220703125
iteration 500, loss 1.6102278232574463
iteration 600, loss 1.5720303058624268
iteration 700, loss 1.677283525466919
iteration 800, loss 1.6530816555023193
iteration 0, loss 1.5921745300292969
iteration 100, loss 1.6115620136260986
iteration 200, loss 1.61182701587677
iteration 300, loss 1.5843383073806763
iteration 400, loss 1.6004774570465088
iteration 500, loss 1.6542959213256836
iteration 600, loss 1.6225818395614624
iteration 700, loss 1.5932132005691528
iteration 800, loss 1.630222201347351
iteration 0, loss 1.6053582429885864
iteration 100, loss 1.6289782524108887
iteration 200, loss 1.6526230573654175
iteration 300, loss 1.6067581176757812
iteration 400, loss 1.5886131525039673
iteration 500, loss 1.6265802383422852
iteration 600, loss 1.623462438583374
iteration 700, loss 1.6097831726074219
iteration 800, loss 1.6577410697937012
iteration 0, loss 1.6546001434326172
iteration 100, loss 1.6415066719055176
iteration 200, loss 1.6309643983840942
iteration 300, loss 1.6304430961608887
iteration 400, loss 1.5870096683502197
iteration 500, loss 1.6812074184417725
iteration 600, loss 1.6223868131637573
iteration 700, loss 1.62808096408844
iteration 800, loss 1.625100016593933
iteration 0, loss 1.603415608406067
iteration 100, loss 1.5886893272399902
iteration 200, loss 1.6477450132369995
iteration 300, loss 1.5898768901824951
iteration 400, loss 1.6132453680038452
iteration 500, loss 1.6436703205108643
iteration 600, loss 1.587090015411377
iteration 700, loss 1.6325448751449585
iteration 800, loss 1.586416244506836
iteration 0, loss 1.6315536499023438
iteration 100, loss 1.6003645658493042
iteration 200, loss 1.6637781858444214
iteration 300, loss 1.619001030921936
iteration 400, loss 1.6552494764328003
iteration 500, loss 1.5951154232025146
iteration 600, loss 1.6794912815093994
iteration 700, loss 1.6293225288391113
iteration 800, loss 1.694155216217041
iteration 0, loss 1.603158950805664
iteration 100, loss 1.5866507291793823
iteration 200, loss 1.6273494958877563
iteration 300, loss 1.6074565649032593
iteration 400, loss 1.6547552347183228
iteration 500, loss 1.6028627157211304
iteration 600, loss 1.6676020622253418
iteration 700, loss 1.643601894378662
iteration 800, loss 1.6440699100494385
iteration 0, loss 1.6324586868286133
iteration 100, loss 1.6316488981246948
iteration 200, loss 1.617590069770813
iteration 300, loss 1.6692289113998413
iteration 400, loss 1.6412333250045776
iteration 500, loss 1.6359320878982544
iteration 600, loss 1.6294625997543335
iteration 700, loss 1.5667307376861572
iteration 800, loss 1.6128374338150024
iteration 0, loss 1.6255220174789429
iteration 100, loss 1.570341944694519
iteration 200, loss 1.6171867847442627
iteration 300, loss 1.5409587621688843
iteration 400, loss 1.6233793497085571
iteration 500, loss 1.59254789352417
iteration 600, loss 1.6034783124923706
iteration 700, loss 1.600432276725769
iteration 800, loss 1.5589367151260376
iteration 0, loss 1.5836297273635864
iteration 100, loss 1.643148422241211
iteration 200, loss 1.651373267173767
iteration 300, loss 1.6389652490615845
iteration 400, loss 1.6306401491165161
iteration 500, loss 1.7072033882141113
iteration 600, loss 1.63970947265625
iteration 700, loss 1.657926321029663
iteration 800, loss 1.580436110496521
iteration 0, loss 1.604544997215271
iteration 100, loss 1.5931040048599243
iteration 200, loss 1.6285009384155273
iteration 300, loss 1.6058777570724487
iteration 400, loss 1.690650463104248
iteration 500, loss 1.6539429426193237
iteration 600, loss 1.6255406141281128
iteration 700, loss 1.6178044080734253
iteration 800, loss 1.5701613426208496
iteration 0, loss 1.622506856918335
iteration 100, loss 1.6411935091018677
iteration 200, loss 1.590636968612671
iteration 300, loss 1.6322424411773682
iteration 400, loss 1.6082053184509277
iteration 500, loss 1.6274398565292358
iteration 600, loss 1.6829270124435425
iteration 700, loss 1.603212594985962
iteration 800, loss 1.660649299621582
iteration 0, loss 1.6047847270965576
iteration 100, loss 1.5858681201934814
iteration 200, loss 1.6259965896606445
iteration 300, loss 1.6305497884750366
iteration 400, loss 1.651929259300232
iteration 500, loss 1.6193419694900513
iteration 600, loss 1.567808985710144
iteration 700, loss 1.591282844543457
iteration 800, loss 1.70734441280365
iteration 0, loss 1.639725923538208
iteration 100, loss 1.5696712732315063
iteration 200, loss 1.5719621181488037
iteration 300, loss 1.590278148651123
iteration 400, loss 1.569460391998291
iteration 500, loss 1.6430331468582153
iteration 600, loss 1.62531578540802
iteration 700, loss 1.6113909482955933
iteration 800, loss 1.556143045425415
iteration 0, loss 1.6140998601913452
iteration 100, loss 1.6636126041412354
iteration 200, loss 1.6064414978027344
iteration 300, loss 1.650495171546936
iteration 400, loss 1.6041324138641357
iteration 500, loss 1.6726593971252441
iteration 600, loss 1.672098994255066
iteration 700, loss 1.6301066875457764
iteration 800, loss 1.6356419324874878
iteration 0, loss 1.6598304510116577
iteration 100, loss 1.597715973854065
iteration 200, loss 1.6476123332977295
iteration 300, loss 1.6488959789276123
iteration 400, loss 1.6860028505325317
iteration 500, loss 1.6334365606307983
iteration 600, loss 1.6347187757492065
iteration 700, loss 1.6313308477401733
iteration 800, loss 1.6265337467193604
iteration 0, loss 1.646846055984497
iteration 100, loss 1.6644341945648193
iteration 200, loss 1.6598331928253174
iteration 300, loss 1.5780243873596191
iteration 400, loss 1.7335972785949707
iteration 500, loss 1.6769404411315918
iteration 600, loss 1.622651219367981
iteration 700, loss 1.6720701456069946
iteration 800, loss 1.6004575490951538
iteration 0, loss 1.6773946285247803
iteration 100, loss 1.6381789445877075
iteration 200, loss 1.6022032499313354
iteration 300, loss 1.6390712261199951
iteration 400, loss 1.6889442205429077
iteration 500, loss 1.6307525634765625
iteration 600, loss 1.6570008993148804
iteration 700, loss 1.6578084230422974
iteration 800, loss 1.6623187065124512
iteration 0, loss 1.6435683965682983
iteration 100, loss 1.629042387008667
iteration 200, loss 1.5876504182815552
iteration 300, loss 1.7012324333190918
iteration 400, loss 1.6096696853637695
iteration 500, loss 1.5939539670944214
iteration 600, loss 1.6170531511306763
iteration 700, loss 1.6096711158752441
iteration 800, loss 1.6044977903366089
iteration 0, loss 1.602934718132019
iteration 100, loss 1.6175537109375
iteration 200, loss 1.6091878414154053
iteration 300, loss 1.582709550857544
iteration 400, loss 1.602419137954712
iteration 500, loss 1.6328117847442627
iteration 600, loss 1.6872566938400269
iteration 700, loss 1.642480492591858
iteration 800, loss 1.6625388860702515
iteration 0, loss 1.6286461353302002
iteration 100, loss 1.6296043395996094
iteration 200, loss 1.645072102546692
iteration 300, loss 1.6190860271453857
iteration 400, loss 1.628755807876587
iteration 500, loss 1.5534816980361938
iteration 600, loss 1.6014291048049927
iteration 700, loss 1.6068394184112549
iteration 800, loss 1.5889639854431152
iteration 0, loss 1.6359024047851562
iteration 100, loss 1.6375553607940674
iteration 200, loss 1.615095853805542
iteration 300, loss 1.600788950920105
iteration 400, loss 1.5867252349853516
iteration 500, loss 1.6731157302856445
iteration 600, loss 1.6580618619918823
iteration 700, loss 1.6149413585662842
iteration 800, loss 1.59714674949646
iteration 0, loss 1.569425344467163
iteration 100, loss 1.6358058452606201
iteration 200, loss 1.700822353363037
iteration 300, loss 1.580517292022705
iteration 400, loss 1.6205211877822876
iteration 500, loss 1.605892300605774
iteration 600, loss 1.6580830812454224
iteration 700, loss 1.6604965925216675
iteration 800, loss 1.6229597330093384
iteration 0, loss 1.6284260749816895
iteration 100, loss 1.5961122512817383
iteration 200, loss 1.5968749523162842
iteration 300, loss 1.6418101787567139
iteration 400, loss 1.660775899887085
iteration 500, loss 1.6287959814071655
iteration 600, loss 1.6665791273117065
iteration 700, loss 1.6653820276260376
iteration 800, loss 1.6225754022598267
iteration 0, loss 1.6443530321121216
iteration 100, loss 1.6335999965667725
iteration 200, loss 1.640493631362915
iteration 300, loss 1.658264398574829
iteration 400, loss 1.6220006942749023
iteration 500, loss 1.6560431718826294
iteration 600, loss 1.6438127756118774
iteration 700, loss 1.6112136840820312
iteration 800, loss 1.6727796792984009
iteration 0, loss 1.6430078744888306
iteration 100, loss 1.6023343801498413
iteration 200, loss 1.527343511581421
iteration 300, loss 1.6666953563690186
iteration 400, loss 1.5761759281158447
iteration 500, loss 1.627508282661438
iteration 600, loss 1.580105185508728
iteration 700, loss 1.678971290588379
iteration 800, loss 1.621553897857666
iteration 0, loss 1.7112250328063965
iteration 100, loss 1.625661015510559
iteration 200, loss 1.5963292121887207
iteration 300, loss 1.5751445293426514
iteration 400, loss 1.6283423900604248
iteration 500, loss 1.643810749053955
iteration 600, loss 1.5609333515167236
iteration 700, loss 1.6531602144241333
iteration 800, loss 1.5865339040756226
iteration 0, loss 1.573052167892456
iteration 100, loss 1.6356029510498047
iteration 200, loss 1.6279847621917725
iteration 300, loss 1.5982848405838013
iteration 400, loss 1.6334857940673828
iteration 500, loss 1.6134599447250366
iteration 600, loss 1.6357542276382446
iteration 700, loss 1.655029296875
iteration 800, loss 1.6523876190185547
iteration 0, loss 1.5689473152160645
iteration 100, loss 1.5728418827056885
iteration 200, loss 1.6427165269851685
iteration 300, loss 1.6189219951629639
iteration 400, loss 1.5880959033966064
iteration 500, loss 1.6215019226074219
iteration 600, loss 1.6417423486709595
iteration 700, loss 1.6306909322738647
iteration 800, loss 1.562706708908081
iteration 0, loss 1.5840476751327515
iteration 100, loss 1.6585817337036133
iteration 200, loss 1.6845108270645142
iteration 300, loss 1.6516607999801636
iteration 400, loss 1.623327374458313
iteration 500, loss 1.6750881671905518
iteration 600, loss 1.5461610555648804
iteration 700, loss 1.573490858078003
iteration 800, loss 1.6102702617645264
iteration 0, loss 1.6651208400726318
iteration 100, loss 1.6690173149108887
iteration 200, loss 1.5871244668960571
iteration 300, loss 1.6158994436264038
iteration 400, loss 1.6504313945770264
iteration 500, loss 1.5854634046554565
iteration 600, loss 1.5930207967758179
iteration 700, loss 1.6612138748168945
iteration 800, loss 1.5956485271453857
iteration 0, loss 1.6624252796173096
iteration 100, loss 1.6666004657745361
iteration 200, loss 1.6595202684402466
iteration 300, loss 1.6482512950897217
iteration 400, loss 1.675458312034607
iteration 500, loss 1.6701377630233765
iteration 600, loss 1.6128344535827637
iteration 700, loss 1.590754747390747
iteration 800, loss 1.6516999006271362
iteration 0, loss 1.6627049446105957
iteration 100, loss 1.7400298118591309
iteration 200, loss 1.592461109161377
iteration 300, loss 1.6999402046203613
iteration 400, loss 1.5955111980438232
iteration 500, loss 1.6290630102157593
iteration 600, loss 1.6206117868423462
iteration 700, loss 1.649467945098877
iteration 800, loss 1.6459711790084839
iteration 0, loss 1.6183056831359863
iteration 100, loss 1.5743184089660645
iteration 200, loss 1.5781524181365967
iteration 300, loss 1.6786161661148071
iteration 400, loss 1.5934451818466187
iteration 500, loss 1.6402947902679443
iteration 600, loss 1.6008403301239014
iteration 700, loss 1.5837883949279785
iteration 800, loss 1.668117642402649
iteration 0, loss 1.6094276905059814
iteration 100, loss 1.672020435333252
iteration 200, loss 1.5999406576156616
iteration 300, loss 1.621042013168335
iteration 400, loss 1.554778814315796
iteration 500, loss 1.6200798749923706
iteration 600, loss 1.6443332433700562
iteration 700, loss 1.6389929056167603
iteration 800, loss 1.6725740432739258
iteration 0, loss 1.6569857597351074
iteration 100, loss 1.637808084487915
iteration 200, loss 1.6661092042922974
iteration 300, loss 1.6520333290100098
iteration 400, loss 1.606752634048462
iteration 500, loss 1.605380892753601
iteration 600, loss 1.5755445957183838
iteration 700, loss 1.640618085861206
iteration 800, loss 1.5695096254348755
iteration 0, loss 1.6362051963806152
iteration 100, loss 1.608987808227539
iteration 200, loss 1.6220728158950806
iteration 300, loss 1.631864309310913
iteration 400, loss 1.6612666845321655
iteration 500, loss 1.5640404224395752
iteration 600, loss 1.5840619802474976
iteration 700, loss 1.6091253757476807
iteration 800, loss 1.591962456703186
iteration 0, loss 1.6318713426589966
iteration 100, loss 1.6419061422348022
iteration 200, loss 1.6060305833816528
iteration 300, loss 1.6275259256362915
iteration 400, loss 1.6221469640731812
iteration 500, loss 1.6108351945877075
iteration 600, loss 1.6123075485229492
iteration 700, loss 1.6101211309432983
iteration 800, loss 1.6508545875549316
iteration 0, loss 1.6815309524536133
iteration 100, loss 1.6156469583511353
iteration 200, loss 1.6214280128479004
iteration 300, loss 1.6802507638931274
iteration 400, loss 1.620216965675354
iteration 500, loss 1.6312909126281738
iteration 600, loss 1.6415562629699707
iteration 700, loss 1.5878878831863403
iteration 800, loss 1.6539710760116577
iteration 0, loss 1.6273176670074463
iteration 100, loss 1.6235451698303223
iteration 200, loss 1.647085428237915
iteration 300, loss 1.6422489881515503
iteration 400, loss 1.6240941286087036
iteration 500, loss 1.6034064292907715
iteration 600, loss 1.5901479721069336
iteration 700, loss 1.6054699420928955
iteration 800, loss 1.6650736331939697
iteration 0, loss 1.6688851118087769
iteration 100, loss 1.6893678903579712
iteration 200, loss 1.6437511444091797
iteration 300, loss 1.6604924201965332
iteration 400, loss 1.6532362699508667
iteration 500, loss 1.6890965700149536
iteration 600, loss 1.5766061544418335
iteration 700, loss 1.668759822845459
iteration 800, loss 1.6793111562728882
iteration 0, loss 1.5960613489151
iteration 100, loss 1.5994815826416016
iteration 200, loss 1.6282097101211548
iteration 300, loss 1.6275501251220703
iteration 400, loss 1.6312885284423828
iteration 500, loss 1.6285457611083984
iteration 600, loss 1.678317666053772
iteration 700, loss 1.6423436403274536
iteration 800, loss 1.6229537725448608
iteration 0, loss 1.6007353067398071
iteration 100, loss 1.5866156816482544
iteration 200, loss 1.6250382661819458
iteration 300, loss 1.5450485944747925
iteration 400, loss 1.6231069564819336
iteration 500, loss 1.6777565479278564
iteration 600, loss 1.6461546421051025
iteration 700, loss 1.6399073600769043
iteration 800, loss 1.5957105159759521
iteration 0, loss 1.611485242843628
iteration 100, loss 1.6481919288635254
iteration 200, loss 1.6080304384231567
iteration 300, loss 1.6063134670257568
iteration 400, loss 1.6162962913513184
iteration 500, loss 1.647957682609558
iteration 600, loss 1.6121217012405396
iteration 700, loss 1.6283105611801147
iteration 800, loss 1.5896539688110352
iteration 0, loss 1.6716492176055908
iteration 100, loss 1.6340882778167725
iteration 200, loss 1.6358484029769897
iteration 300, loss 1.5636367797851562
iteration 400, loss 1.63505220413208
iteration 500, loss 1.7374374866485596
iteration 600, loss 1.6054006814956665
iteration 700, loss 1.6152780055999756
iteration 800, loss 1.6215088367462158
iteration 0, loss 1.563576340675354
iteration 100, loss 1.6896624565124512
iteration 200, loss 1.5955069065093994
iteration 300, loss 1.567086935043335
iteration 400, loss 1.6109092235565186
iteration 500, loss 1.6175938844680786
iteration 600, loss 1.634293794631958
iteration 700, loss 1.6027989387512207
iteration 800, loss 1.7028188705444336
iteration 0, loss 1.5885157585144043
iteration 100, loss 1.5913540124893188
iteration 200, loss 1.5772181749343872
iteration 300, loss 1.6572885513305664
iteration 400, loss 1.602378010749817
iteration 500, loss 1.6194124221801758
iteration 600, loss 1.6010634899139404
iteration 700, loss 1.6347581148147583
iteration 800, loss 1.6320022344589233
iteration 0, loss 1.6855010986328125
iteration 100, loss 1.5604358911514282
iteration 200, loss 1.6343495845794678
iteration 300, loss 1.6980892419815063
iteration 400, loss 1.6083531379699707
iteration 500, loss 1.658678412437439
iteration 600, loss 1.6127750873565674
iteration 700, loss 1.6649750471115112
iteration 800, loss 1.627624273300171
iteration 0, loss 1.5989785194396973
iteration 100, loss 1.6854597330093384
iteration 200, loss 1.614652156829834
iteration 300, loss 1.5755189657211304
iteration 400, loss 1.5995453596115112
iteration 500, loss 1.6418683528900146
iteration 600, loss 1.618891954421997
iteration 700, loss 1.7135233879089355
iteration 800, loss 1.5794962644577026
iteration 0, loss 1.7208040952682495
iteration 100, loss 1.6303465366363525
iteration 200, loss 1.6496951580047607
iteration 300, loss 1.6534897089004517
iteration 400, loss 1.6169469356536865
iteration 500, loss 1.667096734046936
iteration 600, loss 1.5675324201583862
iteration 700, loss 1.6846439838409424
iteration 800, loss 1.5843603610992432
fold 2 accuracy: 0.8194285714285714
iteration 0, loss 1.7191452980041504
iteration 100, loss 1.6499269008636475
iteration 200, loss 1.5520262718200684
iteration 300, loss 1.580399513244629
iteration 400, loss 1.6658735275268555
iteration 500, loss 1.6652323007583618
iteration 600, loss 1.627924919128418
iteration 700, loss 1.631980061531067
iteration 800, loss 1.6186851263046265
iteration 0, loss 1.6257070302963257
iteration 100, loss 1.6210438013076782
iteration 200, loss 1.6578779220581055
iteration 300, loss 1.5840654373168945
iteration 400, loss 1.5604299306869507
iteration 500, loss 1.6103154420852661
iteration 600, loss 1.6342507600784302
iteration 700, loss 1.631542682647705
iteration 800, loss 1.6616407632827759
iteration 0, loss 1.5922528505325317
iteration 100, loss 1.5880872011184692
iteration 200, loss 1.6027894020080566
iteration 300, loss 1.6520893573760986
iteration 400, loss 1.611861228942871
iteration 500, loss 1.6357805728912354
iteration 600, loss 1.5820235013961792
iteration 700, loss 1.5797921419143677
iteration 800, loss 1.6134480237960815
iteration 0, loss 1.6039676666259766
iteration 100, loss 1.5994380712509155
iteration 200, loss 1.6212632656097412
iteration 300, loss 1.6046415567398071
iteration 400, loss 1.5774060487747192
iteration 500, loss 1.5873905420303345
iteration 600, loss 1.576694130897522
iteration 700, loss 1.6810246706008911
iteration 800, loss 1.598541021347046
iteration 0, loss 1.6384260654449463
iteration 100, loss 1.6455152034759521
iteration 200, loss 1.6463191509246826
iteration 300, loss 1.7288258075714111
iteration 400, loss 1.6125153303146362
iteration 500, loss 1.6025971174240112
iteration 600, loss 1.6720519065856934
iteration 700, loss 1.6252717971801758
iteration 800, loss 1.602824330329895
iteration 0, loss 1.6006160974502563
iteration 100, loss 1.6498067378997803
iteration 200, loss 1.6634259223937988
iteration 300, loss 1.6639105081558228
iteration 400, loss 1.604166865348816
iteration 500, loss 1.5583786964416504
iteration 600, loss 1.6403924226760864
iteration 700, loss 1.5612629652023315
iteration 800, loss 1.690821886062622
iteration 0, loss 1.57159423828125
iteration 100, loss 1.5998018980026245
iteration 200, loss 1.6329972743988037
iteration 300, loss 1.6220874786376953
iteration 400, loss 1.615310549736023
iteration 500, loss 1.634964108467102
iteration 600, loss 1.6397637128829956
iteration 700, loss 1.6370584964752197
iteration 800, loss 1.6127824783325195
iteration 0, loss 1.6638681888580322
iteration 100, loss 1.5716309547424316
iteration 200, loss 1.6724016666412354
iteration 300, loss 1.563446283340454
iteration 400, loss 1.5958412885665894
iteration 500, loss 1.587874412536621
iteration 600, loss 1.6710213422775269
iteration 700, loss 1.6243913173675537
iteration 800, loss 1.63381028175354
iteration 0, loss 1.6265240907669067
iteration 100, loss 1.6133743524551392
iteration 200, loss 1.7376433610916138
iteration 300, loss 1.6948238611221313
iteration 400, loss 1.594861626625061
iteration 500, loss 1.6142427921295166
iteration 600, loss 1.6027491092681885
iteration 700, loss 1.5781209468841553
iteration 800, loss 1.7187010049819946
iteration 0, loss 1.594345211982727
iteration 100, loss 1.6271809339523315
iteration 200, loss 1.594928503036499
iteration 300, loss 1.612403392791748
iteration 400, loss 1.6404024362564087
iteration 500, loss 1.5974502563476562
iteration 600, loss 1.6184042692184448
iteration 700, loss 1.643717885017395
iteration 800, loss 1.6996285915374756
iteration 0, loss 1.669640064239502
iteration 100, loss 1.6050546169281006
iteration 200, loss 1.590110182762146
iteration 300, loss 1.6266381740570068
iteration 400, loss 1.6329551935195923
iteration 500, loss 1.6303315162658691
iteration 600, loss 1.603281855583191
iteration 700, loss 1.6049158573150635
iteration 800, loss 1.6279335021972656
iteration 0, loss 1.6049832105636597
iteration 100, loss 1.5676381587982178
iteration 200, loss 1.645783543586731
iteration 300, loss 1.6599845886230469
iteration 400, loss 1.6506043672561646
iteration 500, loss 1.6539868116378784
iteration 600, loss 1.6022186279296875
iteration 700, loss 1.6285525560379028
iteration 800, loss 1.641831636428833
iteration 0, loss 1.6045405864715576
iteration 100, loss 1.616662621498108
iteration 200, loss 1.6752091646194458
iteration 300, loss 1.6355570554733276
iteration 400, loss 1.6673369407653809
iteration 500, loss 1.6900570392608643
iteration 600, loss 1.66798996925354
iteration 700, loss 1.5683581829071045
iteration 800, loss 1.6316256523132324
iteration 0, loss 1.6187809705734253
iteration 100, loss 1.5853453874588013
iteration 200, loss 1.6690330505371094
iteration 300, loss 1.5928516387939453
iteration 400, loss 1.6564496755599976
iteration 500, loss 1.649321436882019
iteration 600, loss 1.6296749114990234
iteration 700, loss 1.6479675769805908
iteration 800, loss 1.6478044986724854
iteration 0, loss 1.5850725173950195
iteration 100, loss 1.6907597780227661
iteration 200, loss 1.6047612428665161
iteration 300, loss 1.6645008325576782
iteration 400, loss 1.6592707633972168
iteration 500, loss 1.5756837129592896
iteration 600, loss 1.5964018106460571
iteration 700, loss 1.70333993434906
iteration 800, loss 1.648719310760498
iteration 0, loss 1.6201367378234863
iteration 100, loss 1.625802993774414
iteration 200, loss 1.6424816846847534
iteration 300, loss 1.6046860218048096
iteration 400, loss 1.740746259689331
iteration 500, loss 1.617100477218628
iteration 600, loss 1.6794893741607666
iteration 700, loss 1.6008632183074951
iteration 800, loss 1.6077262163162231
iteration 0, loss 1.602549433708191
iteration 100, loss 1.6582932472229004
iteration 200, loss 1.632598638534546
iteration 300, loss 1.6074434518814087
iteration 400, loss 1.5965911149978638
iteration 500, loss 1.6489737033843994
iteration 600, loss 1.6315782070159912
iteration 700, loss 1.6022323369979858
iteration 800, loss 1.5853633880615234
iteration 0, loss 1.619410753250122
iteration 100, loss 1.5819514989852905
iteration 200, loss 1.6194491386413574
iteration 300, loss 1.6055797338485718
iteration 400, loss 1.6079699993133545
iteration 500, loss 1.625771164894104
iteration 600, loss 1.6329045295715332
iteration 700, loss 1.600925087928772
iteration 800, loss 1.609236717224121
iteration 0, loss 1.6923699378967285
iteration 100, loss 1.6360881328582764
iteration 200, loss 1.6573771238327026
iteration 300, loss 1.6188814640045166
iteration 400, loss 1.6149437427520752
iteration 500, loss 1.6379708051681519
iteration 600, loss 1.6514201164245605
iteration 700, loss 1.637056827545166
iteration 800, loss 1.62156343460083
iteration 0, loss 1.5724180936813354
iteration 100, loss 1.612424612045288
iteration 200, loss 1.6449708938598633
iteration 300, loss 1.6492829322814941
iteration 400, loss 1.5934150218963623
iteration 500, loss 1.6120556592941284
iteration 600, loss 1.5670242309570312
iteration 700, loss 1.6421880722045898
iteration 800, loss 1.6289267539978027
iteration 0, loss 1.6183357238769531
iteration 100, loss 1.5953880548477173
iteration 200, loss 1.616296410560608
iteration 300, loss 1.7565170526504517
iteration 400, loss 1.632523775100708
iteration 500, loss 1.6047145128250122
iteration 600, loss 1.6128456592559814
iteration 700, loss 1.689741611480713
iteration 800, loss 1.6290154457092285
iteration 0, loss 1.5891224145889282
iteration 100, loss 1.6257790327072144
iteration 200, loss 1.5670760869979858
iteration 300, loss 1.6678671836853027
iteration 400, loss 1.6606056690216064
iteration 500, loss 1.650980830192566
iteration 600, loss 1.6377233266830444
iteration 700, loss 1.6288304328918457
iteration 800, loss 1.7130917310714722
iteration 0, loss 1.6498018503189087
iteration 100, loss 1.6844525337219238
iteration 200, loss 1.6681495904922485
iteration 300, loss 1.7065620422363281
iteration 400, loss 1.6098922491073608
iteration 500, loss 1.6534532308578491
iteration 600, loss 1.6024391651153564
iteration 700, loss 1.6566169261932373
iteration 800, loss 1.6336886882781982
iteration 0, loss 1.6005041599273682
iteration 100, loss 1.6441811323165894
iteration 200, loss 1.6256122589111328
iteration 300, loss 1.5951038599014282
iteration 400, loss 1.6340608596801758
iteration 500, loss 1.7001625299453735
iteration 600, loss 1.5871855020523071
iteration 700, loss 1.6351332664489746
iteration 800, loss 1.6017414331436157
iteration 0, loss 1.6159288883209229
iteration 100, loss 1.6133815050125122
iteration 200, loss 1.5841407775878906
iteration 300, loss 1.6835399866104126
iteration 400, loss 1.669274926185608
iteration 500, loss 1.6229661703109741
iteration 600, loss 1.602667212486267
iteration 700, loss 1.7099767923355103
iteration 800, loss 1.6302012205123901
iteration 0, loss 1.5981981754302979
iteration 100, loss 1.623948097229004
iteration 200, loss 1.671862244606018
iteration 300, loss 1.6399143934249878
iteration 400, loss 1.566739559173584
iteration 500, loss 1.6682789325714111
iteration 600, loss 1.6076658964157104
iteration 700, loss 1.627456784248352
iteration 800, loss 1.6463253498077393
iteration 0, loss 1.6199867725372314
iteration 100, loss 1.5742087364196777
iteration 200, loss 1.6169893741607666
iteration 300, loss 1.6696596145629883
iteration 400, loss 1.6259541511535645
iteration 500, loss 1.61882483959198
iteration 600, loss 1.577858328819275
iteration 700, loss 1.6082903146743774
iteration 800, loss 1.5757118463516235
iteration 0, loss 1.5795159339904785
iteration 100, loss 1.6626853942871094
iteration 200, loss 1.6378300189971924
iteration 300, loss 1.6348605155944824
iteration 400, loss 1.6784031391143799
iteration 500, loss 1.6411380767822266
iteration 600, loss 1.619347095489502
iteration 700, loss 1.6265722513198853
iteration 800, loss 1.605830430984497
iteration 0, loss 1.6607298851013184
iteration 100, loss 1.5789625644683838
iteration 200, loss 1.5802078247070312
iteration 300, loss 1.556098222732544
iteration 400, loss 1.6467477083206177
iteration 500, loss 1.6351356506347656
iteration 600, loss 1.5887762308120728
iteration 700, loss 1.5909423828125
iteration 800, loss 1.6019314527511597
iteration 0, loss 1.5959951877593994
iteration 100, loss 1.6691644191741943
iteration 200, loss 1.664264440536499
iteration 300, loss 1.651863694190979
iteration 400, loss 1.647908091545105
iteration 500, loss 1.6376062631607056
iteration 600, loss 1.6474286317825317
iteration 700, loss 1.6115652322769165
iteration 800, loss 1.6918846368789673
iteration 0, loss 1.6437032222747803
iteration 100, loss 1.5708812475204468
iteration 200, loss 1.6326086521148682
iteration 300, loss 1.6327083110809326
iteration 400, loss 1.6312892436981201
iteration 500, loss 1.6822831630706787
iteration 600, loss 1.706036925315857
iteration 700, loss 1.6968722343444824
iteration 800, loss 1.6346842050552368
iteration 0, loss 1.6279456615447998
iteration 100, loss 1.5688143968582153
iteration 200, loss 1.6439498662948608
iteration 300, loss 1.5802230834960938
iteration 400, loss 1.621640682220459
iteration 500, loss 1.5764046907424927
iteration 600, loss 1.6044809818267822
iteration 700, loss 1.6458964347839355
iteration 800, loss 1.553605556488037
iteration 0, loss 1.5957509279251099
iteration 100, loss 1.614984393119812
iteration 200, loss 1.5988240242004395
iteration 300, loss 1.6218276023864746
iteration 400, loss 1.6731051206588745
iteration 500, loss 1.6281508207321167
iteration 600, loss 1.583683967590332
iteration 700, loss 1.6049928665161133
iteration 800, loss 1.576573371887207
iteration 0, loss 1.6139185428619385
iteration 100, loss 1.588953971862793
iteration 200, loss 1.6247509717941284
iteration 300, loss 1.5926400423049927
iteration 400, loss 1.6136404275894165
iteration 500, loss 1.6821717023849487
iteration 600, loss 1.6200469732284546
iteration 700, loss 1.6161099672317505
iteration 800, loss 1.6227197647094727
iteration 0, loss 1.6176784038543701
iteration 100, loss 1.6091721057891846
iteration 200, loss 1.6258465051651
iteration 300, loss 1.5888752937316895
iteration 400, loss 1.5982552766799927
iteration 500, loss 1.6142083406448364
iteration 600, loss 1.5617702007293701
iteration 700, loss 1.6036763191223145
iteration 800, loss 1.6671785116195679
iteration 0, loss 1.5986615419387817
iteration 100, loss 1.6009678840637207
iteration 200, loss 1.6209899187088013
iteration 300, loss 1.6550123691558838
iteration 400, loss 1.5967414379119873
iteration 500, loss 1.572711706161499
iteration 600, loss 1.6215193271636963
iteration 700, loss 1.58146333694458
iteration 800, loss 1.5848205089569092
iteration 0, loss 1.6217527389526367
iteration 100, loss 1.6146374940872192
iteration 200, loss 1.6210466623306274
iteration 300, loss 1.6312801837921143
iteration 400, loss 1.673026442527771
iteration 500, loss 1.579554796218872
iteration 600, loss 1.6496026515960693
iteration 700, loss 1.6308624744415283
iteration 800, loss 1.5418117046356201
iteration 0, loss 1.6072001457214355
iteration 100, loss 1.6720788478851318
iteration 200, loss 1.6374696493148804
iteration 300, loss 1.658769965171814
iteration 400, loss 1.6492078304290771
iteration 500, loss 1.6443727016448975
iteration 600, loss 1.7240360975265503
iteration 700, loss 1.6243823766708374
iteration 800, loss 1.6714609861373901
iteration 0, loss 1.5877066850662231
iteration 100, loss 1.6373714208602905
iteration 200, loss 1.6360479593276978
iteration 300, loss 1.620416522026062
iteration 400, loss 1.6641067266464233
iteration 500, loss 1.5845444202423096
iteration 600, loss 1.6005630493164062
iteration 700, loss 1.6151723861694336
iteration 800, loss 1.6909390687942505
iteration 0, loss 1.6160115003585815
iteration 100, loss 1.5817129611968994
iteration 200, loss 1.6232728958129883
iteration 300, loss 1.6360081434249878
iteration 400, loss 1.6998988389968872
iteration 500, loss 1.6166300773620605
iteration 600, loss 1.6333330869674683
iteration 700, loss 1.58338463306427
iteration 800, loss 1.635947346687317
iteration 0, loss 1.6500200033187866
iteration 100, loss 1.6323131322860718
iteration 200, loss 1.619192123413086
iteration 300, loss 1.6104116439819336
iteration 400, loss 1.651764154434204
iteration 500, loss 1.6188225746154785
iteration 600, loss 1.596915364265442
iteration 700, loss 1.636631727218628
iteration 800, loss 1.598542332649231
iteration 0, loss 1.5838662385940552
iteration 100, loss 1.6555769443511963
iteration 200, loss 1.5932625532150269
iteration 300, loss 1.658392071723938
iteration 400, loss 1.6557163000106812
iteration 500, loss 1.6241813898086548
iteration 600, loss 1.5842159986495972
iteration 700, loss 1.6092650890350342
iteration 800, loss 1.6257914304733276
iteration 0, loss 1.6313145160675049
iteration 100, loss 1.5677664279937744
iteration 200, loss 1.5703294277191162
iteration 300, loss 1.629510760307312
iteration 400, loss 1.6151344776153564
iteration 500, loss 1.6636929512023926
iteration 600, loss 1.5711743831634521
iteration 700, loss 1.5927470922470093
iteration 800, loss 1.6250578165054321
iteration 0, loss 1.6007298231124878
iteration 100, loss 1.5832195281982422
iteration 200, loss 1.6029576063156128
iteration 300, loss 1.6707261800765991
iteration 400, loss 1.584031581878662
iteration 500, loss 1.6278126239776611
iteration 600, loss 1.5783040523529053
iteration 700, loss 1.6462267637252808
iteration 800, loss 1.7041800022125244
iteration 0, loss 1.609753966331482
iteration 100, loss 1.608847975730896
iteration 200, loss 1.5586979389190674
iteration 300, loss 1.5493589639663696
iteration 400, loss 1.6079344749450684
iteration 500, loss 1.6888600587844849
iteration 600, loss 1.6631516218185425
iteration 700, loss 1.6718151569366455
iteration 800, loss 1.6339890956878662
iteration 0, loss 1.6301422119140625
iteration 100, loss 1.6066709756851196
iteration 200, loss 1.6161694526672363
iteration 300, loss 1.6765832901000977
iteration 400, loss 1.6073951721191406
iteration 500, loss 1.6485953330993652
iteration 600, loss 1.6163966655731201
iteration 700, loss 1.5929056406021118
iteration 800, loss 1.60263192653656
iteration 0, loss 1.616821527481079
iteration 100, loss 1.6149741411209106
iteration 200, loss 1.5836310386657715
iteration 300, loss 1.588815450668335
iteration 400, loss 1.6601605415344238
iteration 500, loss 1.6472524404525757
iteration 600, loss 1.6300859451293945
iteration 700, loss 1.661497712135315
iteration 800, loss 1.6755075454711914
iteration 0, loss 1.6024279594421387
iteration 100, loss 1.6350041627883911
iteration 200, loss 1.6273802518844604
iteration 300, loss 1.5956226587295532
iteration 400, loss 1.5632964372634888
iteration 500, loss 1.5652203559875488
iteration 600, loss 1.6475036144256592
iteration 700, loss 1.6322195529937744
iteration 800, loss 1.6539416313171387
iteration 0, loss 1.6029566526412964
iteration 100, loss 1.6453588008880615
iteration 200, loss 1.626165747642517
iteration 300, loss 1.645442008972168
iteration 400, loss 1.6242612600326538
iteration 500, loss 1.6483845710754395
iteration 600, loss 1.5893187522888184
iteration 700, loss 1.6392648220062256
iteration 800, loss 1.6438896656036377
iteration 0, loss 1.6200473308563232
iteration 100, loss 1.6064884662628174
iteration 200, loss 1.5762479305267334
iteration 300, loss 1.6279302835464478
iteration 400, loss 1.603137493133545
iteration 500, loss 1.6063904762268066
iteration 600, loss 1.6319905519485474
iteration 700, loss 1.59773588180542
iteration 800, loss 1.6629064083099365
fold 3 accuracy: 0.8237857142857142
iteration 0, loss 1.6022536754608154
iteration 100, loss 1.5763534307479858
iteration 200, loss 1.697007417678833
iteration 300, loss 1.603101134300232
iteration 400, loss 1.6895807981491089
iteration 500, loss 1.6151684522628784
iteration 600, loss 1.633644938468933
iteration 700, loss 1.61453378200531
iteration 800, loss 1.6260095834732056
iteration 0, loss 1.544943928718567
iteration 100, loss 1.6568206548690796
iteration 200, loss 1.666321039199829
iteration 300, loss 1.5825554132461548
iteration 400, loss 1.6641767024993896
iteration 500, loss 1.5985454320907593
iteration 600, loss 1.5937747955322266
iteration 700, loss 1.6444973945617676
iteration 800, loss 1.6180484294891357
iteration 0, loss 1.694056749343872
iteration 100, loss 1.6073921918869019
iteration 200, loss 1.60755455493927
iteration 300, loss 1.6171497106552124
iteration 400, loss 1.683866024017334
iteration 500, loss 1.6292858123779297
iteration 600, loss 1.6010518074035645
iteration 700, loss 1.6910912990570068
iteration 800, loss 1.5446256399154663
iteration 0, loss 1.5730987787246704
iteration 100, loss 1.671700119972229
iteration 200, loss 1.5899850130081177
iteration 300, loss 1.6259971857070923
iteration 400, loss 1.6033577919006348
iteration 500, loss 1.6227202415466309
iteration 600, loss 1.6501009464263916
iteration 700, loss 1.6374342441558838
iteration 800, loss 1.629375696182251
iteration 0, loss 1.6093734502792358
iteration 100, loss 1.6471915245056152
iteration 200, loss 1.6336907148361206
iteration 300, loss 1.5968633890151978
iteration 400, loss 1.5963491201400757
iteration 500, loss 1.5790746212005615
iteration 600, loss 1.5896894931793213
iteration 700, loss 1.612359881401062
iteration 800, loss 1.6367290019989014
iteration 0, loss 1.6527475118637085
iteration 100, loss 1.57194185256958
iteration 200, loss 1.6299786567687988
iteration 300, loss 1.680285930633545
iteration 400, loss 1.6459420919418335
iteration 500, loss 1.6279397010803223
iteration 600, loss 1.6511785984039307
iteration 700, loss 1.6133198738098145
iteration 800, loss 1.636971116065979
iteration 0, loss 1.6543768644332886
iteration 100, loss 1.6444454193115234
iteration 200, loss 1.6413123607635498
iteration 300, loss 1.6041744947433472
iteration 400, loss 1.591261625289917
iteration 500, loss 1.6330840587615967
iteration 600, loss 1.66080904006958
iteration 700, loss 1.5645002126693726
iteration 800, loss 1.5676159858703613
iteration 0, loss 1.6622109413146973
iteration 100, loss 1.6354343891143799
iteration 200, loss 1.6843523979187012
iteration 300, loss 1.6192598342895508
iteration 400, loss 1.630642056465149
iteration 500, loss 1.6134239435195923
iteration 600, loss 1.636534571647644
iteration 700, loss 1.5795550346374512
iteration 800, loss 1.6233832836151123
iteration 0, loss 1.6475484371185303
iteration 100, loss 1.6669551134109497
iteration 200, loss 1.587280511856079
iteration 300, loss 1.5671900510787964
iteration 400, loss 1.6048954725265503
iteration 500, loss 1.6201870441436768
iteration 600, loss 1.65322744846344
iteration 700, loss 1.7056964635849
iteration 800, loss 1.5534749031066895
iteration 0, loss 1.6162594556808472
iteration 100, loss 1.5901280641555786
iteration 200, loss 1.6278226375579834
iteration 300, loss 1.6952053308486938
iteration 400, loss 1.6695095300674438
iteration 500, loss 1.6584374904632568
iteration 600, loss 1.6400400400161743
iteration 700, loss 1.6032029390335083
iteration 800, loss 1.6393532752990723
iteration 0, loss 1.7194364070892334
iteration 100, loss 1.632040023803711
iteration 200, loss 1.5980548858642578
iteration 300, loss 1.5832359790802002
iteration 400, loss 1.6324597597122192
iteration 500, loss 1.5967841148376465
iteration 600, loss 1.6371278762817383
iteration 700, loss 1.6447120904922485
iteration 800, loss 1.582564353942871
iteration 0, loss 1.589187741279602
iteration 100, loss 1.6224443912506104
iteration 200, loss 1.6156479120254517
iteration 300, loss 1.6085976362228394
iteration 400, loss 1.6751023530960083
iteration 500, loss 1.5885066986083984
iteration 600, loss 1.5749377012252808
iteration 700, loss 1.569150447845459
iteration 800, loss 1.5737264156341553
iteration 0, loss 1.6555718183517456
iteration 100, loss 1.655501127243042
iteration 200, loss 1.6759439706802368
iteration 300, loss 1.6142823696136475
iteration 400, loss 1.6288518905639648
iteration 500, loss 1.629589557647705
iteration 600, loss 1.592238187789917
iteration 700, loss 1.5911211967468262
iteration 800, loss 1.596785545349121
iteration 0, loss 1.6043012142181396
iteration 100, loss 1.661158800125122
iteration 200, loss 1.6134215593338013
iteration 300, loss 1.6202946901321411
iteration 400, loss 1.570281982421875
iteration 500, loss 1.5465456247329712
iteration 600, loss 1.5972836017608643
iteration 700, loss 1.57402503490448
iteration 800, loss 1.6364974975585938
iteration 0, loss 1.6125481128692627
iteration 100, loss 1.633653163909912
iteration 200, loss 1.6931467056274414
iteration 300, loss 1.6578255891799927
iteration 400, loss 1.5870988368988037
iteration 500, loss 1.639408826828003
iteration 600, loss 1.6419906616210938
iteration 700, loss 1.640368938446045
iteration 800, loss 1.6392579078674316
iteration 0, loss 1.5801321268081665
iteration 100, loss 1.6141562461853027
iteration 200, loss 1.58179771900177
iteration 300, loss 1.6030420064926147
iteration 400, loss 1.5953738689422607
iteration 500, loss 1.6131137609481812
iteration 600, loss 1.5970453023910522
iteration 700, loss 1.6468576192855835
iteration 800, loss 1.6286015510559082
iteration 0, loss 1.6006864309310913
iteration 100, loss 1.7025635242462158
iteration 200, loss 1.5969098806381226
iteration 300, loss 1.6130768060684204
iteration 400, loss 1.6737701892852783
iteration 500, loss 1.6738076210021973
iteration 600, loss 1.6102432012557983
iteration 700, loss 1.5778175592422485
iteration 800, loss 1.5756745338439941
iteration 0, loss 1.613072395324707
iteration 100, loss 1.6422427892684937
iteration 200, loss 1.5737450122833252
iteration 300, loss 1.5991301536560059
iteration 400, loss 1.6182076930999756
iteration 500, loss 1.6729480028152466
iteration 600, loss 1.5975338220596313
iteration 700, loss 1.6221674680709839
iteration 800, loss 1.5945943593978882
iteration 0, loss 1.6452866792678833
iteration 100, loss 1.647228479385376
iteration 200, loss 1.6576673984527588
iteration 300, loss 1.5960174798965454
iteration 400, loss 1.622106671333313
iteration 500, loss 1.691353678703308
iteration 600, loss 1.6250258684158325
iteration 700, loss 1.611828327178955
iteration 800, loss 1.6324445009231567
iteration 0, loss 1.632848858833313
iteration 100, loss 1.6134624481201172
iteration 200, loss 1.5877819061279297
iteration 300, loss 1.5577399730682373
iteration 400, loss 1.5825856924057007
iteration 500, loss 1.6535178422927856
iteration 600, loss 1.6810458898544312
iteration 700, loss 1.632362723350525
iteration 800, loss 1.6175705194473267
iteration 0, loss 1.5881119966506958
iteration 100, loss 1.6541186571121216
iteration 200, loss 1.66370689868927
iteration 300, loss 1.604079246520996
iteration 400, loss 1.6617776155471802
iteration 500, loss 1.6443320512771606
iteration 600, loss 1.589071273803711
iteration 700, loss 1.6413321495056152
iteration 800, loss 1.584682583808899
iteration 0, loss 1.572608470916748
iteration 100, loss 1.5826400518417358
iteration 200, loss 1.6717498302459717
iteration 300, loss 1.6807215213775635
iteration 400, loss 1.609430193901062
iteration 500, loss 1.6861577033996582
iteration 600, loss 1.7120229005813599
iteration 700, loss 1.6322652101516724
iteration 800, loss 1.7036888599395752
iteration 0, loss 1.6594902276992798
iteration 100, loss 1.6446713209152222
iteration 200, loss 1.6067368984222412
iteration 300, loss 1.5858204364776611
iteration 400, loss 1.5389091968536377
iteration 500, loss 1.628933072090149
iteration 600, loss 1.6265919208526611
iteration 700, loss 1.6389975547790527
iteration 800, loss 1.6528518199920654
iteration 0, loss 1.5982871055603027
iteration 100, loss 1.5687246322631836
iteration 200, loss 1.6762919425964355
iteration 300, loss 1.601489782333374
iteration 400, loss 1.6041035652160645
iteration 500, loss 1.7264227867126465
iteration 600, loss 1.623607873916626
iteration 700, loss 1.6692384481430054
iteration 800, loss 1.6422460079193115
iteration 0, loss 1.6063158512115479
iteration 100, loss 1.6316262483596802
iteration 200, loss 1.618601679801941
iteration 300, loss 1.591727614402771
iteration 400, loss 1.5971325635910034
iteration 500, loss 1.6563829183578491
iteration 600, loss 1.5955700874328613
iteration 700, loss 1.5986279249191284
iteration 800, loss 1.6775665283203125
iteration 0, loss 1.6221139430999756
iteration 100, loss 1.5872715711593628
iteration 200, loss 1.6160575151443481
iteration 300, loss 1.6355642080307007
iteration 400, loss 1.6466432809829712
iteration 500, loss 1.6351122856140137
iteration 600, loss 1.6132937669754028
iteration 700, loss 1.578847885131836
iteration 800, loss 1.6301043033599854
iteration 0, loss 1.5773359537124634
iteration 100, loss 1.5597540140151978
iteration 200, loss 1.7212443351745605
iteration 300, loss 1.6438627243041992
iteration 400, loss 1.6478338241577148
iteration 500, loss 1.554100751876831
iteration 600, loss 1.6411640644073486
iteration 700, loss 1.6308361291885376
iteration 800, loss 1.5746170282363892
iteration 0, loss 1.6390407085418701
iteration 100, loss 1.6106685400009155
iteration 200, loss 1.656480073928833
iteration 300, loss 1.5819958448410034
iteration 400, loss 1.6000616550445557
iteration 500, loss 1.5656177997589111
iteration 600, loss 1.6433168649673462
iteration 700, loss 1.5706279277801514
iteration 800, loss 1.69548499584198
iteration 0, loss 1.5967068672180176
iteration 100, loss 1.6010770797729492
iteration 200, loss 1.5697249174118042
iteration 300, loss 1.673900842666626
iteration 400, loss 1.6372630596160889
iteration 500, loss 1.5834509134292603
iteration 600, loss 1.6269114017486572
iteration 700, loss 1.6640801429748535
iteration 800, loss 1.634848952293396
iteration 0, loss 1.6613587141036987
iteration 100, loss 1.6452717781066895
iteration 200, loss 1.7042253017425537
iteration 300, loss 1.5807018280029297
iteration 400, loss 1.668006181716919
iteration 500, loss 1.6086077690124512
iteration 600, loss 1.5738974809646606
iteration 700, loss 1.6563042402267456
iteration 800, loss 1.6121608018875122
iteration 0, loss 1.62494957447052
iteration 100, loss 1.5858829021453857
iteration 200, loss 1.5741698741912842
iteration 300, loss 1.6002647876739502
iteration 400, loss 1.6218433380126953
iteration 500, loss 1.6389776468276978
iteration 600, loss 1.6690220832824707
iteration 700, loss 1.5921803712844849
iteration 800, loss 1.5613596439361572
iteration 0, loss 1.602307677268982
iteration 100, loss 1.6465904712677002
iteration 200, loss 1.5730563402175903
iteration 300, loss 1.6586198806762695
iteration 400, loss 1.6237552165985107
iteration 500, loss 1.578674554824829
iteration 600, loss 1.5480563640594482
iteration 700, loss 1.5592561960220337
iteration 800, loss 1.656429409980774
iteration 0, loss 1.6820595264434814
iteration 100, loss 1.62448251247406
iteration 200, loss 1.6807177066802979
iteration 300, loss 1.620455265045166
iteration 400, loss 1.6655571460723877
iteration 500, loss 1.627744436264038
iteration 600, loss 1.6424880027770996
iteration 700, loss 1.600245475769043
iteration 800, loss 1.6536155939102173
iteration 0, loss 1.6577465534210205
iteration 100, loss 1.6064058542251587
iteration 200, loss 1.6910678148269653
iteration 300, loss 1.6002790927886963
iteration 400, loss 1.560072422027588
iteration 500, loss 1.653817057609558
iteration 600, loss 1.5933241844177246
iteration 700, loss 1.6544373035430908
iteration 800, loss 1.5902973413467407
iteration 0, loss 1.6248136758804321
iteration 100, loss 1.6345667839050293
iteration 200, loss 1.6060434579849243
iteration 300, loss 1.5828585624694824
iteration 400, loss 1.6048964262008667
iteration 500, loss 1.667991042137146
iteration 600, loss 1.6618609428405762
iteration 700, loss 1.6396175622940063
iteration 800, loss 1.610025405883789
iteration 0, loss 1.590484380722046
iteration 100, loss 1.5470176935195923
iteration 200, loss 1.6148823499679565
iteration 300, loss 1.6155132055282593
iteration 400, loss 1.6418397426605225
iteration 500, loss 1.6379293203353882
iteration 600, loss 1.620858907699585
iteration 700, loss 1.6476012468338013
iteration 800, loss 1.6602649688720703
iteration 0, loss 1.6256983280181885
iteration 100, loss 1.6030372381210327
iteration 200, loss 1.6645302772521973
iteration 300, loss 1.5807865858078003
iteration 400, loss 1.6523025035858154
iteration 500, loss 1.6113128662109375
iteration 600, loss 1.7358150482177734
iteration 700, loss 1.6640998125076294
iteration 800, loss 1.6130154132843018
iteration 0, loss 1.5737003087997437
iteration 100, loss 1.6869757175445557
iteration 200, loss 1.5968819856643677
iteration 300, loss 1.599441647529602
iteration 400, loss 1.6597903966903687
iteration 500, loss 1.6664921045303345
iteration 600, loss 1.591261625289917
iteration 700, loss 1.5948225259780884
iteration 800, loss 1.6680163145065308
iteration 0, loss 1.6155816316604614
iteration 100, loss 1.5985113382339478
iteration 200, loss 1.662734866142273
iteration 300, loss 1.5872124433517456
iteration 400, loss 1.628377079963684
iteration 500, loss 1.7089812755584717
iteration 600, loss 1.638472557067871
iteration 700, loss 1.599699854850769
iteration 800, loss 1.6347736120224
iteration 0, loss 1.6073521375656128
iteration 100, loss 1.6652394533157349
iteration 200, loss 1.5726007223129272
iteration 300, loss 1.6544026136398315
iteration 400, loss 1.6236923933029175
iteration 500, loss 1.6281598806381226
iteration 600, loss 1.6356693506240845
iteration 700, loss 1.625596523284912
iteration 800, loss 1.594912052154541
iteration 0, loss 1.6524617671966553
iteration 100, loss 1.6511675119400024
iteration 200, loss 1.6301236152648926
iteration 300, loss 1.610731601715088
iteration 400, loss 1.6669580936431885
iteration 500, loss 1.5899991989135742
iteration 600, loss 1.665191888809204
iteration 700, loss 1.6405575275421143
iteration 800, loss 1.6258665323257446
iteration 0, loss 1.6270534992218018
iteration 100, loss 1.619287133216858
iteration 200, loss 1.6194024085998535
iteration 300, loss 1.587080717086792
iteration 400, loss 1.5706028938293457
iteration 500, loss 1.587259292602539
iteration 600, loss 1.6047215461730957
iteration 700, loss 1.665352702140808
iteration 800, loss 1.6093943119049072
iteration 0, loss 1.6031204462051392
iteration 100, loss 1.613568902015686
iteration 200, loss 1.6146494150161743
iteration 300, loss 1.6448146104812622
iteration 400, loss 1.6234740018844604
iteration 500, loss 1.6414915323257446
iteration 600, loss 1.6327694654464722
iteration 700, loss 1.6201080083847046
iteration 800, loss 1.6762127876281738
iteration 0, loss 1.554081916809082
iteration 100, loss 1.6974987983703613
iteration 200, loss 1.5698109865188599
iteration 300, loss 1.6723800897598267
iteration 400, loss 1.624666452407837
iteration 500, loss 1.6244927644729614
iteration 600, loss 1.6120461225509644
iteration 700, loss 1.6203653812408447
iteration 800, loss 1.6490602493286133
iteration 0, loss 1.5981945991516113
iteration 100, loss 1.5856798887252808
iteration 200, loss 1.5760613679885864
iteration 300, loss 1.60690438747406
iteration 400, loss 1.633123517036438
iteration 500, loss 1.6121983528137207
iteration 600, loss 1.6046251058578491
iteration 700, loss 1.6337405443191528
iteration 800, loss 1.641834020614624
iteration 0, loss 1.6078516244888306
iteration 100, loss 1.6280144453048706
iteration 200, loss 1.586207389831543
iteration 300, loss 1.60649573802948
iteration 400, loss 1.5934256315231323
iteration 500, loss 1.620457410812378
iteration 600, loss 1.6199431419372559
iteration 700, loss 1.6511820554733276
iteration 800, loss 1.6275979280471802
iteration 0, loss 1.6318657398223877
iteration 100, loss 1.6397650241851807
iteration 200, loss 1.6114989519119263
iteration 300, loss 1.5610597133636475
iteration 400, loss 1.610209345817566
iteration 500, loss 1.6025365591049194
iteration 600, loss 1.5705716609954834
iteration 700, loss 1.6446359157562256
iteration 800, loss 1.5950424671173096
iteration 0, loss 1.5965337753295898
iteration 100, loss 1.6701356172561646
iteration 200, loss 1.5807805061340332
iteration 300, loss 1.6617803573608398
iteration 400, loss 1.649895429611206
iteration 500, loss 1.6525243520736694
iteration 600, loss 1.621638298034668
iteration 700, loss 1.5942209959030151
iteration 800, loss 1.7065577507019043
iteration 0, loss 1.6610783338546753
iteration 100, loss 1.6686222553253174
iteration 200, loss 1.6021966934204102
iteration 300, loss 1.602305293083191
iteration 400, loss 1.686970591545105
iteration 500, loss 1.644571304321289
iteration 600, loss 1.637421727180481
iteration 700, loss 1.6531963348388672
iteration 800, loss 1.5568640232086182
iteration 0, loss 1.6846355199813843
iteration 100, loss 1.6170932054519653
iteration 200, loss 1.673953890800476
iteration 300, loss 1.6250314712524414
iteration 400, loss 1.5583946704864502
iteration 500, loss 1.6187753677368164
iteration 600, loss 1.6519135236740112
iteration 700, loss 1.6250172853469849
iteration 800, loss 1.6132891178131104
fold 4 accuracy: 0.8186428571428571
[[0.4764285714285714, 0.49407142857142855, 0.49714285714285716, 0.48314285714285715, 0.486], [0.47314285714285714, 0.42614285714285716, 0.4125714285714286, 0.4139285714285714, 0.412], [0.4692857142857143, 0.47635714285714287, 0.46435714285714286, 0.4835, 0.49078571428571427], [0.42907142857142855, 0.4019285714285714, 0.4075, 0.40485714285714286, 0.40414285714285714], [0.4714285714285714, 0.477, 0.4794285714285714, 0.4759285714285714, 0.4867142857142857], [0.447, 0.4297142857142857, 0.41528571428571426, 0.4337142857142857, 0.4253571428571429], [0.49314285714285716, 0.5115, 0.5233571428571429, 0.5143571428571428, 0.5242857142857142], [0.44342857142857145, 0.462, 0.4637142857142857, 0.4765, 0.4752142857142857], [0.4744285714285714, 0.48364285714285715, 0.48957142857142855, 0.49907142857142855, 0.48964285714285716], [0.6622142857142858, 0.7124285714285714, 0.7195, 0.715, 0.7212142857142857], [0.6077142857142858, 0.6389285714285714, 0.6415, 0.6635, 0.6526428571428572], [0.5947142857142858, 0.6229285714285714, 0.6452142857142857, 0.6687857142857143, 0.7109285714285715], [0.6305714285714286, 0.6497857142857143, 0.6588571428571428, 0.679, 0.7105], [0.6981428571428572, 0.7101428571428572, 0.7108571428571429, 0.7233571428571428, 0.7208571428571429], [0.7823571428571429, 0.8034285714285714, 0.8077857142857143, 0.811, 0.8066428571428571], [0.815, 0.8136428571428571, 0.8162142857142857, 0.8172857142857143, 0.8196428571428571], [0.8037142857142857, 0.8110714285714286, 0.8105714285714286, 0.8156428571428571, 0.8163571428571429], [0.806, 0.8148571428571428, 0.8112142857142857, 0.8191428571428572, 0.8140714285714286], [0.7874285714285715, 0.8137142857142857, 0.8194285714285714, 0.8237857142857142, 0.8186428571428571]]
