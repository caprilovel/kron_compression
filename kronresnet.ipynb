{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from einops import rearrange, reduce, repeat\n",
    "import torchvision\n",
    "from data_factorcy.data_factorcy import loader_generate \n",
    "from utils.decomposition import kron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 8, 3, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv2d = nn.Conv2d(8, 64, 3)\n",
    "test_conv2d.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/zhu.3723/kron_compression/kronresnet.ipynb 单元格 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcse-cnc197017s.coeit.osu.edu/home/zhu.3723/kron_compression/kronresnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         weight \u001b[39m=\u001b[39m kron(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcse-cnc197017s.coeit.osu.edu/home/zhu.3723/kron_compression/kronresnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(x, weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcse-cnc197017s.coeit.osu.edu/home/zhu.3723/kron_compression/kronresnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m KronConv2d(\u001b[39m3\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m7\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39mFalse\u001b[39;00m, [\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m7\u001b[39m, \u001b[39m1\u001b[39m], [\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m7\u001b[39m])\n",
      "\u001b[1;32m/home/zhu.3723/kron_compression/kronresnet.ipynb 单元格 4\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcse-cnc197017s.coeit.osu.edu/home/zhu.3723/kron_compression/kronresnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m weight_shape \u001b[39m=\u001b[39m [out_channels, in_channels, kernel_size, kernel_size]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcse-cnc197017s.coeit.osu.edu/home/zhu.3723/kron_compression/kronresnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# assert each i, a[i] * b[i] == weight[i]\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcse-cnc197017s.coeit.osu.edu/home/zhu.3723/kron_compression/kronresnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39marray(a_shape[\u001b[39m1\u001b[39m:]) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39marray(b_shape[\u001b[39m1\u001b[39m:]) \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39marray(weight_shape)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcse-cnc197017s.coeit.osu.edu/home/zhu.3723/kron_compression/kronresnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mrandn(a_shape))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcse-cnc197017s.coeit.osu.edu/home/zhu.3723/kron_compression/kronresnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mrandn(b_shape))\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "type(resnet50)\n",
    "model = torchvision.models.resnet.ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], num_classes=10)\n",
    "class KronConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias, a_shape, b_shape) -> None:\n",
    "        super().__init__()\n",
    "        weight_shape = [out_channels, in_channels, kernel_size, kernel_size]\n",
    "        # assert each i, a[i] * b[i] == weight[i]\n",
    "        assert np.array(a_shape[1:]) * np.array(b_shape[1:]) == np.array(weight_shape)\n",
    "        self.a = nn.Parameter(torch.randn(a_shape))\n",
    "        self.b = nn.Parameter(torch.randn(b_shape))\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = 1\n",
    "        self.groups = 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        weight = kron(self.a, self.b)\n",
    "        return F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "    \n",
    "KronConv2d(3, 64, 7, 2, 3, False, [3, 8, 3, 7, 1], [3, 8, 1, 1, 7])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25557032"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcu the params of the model \n",
    "def model_params(model):\n",
    "    params = 0\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            params += p.numel()\n",
    "    return params\n",
    "# print the params of the model, for example, 25555 is 25,555\n",
    "model_params(resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the model \n",
    "resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gkpd.convolution import kroneckerconv2d\n",
    "resnet50.conv1\n",
    "nn.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use minist to fine-tune resnet50\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kroneckerconv2d(\n",
    "    3, 64, (7, 7), (2, 2), (3, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/zd/code/compression/kronresnet.ipynb Cell 8\u001b[0m line \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzd31/home/zd/code/compression/kronresnet.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m resnet50\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzd31/home/zd/code/compression/kronresnet.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bzd31/home/zd/code/compression/kronresnet.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzd31/home/zd/code/compression/kronresnet.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzd31/home/zd/code/compression/kronresnet.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torchvision/transforms/transforms.py:270\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    263\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torchvision/transforms/functional.py:360\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    358\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 360\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mnormalize(tensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, inplace\u001b[39m=\u001b[39;49minplace)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torchvision/transforms/functional_tensor.py:940\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[39mif\u001b[39;00m std\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    939\u001b[0m     std \u001b[39m=\u001b[39m std\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 940\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49msub_(mean)\u001b[39m.\u001b[39mdiv_(std)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to the input size expected by ResNet-50\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load pre-trained ResNet-50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "# Modify the last fully connected layer to match the number of classes in MNIST (10)\n",
    "num_ftrs = resnet50.fc.in_features\n",
    "resnet50.fc = nn.Linear(num_ftrs, 10)  # Assuming 10 classes in MNIST\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet50.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Fine-tuning the model\n",
    "num_epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet50.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    resnet50.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet50(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluate the fine-tuned model on the test set\n",
    "resnet50.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = resnet50(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.11093835338565211\n",
      "Epoch 2/5, Loss: 0.021187182665864626\n",
      "Epoch 3/5, Loss: 0.011634554800235977\n",
      "Epoch 4/5, Loss: 0.006403804848863122\n",
      "Epoch 5/5, Loss: 0.004385608656009814\n",
      "Test Accuracy: 99.60%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert single-channel to three channels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load pre-trained ResNet-50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "# Modify the last fully connected layer to match the number of classes in MNIST (10)\n",
    "num_ftrs = resnet50.fc.in_features\n",
    "resnet50.fc = nn.Linear(num_ftrs, 10)  # Assuming 10 classes in MNIST\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet50.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Fine-tuning the model\n",
    "num_epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet50.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    resnet50.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet50(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluate the fine-tuned model on the test set\n",
    "resnet50.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = resnet50(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 11, 2, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (704x2 and 6x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     18\u001b[0m b \u001b[38;5;241m=\u001b[39m rearrange(b, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr b1 b2 ->  b1 (b2 r)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m out2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlinear(x, b)\n\u001b[1;32m     20\u001b[0m out2 \u001b[38;5;241m=\u001b[39m rearrange(out2, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn x1 a1 b2 r -> r (n x1 b2) a1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m out2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(out2, a)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (704x2 and 6x2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from gkpd.tensorops import kron\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "a = torch.randn(3, 2, 2)\n",
    "b = torch.randn(3, 2, 2)\n",
    "x = torch.randn(32, 11, 4)\n",
    "c = kron(a, b)\n",
    "r = 3\n",
    "n = 32\n",
    "out1 = x @ c\n",
    "out1.shape \n",
    "c.shape \n",
    "from einops import rearrange\n",
    "x = rearrange(x , 'n x1 (a1 b1) -> n x1 a1 b1', a1=2, b1=2)\n",
    "print(x.shape)\n",
    "b = rearrange(b, 'r b1 b2 ->  b1 (b2 r)')\n",
    "out2 = F.linear(x, b)\n",
    "out2 = rearrange(out2, 'n x1 a1 b2 r -> r (n x1 b2) a1')\n",
    "out2 = torch.bmm(out2, a)\n",
    "out2 = torch.sum(out2, dim=0).unsqueeze(0)\n",
    "out2 = rearrange(out2, '(n x1 b2) a2 -> n x1 b2 a2', n=n, b2=2, a2=2)\n",
    "print(out1, out2)\n",
    "F.linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2785,  0.1856,  0.2213,  0.7426],\n",
       "          [ 0.5151,  0.2695,  0.2214,  0.5761],\n",
       "          [-0.0203, -0.3220, -0.1053,  0.6436],\n",
       "          [-0.3288, -0.8772, -0.5157, -0.4683]],\n",
       "\n",
       "         [[-0.1491, -0.0418,  0.1228,  0.2301],\n",
       "          [-0.0409, -0.4519, -0.2037,  0.5865],\n",
       "          [ 0.7476,  0.1456,  0.0517,  0.2111],\n",
       "          [-0.2044,  0.0487,  0.1911, -0.0792]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3796, -0.1798, -0.1016,  0.4271],\n",
       "          [ 0.3346,  0.0903,  0.1536,  1.1453],\n",
       "          [ 0.3324,  0.3385,  0.2869, -0.0793],\n",
       "          [ 0.4979,  0.3953,  0.3406,  0.9366]],\n",
       "\n",
       "         [[ 1.3318,  0.7370,  0.4210,  1.2598],\n",
       "          [ 0.6928,  0.0756,  0.0335,  0.6799],\n",
       "          [ 0.7791,  0.4807,  0.3353,  0.8543],\n",
       "          [-0.1847,  0.3920,  0.4750,  0.3823]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0122, -0.3247, -0.1635, -0.3492],\n",
       "          [ 0.4908, -0.3324, -0.2683, -0.0190],\n",
       "          [-0.0077, -0.5009, -0.2482,  0.6191],\n",
       "          [ 0.6104,  0.7908,  0.6164,  0.8604]],\n",
       "\n",
       "         [[ 0.5760, -0.3376, -0.2600,  0.6530],\n",
       "          [-0.0362, -0.7462, -0.4629, -0.0287],\n",
       "          [ 0.2501, -0.5144, -0.3179,  0.6731],\n",
       "          [-1.1025, -1.0331, -0.4429, -0.3082]]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "linear = nn.Linear(3, 4)\n",
    "a = torch.randn(3, 2, 4, 3)\n",
    "linear(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KronLinear(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank=0, structured_sparse=False, bias=True) -> None:\n",
    "        \"\"\"Kronecker Linear Layer\n",
    "\n",
    "        the weight matrix is a kron(a, b) matrix\n",
    "        \n",
    "        Args:\n",
    "            rank (int): the rank of the Kronecker product\n",
    "            a_shape (tuple): the shape of the **a** matrix \n",
    "            b_shape (tuple): the shape of the **b** matrix\n",
    "            structured_sparse (bool, optional): _description_. Defaults to False.\n",
    "            bias (bool, optional): _description_. Defaults to True.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        input_dims = factorize(input_dim)\n",
    "        output_dims = factorize(output_dim)\n",
    "\n",
    "        if rank == 0:\n",
    "            rank = min(*input_dims, *output_dims) // 2 + 1\n",
    "        self.rank = rank\n",
    "        \n",
    "        a_shape = [input_dims[0], output_dims[1]]\n",
    "        b_shape = [input_dims[1], output_dims[0]]\n",
    "        \n",
    "\n",
    "        self.structured_sparse = structured_sparse\n",
    "        if structured_sparse:\n",
    "            self.s = nn.Parameter(torch.randn( *a_shape), requires_grad=True)\n",
    "        self.a = nn.Parameter(torch.randn(rank, *a_shape), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.randn(rank, *b_shape), requires_grad=True)\n",
    "        self.a_shape = a_shape\n",
    "        self.b_shape = b_shape\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.a)\n",
    "        nn.init.xavier_uniform_(self.b)\n",
    "        bias_shape = np.multiply(a_shape, b_shape)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(*bias_shape[1:]), requires_grad=True)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = self.a\n",
    "        if self.structured_sparse:\n",
    "            a = self.s.unsqueeze(0) * self.a\n",
    "        \n",
    "        # a = self.s.unsqueeze(0) * self.a\n",
    "        # w = kron(a, self.b)\n",
    "        x_shape = x.shape \n",
    "        b = self.b\n",
    "        r = self.a_shape[0]\n",
    "        x = torch.reshape(x, (-1, x_shape[-1]))\n",
    "        b = rearrange(b, 'r b1 b2 -> b1 (b2 r)')\n",
    "        x = rearrange(x, 'n (a1 b1) -> n a1 b1', a1=self.a_shape[1], b1=self.b_shape[1])\n",
    "        out = x @ b\n",
    "        out = rearrange(out, 'n a1 (b2 r) -> r (n b2) a1', b2=self.b_shape[2], r=r)\n",
    "        out = torch.bmm(out, a)\n",
    "        out = torch.sum(out, dim=0).squeeze(0)\n",
    "        out = rearrange(out, '(n b2) a2 -> n (a2 b2)', b2=self.b_shape[2])\n",
    "        out = torch.reshape(out, x_shape[:-1] + (self.b_shape[2],))\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.bias is not None:\n",
    "            out += self.bias.unsqueeze(0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.525240153698367"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "21871 / 41640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9, -6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "w =np.array([-2, 1])\n",
    "x1 = [6 ,5]\n",
    "x2 = [6, 3]\n",
    "x1, x2 = np.array(x1), np.array(x2)\n",
    "\n",
    "w = x1 + w\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1101, -1098])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = x1 @ w  + w\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2240, -1132])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhu.3723/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zhu.3723/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight 9408\n",
      "bn1.weight 64\n",
      "bn1.bias 64\n",
      "layer1.0.conv1.weight 4096\n",
      "layer1.0.bn1.weight 64\n",
      "layer1.0.bn1.bias 64\n",
      "layer1.0.conv2.weight 36864\n",
      "layer1.0.bn2.weight 64\n",
      "layer1.0.bn2.bias 64\n",
      "layer1.0.conv3.weight 16384\n",
      "layer1.0.bn3.weight 256\n",
      "layer1.0.bn3.bias 256\n",
      "layer1.0.downsample.0.weight 16384\n",
      "layer1.0.downsample.1.weight 256\n",
      "layer1.0.downsample.1.bias 256\n",
      "layer1.1.conv1.weight 16384\n",
      "layer1.1.bn1.weight 64\n",
      "layer1.1.bn1.bias 64\n",
      "layer1.1.conv2.weight 36864\n",
      "layer1.1.bn2.weight 64\n",
      "layer1.1.bn2.bias 64\n",
      "layer1.1.conv3.weight 16384\n",
      "layer1.1.bn3.weight 256\n",
      "layer1.1.bn3.bias 256\n",
      "layer1.2.conv1.weight 16384\n",
      "layer1.2.bn1.weight 64\n",
      "layer1.2.bn1.bias 64\n",
      "layer1.2.conv2.weight 36864\n",
      "layer1.2.bn2.weight 64\n",
      "layer1.2.bn2.bias 64\n",
      "layer1.2.conv3.weight 16384\n",
      "layer1.2.bn3.weight 256\n",
      "layer1.2.bn3.bias 256\n",
      "layer2.0.conv1.weight 32768\n",
      "layer2.0.bn1.weight 128\n",
      "layer2.0.bn1.bias 128\n",
      "layer2.0.conv2.weight 147456\n",
      "layer2.0.bn2.weight 128\n",
      "layer2.0.bn2.bias 128\n",
      "layer2.0.conv3.weight 65536\n",
      "layer2.0.bn3.weight 512\n",
      "layer2.0.bn3.bias 512\n",
      "layer2.0.downsample.0.weight 131072\n",
      "layer2.0.downsample.1.weight 512\n",
      "layer2.0.downsample.1.bias 512\n",
      "layer2.1.conv1.weight 65536\n",
      "layer2.1.bn1.weight 128\n",
      "layer2.1.bn1.bias 128\n",
      "layer2.1.conv2.weight 147456\n",
      "layer2.1.bn2.weight 128\n",
      "layer2.1.bn2.bias 128\n",
      "layer2.1.conv3.weight 65536\n",
      "layer2.1.bn3.weight 512\n",
      "layer2.1.bn3.bias 512\n",
      "layer2.2.conv1.weight 65536\n",
      "layer2.2.bn1.weight 128\n",
      "layer2.2.bn1.bias 128\n",
      "layer2.2.conv2.weight 147456\n",
      "layer2.2.bn2.weight 128\n",
      "layer2.2.bn2.bias 128\n",
      "layer2.2.conv3.weight 65536\n",
      "layer2.2.bn3.weight 512\n",
      "layer2.2.bn3.bias 512\n",
      "layer2.3.conv1.weight 65536\n",
      "layer2.3.bn1.weight 128\n",
      "layer2.3.bn1.bias 128\n",
      "layer2.3.conv2.weight 147456\n",
      "layer2.3.bn2.weight 128\n",
      "layer2.3.bn2.bias 128\n",
      "layer2.3.conv3.weight 65536\n",
      "layer2.3.bn3.weight 512\n",
      "layer2.3.bn3.bias 512\n",
      "layer3.0.conv1.weight 131072\n",
      "layer3.0.bn1.weight 256\n",
      "layer3.0.bn1.bias 256\n",
      "layer3.0.conv2.weight 589824\n",
      "layer3.0.bn2.weight 256\n",
      "layer3.0.bn2.bias 256\n",
      "layer3.0.conv3.weight 262144\n",
      "layer3.0.bn3.weight 1024\n",
      "layer3.0.bn3.bias 1024\n",
      "layer3.0.downsample.0.weight 524288\n",
      "layer3.0.downsample.1.weight 1024\n",
      "layer3.0.downsample.1.bias 1024\n",
      "layer3.1.conv1.weight 262144\n",
      "layer3.1.bn1.weight 256\n",
      "layer3.1.bn1.bias 256\n",
      "layer3.1.conv2.weight 589824\n",
      "layer3.1.bn2.weight 256\n",
      "layer3.1.bn2.bias 256\n",
      "layer3.1.conv3.weight 262144\n",
      "layer3.1.bn3.weight 1024\n",
      "layer3.1.bn3.bias 1024\n",
      "layer3.2.conv1.weight 262144\n",
      "layer3.2.bn1.weight 256\n",
      "layer3.2.bn1.bias 256\n",
      "layer3.2.conv2.weight 589824\n",
      "layer3.2.bn2.weight 256\n",
      "layer3.2.bn2.bias 256\n",
      "layer3.2.conv3.weight 262144\n",
      "layer3.2.bn3.weight 1024\n",
      "layer3.2.bn3.bias 1024\n",
      "layer3.3.conv1.weight 262144\n",
      "layer3.3.bn1.weight 256\n",
      "layer3.3.bn1.bias 256\n",
      "layer3.3.conv2.weight 589824\n",
      "layer3.3.bn2.weight 256\n",
      "layer3.3.bn2.bias 256\n",
      "layer3.3.conv3.weight 262144\n",
      "layer3.3.bn3.weight 1024\n",
      "layer3.3.bn3.bias 1024\n",
      "layer3.4.conv1.weight 262144\n",
      "layer3.4.bn1.weight 256\n",
      "layer3.4.bn1.bias 256\n",
      "layer3.4.conv2.weight 589824\n",
      "layer3.4.bn2.weight 256\n",
      "layer3.4.bn2.bias 256\n",
      "layer3.4.conv3.weight 262144\n",
      "layer3.4.bn3.weight 1024\n",
      "layer3.4.bn3.bias 1024\n",
      "layer3.5.conv1.weight 262144\n",
      "layer3.5.bn1.weight 256\n",
      "layer3.5.bn1.bias 256\n",
      "layer3.5.conv2.weight 589824\n",
      "layer3.5.bn2.weight 256\n",
      "layer3.5.bn2.bias 256\n",
      "layer3.5.conv3.weight 262144\n",
      "layer3.5.bn3.weight 1024\n",
      "layer3.5.bn3.bias 1024\n",
      "layer4.0.conv1.weight 524288\n",
      "layer4.0.bn1.weight 512\n",
      "layer4.0.bn1.bias 512\n",
      "layer4.0.conv2.weight 2359296\n",
      "layer4.0.bn2.weight 512\n",
      "layer4.0.bn2.bias 512\n",
      "layer4.0.conv3.weight 1048576\n",
      "layer4.0.bn3.weight 2048\n",
      "layer4.0.bn3.bias 2048\n",
      "layer4.0.downsample.0.weight 2097152\n",
      "layer4.0.downsample.1.weight 2048\n",
      "layer4.0.downsample.1.bias 2048\n",
      "layer4.1.conv1.weight 1048576\n",
      "layer4.1.bn1.weight 512\n",
      "layer4.1.bn1.bias 512\n",
      "layer4.1.conv2.weight 2359296\n",
      "layer4.1.bn2.weight 512\n",
      "layer4.1.bn2.bias 512\n",
      "layer4.1.conv3.weight 1048576\n",
      "layer4.1.bn3.weight 2048\n",
      "layer4.1.bn3.bias 2048\n",
      "layer4.2.conv1.weight 1048576\n",
      "layer4.2.bn1.weight 512\n",
      "layer4.2.bn1.bias 512\n",
      "layer4.2.conv2.weight 2359296\n",
      "layer4.2.bn2.weight 512\n",
      "layer4.2.bn2.bias 512\n",
      "layer4.2.conv3.weight 1048576\n",
      "layer4.2.bn3.weight 2048\n",
      "layer4.2.bn3.bias 2048\n",
      "fc.weight 2048000\n",
      "fc.bias 1000\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50\n",
    "resnet = resnet50(pretrained=True)\n",
    "largest_name, largest_param = '', 0\n",
    "# for each layer, print the number of parameters\n",
    "for name, param in resnet.named_parameters():\n",
    "    print(name, param.numel())\n",
    "    if param.numel() > largest_param:\n",
    "        largest_name, largest_param = name, param.numel()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4.0.conv2.weight 2359296\n",
      "torch.Size([512, 512, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(largest_name, largest_param)\n",
    "print(resnet.layer4[2].conv2.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(resnet.fc.weight.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
